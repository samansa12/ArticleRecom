fileaccess characteristics parallel scientific workloads abstractphenomenal improvements computational performance multiprocessors matched comparable gains io system performance imbalance resulted io becoming significant bottleneck many scientific applications one key overcoming bottleneck improving performance multiprocessor file systems design highperformance multiprocessor file system requires comprehensive understanding expected workload unfortunately recently general workload studies multiprocessor file systems conducted goal charisma project remedy problem characterizing behavior several production workloads different machines level individual reads writes first set results charisma project describe workloads observed intel ipsc860 thinking machines cm5 paper intended compare contrast two workloads understanding essential similarities differences isolating common trends platformdependent variances using comparison able gain insight general principles guide multiprocessor filesystem design b introduction growing imbalance computational performance io subsystem performance multiprocessors imbalance resulted io becoming significant bottleneck many scientific applications thus clear need improvements design highperformance parallel file systems enable meet io needs applications successful system designer must possess thorough understanding system likely used understanding systems policies mechanisms optimized cases expected common systems work load designers far forced rely speculation parallel file systems would used extrapolating filesystem characterizations generalpurpose workloads uniprocessor distributed systems scientific workloads vector supercomputers address limitation initiated charisma project june 1993 characterize io scientific multiprocessor applications variety production parallel computing platforms sites 1 work done studying io needs parallel scientific applications typically examining small number selected appli cations charisma project unique recording individual read write requests live multiprogramming parallel workloads far completed characterization studies intel ipsc860 nasas ames research center 1 thinking machines cm5 national center supercomputing applications 2 systems addressed similar set questions ffl job mix look like many jobs run concurrently many processors job use ffl many files read written 1 charisma may found httpwwwcsdartmoutheduresearchcharismahtml ffl typical read write request sizes spaced file accesses sequential way ffl overall implications parallel filesystem design paper address final question integrating results observations across multiple platforms end use results two machinespecific studies try identify observations hold across various multiprocessor platforms pinpoint characteristics appear specific single platform environment next section describe previous studies multiprocessor file systems filesystem workloads describe two platforms examined study section 3 outline research methods section 4 present results section 5 draws overall conclusions background section review many previous studies filesystem workloads outline basic design current multiprocessor file systems finally describe design two platforms traced intel ipsc860 thinking machines cm5 21 workload characterizations classify previous filesystem workload studies characterizing generalpurpose workstations workstation networks scientific vector applications scientific parallel applications generalpurpose workstations uniprocessor file access patterns measured many times floyd ellis 3 4 ousterhout et al 5 measured isolated unix work stations baker et al measured distributed unix system sprite 6 ramakrishnan et al 7 studied access patterns commercial computing environment vaxvms platform studies cover generalpurpose engineering office workloads uniprocessor applications studies identify several characteristics common among uniprocessor filesystem workloads files tend small kilobytes tend accessed small requests tend accessed completely sequentially ie byte file accessed order beginning end scientific vector applications studies specifically examined scientific workloads vector machines del rosario choudhary provide informal characterization grandchallenge applications 8 powell measured set static characteristics file sizes cray1 file system 9 miller katz traced specific iointensive cray applications determine perfile access patterns 10 focusing primarily access rates miller katz also measured secondarytertiary file migration patterns cray 11 giving good picture longterm wholefile access patterns pasquale polyzos studied io intensive cray applications focusing patterns io rate 12 13 studies limited singleprocess applications vector supercomputers studies identify several characteristics common among supercomputer filesystem workloads unlike workstation filesystem workloads files tend large many megabytes gigabytes tend accessed large requests like workstation workloads files typically accessed completely sequentially scientific parallel applications experimental studies io parallel scientific programs running multiprocessors rare crockett 14 kotz 15 hypothesize character parallel scientific filesystem workload reddy banerjee chose five sequential scientific applications perfect benchmarks parallelized eightprocessor alliant finding sequential fileaccess patterns 16 study interesting far need sample size small programs parallelized sequential programs parallel programs per se io parallelized cypher et al 17 studied individual parallel scientific applications measuring temporal patterns io rates galbreath et al 18 present useful highlevel characterization based anecdotal evidence bagrodia et al 19 proposed using pablo analyze characterize specific applications crandall et al performed analysis three scientific applications 20 part charisma project traced parallel io requests live production mix user programs intel ipsc 1 cm5 2 study included one machine programming platform 22 existing parallel file systems single coherent model parallel fileaccess yet emerged parallelio models often closely tied particular machine architecture well programming model nonetheless common characteristics increase parallelism parallel file systems decluster blocks file across many disks accessed parallel extend traditional file abstraction growable addressable linear sequence bytes parallel fileaccess methods common provide io modes specify whether parallel processes share common file pointer 14 21 22 23 24 25 systems based memorymapped interface 26 27 two provide way user specify perprocess logical views file 28 29 provide simd style transfers 30 31 25 18 finally addition shared file pointers mpiio allows applications describe mapping linear file compute nodes running application terms higherlevel data structures 32 clearly industrial research communities yet settled single new model file access thus aspects parallel filesystem workload dependent particular model provided user implications fact study discussed throughout paper whenever dependency apparent 23 systems study useful system designer workload characterization must based realistic workload similar expected used future purposes meant trace multiprocessor file systems use production scientific computing intel ipsc860 nasa ames numerical aerodynamics simulation facility met criterion thinking machines cm5 national center supercomputing applications ncsa 231 intel ipsc860 concurrent file system ipsc860 distributedmemory messagepassing mimd machine compute nodes based intel i860 processor connected hypercube network io handled dedicated io nodes connected single compute node rather directly hypercube interconnect io nodes based intel i386 processor controls single scsi disk drive may also one service nodes handle things ethernet connections interactive shells 33 time study ipsc860 nas 128 compute nodes 10 io nodes compute node 8 mb memory io node 4 mb memory single 760 mb disk drive 34 also single service node handled 10mbit ethernet connection host computer total io capacity 76 gb total bandwidth less 10 mbs concurrent file system cfs stripes file across disks 4 kb blocks requests sent directly compute node issues request appropriate io node service since ipsc mimd machine compute nodes operate independently one another assist programmer coordinating accesses independent compute nodes single shared file cfs provides four io modes mode 0 default mode gives process file pointer mode 1 shares single file pointer among processes mode 2 like mode 1 enforces roundrobin ordering accesses across nodes mode 3 like mode 2 restricts access sizes identical details cfs performance found 21 35 36 232 thinking machines cm5 scalable file system cm5 distributedmemory machine many tens thousands sparcbased processing nodes small number control processors cps processing nodes logically grouped partitions managed cp job executes single partition generally processing node partition executes pro gram although may execute different instructions spmdstyle within individual partitions jobs timeshared processing nodes communicate via two scalable interprocessor communication networks 37 although possible users jobs running different partitions communicate one another rarely done practice cm5 supports variety io devices 37 38 study focuses scalable array sda primary highvolume highbandwidth storage device cm5 ncsa sda expandable raid3 disk system typically provides io bandwidths 33264 mbsec scalable file system sfs enhancement unix file system extensions support parallel io large files although fully general file system sfs optimized parallel highvolume transfer tracing project cm5 ncsa 512 nodes generally divided 5 static partitions size 32 32 64 128 256 nodes partitions cm5 reconfigurable times machine reconfigured single 512node partition node single cpu network interface 4 vector units collective memory size 32 mbnode sda single file system distributed across 118 data disks 1 parity disk total capacity 138 gb logical block size file system 295 kb physical disk block size 59 kb cm5 supports two primary programming models dataparallel controlparallel io model paper characterize io programs written cmf dataparallel fortran dialect cmmd controlparallel messaging library cmf programming model presents single thread control user nodes appear executing identical code though may operating different data cmf io library support routines layered top sfs allow users read write arrays portions thereof sda via either special library calls normal fortran read write statements since single thread control every io request collective whenever application issues io request every node application must participate request issues data distribution io parallelization hidden user cmmd library may used variety familiar programming languages eg c c f77 like ipsc provides user independent thread control processing node cmmd io also layered top sfs like cfs provides variety io modes 39 23 cmmds localindependent mode like mode 0 cfs gives process view file allows process make arbitrary requests file globalindependent mode process private file pointer state shared example one process performs ioctl change blocking mode blocking mode changed every process cmmds synchronoussequential mode like cfss mode 2 every node must participate io request may request different amount data data read written contiguous region file nodes requests satisfied roundrobin order final mode synchronousbroadcast every node accesses exact region file possible write data mode likely used read header information shared configuration file ncsa cmf users outnumber cmmd users factor 7 3 40 3 methods given diversity multiprocessor file systems possible construct architectureindependent workload study thus important study variety platforms comparing contrasting results production workloads multiple platforms may derive several benefits first strong common trends one confidently make generalizations used parallel filesystem design second studying various platforms pinpoints platform environmentdependent characteristics may useful designing new file system similar platform environment section describe methods collecting analyzing data two different platforms 31 trace collection charisma trace file begins header record containing enough information make file selfdescriptive continues series event records one per event ipsc860 one trace file collected entire file system traced io involved concurrent file system means io done standard input output host file system limited sequential ethernet speeds recorded collected data 156 hours period 3 weeks february 1994 trace continuously whole 3 weeks tried get realistic picture whole workload tracing different times day week including nights weekends period covered single trace file ranges minutes 22 hours longest continuously traced period 625 hours tracing usually initiated machine idle cases job running began tracing job traced tracing stopped one two ways manually full system crash machine usually idle trace manually stopped ipsc860 highlevel cfs calls implemented runtime library linked users program instrumented library calls generate event record time called since instrumentation almost entirely within userlevel library jobs whose file accesses traced included system programs eg cp ftp well user programs relinked period tracing instrumented library default users wish applications traced option linking uninstrumented library regardless whether application traced able record job starts ends separate mechanism tracing 3016 jobs run 2237 run single node actually traced least 429 779 multinode jobs least 41 singlenode jobs tremendous number singlenode jobs system programs surprising necessarily undesirable many untraced particular one singlenode job run periodically accounted 800 singlenode jobs simply check status machine way distinguish untraced job traced job cfs io numbers traced jobs lower bound one primary concerns minimize degree measurement perturbed workload reduce network contention local percall overhead buffered event records node sent central trace collector buffer full since large messages ipsc broken 4 kb blocks chose buffer size buffering allowed us reduce number messages sent collector well 90 without stealing much memory user jobs trace records written file system tracing careful minimize effects performance well creating large buffer data collector writing data cfs large sequential blocks since data collector linked noninstrumented library use file system recorded simple benchmarking instrumented library revealed overhead added instrumentation virtually undetectable cases worst case found 7 increase execution time one run nas nht1 applicationio benchmark 41 instrumented library put production use anecdotal evidence suggests noticeable performance loss although collected 700 mb data trace files accounted less 1 total cfs traffic since node buffered 4 kb data sending central data collector raw trace file contained partially ordered list event records ordering records complicated lack synchronized clocks ipsc860 node maintains clock clocks synchronized system startup drifts significantly 42 partially compensated asynchrony timestamping block records left compute node received data collector difference two attempt adjust event order compensate nodes clock drift relative collectors clock technique results better estimation actual event order still approximation much analysis based spatial rather temporal information 32 cm5 trace collection cm5 traced programs two different programming models dataparallel cm fortran cmf programs controlparallel cmmd programs cm5 programming models cfs applications perform io via runtime libraries paper examine discuss io done scalable disk array cmf cfs instrumented runtime cmf io libraries collect traces gathered data single file ipsc cm5 applications trace data written separate file traced nearly cmf applications ran 23day period june 28 1994 july 20 1994 instrumentation mechanism users disable tracing particular job setting environment variable users example industrial partners ncsa requested feature made use thereby applications traced separate mechanism allowed us count total number cmf jobs run tracing period even suppressed trace generation 1943 jobs period 1760 traced neither figure includes programs compiled tracing library installed 1760 jobs traced represent 434 distinct applications run 384 distinct users ipsc attempted reduce effects tracing user population wrote perjob trace files onto serial unix file system avoid contention sda io buffered trace records memory wrote disk large blocks minimize tracing overhead performance measurements taken betatesting indicate instrumentation increased total application execution time less 5 cmmd classify cmf workload general workload cmmd workload selfselecting developed cmmd tracing library thinking machines corporation inhouse version cmmd since developed offsite ncsa systems staff reluctant make default library relied users voluntarily linked programs cmmd tracing library us gather traces traced period two weeks summer 1994 obtained traces 127 jobs representing 29 distinct applications run 11 distinct users volunteers tended heavy users sda relatively sophisticated programmers interested parallelio behavior perhaps classify workload iointensive workload compared general cmf workload difference considered interpreting cmmd data cmmd io implemented clientserver architecture privileged cm 5 host process responsible running server loop monitored cmmd io piggybacking trace records clientserver protocols actual trace records produced cm5 compute nodes communicated host server written local unix file system since communication trace records embedded normal clientserver io protocols believe perturbation minimal section compare contrast ipsc cm5 workloads try identify common trends isolate reasons differences behavior characterize workload top begin examining number jobs machine number use files jobs examine individual io requests addition studying sizes io requests look sequentiality regularity among examine requests higher level try identify specific kinds regular access patterns finally examine file sharing various granularities summary statistics three sets traces may seen table 1 classify files whether actually read written read written within single open period rather mode used open file files opened neither read written closed traced megabytes number files system jobs read written opened read written neither table 1 summary data collected ipsc cm5 41 jobs fig 1 shows amount time machine spent running given number jobs 2 since cm5 much larger user base surprising spent less time idle 2 data overall number jobs cm5 collected 2 weeks may 1995 tracing period since attempt correlate information results paper lack contemporaneousness viewed significant percent total time number jobs figure 1 amount time machine spent given number jobs running data includes jobs even file access could traced ipsc unlike ipsc cm5 timeshared partitions allowed jobs run time although ipsc idle nearly 28 time tracing cm5 idle less 2 time machines actively executing jobs ipsc spent 35 time running single job cm5 spent 6 time running single job means ipsc used run multiple applications simultaneously 25 time cm5 executing multiple jobs 92 time although jobs use file system file system clearly must provide highperformance access many concurrent presumably unrelated jobs uniprocessor file systems tuned situation multiprocessor filesystems research ignored issue focusing optimizing singlejob performance fig 2 shows distribution number compute nodes used job machine although singlenode jobs appear dominate job population ipsc jobs caused daemon run periodically check status machine multiplenode jobs fairly evenly distributed among remaining sizes 64 nodes although ipsc allowed jobs small single node cm5 minimum partition size 32 nodes 60 cmf jobs cm5 used percent jobs number compute nodes ipsc b cm5 51204080percent jobs number compute nodes cmmd figure 2 distribution number compute nodes used jobs workload machines limit choice powers 2 cm5 minimum partition size 32 nodes smallest partition size hand since cmmd workload selfselecting included fairly large iointensive applications observe bias toward large number nodes 50 traced cmmd jobs used 256 nodes clearly file system successful must allow efficient access small sequential jobs large highly parallel jobs variety conditions system loads 42 files two systems studied two different manners file may opened locally globally file said locally opened node accesses file issues independent request open file file locally opened node opens file private view file operations file directly affected nodes using file contrast file said opened globally nodes application collectively issue request open file file globally opened nodes shared view file cfs io model support notion global open file cfs must opened locally discussed section 231 cfs provides several access modes allow files treated globally opened discussing perjob file statistics coalesce local opens issued cfs single global open multiple nodes cfs application issue local open file count local opens single global open since cmf dataparallel language provides single thread control every file operation cmf collective file opens cmf global cmmd allows programmers open files either locally globally since cmmd applications wish open file globally may explicitly since cmmd files opened locally attempt coalesce local opens global opens cfs table note many files written read indeed 2 3 times many speculate programmers traced cfs applications often found easier open separate output file compute node rather coordinating writes common output file hypothesis supported substantially smaller average number bytes written per file 12 mb average bytes read per file 33 mb ipsc difference average number bytes accessed appear cm5 workload cmf jobs read average 278 mbfile wrote average 252 mbfile cmmd applications read 1175 mbfile wrote 1102 mbfile domination writeonly files cm5 appears come partly checkpointing activity partly output files written sda later visualization number bytes read written per file cmf substantially smaller cmmd amount data transferred per file still order magnitude larger observed cfs users seem made use higher disk capacity bandwidth cm5 offers another common trend across three platforms files read written 35 cfs 54 cmmd 58 cmf behavior also common unix file systems 3 may accentuated difficulty coordinating concurrent reads writes file table 2 shows jobs opened files course execution number number jobs files cfs cmf cmmd table 2 among traced jobs number files opened jobs often small 14 although opened many files one cfs job opened 2217 files although cmf required files opened nodes cfs jobs opened large number files opening one file per node although shown table nearly 25 jobs used cmf use files sda applications probably compute intensive io via nfs number files opened per job higher cmmd cmf perhaps due selfselected nature users despite differences absolute numbers files opened appears clear use multiple files per job common therefore although files open concurrently filesystem designers must optimize access several files within job found 061 opens cfs workload temporary files defined file deleted job created rarity temporary files files read written indicates applications chose use files extension memory core solutions many cfs applications computational fluid dynamics codes found outofcore methods general slow workload cm5 exhibited larger number temporary files 38 cmf jobs 49 cmmd jobs difference may indicate ofcore methods common cm5 may caused deletion checkpoint files jobs ran completion fraction files file size bytes cmf figure 3 cumulative distribution function cdf number files size close file size x cdfx represents fraction files x fewer bytes fig 3 shows wide range size files system system 3 files accessed cfs although files larger generalpurpose file system 6 smaller would expect see scientific supercomputing environment 10 files cm5 significantly larger ipsc sizes much evenly distributed one likely reason files cm5 larger availability 20 times disk space 43 io request sizes figures 4 5 show ipsc cm5 vast majority accesses small bytes transferred large accesses indeed 96 reads cfs requested fewer 100 bytes reads transferred 2 data read similarly 90 writes cfs fewer 100 bytes writes transferred 3 data written number small requests surprising due poor performance cfs 36 cmmds interface 3 many small files well several distinct peaks across whole range sizes constant granularity captured detail felt important histogram chose plot file sizes logarithmic scale pseudologarithmic bucket sizes bucket size 10 100 bytes 10 bytes bucket size 100 1000 100 bytes reads fraction data fraction cfs cmmd cmf cmf read size bytes cfs figure 4 cdf number reads request size amount data read request size fraction fraction data writes cmmd cmf write size bytes cfs cfs cmmd figure 5 cdf number writes request size amount data written request size similar cfs compute node issues requests data independently compute nodes requests cmmd somewhat larger cfs 87 reads 96 writes 1000 bytes cmf provides collective model io requests issued compute nodes accordingly would expect see much larger requests cmf either cmmd cfs found however even cmf 97 reads 87 writes 1000 bytes ipsc small requests cm5 known perform poorly although accesses cm5 larger observed ipsc still significantly smaller tens hundreds kilobytes used typical performance analyses systems 36 43 studies shown large io requests common scientific applications running supercomputers seen small requests common scientific applications running parallel computers indeed trend holds across two different parallel machines using three parallel filesystem interfaces two parallel programming models therefore believe preponderance small request sizes observed scientific workloads natural result parallelization fundamental large class parallel applications conclude future parallel file systems must focus providing low latency small requests well high bandwidth large requests 44 sequentiality one common characteristic previous file system workload studies particularly scientific workloads files typically accessed sequentially 5 6 10 define sequential request one begins higher file offset point previous request compute node ended looser definition sequential used studies referred previous studies called sequential call consecutive consecutive request sequential request begins precisely previous request ended figures 6 7 show amount sequential consecutive access files observed workloads figures look pernode access patterns cfs cmmd perjob access patterns cmf three interfaces nearly accesses writeonly files 100 sequential access readonly files also predominantly sequential cmf cmmd several files read nonsequentially several applications cm5 wrote data files forward order read back reverse order behavior accounts least nonsequential accesses machine unsurprisingly readwrite files accessed nonsequentially cmmd cmf cfs cmmd cmf cfs cmmd cmf cfs readonly c readwrite fraction files fraction files fraction files figure cdf sequential access files point line indicates fraction files workload contain indicated percentage sequential accesses looking graphs consecutive access fig 7 find behavior varies systems interfaces cfs cmf nearly 90 writeonly files accessed 100 consecutively cmmd hand 60 writeonly files accessed completely consecutively three interfaces readonly files much less likely accessed consecutively writeonly files least consecutive access found cfs 65 readonly files consecutive accesses cases access readwrite files primarily nonconsecutive one significant reason relatively high percentage consecutive access writeonly files ipsc tendency applications assign different file node writing data single node accesses file frequently cfs readonly c readwrite fraction files fraction files fraction files consecutive cmmd cmf cmmd cmf cmmd cmf cfs cfs figure 7 cdf consecutive access files point line indicates fraction files workload contain indicated percentage consecutive accesses reason node access file nonconsecutively multiple nodes access file happened frequently readonly files cfs files cmmd large number sequential nonconsecutive accesses often result interleaved access interleaved access arises successive records file accessed different nodes perspective individual node bytes must skipped one request next high percentage consecutive access files cmf programs expected looking collective joblevel patterns rather individual nodelevel patterns since io requests cmf applications issued individual nodes sort interleaving unlikely appear percent files number different request sizes cmf cmmd number different interval percent files cmf cmmd figure 8 number different interval request sizes used file across participating nodes files zero interval sizes one access node files zero request sizes opened closed without accessed 45 regularity workloads many small nonconsecutive requests different previously observed workloads traditional uniprocessors supercomputers attempt gain better understanding observed workloads tried identify points regularity intervals first looked interval requests number bytes end one request beginning next consecutive accesses interval size 0 number interval sizes used file across nodes access file shown fig 8 surprising number files around 13 cases read written one request per node ie intervals files 99 cfs 79 cmf 51 cmmd accessed single interval size accessed consecutively ie one interval size 0 remainder 1intervalsize files along 2intervalsize files represent remaining files suggests exists another form highly regular access pattern files 3 different interval sizes regularity complex requests get better feel regularity fig 8 also shows number different request sizes used file cfs exhibited highest degree regularity 90 files accessed one two request sizes cmmd next 75 files accessed one two different request sizes cmf least regular half files accessed two fewer request sizes may indicate cmf users used file store different data structures eg different matrices even cmf 80 files accessed three fewer request sizes combining regularity request sizes regularity interval sizes many applications clearly used regular structured access patterns possibly much data matrix form 46 strided access better understand structure causes regular nonconsecutive access patterns examined trace files evidence strided access patterns 44 461 simplestrided refer series io requests simplestrided access pattern request number bytes file pointer incremented amount request pattern would occur example process parallel application read column data matrix stored rowmajor order could also correspond pattern generated application distributed columns matrix across processors cyclic pattern columns could distributed evenly matrix stored rowmajor order since strided pattern less likely occur singlenode files since could occur files one two accesses looked files three requests multiple nodes fig 9 shows many accesses selected subset cfs cmmd files appeared part simplestrided access pattern since consecutive access could fraction files cmmdwithout cmfwith cmmdwith cfswithout cfswith fraction files figure 9 cumulative distribution files according fraction accesses involved simplestrided pattern plots show frequency strided access consecutive accesses counted strided without considered trivial form strided access interval 0 fig 9 shows frequency strided accesses without consecutive accesses included either case 80 files examined cfs apparently accessed entirely strided pattern strided access also common cmmd 60 files accessed entirely strided nonconsecutive pattern exclude consecutive access appeared almost strided access cmf 20 requests file taking part strided pattern lack strided access cmf surprising since strided access typically caused explicit expression data distribution controlparallel program accordingly remainder discussion focus cfs cmmd define strided segment group requests appear part single simplestrided pattern fig 9 shows percentage requests involved strided segment tell us whether file accessed single filelong strided segment many shorter segments fig 10 shows files strided segments files accessed many strided segments since interested cases file clearly accessed strided pattern figure include consecutive accesses segments fewer requests number requests segment varied machines fig 11 number files number strided segments cfs number strided segments10003000500070000 50 100 150 200 number files figure 10 number different strided segments file ignore segments requests note two plots use different scales1000002000000 number accesses number segments number accesses20060010001400 cfs b cmmd number segments figure 11 head segmentlength distribution plots show number segments given length including short segments 10 fewer accesses shows segments cfs fell range 20 30 requests segments cmmd 55 fig 12 shows files accessed much longer segments machines existence simplestrided patterns interesting potentially useful fact many files accessed multiple short segments suggests level structure beyond described simplestrided pattern 462 nested patterns nestedstrided access pattern similar simplestrided access pattern rather composed simple requests separated regular strides file composed cfs number accesses50150250 200 400 600 800 1000 1200 number segments number accesses50015002500500 1000 1500 2000 2500 number segments figure 12 tail segmentlength distribution strided segments separated regular strides file simplestrided patterns examined last section could called singlynested patterns doublynested pattern could correspond pattern generated application distributed columns matrix stored rowmajor order across processors cyclic pattern columns could distributed evenly across processors simplestrided subpattern corresponds requests generated within row matrix toplevel pattern corresponds distance one row next access pattern could also generated application reading single column data threedimensional matrix higher levels nesting could occur application mapped multidimensional matrix onto set processors maximum level number number nesting cfs files cmmd files table 3 number files use given maximum level nesting table 3 shows frequently nested patterns occurred cfs cmmd file apparent strided accesses zero levels nesting files accessed simplestrided patterns single level nesting interestingly machines far common files exhibit three levels nesting two tendency suggests use multidimensional matrices common systems 47 synchronized access modes although notion synchronized access files built semantics dataparallel cmf case cfs cmmd provide synchronized access files cfs cmmd provide user option using file pointer shared among nodes also provide several modes provides user different semantics governing file pointer shared given regularity request interval sizes ipsc cfss modes see section 231 would seem helpful traces show however 99 files used mode 0 provides node independent file pointer fig 8 gives one hint although different request sizes interval sizes often one something easily supported sharedpointer modes mode 0 also known fastest four modes offered cfs contrast cfs cmmds localindependent mode known slow used 088 total io instead cmmd applications used synchronoussequential mode 78 io synchronousbroadcast mode used 87 total io globalindependent mode used 119 total io speculate used one node subset nodes using file among data one may inclined conclude cm5 applications needed fast synchronous io anecdotal evidence suggests however users frequently wanted independent io willing pay performance penalty cmmd cfs users adopt different io strategies achieve end result high performance illustrates capabilities existing machine may influence user behavior writeblocks readbytes readblocks02061 fraction files percent shared writebytes fraction files percent shared writebytes writeblocks readbytes readblocks02061 fraction files percent shared readblocks readbytes writeblocks writebytes cfs c cmmd figure 13 cdf file sharing nodes readonly writeonly files byte block granularity block size ipsc 4 kbytes block size cm5 kbytes 48 file sharing within jobs file concurrently shared two processes open time uniprocessor distributedsystem workloads concurrent sharing known uncom mon writing concurrently shared files almost unheard 6 parallel file system course concurrent file sharing among processes within job presumably norm concurrent file sharing jobs likely rare indeed traces saw great deal file sharing within jobs concurrent file sharing jobs interesting question individual bytes blocks files shared fig 13 shows frequency byte blocksharing system three cases sharing readonly files writeonly files surprising given complexity coordinating write sharing indeed cfs 70 readonly files bytes shared multiple compute nodes 90 writeonly files bytes shared found similar results cmmd 61 readonly files bytes shared 93 writeonly files none bytes shared cmf least sharing three systems 95 writeonly files bytes shared 60 readonly files 1 fewer bytes shared multiple compute nodes lack sharing likely artifact cmfs dataparallel programming model processors statically assigned nonoverlapping portions matrix even lot bytesharing usually large amount blocksharing overall amount block sharing implies strong interprocess spatial locality suggests caching io nodes may improve system performance 1 5 conclusions recommendations across two machines two programming models covered paper found important similarities differences compared uniprocessor workloads three parallel workloads used much larger files dominated writes although variations magnitude found small request sizes common three parallel workloads uniprocessor workloads compared vectorsupercomputer workloads observed much smaller requests tendency toward nonconsecutive sequential file access finally parallelism leads new interleaved access patterns high interprocess spatial locality io node details results may specific two systems studied workloads two sites believe general conclusions widely applicable scientific workloads running looselycoupled mimd multiprocessors category includes many current multiprocessors ultimately believe filesystem interface must change current interface forces programmer break large parallel io activities small nonconsecutive requests believe controlparallel model support strided io requests programmers interface compute node compute node io node 24 44 strided request effectively increase request size lowers overhead introduces opportunities lowlevel optimization 45 future work believe lowlevel workload analyses conducted important first step towards developing parallel file systems meet needs parallel scientific applications still great deal work done ffl trace platforms reduce likelihood results specific architecture environment ffl study specific applications greater detail workload studies describe parallel file systems used studying individual applications allow us understand used fashion better understand application programmers fundamental needs ffl design implement new interfaces file systems based workload analyses acknowledgments many thanks nas division nasa ames especially jeff becker russell carter fineberg art lazanoff bill nitzberg leigh ann tanner many thanks also orran krieger bernard traversat rest charisma group thank michael welge curtis canada ncsa many thanks ncsa users including greg bryan diane cook tom cortese kathryn johnston chris kuszmaul fady najjar robert sugar also thank kapil mathur david phillimore thinking machines corporation doreen revis duke finally thank many users agreed applications traced r filesystem workload scientific multiprocessor characterizing parallel fileaccess patterns largescale multiprocessor shortterm file reference patterns unix environment directory reference patterns hierarchical file systems trace driven analysis unix 42 bsd file system measurements distributed file system analysis file io traces commercial computing environments high performance io parallel computers problems prospects demos file system inputoutput behavior supercomputer appli cations analysis file migration unix supercomputing environment static analysis io characteristics scientific applications production workload case study scientific application io behavior file concepts parallel io prefetching file systems mimd multi processors study io behavior perfect benchmarks multiprocessor architectural requirements parallel scientific applications explicit communication applicationsdriven parallel io inputoutput instru mentation characterization modeling management policy putoutput characteristics scalable parallel applications concurrent file system highly parallel mass storage system unix file access caching multicomputer environment cmmd io parallel unix io multiprocessor file system interfaces hfs flexible file system largescale mul tiprocessors parallel access files vesta file system ncube parallel io software connection machine model cm2 technical summary parallel file io routines mpiio parallel file io interface mpi ipsc2 ipsc860 users guide nasa ames research center performance measurement concurrent file system intel ipsc2 hypercube performance ipsc860 concurrent file system cmmd reference manual version 30 personal communication global time reference hypercube multiprocessors performance cm5 scalable file system lowlevel interfaces highlevel parallel io diskdirected io mimd multiprocessors tr ctr dean hildebrand peter honeyman directpnfs scalable transparent versatile access parallel file systems proceedings 16th international symposium high performance distributed computing june 2529 2007 monterey california usa len wisniewski brad smisloff nils nieuwejaar sun mpiio efficient io parallel applications proceedings 1999 acmieee conference supercomputing cdrom p14es november 1419 1999 portland oregon united states dean hildebrand lee ward peter honeyman large files small writes pnfs proceedings 20th annual international conference supercomputing june 28july 01 2006 cairns queensland australia thakur william gropp ewing lusk case using mpis derived datatypes improve io performance proceedings 1998 acmieee conference supercomputing cdrom p110 november 0713 1998 san jose ca satyanarayanan carla schlatter ellis adaptation key mobile io acm computing surveys csur v28 n4es dec 1996 jarek nieplocha holger dachsel ian foster implementing noncollective parallel io cluster environments using active message communication cluster computing v2 n4 p271279 1999 myra b cohen charles j colbourn ordering disks double erasure codes proceedings thirteenth annual acm symposium parallel algorithms architectures p229236 july 2001 crete island greece hong scott brandt darrell e long ethan l miller ying lin using memsbased storage computer systemsdevice modeling management acm transactions storage tos v2 n2 p139160 may 2006 gokhan memik mahmut kandemir alok choudhary exploiting interfile access patterns using multicollective io proceedings 1st usenix conference file storage technologies january 2830 2002 monterey ca yijian wang david kaeli profileguided io partitioning proceedings 17th annual international conference supercomputing june 2326 2003 san francisco ca usa ron oldfield david kotz improving data access computational grid applications cluster computing v9 n1 p7999 january 2006 toni cortes sergi girona jess labarta design issues cooperative cache coherence problems proceedings fifth workshop io parallel distributed systems p3746 november 1717 1997 san jose california united states florin isaila guido malpohl vlad olaru gabor szeder walter tichy integrating collective io cooperative caching clusterfile parallel file system proceedings 18th annual international conference supercomputing june 26july 01 2004 malo france thakur william gropp ewing lusk implementing mpiio portably high performance proceedings sixth workshop io parallel distributed systems p2332 may 0505 1999 atlanta georgia united states thomas sterling conclusions beowulf cluster computing linux mit press cambridge 2001 gokhan memik mahmut kandemir weikeng liao alok choudhary multicollective io technique exploiting interfile access patterns acm transactions storage tos v2 n3 p349369 august 2006 emilia rosti giuseppe serazzi evgenia smirni mark squillante models parallel applications large computation io requirements ieee transactions software engineering v28 n3 p286307 march 2002 paolo cremonesi claudio gennaro integrated performance models spmd applications mimd architectures ieee transactions parallel distributed systems v13 n12 p13201332 december 2002 paolo cremonesi claudio gennaro integrated performance models spmd applications mimd architectures ieee transactions parallel distributed systems v13 n7 p745757 july 2002 stergios v anastasiadis kenneth c sevcik michael stumm scalable faulttolerant support variable bitrate data exedra streaming server acm transactions storage tos v1 n4 p419456 november 2005 david kotz diskdirected io mimd multiprocessors acm transactions computer systems tocs v15 n1 p4174 feb 1997 jack dongarra ian foster geoffrey fox william gropp ken kennedy linda torczon andy white references sourcebook parallel computing morgan kaufmann publishers inc san francisco ca