validation augmented lagrangian algorithm gaussnewton hessian approximation using set hardspheres problems augmented lagrangian algorithm uses gaussnewton approximations hessian inner iteration introduced tested using family hardspheres problems gaussnewton model convexifies quadratic approximations augmented lagrangian function thus increasing efficiency iterative quadratic solver resulting method considerably efficient corresponding algorithm uses true hessians comparative study using wellknown package lancelot presented b introduction recent years involved development algorithms based sequential quadratic programming 11 inexact restoration 16 18 minimization problems nonlinear equality constraints bounded variables validation algorithms require comparison well established computer methods type problems include methods family sqp methods first case grg like methods second well methods adopt completely dierent point view case penalty augmented lagrangian algorithms consolidated practical augmented lagrangian method currently available one implemented package lancelot described 4 method used example 11 test reliability new largescale sequential quadratic programming algorithm author supported fapesp grant 9681639 authors supported pronex fapesp grant 9037246 cnpq faep unicamp course mentioned experimental studies felt necessity intervening augmented lagrangian code active way one permitted users lancelot result practical necessity became involved development dierent augmented lagrangian code preserves principles lancelot philosophy also important dierences following lines 4 modern augmented lagrangian method essentially composed three nested algorithms external algorithm updates lagrange multipliers penalty pa rameters decides stopping criteria internal algorithm rules declaring convergence failure overall procedure internal algorithm minimizes augmented lagrangian function bounds variables trust region methods subproblem consists minimization quadratic model intersection two boxes one defines problem trustregion box used 4 implementation third algorithm deals resolution quadratic subproblem lancelot restricts search face determined approximate cauchy point code explores domain subproblem whole second item specifically deals formulation quadratic subproblem one felt strongly desire intervene one hand tried many alternative sparse quasinewton schemes without success hand used surprisingly eective simplification true hessian lagrangian called paper gaussnewton hessian approximation analogy gaussnewton method nonlinear least squares interpreted result excluding hessian sum squares terms involving hessian individual components order validate augmented lagrangian implementation selected family problems particular interest known family hardspheres problems hardspheres problem belongs family sphere packing problems class challenging problems dating beginning seventeenth cen tury tradition famous problems mathematics statements problems elusively simple withstood attacks many worthy mathematicians eg newton hilbert gregory instances remain open problems furthermore related practical problems chemistry biology physics see instance list examples 19 concerning mainly threedimensional problems peruse 1550itemlong bibliography 5 hardspheres problem maximize minimum pairwise distance p points sphere r n problem may reduced nonlinear optimization problem turns might expected mentioned history particularly hard nonconvex problem potentially large number nonoptimal points satisfying kkt conditions thus class problems indexed parameters n p provides suitable set test problems evaluating nonlinear programming codes convenient fact hardspheres problem may regarded feasibility problem associated another famous problem area kissing number problem seeks determine maximum number k n nonoverlapping spheres given radius r n simultaneously touch kiss central sphere radius thus distance obtained solution hardspheres problem given n p greater equal radius sphere points lie one may conclude k n p use known solution threedimensional kissing number problem calibrate code described choose testing code values n p might bring forth new knowledge problem strengthen existing conjectures true alas rigorously established value k n following table known valuesbounds k n given 5 table 1 known values bounds kn 9 306380 paper organized follows section 2 formulate hardspheres problem nonlinear programming problem relate main characteristics albox augmented lagrangian algorithm section 3 explain main algorithmic parameters albox chosen follow previous study 15 section 4 introduce gaussnewton hessian approximation discuss eect use comparison use true hessians lagrangians section 5 describe parameters used lancelot numerical experiments obtained running albox lancelot large number hardspheres problems presented section 6 finally conclusions drawn section 7 2 albox straightforward formulation hardspheres problem leads following maxmin problem r radius sphere centered origin points lie st k 1 vectors k belong r n euclidean norm since answer problem invariant choice positive r let using definition standard inner product r n constraints easy see 1 equivalent st k 2 applying classical trick transforming minimax problems constrained minimization problems reduce 2 nonlinear program min z st z j adding slack variables first set constraints squaring second set equations order avoid nonsmoothness first derivatives obtain min z general form min fx albox augmented lagrangian code developed approximately solves min lx outer iteration augmented lagrangian function associated 5 current approximation lagrange multipliers 0 current vector penalty parameters updated end outer iteration subproblem 6 solved using box boxconstrained solver described 10 iterative method minimizes quadratic approximation objective function intersection original feasible set box x u trust region also box iteration original objective function sufficiently reduced approximate minimizer quadratic corresponding trial point accepted new iterate otherwise trust region reduced main algorithmic dierence box method used 2 box quadratic explored whole intersection original box trust region whereas 2 face determined approximate cauchy point examined albox double precision fortran 77 code aims cope largescale problems reason factorization matrices used quadratic solver used solve subproblems boxconstraint algorithm quacan visits dierent faces domain using conjugate gradients interior face chopped gradients search directions leave faces refer reader 1 9 10 details actual implementation quacan iterations quadratic solver matrixvector product hessian approximation vector computed occasionally additional matrixvector product may neccessary performance albox fact sophisticated algorithms depends choice many parameters sensitive parameters adjusted using kissing problem discuss choices next section similar analysis carried lancelot described section 5 3 choice parameters albox 31 penalty parameters lagrange multipliers vector penalty parameters associated equality constraints updated outer iteration considered two possibilities update component according decrease corresponding component hx using global criterion based hx specific alternatives contemplated assuming x initial point outer iteration x final one 1 increase hx suciently smaller hx 2 increase hx suciently smaller hx preliminary experiments revealed perhaps surprisingly global strat egy 2 better first fact updated components feasibility level hx tends deteriorate next iteration consequentely large number outer iterations becomes necessary words seems strategy based 1 encourages zigzagging havior successive iterates alternatingly satisfying one constraint another thus although original formulation allows one penalty parameter equality constraint practice worked one parameter since initialized value tests indicate 10 adequate initial value updated according rule based tests increased factor 10 sucient improvement feasibility detected considered suciently smaller b means 001b must pointed behavior penalty parameters independent strategy updating lagrange multipliers algorithmic simplicity mind adopted first order formula letting lagrange multiplier start new outer iteration lagrange multipliers penalty parameters previous iteration set 32 stopping criteria boxconstraint solver outer iteration ends one several stopping criteria algorithm solves augmented lagrangian boxconstrained minimization problem 6 reached usual maximum number iterations safeguard set 100 quacan calls consider boxconstraint algorithm box converges continuous projected gradient objective function 6 point x vector defined dierence projection x lx box point x tolerance may change outer iteration tested two strategies one defines dynamically depending degree feasibility current iterate another fixes 10 5 althought conclusive results icosahedron problem better constant strategy used therefore strategy adopted tests incidentally opposite adopted 8 similar augmented lagrangian algorithm used solve linearly constrained problems derived physical applications theoretical justifications inexact minimization subproblems augmented lagrangian context also found 12 13 boxconstraint code admits stopping criteria instance execution may stop progress number consecutive iterations good enough radius trust region becomes small nevertheless best results obtained inhibiting alternative stopping criteria 33 parameters quadratic solver quacan code called minimize quadratic functions augmented lagrangians case subject box constraints eciency lack thereof plays crucial role overall behavior augmented lagrangian algorithm parameters must therefore carefully chosen firstly examine convergence criterion projected gradient quadratic null corresponding point stationary accordingly convergence considered achieved norm projected gradient less fraction corresponding norm initial point case use non continuous projected gradients projections computed feasible box active constraints fractions 110 1100 1100000 tested icosahedron problem first choice provided best behavior one employed subsequently maximum number iterations allowed also important parameter since otherwise may invest much eort solving problems distantly related original one found number variables problem np p suitable delimiter case nonconvergence stopping criteria inhibited radius trust region determines size auxiliary box used quacan nonlinear programming algorithm sensitive choice first trust region radius testing dierent values selected appropriate choice another important parameter 0 1 parameter determines whether next iterate must belong face current one roughly speaking small algorithm tends leave current face soon mild decrease quadratic detected hand 1 algorithm abandons current face current point close stationary point quadratic face rather surprising result icosahedron problem conservative value better smaller values finally quadratic solver hits boundary feasible region extrapolation step may tried depending value extrapolation parameter 1 large new points tried number active bounds may considerably increased extrapolation tried indicated convenient choice hardspheres problem 4 approximate hessian nonlinear optimization problem 4 obtained section 2 version hardspheres problem chosen tests pointed 4 general form min fx whose associated augmented lagrangian thus although 2 lx tends positive definite large close correct lagrange multipliers x close solution case early stages augmented lagrangian calculations hand simplified matrix obtained neglecting term involving second order derivatives constraint functions always positive semidefinite case independently x course always case f convex function another insight bx provided examining problem min fx z current point used box iteration problem 8 obtained replacing original constraints first order linear approx imation bz happens hessian augmented lagrangian associated 8 z furthermore augmented lagrangian associated 8 gradient evaluated z coincide counterparts associated original problem 4 evaluated z matrix vector products 2 lx v bx v seem cumbersome compute first glance taking advantadge structure enables computation done onp time principle using true hessian lagrangian best possible choice since represents better structure true problem however available algorithms minimizing quadratics convex sets much ecient quadratic convex otherwise quacan exception rule therefore interest improving overall performance augmented lagrangian algorithm decided use bx hessian lagrangian approximation results indeed impressive table 2 lists average statistics obtained four eighteen test sets n p pair run fifty random starting points average number outer iterations box iterations function evaluations matrix vector products cpu time seconds minimum distance given runs using exact hessian first row set ones using approximate hessian second row minimum distances obtained close instances minimum distance obtained using approximate hessian smaller one obtained using exact hessian number outer iterations dier much one choice number box iterations consequently number matrix vector products sensibly decreases overall result marked decrease cpu time figure 1 plot average cpu times eighteen tests using exact hessian versus times using approximate hessian also shown line gives best fit data linear ane function namely approximate hessian option implies decrease almost two thirds cpu times table 2 running albox exact first row approximate hessian second row problem size outer box funct mvp cpu min eval time dist 486 3706 5214 156436 0765 1086487225412 22 456 16002 19314 6702022 373141 0998675348042 cpu times using exact hessian cpu times using approx hessian 500 1000 1500 2000 2500 3000400800 figure 1 cpu times using exact hessian xaxis versus using approximate hessian yaxis 5 choice parameters lancelot lancelot allows choice exact approximate first second order derivatives however lancelots manual 3 p111 strongly recommends use exact second derivatives whenever available hand provision user supplied hessian approximation fact ran tests default approximation sr1 results worse obtained using exact second derivatives thus option adopted tests light experiments described previous section provides corroborating evidence eect general purpose consolidated packages designed provide good performance little interference user may convenient use open ended lowlevel interface codes albox user willing get hands dirty latter rawer code might prove competitive may actually outperform former code polished though restrictive finish also experimented several dierent options solving linear equation solver namely without preconditioner diagonal preconditioner band matrix preconditioner best results obtained first option preconditioner another choice slowed algorithm without noticeable improve quality solution requiring exact cauchy point com puted settled use inexact cauchy point option maximum number iterations allowed 1000 finally gradient constraints tolerances chosen albox namely 10 8 fortran compiler option adopted lancelot albox 6 numerical experiments tests run sun sparcstation 20 following main characteristics 128mbytes ram 70mhz 2047 mips 444 mflops results fifty runs n p pair summarized following tables table 3 summarizes statistics machine independent typically involving number iterations number function evaluations exception optimal distances found quotes needed completely accurate since numbers fact depend factors machine precision compiler manufacturer nevertheless certainly provide independent grounds comparison cpu times presented table 4 along optimal distances table 3 presents minimum maximum average amounts number triple box outer box iterations function evaluations quacan iterations matrixvector productsconjugategradient iterations box lancelot respectively first row set corresponds albox second lancelot unfortunately statistics available number function evaluations box iterationsderivative evaluations paired number matrixvector products mvp output albox number conjugategradient iterations cgi produced lancelot since conjugategradient iteration involves matrixvectorproduct table 3 albox lancelot test results problem size outer box function quacan mvp derivative eval iter cgi 4546 21 55 347 25 77 455 309 2343 1064 340 2702 1195 15 61 381 16 71 433 377 1949 992 20 62 380 21 80 434 511 2709 1032 22 58 397 24 66 453 553 1776 1069 4548 27 75 479 34104 642 933 4923 2708 1017 5382 2963 27 84 523 29 96 601 967 4248 2313 4546 32110 602 41140 778 1625 8385 4130 1751 8742 4444 30 91 568 33112 651 1107 5652 3015 22 4543 52115 780 62148 974 5688 16767 10502 6097 17871 11222 45225 1041 49262 1200 5122 37546 12381 37176 1089 39208 1246 4799 29367 14607 2541 45141 864 58183 1079 6282 28049 14077 6769 29825 15009 4542 63180 978 75226 1206 10492 35660 17105 11034 37639 18143 54225 1197 60262 1375 6870 38419 18736 26 4642 51176 954 63216 1172 6765 38932 17185 7317 40932 18237 53266 1314 59311 1505 5094 77233 21796 4543 62206 995 76254 1221 11480 45129 19490 12169 47121 20616 62215 1289 68253 1476 9420 41799 21534 4846 80800 1600 102984 1931 27836471751 63778 29476497038 67020 8533419042 95381 2186 9119 96036 56899 4646 89600 1662 107717 2000 29224326333 67969 30804340424 71261 4749 78700 1958 89815 2313 26692448509 88566 27892472730 92422 9138523124 9945326344 24178160611 85972 4749 90700 2029 106880 2429 34936463883 98266 36614485784102816 4849 93800 2251 117954 2716 36194547421117417 38311577662122924 4746 109700 2123 132887 2561 47402502810109630 49993529036114749 102440 2464 115499 2813 34730200558 although algorithms behave dierently timewise shortly see direct consequence number function evaluations performs best leastsquares fit first degree polynomial gives number function evaluations albox x corresponding amount lancelot whereas similar fit involving cpu times give coecient less third figure 2 plot function evaluation pairs eighteen instances along best fit obtained albox figure 2 number function evaluations lancelot versus albox still providing explanation higher eciency albox comparison mvp versus cgi case best fit gives 110655x number mvp x number cgi suggests although iterations involve matrixvectorproduct cgi substantially costlier timewise mvp performed albox main factor matrixvectorproduct lancelots conjugate gradient iteration deals true hessian whereas one albox involves approximate simpler hessian figure 3 contains line corresponding best linear fit position cgi mvp pairs next table 4 presents similar statistics involving optimal distances encountered cpu times seconds first resp second row n p pair gives numbers obtained albox resp lancelot albox figure 3 number cgis lancelot versus number mvps albox table 4 minimum distances cpu times tests problem size minimum distance 2 points cpu time seconds 10514622 10914262 108323633 0170 1010 0476 10514622 10514622 105146223 0170 1420 0636 09463817 10514622 104515739 0290 1870 0906 22 09529038 09619429 095809771 25150 86570 41290 26 09606935 09779378 096928704 420170 4527652 1000608 09599791 09798367 097025160 807570 4664 information contained table 4 depicted graphically intervals min max distancescpu times represented vertical segments averages indicated diamond symbol albox bullet lancelot graphs left refer distances whereas graphs right refer cpu times min dist time figure 4 albox lancelot results 22 4 26 4 min dist 22 4 26 4 cpu time figure 5 albox lancelot results graphs figures 46 evidence qualitative relative behavior codes notice diamonds bullets always close together graphs left indicating quality optimal solutions obtained codes min dist cpu time figure 6 albox lancelot results similar hand bullets rise faster diamonds graphs right means cpu times lancelot tend higher albox linear fit albox cpu times versus xthe coecient less one third ploted figure 7 confirms cpu times cpu times albox 500 1000 1500 2000 2500400800 figure 7 cpu times lancelot versus albox finally noted cpu times increase sharply function problem size represented instance number constraints tried several fits linear quadratic exponential though none seemed provide good model data quadratic fit best one 7 conclusions main aspects augmented lagrangian methodology solving largescale nonlinear programming problems consolidated works conn gould toint gave origin lancelot package algorithmic framework useful last ten years solving practical problems comparison purposes innovative nonlinear programming methods likely tendency maintained near future present research born result need freedom formulation resolution quadratic subproblems arise lancelotlike approach augmented lagrangian philosophy one hand decided exploit deeply whole trust region means use boxconstraint quadratic solver hand perhaps importantly tested gaussnewton convex simplification quadratic model turned much ecient straight newtonlike version model behind gain eciency fact quadratic solver though able deal nonconvex models far ecient underlying quadratic positive semidefinite hessian usual numerical analysis decision implementation high level algorithm depends current technology solving lowlevel subproblems must warned decision could change new ecient algorithms solving subproblems nonconvex quadratic programming case found main objective use albox solving reallife problems also testing alternative nonlinear programming methods feel deep knowledge implementation details code enable us much exacting testing new codes since possible fine tune standard new code tested present study apart calling readers attention convex simplified gaussnewton like subproblems objective validating code means comparison lancelot using set problems independent interest result comparison seems indicate albox used competitive tool nonlinear programming calculations r adaptive algorithm bound constrained quadratic minimization globally convergent augmented lagrangian algorithm optimization general constraints simple bounds global convergence class trust region algorithms optimization simple bounds lattices groups mathematics science patterns comparing numerical performance two trustregion algorithms largescale boundconstrained minimization augmented lagrangians adaptive precision control quadratic programming equality constraints maximization concave quadratic function box constraints new trustregion algorithm bound constrained minimization nonlinear programming algorithms using trust regions augmented lagrangians nonmonotone penalty parameters analysis implementation dual algorithm constraint optimization dual techniques constraint optimization bounds kissing numbers r n mathematical programming formulations augmented lagrangians resolution packing problems twophase model algorithm global convergence nonlinear pro gramming preconditioning truncatednewton methods linearly constrained spectral gradient methods inexact restoration subproblems nonlinear programming distributing many points sphere tr dual techniques constrained optimization global convergence class trust region algorithms optimization simple bounds globally convergent augmented lagrangian algorithm optimization general constraints simple bounds analysis implementation dual algorithm constrained optimization twophase model algorithm global convergence nonlinear programming lancelot augmented lagrangians adaptive precision control quadratic programming equality constraints ctr graciela croceri graciela n sottosanto mara cristina maciel augmented penalty algorithms based bfgs secant approximations trust regions applied numerical mathematics v57 n3 p320334 march 2007 r andreani friedlander p mello santos boxconstrained minimization reformulations complementarity problems secondorder cones journal global optimization v40 n4 p505527 april 2008 g birgin jos mario martnez largescale activeset boxconstrained optimization method spectral projected gradients computational optimization applications v23 n1 p101125 october 2002 nikhil arora lorenz biegler trust region sqp algorithm equality constrained parameter estimation simple parameter bounds computational optimization applications v28 n1 p5186 april 2004 g birgin j martnez structured minimalmemory inexact quasinewton method secant preconditioners augmented lagrangian optimization computational optimization applications v39 n1 p116 january 2008