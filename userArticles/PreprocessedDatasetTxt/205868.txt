globally convergent successive approximation method severely nonsmooth equations paper presents globally convergent successive approximation method solving fx0 f continuous function step method f approximated smooth function fk pa fkfpa rightarrow 0 k rightarrow infty direction fkxk1fxk used line search sum squares objective approximate function fk constructed nonsmooth equations arising variational inequalities maximal monotone operator problems nonlinear complementarity problems nonsmooth partial differential equations numerical examples given illustrate method b introduction continuous necessarily differentiable function consider system nonlinear equations recent literature nonsmooth equations includes 13 68 10 13 15 17 1921 work supported australian research council f smooth popular method solving 1 damped newton method 49 solve f get k 2 step size ff k 0 1 chosen line search han pang rangaraj 6 generalized damped newton method solve nonsmooth equation 1 using idea iteration function defined damped newton method iteration function let ae oe 2 0 1 given let g r nthetan r n given iteration function solve f get k smallest nonnegative integer global convergence established 6 four assumptions g f g general gx delta nonlinear implies system nonlinear equations generally easier 1 solved step method recently gabriel pang 7 proposed trust region algorithm using iteration functions also required certain assumptions iteration functions establish convergence algorithm poliquin qi 14 proved case nonsmooth optimization assumptions iteration functions actually implied restrictions original function globally convergent methods nonsmooth equations 1013 20 methods either assume conditions much stronger continuity work special problems paper introduce successive approximation method let jj delta jj denote euclidean norm kth step approximate f smooth function f k ff 2 0 1 fixed constant algorithm uses f 0 derivative f x k needed two outstanding advantages new algorithm existing methods first advantage linear approximation made step subproblem system linear equations known globally convergent methods solving nonsmooth equations feature second advantage conditions required establish convergence implement new algorithm general establish global convergence algorithm following assumptions f continuity f boundedness level set nonsingularity f 0 k x k k x accumulation point fx k g implement algorithm require f locally lipschitzian assumptions may construct f k desired accuracy basic tool integration convolution special cases ways construct f k although discuss linear convergence algorithm section 3 intend pursue higher rate convergence method already several superlinearly convergent methods 10111921 superlinear convergence theory 1315 solving nonsmooth equations one may construct hybrid algorithm globally superlinearly convergent using new algorithm known superlinearly convergent algorithm methodology proposed 15 go details construction merit algorithm may solve severely nonsmooth equations nonsmooth equations arising variational inequality problem general convex set maximal monotone operator problem see section 5 section 2 describe successive approximation method prove global convergence section 3 consider rate convergence section 4 discuss construct successive approximation function nonsmooth function f using integration convolution section 5 investigate applications algorithm section 6 give numerical results successive approximation method 2 method global convergence definition 1 let ff 2 0 1 constant kth step iteration methods described section next section call normal decomposition f f k smooth k g k k ff k f whenever f shall give examples normal decompositions section 5 let method described follows successive approximation method sam given ae ff 2 0 1 initial vector x 0 2 r n normal decomposition 1 solve f get k 2 set x smallest nonnegative integer 3 f otherwise construct new normal decomposition k g k1 kminf ffk f assumption 1 level set bounded assumption 2 f 0 nonsingular k lemma 1 suppose f k normal decomposition f exists scalar k 2 0 1 2 0 k proof notice 2 0 k 3 holdslemma 1 indicates sam welldefined assumption 2 theorem 1 suppose assumptions 1 2 hold sam welldefined k let fx k g sequence produced sam furthermore accumulation point x fx k nonsingular large k lim accumulation points x fx k g proof without loss generality may assume f smooth hence k g k k 0 k lemma 1 sam welldefined prove 4 without loss generality assume f kg assume k consists k let k arbitrary nonnegative integer let k j largest number k k j k ff cases follows x k implies 4 holds prove second part theorem k infinite k 0 exists k j 2 k largest number k holds limit righthand side 6 zero proves 5 hence prove 5 suffices prove k infinite suppose k finite assume hence k suppose k 0 subsequence f0 1 g fx converges x 7 condition theorem f 0 since lim k1 delta continuous function fk f 0 uniformly bounded therefore exists l 0 delta continu ous ffi 0 x satisfying ffl since lim k1 k k k k 2 k 0 let 2 0 1 10 11 k 9 12 k k k 2 k 0 2 0 therefore k k k 2 k 0 2 0 implies k k k 2 k 0 ae 8 14 construction algorithm k however 7 construction algorithm nonincreasing k k implies 1 contradicts fact k cannot finite proves 5 final conclusion theorem simply follows 5 continuity f remark 1 may inductively apply proof lemma 1 first part theorem 1 prove 3 4 way may reduce assumption 2 f 0 nonsingular k satisfying x k 2 0 remark 2 17 trust region methods using decomposition f nonsmooth equations presented second one successive approximation used f used classical trust region methods replaced fx k f 0 respectively use successive approximation replace f sam f k also prove global convergence technique 17 3 convergence rate order give convergence rate consider modification sam modified given ae ff 2 0 1 c 2 0 1 1ff initial vector x 0 2 r n normal decomposition 1 solve f get k let x otherwise steps 2 3 sam theorem 2 theorem 1 holds msam proof let k set k 15 holds k finite msam essentially sam hence theorem 1 holds case suppose k infinite let k k i1 two consecutive numbers k k jj otherwise argument similar first part proof theorem 1 k satisfying hence k satisfying shows 5 holds conclusion followsthe proof theorem 2 shows msam globally convergent norm jjf reduces linearly 15 holds infinitely many times following theorem show conditions linear convergence rate realized need normal decomposition large k theorem 3 suppose conditions theorem 1 hold x accumulation point fx k g generated msam suppose exist positive integer k positive numbers fi k k f furthermore k k 15 also holds proof theorem 2 f perturbation lemma x 2 sx r x implies furthermore therefore x f k g repeating proof 3 f smooth 0 quadratic rate convergence damped newton method recovered 17 4 approximation using convolution section next two sections use x represent ith component vector x use x k represent vector without loss generality may assume f r n r n bounded uniformly continuous f continuous bounded uniformly continuous let level set 0 defined section 2 assumption 1 r x 1 equivalent f 0 bounded uniformly continuous hence assume f bounded uniformly continuous let bound jjf jj r modulus continuity f defined continous nondecreasing function 9 x r n call phi r n r kernel function z phi kernel function defined positive number also kernel function phi smooth phi also smooth oe r r onedimensional smooth kernel defined ndimensional smooth kernel function two famous onedimensional smooth kernel functions cauchy see shapiro 24 suppose phi r n r smooth kernel function z z according 52325 f smooth function z furthermore x r n z z ffl 0 let z jjxjjr r z z z z z z z phizdz z jjzjjr therefore section 2 f may choose construct ie normal decomposition required section 2 construct f satisfying 21 need know r 0 19 20 hold phi constructed 18 difficult choose r actually 18 holds need r satisfy z hand f globally lipschitzian constant l 0 20 satisfied section 5 give examples applications f locally lipschitzian construction f 0 beginning section f 0 always globally lipschitzian hence method implemented long f locally lipschitzian 5 applications successive approximation method section discuss applications successive approximation method first two examples appeared literature 13 51 variational inequality problem let c closed convex subset r n continuously differentiable function defined open set r n containing c problem denote vic oe find vector x 2 c system equivalent system nonsmooth equations r n pi c denotes projection c nonsmoothness function f consequence projection operator pi c delta see 13 c polyhedral set operator possesses bdifferentiability properties put use algorithmically see 10 however easy establish properties c general convex set since projection operator lipschitzian modulus 1 use tool integration convolution stated section 4 solve nonsmooth equations 22 successive approximation method 52 maximal monotone operator problem setvalued maximal monotone operator important problem find x 2 r n according theory maximal monotone operator resolvent namely identity operator positive number always singlevalued nonexpansive hence globally 13 moreover solution 23 equivalent nonsmooth equation 1 globally lipschitzian use tool integration convolution approximate f solve equation successive approximation method 53 lc 1 optimization consider r continuously differentiable function let may solve find stationary points 24 f locally lipschitzian called lc 1 function 24 called lc 1 optimization problem many examples lc 1 optimization problems 16 18 ex ample conjugate functions extendvalued strongly convex functions 1 2 0 2 f globally lips chitzian see theorem 25 16 actually extendedvalued convex quadratic functions rewrite 26 z nthetan symmetric positive definite 2 r mthetan b 2 section 6 give numerical example f defined 27 28 54 nonlinear complementarity problem consider nonlinear complementarity problem finding x continuously differentiable problem formulated system nonsmooth equations 1 see 13 give normal decomposition f defined 29 simpler convolution approximation proposed general case let ff 2 0 1 constant let j easy verify f k continuously differentiable g k continuous k g k k 55 piecewise smooth function consider matrix diagonal continuous function system arises nonsmooth partial differential equations general case oe defined piecewise u v smooth functions con stants replace f gammaf hence may assume u 0 p q smooth functions equivalent 30 give normal decomposition f 54 6 numerical experiments section give computation results illustrate sam msam first example 53 example 1 let nonempty convex polyhedra r n let nthetan symmetric positive definite 2 r mthetan b 2 r 2 consider nonsmooth equations p n theta n nonsingular matrix c 0 fixed vector r n proposition 1 let nthetan symmetric positive definite 2 r mthetan b 2 r assume fz az bg empty proof theorem 263 22 continuously differentiable 0 bg moreover conjugate function strongly convex function theorem 235 22 hence 2 0 1 second inequality holds basic property subgra dients let 0 similarly addition 31 32 shows hence therefore generalized case locally strongly convex see 16 proposition 1 f globally lipschitzian modulus 0jj take transformation mentioned beginning section 4 jjf jj hence give normal decomposition f using convolution discussed section 4 let z use cauchy kernel function stated section 4 kth step let jjf solve system linear equations z 5f k z obviously f nondifferentiable x one x boundary z test algorithm sam four dimensional problem 70022 09018 06111 05042 06111 10961 69120 06618 05042 09506 06618 66859c c c 20022 09018 06111 05042 09018 02974 10961 09506 06111 10961 19120 06618 05042 09506 06618 16850c c c 29174 14182 04576 10221 04576 07656 30682 08975 randomly generated let randomly generate x choose c 0 x solution f f nondifferentiable solution x ie boundary z 1 z 2 respectively montecarlo method used calculate integral numerically numerical results shown table 1 random initial points example 2 consider following degenerate nonlinear complementarity problem 8 problem two solutions formulate problem f defined 29 f x differentiable x nondifferentiable x using newton line search method iteration function methodif quote particular definition iteration function gdelta delta given 6 otherwise computational results sam msam shown table 2 used single precision chose 1ff msam table 1 example 1 test problem 1 test problem 2 x 47545 105370 13704 50524 test problem 3 stopping criterion jjf table 2 iteration number k jjx initial 1000 1111 1010 1001 data stopping criterion see three methods globally convergent final iteration numbers sam msam comparable sam msam featured less work iteration sam msam needs solve linear system equations step construct approximation functions locally lipschitzian function generally know construct iteration functions case therefore successive approximation method general acknowledgements thankful professor olvi mangasarian dr rob womersley three referees mr houyuan jiang comments grateful professor carl de boor suggestion convolution upgraded paper r convergence broydenlike methods nonlinear equations nondifferentiable terms parameterized newton method broydenlike method solving nonsmooth equations convergence quasi jr r minimization discontinuous functions mollifier subgradients globally convergent newton methods nonsmooth equations trust region method constrained nonsmooth equations extensions newton quasinewton methods systems pc 1 equations iterative solution nonlinear equations several variables newtons method bdifferentiable equations nesqp robust algorithm nonlinear complementarity problem motivation algorithms iteration function nonsmooth optimization algorithms convergence analysis algorithms solving nonsmooth equations functions lc 1 optimization problems trust region algorithms solving nonsmooth equations superlinear convergent approximate newton methods lc 1 optimization problems nonsmooth version newtons method global convergence damped newtons method nonsmooth equations via path search newtons method class nonsmooth functions convex analysis smoothing approximation functions applications functional analysis mathematical physics tr ctr ilker birbil shucherng fang jiye han entropic regularization approach mathematical programs equilibrium constraints computers operations research v31 n13 p22492262 november 2004 l qi sun smoothing functions smoothing newton method complementarity variational inequality problems journal optimization theory applications v113 n1 p121147 april 2002 smoothing newton method semiinfinite programming journal global optimization v30 n23 p169194 november 2004 liqun qi defeng sun guanglu zhou primaldual algorithm minimizing sum euclidean norms journal computational applied mathematics v138 n1 p127150 1 january 2002 smoothing newton quasinewton methods mixed complementarity problems computational optimization applications v17 n23 p203230 december 2000 bintong chen xiaojun chen global linear local quadratic continuation smoothing method variational inequalities box constraints computational optimization applications v17 n23 p131158 december 2000 xiaojun chen applications smoothing methods numerical analysis optimization focus computational neurobiology nova science publishers inc commack ny 2004