formal analysis rocchios similaritybased relevance feedback algorithm rocchios similaritybased relevance feedback algorithm one important query reformation methods information retrieval essentially adaptive supervised learning algorithm examples spite popularity various applications little rigorous analysis learning complexity literature paper show binary vector space model initial query vector 0 four typical similarities inner product dice coefficient cosine coefficient jaccard coefficient rocchios similaritybased relevance feedback algorithm makes least mistakes used search collection documents represented monotone disjunction ik relevant features terms indimensional binary vector space lcub0 1rcubin arbitrary initial query vector lcub0 1rcubin used makes least mistakes search collection documents linear lower bounds independent choices threshold coefficients algorithm may use updating query vector making classification b introduction research relevance feedback information retrieval long history baezayates ribeironeto 1999 frakes baezayates 1992 ide 1971b ide 1971a raghavan wong 1986 jj rocchio salton 1989 regarded popular query reformation strategy baezayates ribeironeto 1999 central idea relevance feedback improve search performance particular query modifying query step step based users judgments relevance irrelevance documents retrieved vector space model salton 1989 salton et al 1975 extended abstract paper published proceedings eleventh international symposium algorithms computation isaac00 lecture notes computer science 1969 pages 108119 springerverlag december 2000 c 2001 kluwer academic publishers printed netherlands z chen b zhu documents queries represented vectors discretized vector space case relevance feedback essentially adaptive supervised learning algorithm query vector similarity used classify documents relevant irrelevant users judgments relevance irrelevance classied documents used examples updating query vector linear combination initial query vector examples judged user especially inner product similarity used relevance feedback perceptronlike learning algorithm lewis 1991 known jj roc chio 1971 optimal way updating query vector sets relevant irrelevant documents known practically impossible derive optimal query vector full sets relevant irrelevant documents available many dierent variants relevance feedback information retrieval however paper study rocchios similaritybased relevance feedback algorithm jj rocchio 1971 ide 1971a salton 1989 spite popularity various applications little rigorous analysis complexity learning algorithm literature main motivation us investigate learning complexity rocchios similaritybased relevance feedback algorithm wong yao bollmann wong et al 1988 studied linear structure information retrieval designed nice gradient descent procedure compute coecients linear function analyzed performance order update query vector adaptively gradient descent procedure must know user preference practice unknown target learned information retrieval system discussions gradient descent procedure given section 51 main contribution work paper linear lower bounds classication mistakes proved algorithm four typical similarities inner product dice coecient cosine coecient jaccard coecient listed salton 1989 used technically work paper enlightened work kivinen et al 1997 lower bounds perceptron algorithm precisely borrow method developed kivinen et al 1997 constructing example sequence pairwise constant inner prod ucts extend method cope similarities besides inner product also design new method selecting trial sequences prove uniform way lower bounds rocchios similaritybased relevance feedback algorithm pointed lower bounds established paper rocchios similaritybased relevance feedback algorithm based following worst case considerations user acts formal analysis rocchios algorithm 3 adversary algorithm algorithm required precisely search collection documents relevant given search query algorithm allowed receive one document example judged user relevance irrelevant step 1 practical applications contrast worst case considerations user general may act adversary algorithm algorithm usually required search short list top ranked documents relevant given search query step similaritybased relevance algorithm user may judge documents relevance feedback algorithm words appropriate situation realworld information retrieval applications would kind sympathetic oracle model user adversary information retrieval system sympathetic judge provides useful possible information order help system help himher accomplish hisher work hence lower bounds proved paper rocchios similaritybased relevance feedback algorithm may aect algorithms eective applicability realworld problems despite theoretical signicance formal analysis algorithm helps us understand nature algorithm well may nd new strategies improve eectiveness design new algorithms information retrieval recently made progress along line chen 2001 designed two types multiplicative adaptive algorithms user preference retrieval provable better performance one better performance rocchios algorithm learning class linear classiers nonbinary vector space boosts usefulness index term exponentially gradient descent procedure wong et al 1988 boosts usefulness index term linearly refer readers work salton buckley 1990 lewis 1991 discussions perceptronlike learning nature similaritybased relevance feedback algorithm stated lewis 1991 important future direction research information retrieval likely machine learning techniques combine empirical learning use knowledge bases paper organized follows section 2 give formal presentation rocchios similaritybased relevance feedback algorithm section 3 prove several technical lemmas based techniques developed kivinen et al 1997 section 4 prove linear lower bounds rocchios similaritybased relevance feedback algorithm four typical similarities listed salton 1989 last restriction critical proof lower bounds would make analysis easier 4 z chen b zhu section 5 give discussions result also show rocchios algorithm used learn many target document classes conclude paper list several open problems section 6 2 rocchios similaritybased relevance feedback algorithm let r set real values let r set nonnegative real values let n positive integer binary vector space model information retrieval salton 1989 salton et al 1975 collection n features terms used represent documents queries document represented vector 1 n ith component v one ith feature appears zero otherwise query q represented vector v 1 n ith component v q 2 r real value used determine relevance weight ith feature unique vector representations documents queries convenience simply use q stand vector representations v v q respectively similarity general function r n r n r similarity used determine relevance closeness documents search query rank documents according close ness binary vector space model information retrieval salton 1989 salton et al 1975 baezayates ribeironeto 1999 retrieve relevant documents given query vector q respect similarity system searches documents classies similarity values mq higher explicit implicit threshold relevant returns user short list relevant documents highest similarity values information retrieval process fact determined linear classier dened later section composed query vector q similarity realvalued threshold unfortunately realworld information retrieval applications usually ideal query vector cannot generated due many factors limited knowledge users whole document collection typical example realworld problem web search case user may use keywords express documents wanted however nontrivial user web search engine precisely dene collection documents wanted query vector composed set keywords alternative formal analysis rocchios algorithm 5 solution query formation problem stated salton 1989 conduct searches iteratively rst operating tentative query formation ie initial query vector improving formations subsequent searches based evaluations previously retrieved materials type methods automatically generating improved query formation called relevance feedback one particular wellknown example rocchios similaritybased relevance feedback jj rocchio 1971 ide 1971a salton 1989 rocchios similaritybased relevance feedback algorithm works step step adaptive renement fashion follows starting initial query vector q 1 algorithm searches documents close q 1 according similarity ranks mq nally presents short list top ranked documents user user examines returned list documents judges documents relevant irrelevant step 1 assume list documents user judged x algorithm updates query vector q coecients j algorithm uses updated query vector q similarity search relevant documents ranks documents according presents top ranked documents user practice threshold explicitly implicitly used select highly ranked documents practically coecients j may xed baezayates ribeironeto 1999 salton 1989 following four typical similarities listed salton 1989 q x 2 r n dice coefficient make denitions valid arbitrary q x dene similarity two zero vectors zero ie pointed cosine similarity 3 nothing inner product similarity 1 vectors normalized also easy show jaccard similarity 4 strictly monotonic 6 z chen b zhu transformation dice similarity 2 implies inner product similarity cosine similarity may achieve equivalent classication document document respect given query vector subtle important dierence two similarities dierent rank values may obtained document respect given query vector dierent ranking gaps obtained documents example rank gap documents x based inner product similarity 1 08 rank gap based similarity 3 may analogously analysis applies dice similarity jaccard similarity dierent rank gaps pairs documents based dierent similarities dene dierent user preference structures documents hence dierent document groups clusters may obtained therefore dierent information retrieval performances might achieved dierent similarities words performance information retrieval system concerned cosine inner product similarities may regarded two equivalent similarities may jaccard dice similarities wong et al 1988 user preference studied terms structures weak order particular linear order designing adaptive information retrieval system system performance denitely negligible factor best knowledge know provable theoretical results uences dierent similarities performance similaritybased relevance feedback linear lower bounds proved paper tell us worst case four dierent similarities aect performance rocchios relevance feedback know result average case stated baezayates ribeironeto 1999 main advantage relevance feedback simplicity good results simplicity due fact modied term weights query vector components computed directly set retrieved documents good results observed experimentally due fact modied query vector ect portion intended query semantics similaritybased relevance feedback algorithm essentially adaptive supervised learning algorithm examples salton buck ley 1990 lewis 1991 goal algorithm learn unknown classier classify documents relevant irrelevant learning performed modifying updating query vector serves hypothetical representation collection relevant documents method updating query vector similar formal analysis rocchios algorithm 7 perceptron algorithm given necessary formal denitions following definition 1 let r n r n r similarity classier respect ndimensional binary vector space f0 1g n triple q q 2 r n query vector r threshold classier q classies documents irrelevant otherwise clas sier called linear classier respect similarity linear function r n r n r simplicity may call q classier linear classier linear following examples classiers particular q linear classier three 1 linear linear definition 2 adaptive supervised learning algorithm learning target classier q ndimensional binary vector space f0 1g n examples game played algorithm user step step fashion query vector q threshold unknown algorithm similarity step 1 gives classier q hypothesis target classier user q 2 r n 2 r hypothesis equivalent target user says yes conclude learning process otherwise user presents example target classier hypothesis classier dier x case say algorithm makes mistake step 1 algorithm constructs new hypothetical classier user based received examples x learning complexity mistake bound algorithm worst case maximum number examples may receive user order learn classier readers familiar online learning equivalence queries angluin 1987 littlestone 1988 adaptive supervised 8 z chen b zhu learning algorithm dened proper online learning algorithm learning class classiers equivalence queries ndimensional binary vector space give formal denition rocchios similaritybased relevance feedback algorithm definition 3 rocchios similaritybased relevance feedback algorithm adaptive supervised learning algorithm learning classier ndimensional binary vector space f0 1g n examples let q 1 initial query vector step 1 algorithm presents classier q hypothesis target classier user 2 r threshold query vector q modied follows assume beginning step algorithm received sequence examples x algorithm uses following modied query vector q next classication called additive updating factors please note denition generalized version rocchios original algorithm denition function r n r n r used similarity arbitrary real values used computing updated query vector nally denition allows adaptive learning target obtained remark 4 would like give following remarks roc chios similaritybased relevance feedback algorithm denition rocchios similaritybased relevance feedback algorithm use realvalued threshold realvalued additive updating factors step practice additive updating factors j may xed 1 05 promote relevance relevant examples 1 05 demote irrelevance irrelevant examples baezayates ribeironeto 1999 salton 1989 0 usually set 1 also practice threshold usually implicitly used selecting short list top ranked documents b purpose worst case analysis mistake bounds algorithm denition allow algorithm receive one example step consider user acts adversary algorithm algorithm required formal analysis rocchios algorithm 9 precisely learn search target linear classier practical applications contrast worst case consid erations step algorithm may receive several examples user user general may act adversary algorithm algorithm usually required search short list top ranked relevant documents hence lower bounds proved paper may aect algorithms eective applicability realworld problems formal analysis algorithm helps us understand nature well may able nd new strategy improve eectiveness design new algorithms information retrieval c similarity inner product two vectors rocchios algorithm similar rosenblatts perceptron algorithm rosenblatt 1958 use sets documents represented monotone disjunctions relevant features study mistake bounds rocchios algorithm ecient learnability monotone disjunctions relevant features attributes extensively studied machine learning example littlestone 1988 although simple mat monotone disjunctions common ways expressing search queries especially case web search existing popular search engines support disjunctions keywords search query formations k 1 k n classiers dened precisely classify monotone disjunction k relevant features ie precisely classify whether given document satises monotone disjunction 2 choose vector u 2 r n components zero except positions one easy verify 2 f0 1g n following four expressions necessary sucient condition deciding whether satises 2 kn z chen b zhu implies u 1 respectively classiers 2 3 technical lemmas technique used kivinen et al 1997 prove linear lower bounds perceptron algorithm general linear additive online learning algorithms construction example sequence l pairwise constant inner products given initial query vector weight vector used kivinen et al 1997 linear classier inner product similarity initial query vector linear classier dier sequence b perceptron algorithm makes one mistake one two examples every pair b words pair sequence b preserves classication dierence linear classier initial query vector perceptron algorithm examples b used update query vector shown kivinen et al 1997 row vectors hadamard matrices used construct required example sequence b borrow technique kivinen et al 1997 prove linear lower bounds rocchios similaritybased relevance feedback algorithm however one must note similarity example jaccard coefcient similarity inner product similarity used pairs sequence b used kivinen et al 1997 may still preserve classication dierence target classier may necessarily linear initial query vector rotation invariant concept kivinen et al 1997 general applicable learning algorithms nonzero initial query vector applicable nonrotation variants linear additive learning algorithms therefore need design new methods constructing example sequences applicable rocchios similaritybased relevance feedback algorithm arbitrary initial query vector four similarities dened section 2 following extend denition 7 given kivinen et al 1997 deal similarity definition 5 let sequence l z 0 z 00 f0 1g n let q 1 2 r n query vector similarity u classier dene q r say query vector q classier u dier sequence b formal analysis rocchios algorithm 11 respect either prove weaker version lemma 8 kivinen et al 1997 initial query vector weight vector term required dier target linear classier example sequence whereas require query vectors initial one updated ones dier target classier general may linear use pairwise constant inner product property may property similarities lemma 6 let r n r n r similarity let q 1 2 r n initial query vector let sequence l z 0 z 00 f0 1g n classier u domain f0 1g n q dened denition 3 dier b respect similaritybased relevance feedback algorithm makes least l mistakes learning classier u proof let rocchios similaritybased relevance feedback algorithm learning u consider trial sequence x 2 fz 0 g classication value x determined unknown target classier u words classies x relevant step 1 l hypothesis algorithm according expression 1 denition 3 assumption q dier sequence b respect either mq z rst case mq z 0 adversary chooses x case ie target classier classies x irrelevant hypothesis issued classies relevant thus makes z chen b zhu mistake mq z 0 adversary chooses x case ie target classier classies x relevant hypothesis issued classies irrelevant thus makes mistake second case mq z 0 manner show learning algorithm makes mistake either z 0 z 00 therefore makes l mistakes trial sequence means makes least l mistakes learning unknown target classier u mwe follow approach kivinen et al 1997 construct example sequences row vectors hadamard matrices sequences essentially applicable rocchios similaritybased relevance feedback algorithm zero initial vector used nonzero initial vector used let n identity matrix order n definition 7 hadamard matrix h n order n n n matrix elements f1 1g h n normalized rst row rst column consist ones 2 implies two distinct rows columns h n orthogonal normalized hadamard matrices constructed follows let 1 two examples hadamard matrices given follows formal analysis rocchios algorithm 13 following property follows 3 4 proposition 8 tth row normalized hadamard matrix h n definition 9 let positive integers k tth row normalized hadamard dene bh sequence z 0 z 0 z 00 proposition 10 let positive integers k let sequence dened denition 9 j 1 proof prove b c coped similarly proposition 8 h similarly 2 14 z chen b zhu h introduce new method constructing example sequences applicable rocchios similaritybased relevance feedback algorithm four similarities arbitrary initial query vector expand hadamard matrix adding rows columns zeroes exchange rows columns expanded matrix according initial query vector new method given proof proposition 11 proposition 11 given positive integers k query vector q 2 f0 1g n sequence v 0 v 00 f0 1g n sequence satises three properties given proposition 10 occurrence 2 replaced 2 1 occurrence z replaced v respectively furthermore q least 2 1 zero components 1 q v proof given vector positive integers k means choose 2 1 components q denoted either one zero dene 2 1 2 1 hadamard matrix move rst c n rows respectively process achieved sequence exchanges two rows words n n transformation matrix ac n work aa move rst 2 1 columns ac n formal analysis rocchios algorithm 15 columns respectively similarly process achieved sequence exchanges two columns moreover work j 1 row ac n 1 2 1 component j denoted x fact sth component jth row hadamard matrix h 2 1 words j zero components except 2 1 components x forming subvector jth row h 2 1 nally construct j j changing 1 components zero keeping components also construct v 00 j j changing 1 components one one components zero keeping components words v 0 constructed follows rst construct z 0 j z 00 j 2 1 2 1 hadamard matrix denition 10 construct v 0 j adding n 2 1 zero components end z 0 j moving rst 2 1 components z 0 exchanging columns similarly construct v 00 j adding n 2 1 zero components end z 00 j moving rst 2 1 components z 00 j exchanging columns hence proposition 11 follows proposition 8 manner similar proposition 10 2 following two lemmas show sequence bh enables query vector q preserve 1 similarity pair z 0 z 00 bh zero initial query vector used sequence enables query vector q preserve 1 similarity pair v 0 v 00 dq 1 arbitrary initial query vector q 1 used lemma 12 positive integers k let bh sequence dened denition 9 let initial query vector q 2 r g z chen b zhu proof 1 j 2 x b c 1 1 2 lemma 13 let positive integers k given initial query vector q 1 2 f0 1g n let dq 1 sequence given proposition 11 g q 1 least 2 1 zero components proof 1 g proposition 11 properties similar b c proposition 10 hold sequence dq 1 hence 1 2 2 1 proposition 11 formal analysis rocchios algorithm 17 thus 2 2 1 least 2 1 zero components proposition 11 following lemma allows us choose examples subdomain force learning algorithm make mistakes lemma 14 given n k 1 0 adversary strategy forces adaptive supervised learning algorithm make least k 1 mistakes learning class disjunctions k 1 variables fx 1 g binary vector space f0 1g n moreover adversary chooses examples vector space nonzero values variables fx 1 g proof given adaptive supervised learning algorithm step 1 k 1 adversary uses example x defeat learning algorithm follows x zero components except th components one learning algorithm classies x relevant adversary classies irrelevant otherwise adversary classies relevant 2 one may easily note strategy used prove lemma 14 generalized prove following fact given adaptive supervised learning algorithm makes least n mistakes learn class disjunctions n variables fx binary vector space f0 1g n algorithm forced make one mistake determine whether n variables target disjunction n variables unfor tunately one must note kind strategy cannot used prove adaptive supervised learning algorithm rocchios algorithm makes least n mistakes learning disjunction k variable k less n especially k small constant reason must design sophisticated methods paper prove linear lower bounds rocchios algorithm learning disjunctions k variables k value 1 n example k 1 3 log n n disjunctions small number variables common ways users specify information needs example realworld web search number keywords used query session usually small problem learning disjunctions small number variables z chen b zhu studied many researchers see example work littlestone 1988 careful readers may observed rule choice decomposition proposition 8 lemma 13 section 14 simply used prove linear lower bounds adaptive supervised learning algorithm rocchios algorithm learn disjunctions variables binary vector space however pointed paragraph k must reply choice 6 0 decomposition prove linear lower bounds example case choice 6 0 prove rocchios algorithm makes least mistakes learning disjunctions one variable zero initial query vector used show next section proof accomplished construction 2 pairs examples row vectors hadamard strategy lemma 14 like applicable case learning disjunctions one variable case learning disjunctions small number variables 4 linear lower bounds ready prove linear lower bounds rocchios similaritybased relevance feedback algorithm four typical similarities used throughout section let two positive integers k let u vector f0 1g n rst component one last k 1 components k 1 fewer ones however onecomponents specied point components zero given query vector 2 1 components either zero one dene uq 1 vector f0 1g n 1 th component one j th components zero among remaining n 2 1 components k 1 one components setting components one determined point note respectively monotone disjunction k relevant features use eu euq 1 denote monotone disjunctions represented u uq 1 respectively lemma 15 let 2 example sequence dened denition 9 similarity 1 4 formal analysis rocchios algorithm 19 2 r query vector classier dier bh respect arbitrary values r x g proof noted section 2 1 respectively classiers monotone disjunction eu k relevant features follows denition 9 rst component z 0 one rst z 00 zero last k components z 0 z 00 zero 1 2 hence 1 2 propositionm 1 u z 0 kn lemma 12 8 12 1 2 q classier u 12 dier sequence bh respect 1 similarity 2 2 proposition 10 lemma 12 z 00 according denition thus 1 2 hence 9 12 query vector q classier u 2k dier sequence bh respect 2 z chen b zhu similarity 3 2 proposition 10 lemma 12 similarity according denition thus 12 query vector q classier u 1 dier sequence bh respect 3 finally similarity 4 2 proposition thus 1 2 4 12 query vector q classier u 1k dier sequence bh respect 4 2 lemma 16 given initial query vector example sequence dened proposition 11 similarity 1 4 2 r query vector classier dier dq 1 respect arbitrary values r x g moreover q 1 least 2 1 zero components q 1 dier respect formal analysis rocchios algorithm 21 proof proof lemma 15 need replace u uq 1 z v 2 2 1 respectively also need use proposition 11 lemma 13 complete proofwe prove following main results paper theorem 17 let positive integers k given similarity similaritybased relevance feedback algorithm makes least n mistakes learning class monotone disjunctions k relevant features binary vector space f0 1g n initial query vector similarity used proof let rocchios similaritybased relevance feedback algorithm similarity initial query vector q analyze number mistakes must make learning eu disjunction k relevant features represented u noted 2 r classier classies eu ie logically equivalent eu lemma 15 classier query vector q 1 2 dier example sequence respect similarity thus lemma 6 adversary use examples bh force algorithm make 2 mistakes note last k 1 components examples bh zero last components u unspecied may k 1 one components hence lemma 14 adversary force make least k 1 mistakes learn values last k 1 components u putting together makes least 2 mistakes learning eu 2 theorem 18 let positive integers k given similarity similaritybased relevance feedback algorithm makes least nk 32 mistakes learning class monotone disjunctions k relevant features binary vector space f0 1g n arbitrary initial query vector q 1 2 f0 1g n similarity used moreover initial query vector q 1 least 2 1 zero components algorithm makes least n mistakes proof let rocchios similaritybased relevance feedback algorithm similarity arbitrary initial query vector analyze number mistakes must make learning euq 1 disjunction k relevant features represented uq 1 noted 2 r 22 z chen b zhu classier classies euq 1 ie logically equivalent euq 1 lemma 16 classier uq 1 query vector q 2 2 dier example sequence dq 1 respect similarity q 1 least 2 1 zero components classier q 1 also dier respect thus lemma 6 adversary use examples dq 1 force algorithm make 2 1 1 mistakes 2 1 mistakes q 1 least 2 1 zero components note positions components examples dq 1 positions zero components u components unspecied may k 1 one components hence lemma 14 adversary force make least k 1 mistakes learn values components u positions putting together makes least 2 1 nk 32 mistakes learning eu q 1 least 2 1 zero components makes least 2 1 mistakes 2 lower bounds obtained theorems 17 independent choices threshold coecients rocchios similaritybased relevance feedback algorithm may use updating query vector making classication 5 discussions 51 gradient descent procedure pointed wong et al 1988 one primary concern information retrieval ensure documents relevant user information needs ranked ahead less relevant ones means ranking acceptable guarantee less preferred documents listed front preferred ones ranking called acceptable ranking wong et al 1988 let denote user preference relation document vector space wong yao bollmann designed nice gradient descent procedure compute query vector q satisfying called dierence vector documents 0 gradient descent procedure procedure outlined follows choose initial query vector q 0 let formal analysis rocchios algorithm 23 ii let q k query vector kth step identify set dierence vectors solution vector terminate procedure iii let b back step ii gradient descent procedure nice adaptive algorithm computing query vector however must know user preference relation ahead time perform exhaustive search identify set q k step ii exhaustive search exponential time complexity practice machine learning setting user preference relation unknown target must learned information retrieval system sense gradient descent procedure adaptive learning process hand rocchios similaritybased relevance feedback algorithm formally dened section 2 adaptive learning algorithm without priori knowledge user preference updating process query vector iteration linear time complexity 52 learning document classes paper study learning document class represented monotone disjunction index features terms rocchios algorithm fact rocchios algorithm easily used learn target document classes class represented conjunction disjunctions index features terms conjunctions disjunctions common forms search queries constructed humans give several examples examples know lower bounds proved previous section learning performance rocchios algorithms hold learning cases example 1 learning arbitrary disjunctions arbitrary disjunction general case monotone disjunctions may negated index features negated index feature disjunction means feature occur desired documents let arbitrary disjunction let z document vector makes g false ie gz must z z chen b zhu 1 use z transform g monotone disjunction please also note z used transform f back g following manner transformation methods tell us g learned follows first use rocchios algorithm initial query vector q learn g document vector judged user irrelevant one use z transform obtained document vector continue learning kind transformation algorithm actually learns function f observed f easily transformed g document vector z process implies learning arbitrary disjunction rocchios algorithm performance learning monotone disjunction example 2 learning monotone conjunctions let monotone conjunction ie index features occurred g positive negation g disjunction note vector z used transform g monotone disjunction transform f back g hence learn g need learn negation g example 1 done rocchios algorithm example 3 learning arbitrary conjunctions let arbitrary conjunction negation g g formal analysis rocchios algorithm 25 arbitrary disjunction learn g need learn negation g example 1 done rocchios algorithm example 4 learning disjunctions conjunctions let disjunction conjunctions g conjunction 1 using virtual variable technique developed maass warmuth 1998 one learn g follows introduce one new input variable virtual variable possible 3 n conjunctions constructed variables x values x known value virtual variable easy determined help virtual variables g monotone disjunction thus learned rocchios algorithm example 5 learning conjunctions disjunctions let conjunction disjunctions f disjunction 1 negation f disjunctions conjunctions example 4 virtual variables technique developed maass warmuth 1998 used learn negation f hence f 53 remark optimal criterion rocchios algorithm according jj rocchio 1971 salton 1989 construction ideal query vector would target maximize average querydocument similarity relevant documents time minimize average querydocument similarity irrelevant documents known jj rocchio 1971 salton 1989 appropriate assumptions ideal query vector form r rel nonrel r n r assumed number relevant irrelevant documents summations range sets normalized relevant irrelevant documents respectively however optimal query vector cannot adopted practice sets relevant irrelevant documents respect queries known 26 z chen b zhu exhaustive search recall gradient descent procedure wong et al 1988 similar problem hand optimal query vector adaptively approached following see salton 1989 r 0 set documents judged user relevant end iteration n 0 set documents judged irrelevant two approximation formulas 1 updating factors coecients formalization rocchios algorithm given section 2 arbitrary updating factors allowed lower bounds proved fact independent choices updating factors lower bounds hold even 1 1 used updating factors 54 counting arguments k 1 k n given disjunction k variables easy know 2 n k documents binary vector space f0 1g n irrelevant q ie vectors documents make q false 2 n 2 n k documents relevant q ie vectors documents make q true k small say 3 huge value large n huge number documents irrelevant q one might ask whether kind observation helps us nd simple ways prove linear lower bounds rocchios algorithm example adversary could response almost formulated query produce document matches query degree relevant matter fact sort idea essence example decision tree technique developed littlestone 1988 prove lower bounds general learning algorithms decision tree inner node labeled document vector binary vector space f0 1g n two edges leaving inner node labeled relevant irrelevant respectively formal analysis rocchios algorithm 27 leaf labeled disjunction k variables way disjunction leaf consistent labels along path leading root leaf number inner nodes along path depth leaf assume depth every leaf decision tree least mistake bound learning algorithm learning disjunctions k variables least readers refer littlestone 1988 proof statement decision tree technique useful reduces problem proving lower bound every learning algorithm problem constructing single decision tree required depth latter usually done counting arguments estimate depth decision tree best knowledge maass turan 1994 best article prove lower bounds learning threshold gate general case disjunction k variables article presents several powerful counting arguments estimate depth decision tree class threshold gates unfortunately counting arguments help us obtain linear lower bounds rocchios algorithm reason many monotone disjunctions k binary variables one easily nd number monotone disjunctions k binary variables least depth decision tree class monotone disjunctions k binary variables log 2 ck example log log log log 2 ck log 2 cn n k n therefore counting arguments cannot yield linear lower bounds rocchios algorithm k much less n especially k small constant example lower bound produced counting argument log 2 n however noncounting technique use paper yield linear lower bounds rocchios algorithm k 1 k n even recall disjunctions small number variables common 28 z chen b zhu ways users specify information needs example realworld web search number keywords used query session usually small problem learning disjunctions small number variables studied many researchers see example work littlestone 1988 6 conclusions open problems rocchios similaritybased relevance feedback algorithm one popular query reformation method information retrieval used various applications essentially adaptive supervised learning algorithm examples however little rigorous analysis learning complexity paper prove linear lower bounds rocchios similaritybased relevance feedback algorithm four typical similarities listed salton 1989 used linear lower bounds proved worst case analysis may aect algorithms eective applicability realworld problems lower bounds help us understand nature algorithm well may nd new strategies improve eectiveness rocchios algorithm design new algorithms information retrieval one possible way use winnow2 littlestone 1988 algorithm alternative similaritybased relevance feedback algorithm applications information retrieval example recent research building realtime intelligent web search engines chen et al 2000 chen meng 2000 used tailored version winnow2 list following open problems future research problem 1 lower bound theorem holds arbitrary initial query vector q 1 2 f0 1g n choosing zeroone initial query vector common practice applications example web search initial zeroone query vector may constructed query words submitted user initial query vector q 1 chosen r n prove lower bound similar tedious approach similarities know whether lower bound still holds 4 problem 2 paper proved linear lower bounds rocchios similaritybased relevance feedback algorithm binary vector space know whether approach extended study learning complexity rocchios algorithm arbitrary discretized vector space problem 3 linear lower bounds established roc chios algorithm binary vector based worstcase anal formal analysis rocchios algorithm 29 ysis would interesting analyze averagecase learning complexity rocchios algorithm feel problem challenging nontrivial average case analysis reply realistic models document distribution index term distribution user preference distributions well feel easy model distributions analyze complexity distributions probabilistic corpus model proposed papadimitrious et al 2000 may shed light problem problem 4 although follows linear lower bounds winnow2 algorithm littlestone 1988 better worst case learning complexity authors know provable theoretical analysis average learning complexity winnow2 algorithm similaritybase relevance feedback algorithm pre cisely know whether winnow2 algorithm performs better average rocchios similaritybased relevance feedback algorithm acknowledgements early 1997 dr stanley sclaro asked rst author whether relevance feedback algorithm used help user search desired world wide web documents two dozens examples judged user time research group implemented imagerover taycher et al 1997 sclaro et al 1997 image search engine users relevance feedback author colleagues started build intelligent search tools yarrow chen meng 2000 websail chen et al 2000 features chen et al 2001 help information retrieval machine learning techniques dr sclaros question together authors research building intelligent web search tools inspired work paper authors would also acknowledge example sequence selection method developed kivinen et al 1997 proving linear lower bounds perceptron algorithm key breakthrough proofs without knowing method would take longer authors nish work paper thank three anonymous referees journal editor professor paul b kantor critical valuable questions comments helping us revise paper thank one referee informing us reference wong et al 1988 z chen b zhu r queries concept learning modern information retrieval multiplicative adaptive algorithms user preference retrieval yarrow realtime client site meta search learner websail online learning web search ieee press information retrieval data structures algorithms relevance feedback information retrieval smart retrieval system experiments automatic document processing perceptron algorithm vs winnow linear vs logarithmic mistake bounds input variables relevant learning intelligent information retrieval learning quickly irrelevant attributes abound new linearthreshold algorithm information computation latent semantic indexing probabilistic analysis critical analysis vector space model information retrieval perceptron probabilistic model information storage organization brain automatic text processing transformation vector space model automatic indexing cascia sclaro linear structures information retrieval tr ctr ying liu dengsheng zhang guojun lu weiying survey contentbased image retrieval highlevel semantics pattern recognition v40 n1 p262282 january 2007