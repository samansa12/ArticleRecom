performance comparison three modern dbms architectures introduction powerful workstations connected local area networks lans inspired new database management system dbms architectures offer high performance characteristics authors examine three software architecture configurations clientserver cs radunify type dbms ru enhanced clientserver ecs specific functional components design rationales discussed three simulation models used provide performance comparison different job workloads simulation results show ru almost always performs slightly better cs especially light workloads ecs offers significant performance improvement cs ru reasonable update rates ecs cs ru performance ratio almost proportional number participating clients less clients authors also examine impact certain key parameters performance three architectures show ecs scalable two b introduction centralized dbmss present performance restrictions due limited resources early eighties lot research geared towards realization database chines specialized expensive hardware software used built complex systems would provide high transaction throughput rates utilizing parallel processing accessing multiple disks recent years though observed different trends research technology local area networks matured workstations became fast inexpensive data volume requirements continue grow rapidly 2 light developments computer systemsand dbmss particular order overcome long latencies adopted alternative configurations improve performance paper present three configurations dbmss strive high throughput rates namely standard clientserver23 radunify type dbms19 enhanced clientserver architecture16 primary goal study examine performance related issues three architectures different workloads achieve develop closed queuing network models architectures implement simulation packages experiment different workloads expressed context job streams analyze simulated performance ratios derive conclusions system bottlenecks finally show light update rates 15 enhanced clientserver offers performance almost proportional number participating workstations configuration 32 less workstations hand radunify performs almost always slightly better pure clientserver architecture section 2 survey related work section 3 discusses three dbms architectures identifies specific functional components section 4 propose three closed queuing network models three configurations talk briefly implemented simulation packages section 5 presents different workloads simulation experiments discusses derived performance charts conclusions found section 5 related work number studies trying deal similar issues like investigate roussopoulos kang 18 propose coupling number workstations mainframe workstations mainframe run dbms workstations free selectively download data mainframe paper describes protocol caching maintenance cached data hagman ferrari 11 among first tried split functionality database system offload parts dedicated backend machines instrumented ingres dbms assigned different layers dbms two different machines performed several experiments comparing utilization rates cpus disks network among results found generally 60 overhead disk io suspected overhead similar size cpu cycles attribute findings mismatch operating dbms system cooperation server number workstations engineering design environment examined 13 dbms prototype supports multilevel communication workstations server tries reduce redundant work ends described dewitt et al 8 examine performance three workstationserver architectures objectoriented dbms point view three approaches building server proposed object server file server detailed simulation study presented different loads concurrency control report page file server gain object clustering page object servers heavily dependent size workstation buffers finally file page servers perform better experiments write type transactions response time measurements wilkinson niemat 26 propose two algorithms maintaining consistency workstation cached data algorithms based cache notify locks new lock compatibility matrices proposed novel point work server concurrency control cache consistency treated unified approach simulation results show cache locks always give better performance twophase locking notify locks perform better cache locks whenever jobs cpu bound alonso et al 2 support idea caching improves performance information retrieval systems considerably introduce concept quasicaching different caching algorithms allowing various degree cache consistency discussed studied using analytical queuing models delis roussopoulos 6 simulation approach examine performance server based information systems light updates show architecture offers significant transaction processing rates even considerable updates server resident data 17 describe modern clientserver dbms architectures report preliminary results performance 7 examine scalability three clientserver dbms configurations carey et al 5 examine performance five algorithms maintain consistency cached data clientserver dbms architecture important assumption work client data maintained cache memory disk resident wang rowe 25 similar study examine performance five cache consistency algorithms clientserver configuration simulation experiments indicate either two phase locking certification consistency algorithm offer best performance almost cases work indirectly related issues examined paper goda distributed filing system project 20 cache coherence algorithms described 3 ecs modelpresented hereis slightly modified abstraction system design described 18 discussed 16 paper extend work three ways first give relative performance measures two existing dbms configurations analyze role key system parameter values finally provide insights scalability architectures 3 modern dbms architectures briefly review three alternatives modern database system architectures highlight differences number reasons made configurations reality 1 introduction inexpensive extremely fast processors workstations large amount main memory medium size disk 2 evergrowing volume operational databases 3 need data staging extracting user class portion database defines users operational region although architectures general described relational model 31 clientserver architecture cs clientserver architecturecs extension traditional centralized database system model originated engineering applications data mostly processed powerful clients centralized repositories check protocols predominantly used maintaining data consistency cs database 23 client runs application workstation client database access server depicted figure 1a one client shown sake presentation communication server clients done remote calls local area network lan 22 applications processing carried client sites leaving server free carry database work model also applicable single machine one process runs server others run clients configuration avoids transmission data network obviously puts burden system load server paper assume server client processes run different hardware 32 radunify type dbms architectureru broad availability high speed networks fast processing diskless workstations principal reasons introduction radunify type dbms ar chitectureru presented rubinstein et al 19 main objective configuration improve response time utilizing client processing capability memory architecture depicted figure 1b role server execute low level dbms operations locking page readswrites figure 1b suggests server maintains lock data manager client performs query processing determines plan executed datapages retrieved sent diskless client memory carrying rest processing study experimental results presented mostly look operations perform much better traditional clientserver configurations also acknowledged architecture may perform well assumption light update loads specifically systems prototype required one server database writer permitted time novel point architecture introduction client cache memory used database processing therefore clients use cpu process pages resident caches server degenerates file server lock manager paper 19 suggests transfer pages appropriate client memory gives improved response times least case small medium size retrieval operations naturally dependent size client cache memory 33 enhanced clientserver architecture ecs ru architecture relieves cpu load server little biggest bottleneck io data manager enhanced clientserver architecture reduces cpu io load caching query results incorporating client fullfledged dbms incremental management cached data 15 16 shared database server dbms commun software cs server commun software application soft client commun software application soft server dbms commun software shared database shared database server dbms locking data managers commun software commun software application soft cached data lan lan lan radunify server ecs server client client client client dbms client dbms figure 1 cs ru ecs architectures initially clients start empty database caching query results time permits user create local database subset pertinent users application essentially client database partial replica server database furthermore user integrate herhis local database private data accessible others caching general presents advantages disadvantages two major advantages eliminates requests data server boosts performance client cpus working local data copies presence updates though system needs ensure proper propagation new item values appropriate clients figure 1c depicts new architecture updates directed execution server primary site pages modified read main memory updated flushed back server disk every server relation associated update propagation log consists timestamped inserted tuples timestamped qualifying conditions deleted tuples updated committed tuples recorded logs amount bytes written log per update generally much smaller size pages read main memory queries involving server relations transmitted processed initially server result query cached local relation first time local relation bound server relations used extracting result every binding recorded server dbms catalog stating three items interest participating server relations applicable conditions relations timestamp condition essentially filtering mechanism decides qualifying tuples particular client timestamp indicates last time client seen updates may affect cached data two possible scenarios implementing binding catalog information first approach server undertakes whole task case server maintains information caches time second alternative client individually keeps track binding information releases servers dbms responsibility keep track large number cached data subsets different update statuses multiplies quickly number clients query processing bound data preceded request incremental update cached data server required look portion log maintains timestamps greater one seen submitting client possible done binding information demanding client available first implementation alternative used done readily however second solution followed client request accompanied along proper binding template enable server perform correct tuple filtering relevant fractions increments modifications propagated clients site set algorithms carry tasks based incremental access methods relational operators described 15 involve looking update logs server transmitting differential files 21 significantly reduces data transmission network transmits increments affecting bound object compared traditional cs architectures query results continuously transmitted entirety important point characteristics concurrency control mechanism assumed ecs architecture first since updates done server 2oe locking protocol assumed running server dbms also suggested recent study 25 time commercial dbmss reveal 2oe commit protocol assume updates single server transactions second assume update logs servers locked therefore server process multiple concurrent requests incremental updates 4 models dbms architectures section develop three closed queuing network models one three architectures implementation simulation packages based first discuss common components models specific elements closed network 41 common model components dbms locking models maintain w orkload generator part either client workstation w orkload generator responsible creation jobs either read write type every job consists database operation selec tion projection join update operations represented sqllike language developed part simulation interface interface specifies type operation well simple join page selectivities role w orkload generator randomly generate client job initial menu jobs also responsible maintaining mixing read write types operations job finishes suc cessfully w orkload generator submits next job process continues entire sequence queriesupdates processed accurately measure throughput assume sites queryupdate jobs continuous thinktime jobs three models network manager performs two tasks 1 routes messages acknowledgments server clients 2 transfers resultsdatapagesincrements corresponding client site depending csruecs architecture respectively results answers queries requesting jobs datapages pages requested ru query processing workstation site increments new records tuple networkparameters meaning v alue time overhead 10 msec remote procedure call net rate network speed 10 mbitssec mesg length average size 300 bytes requesting messages table 1 network parameters identifiers deleted records used ecs model incremental maintenance cached data related parameters network manager appear table 1 time overhead every remote call represented init time 14 net rate networks transfer ratembitssec average size job request message mesg length locking page level two types locks models shared exclusive use lock compatibility matrix described 10 standard twophase locking protocol 42 clientserver model figure 2 outlines closed queueing network cs model extension model proposed 1 consists three parts server network manager client one client depicted figure client model runs application programs directs dbms inquiries updates network manager cs server network manager routes requests server transfers results queries appropriate client nodes job submitted clients site passes send queue network servers input queue finally arrives ready queue system awaits service maximum multiprogramming degree assumed model limits maximum number jobs concurrently active competing system resources server pending jobs remain ready queue number active jobs becomes less multiprogramming level mpl job becomes active queued concurrency control manager ready compete system resources cpu access disk pages lock tables considered memory resident structures presence multiple jobs cpu allocated workload generator server client commit queue cmt output queue receive queue send queue mpl concurrency control queue ccm update queue upd blocked queue read queue rd abort queue abrt update blocked operation read abort operation ready queue input queue processing queue network manager figure 2 queuing model cs architecture roundrobin fashion physical limitations server main memory client main memory see later allocated database buffering may contribute extra data page accesses disk number page accesses determined formulas given 24 depend type database operation jobs serviced queues fifo discipline concurrency control manager ccm attempts satisfy locks requested job several possibilities depending happens set locks requested requested locks acquired corresponding page requests queued readqueue processing io rd module pages read rd buffered p rocessingqueue cpu works prc jobs queued p rocessingqueue pages locked successfully part request processed normally rdprc remaining part reenter concurrency control manager queue obviously due lock conflict routed ccm blocked queue lock acquired conflict ceases exist update request corresponding pages exclusively locked subsequently buffered updated upd job unsuccessful obtaining lock queued blocked queue lock request passes unsuccessfully predefined number times blocked queue deadlock detection mechanism serverparameters meaning v alue server cpu mips processing power server 21mips disk tr average disk transfer time 12msec main memory size server main memory 2000 pages instr per lock instructions executed per lock 4000 instr selection instructions executed per selected page 10000 instr projection instructions per projected page 11000 instr join instructions per joined page 29000 instr update instructions per updated page 12500 instr rd page instructions read page disk 6500 instr wr page instructions write page disk 8000 ddlock search deadlock search time kill time time required kill job 02 sec mpl multiprogramming level 10 table 2 server parameters triggered detection mechanism simply looks cycles waitfor graph maintained throughout processing jobs cycle found job least amount processing done far aborted restarted aborted jobs release locks rejoin systems ready queue finally job commits changes reflected disk locks released job departs server related parameters appear table 2 parameters also applicable two models selfdescribing give processing overheads time penalties ratios ranges system variables two issues mentioned first queuing model assumes average value disk access thereby avoiding issues related data placement disks second whenever deadlock detected system timing charged amount equal ddlock search active jobs kill time therefore deadlock overhead proportional number active jobs database parameters applicable three models described set parameters shown table 3 relation database consists following information unique name rel name number tuples card size tuples rel size stream mix indicates composition streams submitted w orkload generators terms read write jobs note also size server main memory defined hold portion disk resident database 25 case multiprogramming equal onewe applied fraction concept later sizes client main mem dataparameters meaning v alue name db name database testdatabase dp size size data page 2048 bytes num rels number relations 8 fill factor page filling factor 98 rel name name relation r1 r2 r8 card cardinality every relation 20k rel size size relation tuple bytes 100 stream mix queryupdate ratio 10 table 3 data parameters ories selected parameter values every segment multiprogrammed main memory equal 200 pages main memorympl 43 radunify model abort queue abrt ready queue concurrency control queue ccm upd update queue read queue rd input queue send queue workload generator rec queue network manager commit queue cmt output queue radunify server diskless client blocked queue mpl processing queue aborted committed job figure 3 queuing model radunify architecture figure 3 depicts closed network model radunify type architecture differences cs model ffl every client uses cache memory buffering datapages query processing ffl one writer time allowed update server database every client working finite capacity cache time wants request additional page forwards request server ffl handling aborts done slightly different manner w orkload generator creates jobs proper sendqueue network servers inputqueue directed servers readyqueue functionality mpl processor modified account single writer require ment one writer may coexist one readers time functionality upd rd processors corresponding queues well queue blocked jobs similar previous model prominent difference previous model query job pages read readqueue rd service module network directed workstation buffers awaiting processing architecture capitalizes workstations processing capability server becomes device responsible locking page retrieval make low level database system opera tions write type jobs executed solely server serviced updatequeue upd service module soon pages buffered client site p rocessingqueue local cpu may commence processing whenever available internal loop ru client model corresponds subsequent requests page may reside client cache since size cache finite may force request page many timesdepending type job 24 replacement performed either least recently used lru discipline way specific database operations call ie case sortmerge join operation waitfor graph processes executing maintained server responsible component deadlock detection deadlock found job selected killed job queued abortqueue abrt processing element releases locks process sends abort signal appropriate client client abandons processing current job discards datapages fetched far server instructs w orkload generator resubmit job processes commit notify proper signal diskless client component correct job scheduling takes place presence incomplete job still either active pending server w orkload generator takes action awaiting either commit abort signal 44 enhanced clientserver model closed queuing model ecs architecture shown figure 4 major difference previous models ffl server extended facilitate incremental update propagation cached data andor caching new data every time client sends request appropriate sections server logs need shipped server back client action decides whether recent changes latest seen requesting client addition client may demand new data previously cached local disk processing application ready queues mpl concurrency control queue ccm update queue upd blocked queue abort queue abrt update blocked operation abort operation receive messages network manager read type xaction send queue workload generator commit queue cmt rec queue update processing upd input queue output queue messages ecs server ism data access processing client rdm logrd queue figure 4 queuing model ecs architecture initially clientdisk cached data server relations defined using parameter ff rel corresponds percentage server relation cached participating client jobs initiated client sites always dispatched server processing message output queue send queue server receives requests input queue via network forwards ready queue job finally scheduled concurrency control manager ccm type determined job write type serviced update blocked abort queues upd abrt processing elements commit time updated pages flushed disk fraction write log fract pages appended log modified relation assuming fraction page tuples modified per page time read job routed logrd queue queue accommodated log read manager logrdm decides pertinent changes need transmitted job evaluated client new portion data downloaded client increments decided examined pieces logs read filtered client applicable conditions factor decides amount new data cached per job defined every client individually parameter cont caching perc last factor determines percentage new pages cached every time parameter set zero initial set experiment log read manager sends queue either requested data increments along newly read data acknowledgment model clients requires modification due client disk existence transactions commence respective workload generators sent network server servers answer received two possibilities exist first data transferred server acknowledgment increments exist case clients dbms goes execution query hand incremental changes received server accommodated increment service module ism responsible reflecting local disk service query evaluation provided loop repeating data page accesses processing clearly overhead involved whenever updates server affect clients cached data new data cached table 4 shows additional parameters models average client disk access time client cpu processing power client main memory size described client dist tr client cpu mips client main memory respectively num clients number clients participating system configuration every experiment instr log page number instructions executed ecs server order process log page clientparameters meaning v alue applicable model client disk tr average disk transfer time 15 msec ecs client cpu mips processing power client 20 mips ru ecs client main memory size client main memory 500 pages ru ecs initial cached fraction 030 040 ecs server relation rel instr log page instructions process 5000 ecs log page log fract fraction updated pages 010 ecs recorded log cont caching perc continuous caching factor 0 ecs clients number clients 48162456 cs ru ecs table 4 additional cs ru ecs parameters 45 simulation packages simulation packages written c sizes range 46k 54k lines source code twophase locking protocol used implemented timeout mechanisms detecting deadlocks aborted jobs scheduled immediately delayed number rounds restart execution time three simulators complete workload one day decstation 5000200 also ensurethrough method batch means9that simulations reach stability confidence 96 5 simulation results section discuss performance metrics describe experiments conducted using three simulators system data related parameter values appear tables 1 4 51 performance metrics queryupdate streams work load settings main performance criteria evaluation average throughput jobsmin throughput speedup defined ratio two throughput values speedup related throughput gap 12 measures relative performance two system configurations several job streams also measure server disk access reduction ratio server disk accesses performed two system configurations cache memory hits various system resource utilizations database experiments consists eight base relations every relation cardinality 20000 tuples requires 1000 disk pages main memory server retain 2000 pages multiprogrammed jobs client retain 500 pages database processing main memory case ru ecs configurations initially database resident servers disk time progresses parts cached either cache memory ru clients disk main memory ecs clients order evaluate three dbms architectures w orkload generator creates several workloads using mix queries modifications queryupdate streamqus sequence query update jobs mixed predefined ratio first two experiments paper mix ratio 10 qus includes one update per ten queries qus jobs randomly selected predetermined set operations describe workload every client submits execution qus terminates jobs completed successfully exactly qus submitted configurations lenght quss selected 132 jobs since gave confidence 96 results goal examine system performances diverse parameter settings principal resources dbms configurations compete cpu cycles disk ac cesses qus consists three groups jobs two queries updates since mix jobs plays important role system performance boral dewitt show 4 chose small large queries first two experiments included paper correspond low high workloads small size query set sqs consists 8 selections base relations tuple selectivity 5 2 done clustered attributes 4 2way join operations join selectivity 02 large size query set lqs consists 8 selections tuple selectivity equal selections tuple selectivity 404 selections done clustered attributes 3 projections finally 4 2way joins join selectivity 40 update jobs u made 8 modifications varying update rates 4 use clustered tributes update rates give percentage pages modified relation update experiments update rates set following values 0 modifications 2 4 6 8 given group classification formulate two experiments sqsu lqsu job stream particular experiment say sqsu x update rate consists queries sqs group updates u modify x throughput cs ru throughput rates sqsu clients figure 5 cs ru throughput server relation pages another set experiments designed around wisconsin benchmark found 16 52 experiments sqsu lqsu figure 5 throughput rates cs ru architectures presented number clients varies 4 56 clearly two groups curves cs located lower part chart ru located upper part chart overall could say throughput averages 116 jobs per minute cs 237 ru case 4 16 cs clients throughput increases cs configuration capitalize upon resources 24 cs clients observe throughput decline nonzero update streams attributed high contention higher number aborted restarted jobs ru 0 curve always remains range 32 33 jobsmin since client cache memories essentially provide much larger memory partitions corresponding ones cs one writer time requirement ru configuration plays major role almost linear decrease throughput performance nonzero update curves number clients increases note 56 clients ru performance values obtained qus 4 8 update rates cs counterparts evident figure since ru utilizes cache throughput ecs throughput sqsu clients figure memories cpu clients dbms page processing performs considerably better cs counterpart except case many clients submitting nonzero update streams ru throughput rates comparable obtained cs configuration average ru throughput improvement experiment calculated 204 times higher cs figure 6 shows ecs throughput identical streams 0 update curve shows throughput system increases almost linearly number clients benefit due two reasons 1 clients use local disks achieve maximal parallel access already cached data 2 server carries negligible disk operations updates therefore logs empty handles short messages acknowledgments routed network data movement across network observed case update rate increases 2 4 level achieved throughput rates remains high increases almost linearly number workstations holds stations observe small decline due higher conflict rate caused increased number updates similar declines observed others 0 update curve figures 5 6 see performance ecs significantly higher cs ru ecs maximum processing rate 13164 jobsmin 56 clients attached single server 0 updates maximum throughput 0 rucs 2 rucs 4 rucs 6 rucs 8 rucs 0 ecscs 2 ecscs 4 ecscs 6 ecscs 8 ecscs21e015throughput clients figure 7 ecscs rucs throughput speedup value cs 126 jobsmin ru 329 jobsmin number workstations observe decline job throughput termed maximum throughput threshold mtt varies update rate instance 2 curve comes 35 workstations 6 8 curves appears region around 20 workstations mtt greatly depends type submitted jobs composition qus well server concurrency control manager sophisticated flexible manager 26 one used simulation package would increase mtt values figure 7 depicts throughput speedup ru ecs architectures cs axis depicted logarithmic scale suggests average throughput ecs almost proportional number clients least light update streams 0 2 4 range 4 32 stations 4 update stream relative throughput ecs 17 times better csru counterparts 56 clients worth noting even worst case 8 updates ecs system performance remains 9 times higher cs decline though starts earlier clients still maintains 10 times higher job processing capability lower part figure 7 ru cs speedup shown although ru performs generally better cs stsu workload reasons given earlier corresponding speedup notably much smaller ecs principal reasons 2 ecscs 4 ecscs 6 ecscs 8 ecscs1e015rucs reduction curves reduction clients server disk reduction sqsu figure 8 ecscs rucs disk reduction superiority ecs loading server disk operations parallel use local disks figure 8 supports hypotheses presents server disk access reduction achieved ecs cs similarly one achieved ru cs ecs server disk accesses 0 update streams cause insignificant disk access assuming pertinent user data cached already workstation disks 2 update rate quss reduction varies 102 32 times entire range clients expected reduction drops update intensive streams ie curves 4 6 8 however even 8 updates disk reduction ranges 26 8 times significant gain disk reduction rates ru architecture cs vary 24 26 times throughout range clients qus curves achieved predominantly use client cache memories larger corresponding main memory multiprogramming partitions cs configuration avoiding many page replacements figure 9 reveals one greatest impediments performance ecs log operations graph shows percentage log pertinent disk performed log reads writes total number server disk accesses presense 40 clients log disk operations constitute large fraction server disk accesses around 69 percentage log accesses total server disk accesses clients percentage figure 9 percentage server page accesses due log operations figure depicts time spent network configurations update rates 0 4 8 ecs model causes least amount traffic network increments results small size ru models causes highest network traffic datapages must transferred workstation processing hand cs transfers qualifying recordsresults 56 clients ru network traffic almost double cs 215 7 times ecs figure 11 summarizes results ltsu experiment although nature query mix changed considerably notice speedup curves compare figure 7 difference nonzero update curves moved upwards closer 0 curve mtts appear much later 50 clients positive speedup deviations observed nonzero curves compared rates seen figure 7 interesting note gains produced ru model lower stsu experiment offer following explanation number pages processed ru server disk increases significantly larger selectionjoin selectivities projection operators involved experiment imposing delays server site disk utilization ranges 93 workstation cpus contribute much system average throughput 4 ecs 8 ecs21e0552ru related curves cs related curves clients time spent network sqsu figure 10 time spent network 53 effect model parameters section vary one one system parameters significant impact performance examined architectures run sqsu experiment finally compare obtained results figure 7 parameters vary client disk access time 9msec corresponds 40 reduction average access time figure 12 presents throughput speedup achieved ecs experiments using new client access time ecs results depicted figure 6 surprisingly new ecs performance values increased average 123 factor curves represents serious gains noupdate curve indicates constant 53 throughput rate improvement rest curves indicate significant gains range 4 40 clients 40 clients gains insignificant explanation large number updates increases linearly number submitting clients imposes serious delays server naturally ru cs throughput values affected experiment server disk access time set 7 msec observe one8 nonzero update curves approach 0 curve mtts area appear much later 40 stations providing faster disk access time server jobs executed faster cause 0 ecscs 2 ecscs 4 ecscs 6 ecscs 8 ecscs 0 rucs21e015 rest rucs curves throughput clients figure 11 rucs ecscs throughput speedups lqsu fewer restarts client cpu set 110mips results similar figure 6 exception notice average increase 1516 jobsmin throughput rate update curves indicates extremely fast processing workstations alone moderate impact performance ecs architecture server cpu set 90mips observe shifting curves towards top right corner graph pushes mtts right allows clients work simultaneously two reasons behavior 1 updates mildly consuming cpu therefore providing faster cpu finish much faster fast cpu results lower lock contention turn leads fewer deadlocks server log processing also carried faster well 54 experiments section perform four experiments examine role clean update workloads specially designed quss small number clients submits updates effect continuous caching parameter cont caching perc ecs model well ability models scaleup pure update workload experiment update rate 6 remains constant throughput ecs speedup client disk access 9msec 15msec clients figure 12 ecs speedup client disk access time 9msec client access time 15 msec qus made update jobs pure update workload figure 13 gives throughput rates configurations cs ecs models show similar curves shapewise range 410 clients throughput rates increase beyond point influence extensive blocking delays created deadlocks performance declines considerably 56 clients ecs achieved half throughput obtained 8 clients used overall cs better ecs latter also write logs extra disk page accesses commit time also interesting note almost linear decline performance ru configuration since strictly one writer time set experiments jobs sequenced mpl processor model executed one time due lack readers concurrency assists cs ecs clients lower range 4 8 achieve better throughput rates ru counterparts later advantages job concurrency many writers time csecs cases diminish significantly limited number update clients experiment number clients dedicated updates rest clients submit read requests simulates environments large number readonly users constant number updaters class databases includes systems used stock cs ru throuhgput clients pure update workload figure 13 quss consist update jobs markets figure 14 presents throughput speedup results sqsu experiment note three clients experiment designated writers remaining query database rucs curves remain performance value levels observed figure 7 exception number clients increases effect writers amortized readers thus nonzero update curves converge 0 rucs curve clients ecscs curves suggest spectacular gains ecs configuration performance system increases almost linearly number participating clients update curves note well mtts disappeared graph naturally mtts appear much later 56 clients used configuration shown figure changing data locales ecs clients far considered case clients continuously demand new data server cached disks goal experiment address issue try qualify degradation ecs performance carry experiment use parameter cont caching perc ccp represents percentage new pages relations involved query cached client disk parameter enabled us simulate constantly changing client working set data figure 15 shows 0 2 6 curves figure 6 ccp 0 superimposed curves throughput limited number update job strems clients 0 rucs 6 ecscs 8 ecscs 4 ecscs 2 ecscs 0 ecscs 8 rucs figure 14 throughput speedup rates three writer clients per experiment experiment sqsu ccp equal 1 2 taken account every query requires new part server data percentage contributes large numbers additional disk accesses augmented processing time part server fact makes almost linear format original 0 curve disappear specifically 56 clients original experiment attain 13163 jobsmin ccp1 achieve 4161 jobsmin accomplish 2066 jobsmin performance degradation heavy updating qus ie 6 less noteworthy since already serious server blocking ability scaleup also interested scaleup behavior architectures presence large number workstations purpose ran experiment number clients ranges 4 120 per server figure 16 depicts resulting curves stsu experiment graph indicates beyond mtts region speedup ecs cs gradually decreasing 40 clients update intesive quss speedup decline less sharp 2 curve detarioration due saturation servers resources great client number creates note however even 120 clients ecs architecture process 61 199 times jobs cs architecture nonzero update curves 0 update curve shows saturation continues increase almost linearly major reasons performance cs configuration 0 ecs 2 ecs 0 ecs 2 ecs 6 ecs throughput sqsu experiment three contcachingperc ccp values clients 6 ecs 6 ecs 2 ecs figure 15 experiments three values ccp parameter 56 clients remains stable range 900 125 jobmin due high utilization resources network shows signs extremely heavy utilization gains ru configuration remain levels reported experiment stsu 6 conclusions presented modeled compared three contemporary dbms architectures aiming high throughput rates simulation results show following characteristics ffl radunify architecture gives average 21 times performance improvement cs utilizing main memories cpus clients ffl although ecs performance declines qus queries updates ratio decreases still offers serious speedup rates two architectures pure update workloads ecs gives worst performance clients ffl light update rates 15 performance speedup ecs cs ru almost proportional number participating workstations 100030005000model scalability experiment throughput clients 0 ecscs 2 ecscs 4 ecscs 6 ecscs 8 ecscs 0 rucs rest rucs curves figure scalability experiment range 432 experiments streams without updates ecscs speedup increases almost linearly number workstations ffl either heavier update rates many concurrently imposed modifications ecs server becomes system bottleneck introduced maxi mum throughput thresholds mtts identify bottleneck points respect update rate ffl faster workstation disk access time improves performance ecs ffl ecs performance diminished significantly whenever clients constantly demand new data elements server ffl database environments exclusive writers number readers offer good performance results ecs configuration ffl pure update workloads ecs architecture scalable two future work includes experimentation using local databases workstations exploring behavior configurations different server concurrency control protocols developing periodic update propagation strategies bringing transaction throughput close 0 update curves aknowledgements authors grateful anonymous referees valuable comments suggestions well jennifer carle george panagopoulos commenting earlier versions paper r models studing concurrency control performance alternatives implications data caching issues information retrieval system cache coherence protocols evaluation using multiprocessor simulation model methodology database system performance evaluation data caching tradeoffs clientserver dbms architectures server based information retrieval systems light update loads performance scalability clientserver database architec tures study three alternative workstation server architectures objectoriented database systems computer systems performance evaluation granularity locks degrees consistency shared database performance analysis several backend database architectures performance considerations operating system transaction manager cooperative object buffer management advanced information management prototype environment developing faulttolerant software incremental access method view cache concept evaluation enhanced workstationserver dbms architecture modern clientserver dbms architectures principles techniques design adms benchmarking simple database operations highly available file system distributed workstation environment differential files application maintenance large databases unix networking programming architectures future data base systems database knowledgebase systems volume volumeiithevolume new technologies cache consistency concurrency control clientserver dbms architecture maintaining consistency clientcached data tr performance analysis several backend database architectures cache coherence protocols evaluation using multiprocessor simulation model principles techniques design admsf6wwp concurrency control performance modeling alternatives implications benchmarking simple database operations performance considerations operating system transaction manager coda data caching issues information retrieval system study three alternative workstation server architectures objectoriented database systems maintaining consistency clientcached data architecture future data base systems unix network programming environment developing faulttolerant software incremental access method viewcache data caching tradeoffs clientserver dbms architectures cache consistency concurrency control clientserver dbms architecture evaluation enhanced workstationserver dbms architecture differential files principles database knowledgebase systems methodology database system performance evaluation cooperative object buffer management advanced information management prototype performance scalability clientserver database architectures ctr svend frlund pankaj garg designtime simulation largescale distributed object system acm transactions modeling computer simulation tomacs v8 n4 p374400 oct 1998 vinay kanitkar alex delis time constrained push strategies clientserver databases distributed parallel databases v9 n1 p538 january 1 2001 vinay kanitkar alex delis realtime processing clientserver databases ieee transactions computers v51 n3 p269288 march 2002 alexander thomasian distributed optimistic concurrency control methods highperformance transaction processing ieee transactions knowledge data engineering v10 n1 p173189 january 1998 alex delis nick roussopoulos performance scalability clientserver database architectures proceedings 18th international conference large data bases p610623 august 2327 1992 jeho park vinay kanitkar alex delis logically clustered architectures networked databases distributed parallel databases v10 n2 p161198 september 2001 alex delis nick roussopoulos techniques update handling enhanced clientserver dbms ieee transactions knowledge data engineering v10 n3 p458476 may 1998 vinay kanitkar alex delis efficient processing client transactions realtime distributed parallel databases v17 n1 p3974 january 2005 alfredo goi arantza illarramendi eduardo mena jos miguel blanco optimal cache federated database system journal intelligent information systems v9 n2 p125155 septoct 1997