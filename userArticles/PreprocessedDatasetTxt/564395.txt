bayesian online classifiers text classification filtering paper explores use bayesian online classifiers classify text documents empirical results indicate classifiers comparable best text classification systems furthermore online approach offers advantage continuous learning batchadaptive text filtering task b introduction faced massive information everyday need automated means classifying text documents since handcrafting text classifiers tedious process machine learning methods assist solving problem15 7 27 yang liu27 provides comprehensive comparison supervised machine learning methods text classification paper show certain bayesian classifiers comparable support vector machines23 one best methods reported 27 particular evaluate bayesian online perceptron17 20 bayesian online gaussian process3 text classification filtering initial training set large online approaches useful allow continuous learning without storing previously permission make digital hard copies part work personal classroom use granted without fee provided copies made distributed profit commercial advantage copies bear notice full citation first page copy otherwise republish post servers redistribute lists requires prior specific permission andor fee sigir02 august 1115 2002 tampere finland seen data continuous learning allows utilization information obtained subsequent data initial training bayes rule allows online learning performed principled way16 20 17 evaluate bayesian online perceptron together information gain considerations batchadaptive filtering task18 2 classification filtering text classification task defined lewis9 set predefined categories set documents category document set partitioned two mutually exclusive sets relevant irrelevant documents goal text classification system determine whether given document belongs predefined cate gories since document belong zero one categories system collection binary classi fiers one classifier classifies one category text retrieval conference trec task known batch filtering consider variant batch filtering called batchadaptive filtering18 task testing document retrieved classifier relevance judgement fed back classifier feedback used improve classifier 21 corpora data text classification use modapte version reuters21578 corpus 1 unlabelled documents removed version 9603 training documents test documents following 7 27 categories least one document training test set retained reduces number categories 90 batchadaptive filtering attempt task trec 918 ohsumed collection6 used evaluate ohsu topicset consists 63 topics training test material consist 54710 293856 documents respectively addition topic statement topic purpose treated additional training document topic use title abstract author source sections documents training testing 22 representation various ways transform document representation convenient classification use available via httpwwwdaviddlewiscomresources testcollectionsreuters21578 bagofwords approach retain frequencies words tokenisation stemming stopwords moval frequencies normalized using various schemes19 6 use ltc normalization l l id jterms l jd subscripts denote ith term dth document respectively tf id frequency ith term dth document n documentfrequency ith term n total number documents 23 feature selection metric given set candidate terms select features set using likelihood ratio binomial distribution advocated dunning5 number relevant nonrelevant training documents contain term r n number relevant nonrelevant training documents n total number training documents asymptotically 2 ln 2 distributed 1 degree freedom choose terms 2 ln 1213 ie 005 significance level details feature selection procedures given section 4 24 performance measures evaluate text classification system use f1 measure introduced van rijsbergen22 measure combines recall precision following way number correct positive predictions number positive examples number correct positive predictions number positive predictions recall ease comparison summarize f1 scores dierent categories using micro macroaverages f1 scores11 27 categories documents average withincategory f1 values micro macroaverage f1 emphasize performance system common rare categories spectively using averages observe eect dierent kinds data text classification system addition comparing two text classification systems use micro signtest stest macro signtest stest two significance tests first used comparing text classification systems 27 stest compares binary decisions made systems stest compares withincategory f1 values similar f1 averages stest stest compare performance two systems common rare categories respectively evaluate batchadaptive filtering system use t9p measure trec918 number correct positive predictions max50 number positive predictions precision penalty retrieving 50 documents 3 bayesian online learning section based work opper17 solla suppose document described vector x relevance indicator x category given label 1 1 1 1 indicates irrelevant relevant respectively given instances past data predictive probability relevance document described x z da pyx apadm introduced classifier assist us prediction bayesian approach random variable probability density padm integrate possible values obtain prediction aim obtain reasonable description bayesian online learning framework16 20 17 begin prior pad0 perform incremental bayes updates obtain posterior data arrives py t1 x t1 apad r da py t1 x t1 apad make learning online explicit dependence posterior pad t1 past data removed approximating distribution paa t1 t1 characterizes distribution time 1 exam ple paa t1 gaussian t1 refers mean covariance hence starting prior new example t1 x t1 comprises two steps update posterior using bayes rule approximate updated posterior parameterisation approximation step done minimizing kullbackleibler distance approximating approximated distributions amount information gained learning new example expressed kullbackleibler distance posterior prior distribu igy t1 x t1d z da pad t1 log 2 z da paa t1 log 2 instances data replaced summaries approximation simplify notation henceforth use p denote paa averages taken paa respectively example predictive probability rewritten z da pyx ap pyx following sections scalar field used simplify notation calculation 31 bayesian online perceptron consider case describes perceptron define likelihood probit model ya x 0 fixed noise variance cumulative gaussian distribution z u p0 spherical unit gaussian p gaussian approximation opper16 17 solla obtain following updates equating means covariances paa t1 paa t1 x t1 py t1 h t1 h 311 algorithm training bayesian online perceptron data involves successive calculation means covariances c posteriors 1 1 initialize 0 0 c0 1 identity matrix ie spherical unit gaussian centred origin 2 3 t1 relevance indicator document x t1 4 calculate t1 t1 h py t1 h 5 calculate 6 calculate py t1 h 7 calculate 2 py t1 h py t1 h 8 calculate t1 c t1 prediction datum x simply involves calculation pyx 32 bayesian online gaussian process gaussian process gp constrained problems small data sets recently csato opper3 williams seeger24 introduced ecient eective approximations full gp formulation section outline approach 3 gp framework describes function consisting function values ax using probit model likelihood expressed 0 described section 31 addition p0a gp prior specifies gaussian distribution zero mean function covariancekernel function k0 x x function space p also gaussian process csato opper obtain following updates equating means covariances paa t1 paa t1 x t1 py t1 h t1 h notice similarities updates section 31 main dierence kernel trick introduced equations new inputs x t1 added sequentially system via 1th unit vector e t1 results quadratic increase matrix size drawback large data sets text classification csato opper overcome introducing sparseness gp idea replace e t1 projection approximation introduces error used decide employ approximation hence time algorithm holds set basis vec tors usually desirable limit size set accommodate csato opper describe procedure removing basis vector set reversing process adding new inputs lack space algorithm bayesian online gaussian process given reader referred 3 information 4 evaluation 41 classification reuters21578 evaluation compare bayesian online per ceptron bayesian online gaussian process support vector machines svm23 svm one best performing learning algorithms reuters21578 corpus7 27 bayesian methods described section 3 svm use sv light package joachims8 since svm batch method fair comparison online methods iterated training data 3 times testing 2 411 feature selection reuters21578 corpus select features category set words 2 ln 1213 prune using top 300 features reduces computation time required calculation covariances bayesian classifiers since svm known perform well many features svm classifiers also use set words occur least 3 training documents7 gives us 8362 words note words noncategory specific 412 thresholding probabilistic outputs bayesian classifiers used various ways direct way use bayes decision rule determine relevance document described x 3 however discussed 10 26 optimal chosen evaluation measure therefore addition 05 thresholding also empirically optimise threshold category f1 measure training documents scheme shall call maxf1 also employed 27 thresholding knn llsf classifiers dierence approach threshold 27 calculated validation set use validation set feel rare categories hard obtain reasonable validation set training documents bayesian classifiers also perform analytical threshold optimisation suggested lewis10 scheme shall call expectedf1 threshold category selected optimise expected f1 otherwise threshold p probability assigned document classifier set test docu ments set test documents probabilities higher threshold note expectedf1 applied probabilities test documents assigned hence classification done batch unlike first two schemes classification done online 413 results discussion section a2 discussion number passes 3 svm minimise structural risks would classify document relevant w x hyperplane b bias section a3 discussion jitter terms ij table 1 description methods description 4 perceptron fixed feature bias table 2 micromacroaverage f1 perceptron 8512 4523 8669 5216 8644 5308 table 1 lists parameters algorithms used evaluation table 2 3 tabulate results two sets results svm labeled svma svm b latter uses set features bayesian classifiers ie using 2 ln measure former uses set 8362 words features table 2 summarizes results using f1 averages table 3 compares classifiers using stest stest maxf1 thresholds used classification decisions row tables compares method listed first column methods significance levels 27 used several observations made generally maxf1 thresholding increases performance systems especially rare categories bayesian classifiers expectedf1 thresholding improves performance systems rare categories perceptron implicitly implements kernel used gp1 hence similar results maxf1 thresholding feature selection impedes performance svm table 2 svm 8362 features slightly lower microaverage f1 bayesian classifiers however stests table 3 show bayesian classifiers outperform svm significantly many common cat egories hence addition computing average f1 measures useful perform sign tests shown table 3 limited features bayesian classifiers outperform svm common rare categories based sign tests bayesian classifiers outperform using 8362 words common categories vice versa rare categories table 3 steststest using maxf1 thresholding perceptron means pvalue 001 means 001 pvalue 005 means pvalue 005 last observation suggests one use bayesian classifiers common categories svm rare ones 42 filtering ohsumed section bayesian online perceptron considered order avoid numerical integration information gain measure instead probit model section 31 use simpler likelihood model outputs flipped fixed probability update equations also change accordingly eg py t1 h t1 h using likelihood measure express information gained datum t1 x t1 log c log 2 c use evaluation following sections describe algorithm detail simplify presen tation divide batchadaptive filtering task batch adaptive phases 421 feature selection adaptation batch phase words 2 ln 1213 selected features adaptive phase obtain feedback update features adding new words 2 1213 feature added distribution perceptron extended one dimension 422 training classifier batch phase classifier iterated training documents 3 times addition relevant documents collected use adaptive phase adaptive phase retrieved relevant documents added collection document retrieved classifier trained document given relevance judgement classifier trained irrelevant documents time prevent forgetting relevant documents due limited capacity whenever train irrelevant document would also train past relevant document past relevant document chosen successively collection relevant documents needed also new features might added since relevant document last trained hence classifier would able gather new information document due additional features note past relevant document need chosen successive order instead chosen using probability distribution collection desirable handling topicdrifts evaluate eectiveness strategy retraining past retrieved relevant documents denote use rel though use means algorithm longer online asymptotic eciency unaected since one past document used training instance 423 information gain testing two reasons retrieve document first relevant ie represents document second although document deemed irrelevant classifier classifier would gain useful information document using measure igy xd calculate expected information gain 0408n ret q target number figure 1 versus nret tuned t9p document deemed useful expected information gain least optimizing t9p measure ie targeting 50 documents choose n ret total number documents system retrieved figure 1 plots n ret note kind active learning willingness tradeo precision learning decreases n ret use information gain criteria denoted ig test eectiveness information gain strat egy alternative one alternative denoted rnd randomly select documents retrieve based probability 50n ret 293856 otherwise 293856 number test documents 424 results discussion table 4 lists results seven systems first two microsoft research cambridge fudan university spectively runs trec9 task third system described full ie bayesian online perceptron retraining past retrieved relevant documents use information gain rest bayesian online perceptron dierent combinations strategies besides t9p measure sake completeness table 4 also lists measures used trec9 taken together measures show bayesian online percep tron together consideration information gain competitive method systems rel collection past known relevant documents kept although microsoft uses collection query reformulation another collection previously seen documents used threshold adaptation fudan maintains collection past retrieved documents uses collection query adaptation reports results run ok9bfr2po report results slightly better run ok9bf2po average number relevant documents retrieved average number features pptronrelig pptronig pptronrnd pptron figure 2 variation number features relevant documents retrieved plots pptronrelig pptronig close plots pptronrnd pptron typical operational system retrieved relevant documents usually retained irrelevant documents usually discarded therefore rel practical strategy adopt figure 2 plots average number features adaptive phase see features constantly added relevant documents seen classifier retrained past documents new features enable classifier gain new information documents compare results pptronrel pptron table 4 find training past documents causes number relevant documents retrieved drop 5 similarly pptronrelig pptronig drop 8 table 5 breaks retrieved documents classifier deems relevant classifier actually querying information pptronig pptronrnd table shows none documents randomly queried relevant documents surprising since average 0017 test documents relevant contrast information gain strategy able retrieve 313 relevant documents 261 documents queried significant result consider pptronig table 4 shows pptron information gain strategy removed 731 relevant documents retrieved hence although documents queried irrelevant information gained queries helps recall classifier ie 815 documents versus 731 documents important reaching target 50 documents mackay13 noted phenomenon querying irrelevant documents edges input space suggested maximizing information defined region interest instead finding region batch adaptive filtering remains subject research comparing four plots figure 2 find average information gain strategy causes 3 features discovered number relevant documents retrieved consequence better recall table 4 results batchadaptive filtering optimized t9p measure microsoft 5 fudan pptronrelig pptronig pptronrnd pptronrel pptron total retrieved 3562 3251 2716 2391 2533 1157 1057 relevant retrieved 1095 1061 1227 1128 732 772 731 macroaverage recall 395 379 362 333 200 208 200 macroaverage precision 305 322 358 358 216 619 623 mean t9p 305 317 313 298 192 215 208 mean utility 4397 1079 15318 15762 5349 18397 17730 mean t9u 4397 1079 15318 15762 5349 18397 17730 mean scaled utility 0596 0461 0025 0016 0397 0141 0138 zero returns table 5 breakdown documents retrieved pptronig pptronrnd numbers latter brackets relevant relevant total docs retrieved perceptron classifier proper 815 732 378 345 1193 1077 docs retrieved information gain random strategy 313 0 885 1456 1198 1456 total 1128 732 1263 1801 2391 2533 5 conclusions work implemented tested bayesian online perceptron gaussian processes text classification prob lem shown performance comparable svm one best learning algorithms text classification published literature also demonstrated eectiveness online learning information gain trec9 batchadaptive filtering task results text classification suggest one use bayesian classifiers common categories maximum margin classifiers rare categories partitioning categories common rare ones optimal way interesting problem svm employed use relevance feedback drucker et al 4 retrieval groups 10 doc uments essence form adaptive routing would instructive see bayesian classifiers perform without storing many previously seen documents would also interesting compare merits incremental svm21 1 bayesian online classifiers acknowledgments would like thank lehel csato providing details implementation gaussian process wee meng soon assisting data preparation yiming yang clarifying representation used 27 loo nin teow proofreading manuscript would also like thank reviewers many helpful comments improving paper 6 r incremental decremental support vector machine learning analysis binary data relevance feedback using support vector machines accurate methods statistics surprise coincidence ohsumed interactive retrieval evaluation new large test collection research text categorization support vector machines learning many relevant features making largescale svm learning practical representation learning information retrieval evaluating optimizing automomous text classification systems training algorithms linear text classifiers bayesian interpolation monte carlo implementation gaussian process models bayesian regression classification feature selection online versus bayesian approach online learning trec9 filtering track final report optimal perceptron learning online bayesian approach incremental learning support vector machines information retrieval nature statistical learning theory using nystrom method speed kernel machines bayesian mean field algorithms neural networks gaussian processes study thresholding strategies text categorization tr termweighting approaches automatic text retrieval representation learning information retrieval bayesian interpolation informationbased objective functions active data selection ohsumed nature statistical learning theory evaluating optimizing autonomous text classification systems training algorithms linear text classifiers feature selection perception learning usability case study text categorization making largescale support vector machine learning practical bayesian approach online learning optimal perceptron learning reexamination text categorization methods study thresholding strategies text categorization information retrieval text categorization suport vector machines relevance feedback using support vector machines ctr kian ming adam chai expectation fmeasures tractable exact computation empirical observations properties proceedings 28th annual international acm sigir conference research development information retrieval august 1519 2005 salvador brazil hwanjo yu chengxiang zhai jiawei han text classification positive unlabeled documents proceedings twelfth international conference information knowledge management november 0308 2003 new orleans la usa reylong liu dynamic category profiling text filtering classification information processing management international journal v43 n1 p154168 january 2007 randa kassab jeancharles lamirel towards synthetic analysis users information need effective personalized filtering services proceedings 2007 acm symposium applied computing march 1115 2007 seoul korea vaughan r shanks hugh e williams adam cannane indexing fast categorisation proceedings twentysixth australasian conference computer science research practice information technology p119127 february 01 2003 adelaide australia reylong liu wanjung lin adaptive sampling thresholding document filtering classification information processing management international journal v41 n4 p745758 july 2005 aynur dayanik david lewis david madigan vladimir menkov alexander genkin constructing informative prior distributions domain knowledge text classification proceedings 29th annual international acm sigir conference research development information retrieval august 0611 2006 seattle washington usa new topic identification using multiple linear regression information processing management international journal v42 n4 p934950 july 2006 franca debole fabrizio sebastiani analysis relative hardness reuters21578 subsets research articles journal american society information science technology v56 n6 p584596 april 2005