efficient sparse lu factorization partial pivoting distributed memory architectures abstracta sparse lu factorization based gaussian elimination partial pivoting gepp important many scientific applications still open problem develop high performance gepp code distributed memory machines main difficulty partial pivoting operations dynamically change computation nonzero fillin structures elimination process paper presents approach called parallelizing problem distributed memory machines approach adopts static symbolic factorization avoid runtime control overhead incorporates 2d lu supernode partitioning amalgamation strategies improve caching performance exploits irregular task parallelism embedded sparse lu using asynchronous computation scheduling paper discusses compares algorithms using 1d 2d data mapping schemes presents experimental studies crayt3d t3e performance results set nonsymmetric benchmark matrices encouraging achieved 6878 gflops 128 t3e nodes best knowledge highest performance ever achieved challenging problem previous record 2583 gflops shared memory machines 8 b currently computer science department university illinois urbanachampaign pivoting operations interchange rows based numerical values matrix elements elimination process impossible predict precise structures l u factors without actually performing numerical factorization adaptive irregular nature sparse lu data structures makes efficient implementation algorithm hard even modern sequential machine memory hierarchies several approaches used solving nonsymmetric systems one approach unsymmetricpattern multifrontal method 5 25 uses elimination graphs model irregular parallelism guide parallel computation another approach 19 restructure sparse matrix bordered block upper triangular form use special pivoting technique preserves structure maintains numerical stability acceptable levels method implemented illinois cedar multiprocessors based aliant shared memory clusters paper focuses parallelization issues given column ordering row interchanges maintain numerical stability parallelization sparse lu partial pivoting also studied 21 shared memory machine using static symbolic lu factorization overestimate nonzero fillins avoid dynamic variation lu data structures approach leads good speedups 6 processors sequent machine work needed assess performance sequential code far know published results parallel sparse lu popular commercial distributed memory machines crayt3dt3e intel paragon ibm sp2 tmc cm5 meiko cs2 one difficulty parallelization sparse lu machines utilize sophisticated uniprocessor architecture design sequential algorithm must take advantage caching makes previously proposed techniques less effective hand parallel implementation must utilize fast communication mechanisms available chines easy get speedups comparing parallel code sequential code fully exploit uniprocessor capability easy parallelize highly optimized sequential code one sequential code superlu 7 uses supernode approach conduct sequential sparse lu partial pivoting supernode partitioning makes possible perform numerical updates using blas2 level dense matrixvector multiplications therefore better exploit memory hierarchies superlu performs symbolic factorization generates supernodes fly factorization proceeds umfpack another competitive sequential code problem neither superlu umfpack always better 3 4 7 ma41 code sparse matrices symmetric patterns regarded high quality deliver excellent megaflop performance paper focus performance analysis comparison superlu code since structure code closer superlu paper present approach called considers following key strategies together parallelizing sparse lu algorithm 1 adopt static symbolic factorization scheme eliminate data structure variation caused dynamic pivoting 2 data regularity sparse structure obtained symbolic factorization scheme efficient dense operations used perform computation impact nonzero fillin overestimation overall elimination time minimized 3 develop scheduling techniques exploiting maximum irregular parallelism reducing memory requirements solving large problems observe current commodity processors memory hierarchies highly optimized blas3 subroutine usually outperforms blas2 subroutine implementing numerical operations 6 9 afford introduce extra blas3 operations redesigning lu algorithm new algorithm easy parallelized sequential performance code still competitive current best sequential code use static symbolic factorization technique first proposed 20 21 predict worst possible structures l u factors without knowing actual numerical values develop 2d lu supernode partitioning technique identify dense structures l u factors maximize use blas3 level subroutines dense structures also incorporate supernode amalgamation 1 10 technique increase granularity computation exploiting irregular parallelism redesigned sparse lu algorithm experimented two mapping methods one uses 1d data mapping uses 2d data mapping one advantage using 1d data mapping corresponding lu algorithm easily modeled directed acyclic task graphs dags graph scheduling techniques efficient runtime support available schedule execute dag parallelism 15 16 scheduling executing dag parallelism difficult job parallelism sparse problems irregular execution must asynchronous important optimizations overlapping computation communication balancing processor loads eliminating unnecessary communication overhead graph scheduling excellent job exploiting irregular parallelism leads extra memory space per node achieve best performance also 1d data mapping expose limited parallelism due restrictions also examined 2d data mapping method asynchronous execution scheme exploits parallelism memory constraints implemented sparse lu algorithms conducted experiments set nonsymmetric benchmark matrices crayt3d t3e experiments show approach quite effective delivering good performance terms high megaflop numbers particular 1d code outperforms current 2d code processors sufficient memory 2d code potential solve larger problems produces higher megaflop numbers rest paper organized follows section 2 gives problem definition section 3 describes structure prediction 2d lu supernode partitioning sparse lu factorization section 4 describes program partitioning data mapping schemes section 5 addresses asynchronous computation scheduling execution section 6 presents experimental results section 7 concludes paper find ja mk 03 singular stop row k row ik 6 0 kj 6 0 ik 6 0 figure 1 sparse gaussian elimination partial pivoting lu factorization preliminaries figure shows nonsingular matrix factored two matrices l u using gepp elimination steps controlled loop index k elements manipulated step k use row indexing j column indexing convention used rest paper step elimination process row interchange may needed maintain numerical stability result lu factorization process expressed l unit lower triangular matrix u upper triangular matrix p permutation matrix contains row interchange information solution linear system solved two triangular solvers triangular solvers much less time consuming gaussian elimination process caching behavior plays important role achieving good performance scientific computations better exploit memory hierarchy modern architectures supernode partitioning important technique exploit regularity sparse matrix computations utilize blas routines speed computation successfully applied cholesky factorization 26 30 31 difficulty nonsymmetric factorization supernode structure depends pivoting choices factorization thus cannot determined advance superlu performs symbolic factorization identifies supernodes fly also maximizes use blas level operations improve caching performance sparse lu however challenging parallelize superlu distributed memory machines using precise pivoting information elimination step certainly optimize data space usage reduce communication improve load balance benefits could offset high runtime control communication overhead strategy static data structure prediction 20 valuable avoiding dynamic symbolic factorization identifying maximum data dependence patterns minimizing dynamic control overhead use static strategy approach overestimation introduce extra fillins lead substantial amount unnecessary operations numerical factorization observe superlu 7 dgemv routine blas2 level dense matrix vector multiplication accounts 78 98 floating point operations excluding symbolic factorization part also fact blas3 routine dgemm matrixmatrix multiplication usually much faster blas1 blas2 routines 6 crayt3d matrix size theta 25 dgemm achieve 103 mflops dgemv reaches 85 mflops thus key idea approach could find way maximize use dgemm using static symbolic factorization even overestimated nonzeros extra numerical operations overall code performance could still competitive superlu mainly uses dgemv 3 storage prediction dense structure identification 31 storage prediction purpose symbolic factorization obtain structures l u factors since pivoting sequences known numerical factorization way allocate enough storage space fillins generated numerical factorization phase overestimate given sparse matrix zerofree diagonal simple solution use cholesky factor l c shown structure l c used upper bound structures l u factors regardless choice pivot row step 20 turns bound tight often substantially overestimates structures l u factors refer table 1 instead consider another method 20 basic idea statically consider possible pivoting choices step space allocated possible nonzeros would introduced pivoting sequence could occur numerical factorization summarize symbolic factorization method briefly follows nonzero structure row defined set column indices nonzeros fillins present given n theta n matrix since nonzero pattern row change factorization proceeds use r k denote structure row step k factorization k denote structure matrix step k k ij denotes element ij k notice structures row whole matrix cover structures l u factors addition process symbolic factorization assume exact numerical cancelation occurs thus ij structurally nonzerog also define set candidate pivot rows step k follows ik structurally nonzerog assume kk always nonzero nonsingular matrix zerofree diagonal always possible permute rows matrix permuted matrix zerofree diagonal 11 though symbolic factorization work matrix contains zero entries diagonal preferable makes overestimation generous symbolic factorization process iterate n steps step k row structure updated r essentially structure candidate pivot row step k replaced union structures candidate pivot rows except column indices less k way guaranteed resulting structure n able accommodate fillins introduced possible pivot sequence simple example figure 2 demonstrates whole process nonzero fillin figure 2 first 3 steps symbolic factorization sample 5 theta 5 sparse matrix structure remains unchanged steps 4 5 symbolic factorization applied ordering performed matrix reduce fillins ordering currently using multiple minimum degree ordering also permute rows matrix using transversal obtained duffs algorithm 11 make zerofree diagonal transversal often help reduce fillins 12 tested storage impact overestimation number nonsymmetric testing matrices various sources results listed table 1 fourth column table original number nonzeros fifth column measures symmetry structure original matrix bigger symmetry number nonsymmetric original matrix unit symmetry number indicates matrix symmetric matrices nonsymmetric numerical values compared number nonzeros obtained static approach number nonzeros obtained superlu well cholesky factor matrices results table 1 show overestimation usually leads less 50 extra nonzeros superlu scheme extra nonzeros imply additional computational cost example one either check symbolic nonzero actual nonzero numerical factorization directly perform arithmetic operations could unnecessary aggregate floating point operations maximize use blas3 subroutines sequential code performance still competitive even fifth column table 1 shows floating operations overestimating approach high 5 times results section 6 show actual ratios running times much less thus necessary beneficial identify dense structures sparse matrix static symbolic factorization noted cases static symbolic factorization leads excessive overestimation example memplus matrix 7 case static scheme produces 119 times many nonzeros superlu fact case ordering superlu applied based instead otherwise overestimation ratio 234 using superlu also another matrix wang3 7 static scheme produces 4 times many nonzeros superlu code still produce 1 gflops 128 nodes t3e paper focuses development high performance parallel code overestimation ratios high future work study ordering strategies minimize overestimation ratios factor entriesjaj superlu matrix 9 e40r0100 17281 553562 1000 1476 1732 2648 117 311 table 1 testing matrices statistics 32 2d lu supernode partitioning dense structure identification supernode partitioning commonly used technique improve caching performance sparse code 2 symmetric sparse matrix supernode defined group consecutive columns nested structure l factor matrix excellent performance achieved 26 30 31 using supernode partitioning cholesky factorization however definition directly applicable sparse lu nonsymmetric matrices good analysis defining unsymmetric supernodes l factor available 7 notice supernodes may need broken smaller ones fit cache expose parallelism superlu approach l supernode partitioning regular dense structures u factor could make possible use blas3 routines see figure 3a however approach dense columns subcolumns u factor identify static symbolic factorization see figure 3b u partitioning strategy explained follows l supernode partition obtained sparse matrix ie set column blocks possible different block sizes partition applied rows matrix break supernode panel submatrices offdiagonal submatrix l part either dense block contains dense blocks furthermore following theorem identifies dense structure patterns u factors key maximizing use blas3 subroutines algorithm b figure 3 illustration dense structures u factor superlu approach b dense structures u factor approach following theorem show 2d lu partitioning strategy successful rich set dense structures exploit following notations used rest paper ffl l u partitioning divides columns n column blocks rows n row blocks whole matrix divided n theta n submatrices submatrices u factor denote u ij 1 submatrices l factor denote l ij 1 denotes diagonal submatrix use ij denote submatrix necessary distinguish l u factors ffl define si starting column row number ith column row block convenience define sn ffl subcolumn subrow column row submatrix simplicity use global column row index denote subcolumn subrow submatrix example subcolumn k submatrix block u ij means subcolumn submatrix global column index k 1 similarly use ij indicate individual nonzero element based global indices compound structure l u submatrix subcolumn subrow ffl compound structure nonzero contains least one nonzero element fillin use ij 6 0 indicate block ij nonzero notice algorithm needs operate nonzero compound structures compound structure structurally dense elements nonzeros fillins following differentiate nonzero fillin entries considered nonzero elements theorem 1 given sparse matrix zerofree diagonal static symbolic factorization 2d lu supernode partitioning performed nonzero submatrix u factor contains structurally dense subcolumns proof recall p k set candidate pivot rows symbolic factorization step k given supernode spanning column k k definition fact step k static symbolic factorization affect nonzero patterns submatrix k1nk1n zerofree diagonal notice step k final structures row 2 p k updated symbolic factorization procedure r structure row k k interested nonzero patterns u part excluding part belonging l kk call partial structure ur thus ur seen kth step updating ur k knowing structure row k unchanged step k need prove ur k ks shown infer nonzero structures rows k k subcolumns u part either structurally dense zero since p k oe p k1 clear similarly show ur ks k theorem shows lu partitioning generate rich set structurally dense subcolumns even structurally dense submatrices u factor also incorporate result supernode amalgamation section 33 experiments indicate 64 numerical updates performed blas3 routine dgemm shows effectiveness lu partitioning method figure 4 demonstrates result supernode partitioning 7 theta 7 sample sparse matrix one see submatrices upper triangular part matrix contain structurally dense subcolumns based theorem show structural relationship two submatrices supernode column block useful implementing algorithm detect nonzero structures efficiently numerical updating corollary 1 given two nonzero submatrices u ij u k u ij structurally dense subcolumn k u 0 j also structurally dense nonzero figure 4 example lu supernode partitioning proof corollary illustrated figure 5 since l 0 nonzero must structurally dense subrow l 0 lead nonzero element subcolumn k u subcolumn k u ij structurally dense according theorem 1 subcolumn k u 0 j structurally dense u figure 5 illustration corollary 1 corollary 2 given two nonzero submatrices u ij u structurally dense u must structurally dense proof straightforward using corollary 1 33 supernode amalgamation tested sparse matrices average size supernode lu partitioning small 15 2 columns results fine grained tasks amalgamating small supernodes lead great performance improvement parallel sequential sparse codes improve caching performance reduce interprocessor communication overhead could many ways amalgamate supernodes 7 30 basic idea relax restriction columns supernode must exactly nonzero structure diagonal amalgamation usually guided supernode elimination tree parent could merged children merging introduce many extra zero entries supernode row column permutations needed parent consecutive children however column permutation introduced amalgamation method could undermine correctness static symbolic factorization used simpler approach require permutation approach amalgamates consecutive supernodes nonzero structures differ small number entries performed efficient manner time complexity 27 control maximum allowed differences amalgamation factor r experiments show r range gives best performance tested matrices leads improvement execution times sequential code reason getting bigger supernodes getting larger dense structures although may zero entries taking advantage blas3 kernels notice applying supernode amalgamation dense structures identified theorem 1 strictly dense call almostdense structures still use result theorem 1 minor revision summarized following corollary results presented section 6 obtained using amalgamation strategy corollary 3 given sparse matrix supernode amalgamation applied static symbolic factorization 2d lu supernode partitioning performed nonzero submatrix u factor contains almoststructurallydense subcolumns 4 program partitioning task dependence processor mapping dividing sparse matrix submatrices using lu supernode partitioning need partition lu code accordingly define coarse grained tasks manipulate partitioned dense data structures program partitioning column block partitioning follows supernode structures typically two types tasks one f actork factorize columns kth column block including finding pivoting sequence associated columns updatek j apply pivoting sequence derived f actork jth column block modify jth column block using kth column block instead performing row interchange right part matrix right pivoting search technique called delayedpivoting used 6 technique pivoting sequence held factorization kth column block completed pivoting sequence applied rest matrix ie interchange rows delayedpivoting important especially parallel algorithm equivalent aggregating multiple small messages larger one owner kth column block sends column block packed together pivoting information processors outline partitioned sparse lu factorization algorithm partial pivoting described figure 6 code f actork summarized figure 7 uses blas1 blas subroutines computational cost numerical factorization mainly dominated tasks function task updatek j presented figure 8 lines 05 using dense matrix multiplications 2 perform task f actork perform task updatek j figure partitioned sparse lu factorization partial pivoting 3 find pivoting row column row row column block k 5 scale column update rest columns column block figure 7 description task f actork use directed acyclic task graphs dags model irregular parallelism arising partitioned sparse lu program dags constructed statically numerical factorization previous work exploiting task parallelism sparse cholesky factorization used elimination trees eg 28 30 good way expose available parallelism pivoting required sparse lu elimination tree directly reflect available paral lelism dynamically created dags used modeling parallelism guiding runtime execution nonsymmetric multifrontal method 5 25 given task definitions figures 6 7 8 define structure sparse lu task graph following four properties necessary ffl n tasks f actork 1 k n ffl task updatek dense matrix total nn gamma 12 updating tasks ffl dependence edge f actork task updatek j 02 interchange rows according pivoting sequence lower triangular part l kk 04 submatrix u kj dense else dense subcolumn c u u kj nonzero submatrix ij submatrix u kj dense else dense subcolumn c u u kj b corresponding dense subcolumn ij figure 8 description task updatek j ffl dependence updatek k 0 f actork 0 exists task updatet k 0 add one property necessary simplifies implementation property essentially allow exploiting commutativity among update tasks however according experience cholesky factorization 16 performance loss due property substantial 6 average graph scheduling used ffl dependence updatek j updatek exists task updatet j figure 9a shows nonzero pattern partitioned matrix shown figure 4 figure 9b corresponding task dependence graph 1d data mapping 1d data mapping submatrices l u part column block reside processor column blocks mapped processors cyclic manner based scheduling techniques graph scheduling tasks assigned based ownercompute rule ie tasks modify column block assigned processor owns column block one disadvantage mapping serializes computation single f actork words single f actork updatek task performed b3 4 5125 figure 9 nonzero pattern example matrix figure 4 b dependence graph derived partitioning result convenience f used denote f actor u used denote update one processor mapping strategy advantage pivot searching subrow interchange done locally without communication another advantage parallelism modeled dependence structure effectively exploited using graph scheduling techniques data mapping literature 2d mapping shown scalable 1d sparse cholesky 30 31 however several difficulties apply 2d blockoriented mapping case sparse lu factorization even static structure predicted firstly pivoting operations row interchanges require frequent wellsynchronized interprocessor communication submatrices column block assigned different processors effective exploitation limited irregular parallelism 2d case requires highly efficient asynchronous execution mechanism delicate message buffer management secondly difficult utilize schedule possible irregular parallelism sparse lu lastly manage low space complexity another issue since exploiting irregular parallelism maximum degree may need buffer space 2d algorithm uses simple standard mapping function scheme p available processors viewed two dimensional grid c nonzero submatrix block ij could l block u block assigned processor p mod pr j mod pc 2d data mapping considered scalable 1d data mapping enables parallelization single f actork updatek j task p r processors discuss 2d parallelism exploited using asynchronous schedule execution 5 parallelism exploitation 51 scheduling runtime support 1d methods discuss 1d sparse lu tasks scheduled executed parallel time minimized george ng 21 used dynamic load balancing algorithm shared memory machine distributed memory machines dynamic adaptive load balancing works well problems coarse grained computations still open problem balance benefits dynamic scheduling runtime control overhead since task data migration cost expensive sparse problems mixed granularities use task dependence graphs guide scheduling investigated two types scheduling schemes ffl computeahead scheduling ca use blockcyclic mapping tasks computeahead execution strategy demonstrated figure 10 idea used speed parallel dense factorizations 23 executes numerical factorization layer layer based current submatrix index parallelism exploited concurrent updating order overlap computation communication f actork executed soon f actork updatek k pivoting sequence column block k next layer communicated early possible ffl graph scheduling order task execution within processor using graph scheduling algorithms 36 basic optimizations balancing processor loads overlapping computation communication hide communication latency done utilizing global dependence structures critical path information 01 column block 1 local 02 perform task f actor1 broadcast column block 1 pivoting sequence local receive column block k pivoting choices rows according pivoting sequence perform task f actork broadcast column block k pivoting sequence local column block k received receive column block k pivoting choices rows according pivoting sequence perform task updatek j figure 10 1d code using computeahead schedule graph scheduling shown effective exploiting irregular parallelism applications eg 15 16 graph scheduling outperform ca scheduling sparse lu constraint ordering f actor tasks demonstrate point using lu task graph figure 9 example gantt charts ca schedule schedule derived graph scheduling algorithm listed figure 11 assumed task computation weight 2 edge communication weight 1 easy see scheduling approach produces better result ca schedule look ca schedule carefully see reason ca look ahead one step execution task f actor3 placed update1 5 hand graph scheduling algorithm detects f actor3 executed update1 5 leads better overlap communication computation p1a figure 11 schedule derived graph scheduling algorithm b computeahead schedule convenience f used denote f actor u used denote update however implementation ca algorithm much easier since efficient execution sparse task graph schedule requires sophisticated runtime system support asynchronous communication protocols used rapid runtime system 16 parallelization sparse lu using graph scheduling key optimization use remote memory accessrma communicate data object two processors incur copyingbuffering data transfer since low communication overhead critical sparse code mixed granularities rma available modern multiprocessor architectures crayt3d 34 t3e 32 meiko cs2 15 since rma directly writes data remote address possible content remote address still used tasks execution remote processor could incorrect thus general computation permission write remote address needs obtained issuing remote write however rapid system handshaking process avoided carefully designed task communication protocol 16 property greatly reduces task synchronization cost shown 17 rapid sparse code deliver 70 speedup predicted scheduler crayt3d addition using rapid system greatly reduces amount implementation work parallelize sparse lu 01 let rno cno 2d coordinates processor perform scaleswapk perform update perform update 2dk j figure 12 spmd code 2d asynchronous code 52 asynchronous execution 2d code discussed previously 1d data mapping expose parallelism maximum extent another issue timeefficient schedule may spaceefficient specifically support concurrency among multiple updating stages rapid ca code multiple buffers needed keep pivoting column blocks different stages processor therefore given problem per processor space complexity 1d codes could high os 1 1 space complexity sequential algorithm sparse lu processor worst case may need space holding entire matrix rapid system 16 also needs extra memory space hold dependence structures based observation goal 2d code reduce memory space requirement exploiting reasonable amount parallelism solve large problem instances efficient way section present asynchronous 2d algorithm substantially overlap multistages updating memory requirement much smaller 1d methods figures 12 shows main control algorithm spmd coding style figure 13 shows spmd code f actork executed processors column k mod p c recall algorithm uses 2d blockcyclic data mapping coordinates processor owns mod also divide function update figure 8 two parts scaleswap scaling delayed row interchange submatrix kn k1n shown figure 14 update 2d submatrix updating shown figure 15 figures statements involve interprocessor communication marked seen computation flow 2d code still controlled pivoting tasks f actork order execution f actork sequential update 2d tasks computation comes execute parallel among processors asynchronous parallelism comes two levels first single stage tasks update 2dk find local maximum element column 05 send subrow within column block k containing local maximum processor p k mod pr k mod pc 06 processor owns l kk 07 collect local maxima find pivot row 08 broadcast subrow within column block k along processor column interchange subrow subrow necessary scale local entries column update local subcolumns column 12 multicast pivot sequence along processor row 13 processor owns l kk multicast l kk along processor row 14 multicast part nonzero blocks l k1n k owned processor along processor row figure 13 parallel execution f actork 2d asynchronous code executed concurrently processors addition different stages update 2d tasks update 2dk also overlapped idea computeahead scheduling also incorporated ie f actork 1 executed soon update finishes detailed explanation pivoting scaling swapping given line 5 figure 13 whole subrow communicated processor reports local maximum processor owns l kk block let current global column number pivoting conducted without synchronization processor locally swap subrow subrow contains selected pivoting element shortens waiting time conduct updating little communication volume however line 08 processor p k mod pr k mod pc must send original subrow owner subrow swapping selected subrow processors well updat ing f actor tasks synchronizations take place lines 05 07 08 processor reports local maximum p k mod pr k mod pc p k mod pr k mod pc broadcasts subrow containing global maximum along processor column task scaleswap main role scale u k k1n perform delayed row interchanges remaining submatrices k1n k1n examine degree parallelism exploited algorithm determining number updating stages overlapped using information also determine extra buffer space needed per processor execute algorithm correctly define stage overlapping degree receive pivot sequence p rno k mod pc 04 processor part row pivot row column 05 interchange nonzero parts row row owned processor 08 cno 6 k mod p c receive l kk p rno k mod pc scale nonzero blocks u k kn owned processor 10 multicast scaling results along processor column 11 cno 6 k mod p c receive l kn k p rno k mod pc 12 rno 6 k mod p r receive u k kn p k mod pr cno figure 14 task scaleswapk 2d asynchronous code 1 update 2dk using l ik u kj figure 15 update 2dk j 2d asynchronous code updating tasks exist tasks update 2dk update 2dk executed concurrentlyg update 2dk denotes set update 2dk tasks theorem 2 asynchronous 2d algorithm p processors p 1 reachable upper bound overlapping degree p c among processors reachable upper bound overlapping degree within processor column minp r gamma proof use following facts proving theorem ffl fact 1 f actork executed processors column number k mod p c processors column synchronized processor completes f actork processor still update shown figure 13 update tasks belonging processor 1 must completed processor ffl fact 2 scaleswapk executed processors row number k mod p r processor completes scaleswapk update tasks belonging processor 0 must completed processor part 1 first show update 2d tasks overlapped degree p c among processors trivial based fact 1 p c 1 imagine scenario processors column 0 finished task f actork still working update processors column 1 could go ahead execute update 2dk tasks processors column 1 finish update 2dk k1 task execute f actork1 finishing update 2dk tasks processors column 2 could execute update 2dk finally processors column p c gamma 1 could execute f actork moment processors column 0 may still working update thus overlapping degree p c show contradiction maximum overlapping degree p c assume moment exist two updating stages executed concurrently update 2dk update must completed without loss generality assuming processors column 0 execute f actork 0 according fact 1 update completed moment since block cyclic mapping used easy see processor column performed one f actorj tasks completed processors concurrent stage update 2dk k must satisfy contradiction part 2 first show overlapping degree minp r gamma achieved within processor column convenience illustration consider scenario delayed row interchanges scaleswap take place locally without communication within processor column therefore interprocessor synchronization going within processor column except f actor tasks assuming imagine moment processors column 0 completed f actors p 00 finished scaleswaps starts executing update 2ds mod processors column 1 execute update 1 p 10 start scaleswaps update 2ds following reasoning update 2ds finished processors column could complete previous update 2d tasks scaleswapsp r gamma 1 start update 2dsp r gamma 1 p 00 may still working update 2ds thus overlapping degree obviously reasoning stop processors column f actors 1 case p pc gamma10 start update 2ds pr gamma10 could still working update 2ds gamma compute ahead scheduling hence overlapping degree p c need show upper bound overlapping degree within processor column already shown proof part 1 overall overlapping degree less p c overlapping degree within processor column prove also less 1 use similar proof part 1 except using scaleswapk replace f actork using fact 2 instead fact 1 knowing degree overlapping important determining amount memory space needed accommodate communication buffers processor supporting asynchronous execu tion buffer space additional data space needed distribute original matrix four types communication needs buffering 1 pivoting along processor column lines 05 07 08 figure 13 includes communicating pivot positions multicasting pivot rows call buffer purpose pbuffer 2 multicasting along processor row line 12 13 14 figure 13 communicated data includes l kk local nonzero blocks l k1n k pivoting sequences call buffer purpose cbuffer 3 row interchange within processor column line 05 figure 14 call buffer ibuffer 4 multicasting along processor column line 10 figure 14 data includes local nonzero blocks row panel call buffer rbuffer assume p r based experimental results setting p r always leads better performance thus overlapping degree update 2d tasks within processor row p c overlapping degree within processor column p r gamma 1 need p c separate cbuffers overlapping among different columns rbuffers overlapping among different rows estimate size cbuffer rbuffer follows assuming sparsity ratio given matrix fillin maximum block size bsize cbuffer size maxfspace local nonzero blocks l knk similarly rbuffer size local nonzero blocks u ignore buffer size pbuffer ibuffer small size pbuffer bsize delta bsize size ibuffer delta np c thus total buffer space needed asynchronous execution c notice sequential space complexity practice set p c p 2 therefore buffer space complexity processor 25 small large matrix benchmark matrices tested buffer space less 100 k words given sparse matrix matrix data evenly distributed onto p processors total memory requirement per processor 1 p o1 considering n ae p n ae bsize leads us conclude 2d asynchronous algorithm space scalable 6 experimental studies experiments originally conducted crayt3d distributed memory machine san supercomputing center node t3d includes dec alpha ev421064 processor 64 mbytes memory size internal cache 8 kbytes per processor blas3 matrixmatrix multiplication routine dgemm achieve 103 mflops blas2 matrixvector multiplication routine dgemv reach 85 mflops numbers obtained assuming data cache using cache readahead optimization t3d matrix block size chosen 25 communication network t3d 3d torus cray provides shared memory access library called shmem achieve 126 mbytess bandwidth 27s communication overhead using shmem put primitive 34 used shmem put communications implementations also conducted experiments newly acquired crayt3e san diego supercomputing center t3e node clock rate 300 mhz 8kbytes internal cache 96kbytes second level cache 128 mbytes main memory peak bandwidth nodes reported 500 mbytess peak round trip communication latency 05 2 33 observed block size 25 dgemm achieves 388 mflops dgemv reaches 255 mflops used block size 25 experiments since block size large available parallelism reduced section mainly report results t3e occasions absolute performance concerned also list results t3d see approach scales underline architecture upgraded results obtained t3e unless explicitly stated calculating mflops achieved parallel algorithms include extra floating point operations introduced overestimation use following formula achieved operation count obtained superlu parallel time algorithm t3d t3e operation count matrix reported running superlu code sun workstation large memory since superlu code cannot run large matrices single t3d t3e node due memory constraint also compare sequential code superlu make sure code using static symbolic factorization slow prevent parallel version delivering high megaflops 61 impact static symbolic factorization sequential performance study introduction extra nonzero elements static factorization substantially affects time complexity numerical factorization compare performance sequential code superlu code performance table 2 1 matrices table 1 1 times table include symbolic preprocessing cost times superlu include symbolic factorization superlu fly implementation static symbolic preprocessing executed single t3d t3e node also introduce two matrices show well method works larger matrices denser matrices one two matrices b33 5600 truncated bcsstk33 current sequential implementation able handle entire matrix due memory constraint one dense1000 matrix approach superlu exec time ratio seconds mflops seconds mflops superlu sherman5 287 094 881 269 239 078 1057 324 121 122 sherman3 606 203 1018 304 427 168 1446 367 156 121 jpwh991 211 069 824 252 162 056 1066 310 134 123 goodwin 4372 170 153 394 dense1000 1048 404 636 1650 196 839 340 794 053 048 table 2 sequential performance versus superlu implies data available due insufficient memory though static symbolic factorization introduces lot extra computation shown table 1 performance 2d lu partitioning consistently competitive highly optimized superlu absolute single node performance achieved approach t3d t3e consistently range 5 gamma 10 highest dgemm performance matrices small medium sizes considering fact sparse codes usually suffer poor cache reuse performance reasonable addition amount computation testing matrices table 2 small ranging 107 million double precision floating operations since characteristic approach explore dense structures utilize blas3 kernels better performance expected larger denser matrices verified matrix b33 5600 even larger matrices vavasis3 cannot run one node shown later 2d code achieve 328 mflops per node 16 t3d processors notice megaflops performance per node sparse choleksy reported 24 16 t3d nodes around 40 mflops also good indication singlenode performance satisfactory present quantitative analysis explain competitive superlu assume speed blas2 kernel 2 secondf lop speed blas3 kernel 3 secondf lop total amount numerical updates c f lops superlu c 0 f lops apparently simplicity ignore computation scaling part within column contributes little total execution time hence efficient example preprocessing time 276 seconds single node t3e largest matrix tested vavasis3 symbolic time spent dynamic symbolic factorization superlu approach ae percentage numerical updates performed dgemm let j ratio symbolic factorization time numerical factorization time superlu simplify equation 1 following estimate j 082 tested matrices based results 7 17 also measured ae approximately ae 067 ratios number floating point operations performed superlu tested matrices available table 1 average value c 0 398 plug typical parameters equation 2 3 lop get 193 t3e lop get 168 estimations close ratios obtained table 2 discrepancy caused fact submatrix sizes supernodes nonuniform leads different caching performance submatrices uniform sizes expect prediction accurate instance dense case c 0 exactly 1 ratio calculated 048 t3d 042 t3e almost ratios listed table 2 analysis shows using blas3 much possible makes competitive superlu suppose machine dgemm outperforms dgemv substantially ratio computation performed dgemm high enough could faster superlu matrices last two entries table 2 already shown 62 parallel performance 1d codes subsection report set experiments conducted examine overall parallel performance 1d codes effectiveness scheduling supernode amalgamation performance list mflops numbers 1d rapid code obtained various number processors several testing matrices table 3 entry implies data available due memory constraint know megaflops dgemm t3e 37 times large t3d rapid code using upgraded machine speeded 3 times average satisfactory machine performance rapid code increases number processors increases speedups compared pure sequential code applicable reach 177 64 t3d nodes 241 64 t3e nodes 32 64 nodes performance gain small except matrices goodwin e40r0100 b33 5600 much larger problems rest reason small tested matrices enough amount computation parallelism saturate large number processors elimination process proceeds toward end belief better scalable performance obtained larger matrices currently available memory node t3d t3e limits problem size solved current version rapid code matrix p2 p4 p8 p16 p32 p64 sherman5 147 444 258 790 408 1331 538 1686 649 2107 684 2299 sherman3 164 514 300 907 457 1435 611 1928 643 1990 663 2127 jpwh991 133 414 232 756 405 1242 512 1739 580 1932 600 2173 orsreg1 174 534 306 906 512 1603 687 2156 753 2233 753 2316 goodwin 296 736 540 1357 879 2380 1364 3737 1820 5226 2181 6558 table 3 absolute performance mflops 1d rapid code effectiveness graph scheduling compare performance 1d ca code 1d rapid code figure 16 axis stands parallel time 2 4 processors certain cases computeahead code slightly faster rapid code number processors 4 rapid code runs faster processors involved bigger performance gap tends reason small number processors sufficient tasks making processors busy compute ahead schedule performs well rapid code suffers certain degree system overhead larger number processors schedule optimization becomes important since limited parallelism exploit effectiveness supernode amalgamation examined effective supernode amalgamation strategy using 1d rapid code let pt pt parallel time without supernode amalgamation respectively parallel time improvement ratios t3e several testing matrices listed table 4 similar results t3d 17 apparently supernode amalgamation brought significant improvement due increase supernode size implies increase task granularities important obtaining good parallel performance 22 comparison rapid code 1d ca code proc sherman5 sherman3 0101030507comparison rapid code 1d ca code proc jpwh991 x goodwin figure impact different scheduling strategies 1d code approach matrix p1 p2 p4 p8 p16 p32 sherman5 47 47 46 50 40 43 sherman3 20 25 23 28 22 14 jpwh991 48 48 48 50 47 40 table 4 parallel time improvement obtained supernode amalgamation 63 2d code performance mentioned 2d code exploits parallelism maintains lower space complexity much potential solve large problems show absolute performance obtained large matrices t3d table 5 since matrices cannot fit small number processors list results 16 processors maximum absolute performance achieved 64 nodes t3d 148 gflops translated 231 mflops per node nodes pernode performance 328 mflops table 6 shows performance numbers t3e 2d code achieved 6878 gflops 128 nodes 64 nodes megaflops t3e 31 34 times large t3d considering dgemm megaflops t3e 37 times large t3d code performance using upgraded machine good notice 1d codes cannot solve last six matrices table 6 matrices solvable using 1d rapid 2d codes compare average parallel time differences computing result figure 17 1d rapid code achieves matrix timesec mflops timesec mflops timesec mflops goodwin 60 1107 46 1452 36 1848 ex11 879 3050 534 5018 334 8026 raefsky4 1298 2429 760 4138 432 7192 table 5 performance results 2d code large matrices t3d matrix p8 p16 p32 p64 p128 time mflops time mflops time mflops time mflops time mflops goodwin 31 2152 19 3446 13 4963 11 5992 09 7152 ex11 507 5288 283 9462 162 16542 99 27031 64 41822 raefsky4 794 3912 432 7189 241 12907 139 22333 86 35929 inaccura 168 2446 99 4152 63 6558 39 10480 30 13914 af23560 223 2854 129 4929 812 7843 57 11232 42 15127 table performance results 2d asynchronous algorithm t3e times seconds better performance uses sophisticated graph scheduling technique guide mapping column blocks ordering tasks results better overlapping communication computation performance difference larger matrices listed left figure 17 compared right figure 17 partially explain reason analyzing load balance factors 1d rapid code 2d code figure 18 load balance factor defined work total p delta work max 31 count work updating part major part computation 2d code better load balance make impact lacking efficient task scheduling verified figure 17 figure 18 one see load balance factor 2d code close rapid code eg lnsp3937 performance rapid code much better 2d code load balance factor 2d code significantly better rapid code eg jpwh991 orserg1 performance differences smaller synchronous versus asynchronous 2d code using global barrier 2d code elimination step simplify implementation cannot overlap computations among different updating stages compared parallel time reductions asynchronous code synchronous code testing matrices table 7 shows asynchronous design improves performance significantly especially large number processors t3e demonstrates importance exploiting parallelism using asynchronous execution experiment 0204comparison rapid code 2d code proc sherman5 sherman3 comparison rapid code 2d code proc jpwh991 x goodwin figure 17 performance improvement 1d rapid 2d code balance comparison rapid vs 2d proc load balance factor x sherman3 balance comparison rapid vs 2d proc load balance factor x jpwh991 figure 18 comparison load balance factors 1d rapid code 2d code data t3d 14 7 concluding remarks paper present approach parallelizing sparse lu factorization partial pivoting distributed memory machines major contribution paper integrate several techniques together static symbolic factorization scheduling asynchronous parallelism 2d lu supernode partitioning techniques effectively identify dense structures maximize use blas3 subroutines algorithm design using ideas able exploit data regularity open irregular problem achieve 6878 gflops 128 t3e nodes highest performance known challenging problem previous record 2583 gflops shared memory machines 8 matrix p2 p4 p8 p16 p32 p64 sherman5 77 64 194 281 259 241 sherman3 102 124 203 227 260 250 jpwh991 87 100 238 333 357 286 orsreg1 61 77 175 280 205 282 goodwin 54 141 142 246 260 302 table 7 performance improvement 2d asynchronous code 2d synchronous code comparison results show 2d code better scalability 1d codes 2d mapping exposes parallelism carefully designed buffering scheme 1d rapid code still outperforms 2d code sufficient memory since scheduling execution techniques 2d code simple competitive graph scheduling recently conducted research developing space efficient scheduling algorithms retaining good time efficiency 18 still open problem develop advanced scheduling techniques better exploit parallelism 2d sparse lu factorization partial pivoting issues related work need studied example alternative parallel sparse lu based schur complements 13 static estimation parallelism exploitation sparse qr 29 35 noted static symbolic factorization could fail practical input matrix nearly dense row lead almost complete fillin whole matrix might possible use different matrix reordering avoid fortunately case matrices tested therefore approach applicable wide range problems using simple ordering strategy interesting future study ordering strategies minimize overestimation ratios consistently deliver good performance various classes sparse matrices acknowledgment work supported nsf ria ccr9409695 nsf cda9529418 uc micro grant matching sun nsf career ccr9702640 arpa dabt6393c0064 rutgers hpcd project would like thank kai shen efficient implementation static symbolic factorization algorithm xiaoye li jim demmel helpful discussions providing us testing matrices superlu code cleve ashcraft tim davis apostolos gerasoulis esmond ng ed rothberg rob schreiber horst simon chunguang sun kathy yelick anonymous referees valuable comments r influence relaxed supernode partitions multifrontal method progress sparse matrix methods large sparse linear systems vector supercomputers users guide unsymmetricpattern multifrontal package umfpack personal communication unsymmetricpattern multifrontal method sparse lu factor ization numerical linear algebra parallel processors supernodal approach sparse partial pivoting asynchronous parallel supernodal algorithm sparse gaussian elimination extended set basic linear algebra subroutines multifrontal solution indefinite sparse symmetric systems equations algorithms obtaining maximum transversal personal communication structural representations schur complements sparse matrices comparison 1d 2d data mapping sparse lu factorization partial pivoting efficient runtime support irregular task computations mixed granularities sparse lu factorization partial pivoting distributed memory machines space time efficient execution parallel irregular computations parallel solution nonsymmetric sparse linear systems using h symbolic factorization sparse gaussian elimination partial pivoting parallel sparse gaussian elimination partial pivoting granularity clustering directed acyclic task graphs scientific computing introduction parallel computing compilers highly scalable parallel algorithms sparse matrix factorization parallel unsymmetricpattern multifrontal method parallel algorithms sparse linear systems parallel sparse gaussian elimination partial pivoting 2d data mapping computational models task scheduling parallel sparse cholesky factorization distributed sparse gaussian elimination orthogonal factorization exploiting memory hierarchy sequential parallel sparse cholesky factorization improved load distribution parallel sparse cholesky fac torization synchronization communication t3e multiprocess cray t3e network adaptive routing high performance 3d torus decoupling synchronization data transfer message passing systems parallel computers parallel sparse orthogonal factorization distributedmemory multiprocessors pyrros static task scheduling code generation messagepassing multiprocessors tr ctr kai shen xiangmin jiao tao yang elimination forest guided 2d sparse lu factorization proceedings tenth annual acm symposium parallel algorithms architectures p515 june 28july 02 1998 puerto vallarta mexico xiaoye li james w demmel making sparse gaussian elimination scalable static pivoting proceedings 1998 acmieee conference supercomputing cdrom p117 november 0713 1998 san jose ca patrick r amestoy iain duff jeanyves lexcellent xiaoye li analysis comparison two general sparse solvers distributed memory computers acm transactions mathematical software toms v27 n4 p388421 december 2001 xiaoye li james w demmel superludist scalable distributedmemory sparse direct solver unsymmetric linear systems acm transactions mathematical software toms v29 n2 p110140 june