scalable feature mining sequential data classification algorithms difficult apply sequential examples text dna sequences vast number features potentially useful describing example past work feature selection focused searching space subsets available features intractable large feature sets authors adapt data mining techniques act preprocessor select features standard classification algorithms naive bayes winnow apply algorithm number data sets experimentally show features produced algorithm improve classification accuracy 20 b introduction many real world datasets contain irrelevant redundant attributes may data collected without data mining mind attribute dependences known priori data collection well known many data mining methods like classification clustering etc degrade prediction accuracy trained datasets containing redundant irrelevant attributes features selecting right feature set improve accuracy also reduce running time predictive algorithms lead simpler understandable models good feature selection thus one fundamental data preprocessing steps data mining research feature selection todate focused nonsequential domains problem may defined selecting optimal feature subset size full ddimensional feature space ideally selected subset maximize optimization criterion classification accuracy faithfully capture original data distribution subset search space exponentially large number features contrast traditional nonsequential data focus sequence data example represented sequence events event might described set predicates ie dealing categorical sequential domains examples sequence data include text dna sequences web usage data multiplayer games plan execution traces sequential domains features ordered sets partial event descriptions example sequential feature describes chess game black moves knight white moves bishop square d6 feature holds chess games others thus might used classify chess games example ones played experts vs ones played novices selecting right features sequential temporal domains even challenging nonsequence data original feature set undefined potentially infinite number sequences arbitrary length categorical attributes dimensions even restrict maximum sequence length k potentially od k subsequences dimensions complexity consider maximum subsequence length opposed 2 nonsequential case goal feature selection sequential domains select best subset sequential features k possible sequential features ie subsequences composed attributes describing individual events motivated use data mining techniques set possible features exponentially large alternative conception problem constructing new features primitives describing events new features augment dimensionality original space effectively pulling apart examples class making easily distinguishable classification algorithms course process constructing features primitives equivalent process selecting features space combinations primitives input system set labeled training sequences output function maps new sequence label words interested selecting constructing features sequence classification order generate function algorithm first uses sequence mining portion training data discovering frequent distinctive sequences uses sequences features feed classification algorithm winnow naive bayes generate classifier remainder data past work rules produced data mining algorithms used construct classifiers primarily ordering rules decision lists eg 1 2 merging general rules occur training data eg 3 paper convert patterns discovered mining algorithm set boolean features feed standard classification algorithms classification algorithms turn assign weights features allows evidence different features combined order classify new example two main contributions paper first combine two powerful data mining paradigms sequence mining efficiently search patterns correlated target classes classification algorithms learn weigh evidence different features classify new examples second present scalable feature mining algorithm handle large datasets thousands items millions records additionally present criteria selecting features present pruning rules allow efficient mining features present featuremine scalable algorithm capable handling large diskresident datasets mines good sequential features integrates pruning constraints algorithm instead postprocessing enables efficiently search large pattern spaces 11 example poker lets first preview main ideas paper simple illustrative example suppose observe three people playing poker betting sequences outcomes example p1 bets 3 p2 calls p3 raises 2 p1 raises 1 p2 folds p3 calls p1 wins example p2 passes p3 bets 1 p1 folds p2 raises 2 p3 raises 2 p2 calls p3 wins objective learn function predicts likely win given betting sequence task resembles standard classification given labeled training examples must produce function classifies new unlabeled examples many classifiers require however examples represented vectors featurevalue pairs paper addresses problem selecting features represent betting sequences first consider obvious poor feature set let n length longest betting sequence represent betting sequences 3n features generating distinct feature every 0 n person made ith bet type ith bet amount ith bet section 3 show experimentally feature set leads poor classification one problem features individual feature express particular complete bidding sequence took place interesting subsequence occurred feature feature dollars first feature would important p 1 tends win whenever raises twice classifier could construct boolean expression features described capture notion p 1 raises twice expression would disjuncts since need disjunct p 1 raises ith bet jth believe important consider partial specifications difficult know advance whether p 1 raises twice p 1 raises 2 raises 3 useful feature alternative use much larger feature set 3 players 4 bids 5 different amounts 4 5 specifications bet someone bets 3 chain partial specifications together relation p 1 raises someone bets 3 number features length k 120 k problem feature set large sets 10000 features considered large classification algorithms furthermore irrelevant redundant features reduce classification accuracy 4 adopt middle ground two extremes use data mining techniques search second huge feature set select subset show criteria similar used general knowledge discovery task works well deciding features useful 2 data mining features formulate present algorithm feature mining formulation involves specifying language features express sequences partial descriptions events gaps p 1 raises later bid folds present criteria selecting subset features used classification entire set expressed language finally describe featuremine algorithm efficiently mine features selected criteria begin adopting following terminology closely resembles used sequence mining eg 5 let f set distinct features finite set possible values let contain unique element every possible featurevalue pair sequence ordered list subsets example fa b cg example sequence would ab bc sequence denoted sequence element subset length sequence width maximum size 1 n say subsequence denoted exists integers j example ab c subsequence ab bc let h set class labels example pair h ci sequence c 2 h label example unique identifier eid timestamp occurred example h ci said contain sequence input database consists set examples means data look multiple sequences composed sets items frequency sequence denoted fr fraction examples contain let sequence c class label confidence rule c denoted conf c conditional probability c label example given contains sequence conf c c subset examples class label c sequence said frequent frequency userspecified min freq threshold rule said strong confidence userspecified min conf threshold goal mine frequent strong patterns figure 1 shows database examples 7 examples 4 belonging class c 1 3 belonging class c 2 general two classes looking different min freq sequences class example c frequent class c 2 frequent class c 1 rule c c 2 confidence rule confidence sequence classifier function sequences class labels h classifier evaluated using standard ba aa 100 75 75 75 75 100 100 100 eid c time c ac 67 67 100 class 6 c2 examples new boolean features class eid figure 1 examples b new database boolean features metrics accuracy coverage finally describe frequent sequences 1 n used features classification recall input standard classifiers example represented vector featurevalue pairs represent example sequence vector featurevalue pairs treating sequence boolean feature true iff example suppose features f sequence ab bd bc would represented hf 1 truei hf falsei sequence abcd would represented truei note features skip steps feature bc holds ab bd bc figure 1b shows new dataset created frequent sequences example database figure 1 use frequent sequences features example general use good subset frequent sequences features described 21 selection criteria mining specify selection criteria selecting features use classification objective find sequences representing examples sequences yield highly accurate sequence classifier however want search space subsets features 4 instead want evaluate new sequential feature isolation pairwise comparison candidate features certainly criteria selecting features might depend domain classifier used believe however following domainandclassifier independent heuristics useful selecting sequences serve features features frequent features distinctive least one class feature sets contain redundant features intuition behind first heuristic simply rare features definition rarely useful classifying examples problem formulation heuristic translates requirement features minimum frequency training set note since use different min freq class patterns rare entire database still frequent specific class ignore patterns rare class intuition second heuristic features equally likely classes help determine class example belongs course conjunction multiple nondistinctive features distinctive case algorithm prefers use distinctive conjunction feature rather non distinctive conjuncts encode heuristic requiring selected feature significantly correlated least one class frequent motivation third heuristic two features closely correlated either useful classification show reduce number features time needed mine features pruning redundant rules addition wanting prune features provide information also want prune feature another feature available provides strictly information let mfd set examples contain feature f say feature f 1 subsumes feature respect predicting class c data set iff mf intuitively f 1 subsumes f 2 class c f 1 superior f 2 predicting c f 1 covers every example c training data f 2 covers f 1 covers subset nonc examples f 2 covers note feature f 2 better predictor class c f 1 even f 1 covers examples c f 2 example every example f 2 covers c half examples f 1 covers c case neither feature subsumes third heuristic leads two pruning rules first pruning rule extend ie specialize feature 100 accuracy let f 1 feature contained examples one class specializations f 1 may pass frequency confidence tests definition feature mining subsumed f 1 following lemma follows definition subsume justifies pruning rule respect class c next pruning rule concerns correlations individual items recall examples represented sequence sets say examples b occurs every set every sequence occurs following lemma states b feature containing set b subsumed one generalizations lemma 2 let subsumed precompute set b relations immediately prune feature search contains set b section 3 discuss b relations arise show crucial success approach problems define feature mining task inputs featuremine algorithm set examples parameters min freq maxw max l output nonredundant set frequent distinctive features width maxw length max l formally feature mining given examples parameters min freq maxw max l return feature set f every feature f every class c significantly greater jd c jjdj f contains f contains feature subsumes f respect class c j data set use chisquared test determine significance 22 efficient mining features present featuremine algorithm leverages existing data mining techniques efficiently mine features set training examples sequence mining algorithms designed discover highly frequent confident patterns sequential data sets well suited task featuremine based recently proposed spade algorithm 5 fast discovery sequential patterns spade scalable diskbased algorithm handle millions example sequences thousands items consequently featuremine shares properties well construct featuremine adapted spade algorithm search databases labeled examples featuremine mines patterns predictive classes database simultaneously opposed previous approaches first mine millions patterns apply pruning postprocessing step featuremine integrates pruning techniques mining algorithm enables search large space previous methods would fail featuremine uses observation subsequence relation defines partial order sequences say general specific relation monotone specialization relation respect frequency fr ie frequent sequence subsequences also frequent algorithm systematically searches sequence lattice spanned subsequence relation general specific sequences depthfirst manner figure 2 shows frequent sequences example database eid suffixjoins idlists time time time time aa ba bb ac intersect ab bb intersect b figure 2 frequent sequence lattice frequency computation frequency computation featuremine uses vertical database layout associate item x sequence lattice idlist denoted lx list example ids eid event time time pairs containing item figure 2 shows idlists items b given sequence idlists determine support ksequence simply intersecting idlists two k 1 length subsequences check cardinality resulting idlist tells us whether new sequence frequent two kinds intersections temporal equality example figure 2 shows idlist b obtained performing temporal intersection idlists b ie lb done looking within eid occurs b listing occurrences hand idlist obtained equality intersection ie lab b check see two subsequences occur within eid time additional details found 5 also maintain class index table indicating classes example using table able determine frequency sequence classes time example occurs eids f1 2 3 4 5 6g however eids f1 2 3 4g label c 1 f5 6g label c 2 thus frequency 4 c 1 2 c 2 class frequencies pattern shown frequency table use limited amount mainmemory featuremine breaks sequence search space small independent manageable chunks processed memory accomplished via suffixbased partition ing say two k length sequences equivalence class partition share common k 1 length suffix partitions fa b cg based length 1 suffixes called parent partitions parent partition independent sense complete information generating frequent sequences share suffix example class x elements possible frequent sequences next step obvious item q lead frequent sequence suffix x unless qx q x also x parent partition p enumeratefeaturesp elements elements rulepruner maxw accuracyr 100 return true return false figure 3 featuremine algorithm feature enumeration featuremine processes parent partition depthfirst manner shown pseudocode figure 3 input procedure partition along idlist elements frequent sequences generated intersecting idlists distinct pairs sequences partition checking cardinality resulting idlist min supc sequences found frequent class c current level form partitions next level process repeated frequent sequences enumerated integrated constraints featuremine integrates pruning constraints mining algorithm instead applying pruning postprocessing step shall show allows featuremine search large spaces efficiently would infeasible otherwise ruleprune procedure eliminates features based two pruning rules also based length width constraints first pruning rule tested time extend sequence new item exists efficient onetime method applying b rule idea first compute frequency 2 length sequences b remove ab suffix partition b guarantees point future ab appear together set sequence 3 empirical evaluation describe experiments test whether features produced system improve performance winnow 6 naive bayes 7 classification algorithms winnow multiplicative weightupdating algorithm used variant winnow maintains weight w ij feature f class c j given example activation level class c j x 1 feature f true example 0 otherwise given example winnow outputs class highest activation level training winnow iterates training examples winnows classification training example agree label winnow updates weights feature f true example multiplies weights correct class constant 1 multiples weights incorrect classes constant 1 experiments learning often sensitive values used chose values based common literature small amount experimentation winnow actually used prune irrelevant features example run winnow large feature sets say 10000 throw away features assigned weight 0 near 0 however practical sequence classification since space potential features exponential naivebayes classifier feature f class c j naive bayes computes pf jc j fraction training examples class c j contain f given new example features f 1 f n true naive bayes returns class maximizes p though naive bayes algorithm appears make unjustified assumption features independent shown perform surprisingly well often well better c45 8 describe domains tested approach discuss results experiments 301 random parity problems first describe nonsequential problem standard classification algorithms perform poorly problem every feature true exactly half examples class way solve problem discover combinations features correlated different classes intuitively construct problem generating n randomlyweighted meta features composed set actual observable features parity observable features determines whether corresponding meta feature true false class label instance function sum weights meta features true thus order solve problems featuremine must determine observable features correspond meta feature important discover meta features higher weights ones lower weights additionally increase difficulty problem add irrelevant features bearing class instance formally problem consists n parity problems size l distracting irrelevant features every boolean feature f ij additionally 0 k l irrelevant boolean feature k generate instance randomly assign relevant irrelevant boolean true false 5050 probability example instance nml features 2 nml distinct instances possible instances equally likely also choose n weights w 1 wn uniform distribution 0 1 used assign instance one two class labels follows instance credited weight w iff ith set features even parity score instance sum weights w number true features f i1 f im even instances score greater half sum weights instance assigned class label otherwise assigned note 1 feature indicative class label parity problems hard classifiers job featuremine essentially figure features grouped together example features produced featuremine results shown table 1 include f 11 true f 12 true f 41 true f 42 false used min freq 02 05 302 forest fire plans featuremine algorithm originally motivated task plan monitoring stochastic domains probabilistic planners construct plans high probability achieving goal task monitoring watch plan executed predict advance whether plan likely succeed fail facilitate replanning order build monitor given plan goal first simulate plan repeatedly generate execution traces label execution trace success failure depending whether goal holds final state simulation use execution traces input featuremine plan monitoring monitoring probabilistic process simulate attractive area machine learning essentially unlimited supply training data although cannot consider possible execution paths number paths exponential length plan process generate arbitrary numbers new examples monte carlo simulation problem overfitting reduced test hypotheses fresh data sets example domain constructed simple forestfire domain based loosely phoenix fire simulator 9 execution traces available email contact leshmerlcom use grid representation terrain grid cell contain vegetation water base example terrain shown figure 4 beginning bbbb dd dd dd bbdbbd dbbd dbbd dbd bbbbwwwwww dbbd dbbd dd wwwwwwwwwwww wwwwww wwwwww wwwwww wwwwwwwwwwww wwwwww wwwwww wwwwww wwwwww www www www time figure 4 ascii representation several time slices example simulation fire world domaina indicates fire b indicates base b indicates bulldozer indicates place bulldozer dug fire line w indicates water unburnable terrain simulation fire started random location iteration simulation fire spreads stochastically probability cell igniting time calculated based cells vegetation wind direction many cells neighbors burning time 1 additionally bulldozers used contain fire example terrain handdesigned plan bulldozers dig fire line stop fire bulldozers speed varies simulation simulation example simulation looks like time0 ignite x3 y7 time0 moveto bd1 x3 y4 time0 moveto bd2 x7 y4 time0 digat bd2 x7 y4 time6 ignite x4 y8 time6 ignite x3 y8 time8 digat bd2 x7 y3 time8 ignite x3 y9 time8 digat bd1 x2 y3 time8 ignite x5 y8 time32 ignite x6 y1 time32 ignite x6 y0 tag plan success none locations bases burned final state failure otherwise train plan monitor predict time k whether bases ultimately burned include events occur time k training examples example features produced featuremine domain first sequence holds bulldozer bd1 moves second column time 6 second holds fire ignites anywhere second column bulldozer moves third row time 8 many correlations used second pruning rule described section 22 arise data sets example arises one test plans bulldozer never moves eighth column fire data 38 boolean features describe event thus number composite features search 38 l experiments reported used min experiment winnow winnowtf winnowfm bayes bayestf bayesfm parity parity parity spelling vs 70 na 94 75 na 78 spelling vs 86 na 94 66 na 90 spelling vs 83 na 92 79 na 81 spelling youre vs 77 na 86 77 na 86 table 1 classification results average classification accuracy using different feature sets represent examples legend tf uses features obtained times features approach fm uses features produced featurem ine highest accuracy obtained features produced featuremine algorithm standard deviations shown parentheses following average except spelling problems one test training set used 303 contextsensitive spelling correction also tested algorithm task correcting spelling errors result valid words substituting 10 test chose two commonly confused words searched sentences 1 millionword brown corpus 11 containing either word removed target word represented word word partofspeech tag brown corpus position relative target word example sentence politics translated wordand tagcc pos2 wordthen tagrb pos1 wordis tagbez pos1 wordpolitics tagnn pos2 example features produced featuremine include pos3 wordthe indicating word occurs least 3 words target word indicating noun occurs within three words target word features reasons obvious us significantly correlated either training set vs dataset 3802 training examples 944 test examples 4692 featurevalue pairs vs dataset 2917 training examples 755 test examples 5663 featurevalue pairs vs dataset 2019 training examples 494 test examples 4331 featurevalue pairs finally youre vs dataset 647 training examples 173 test examples 1646 featurevalue pairs n number featurevalue pairs search n maxw experiments reported used min 2 31 results test parity fire domains generated 7000 random training examples mined features 1000 examples pruned features pass chisquared significance test correlation class feature frequent 2000 examples trained classifier remaining 5000 examples tested 1000 additional examples results tables 1 2 averages 2550 tests spelling correction experiment evaluated selected features features fire world time 10 64766 553 spelling vs 782264 318 table 2 mining results number features considered returned featuremine experiment cpu seconds cpu seconds cpu seconds features features examined features pruning examined examined pruning pruning pruning b pruning pruning random 320 337 337 1547122 1547122 1547122 fire world 58 hours 560 559 25336097 511215 511215 spelling 490 407 410 1126114 999327 971085 table 3 impact pruning rules running time nodes visited featuremine without b pruning results taken one data set example used examples brown corpus roughly 10004000 examples per word set split 8020 sentence training test sets mined features 500 sentences trained classifier entire training set table 1 shows features produced featuremine improved classification performance compared using feature set produced featuremine using primitive features ie features length 1 fire domain also evaluated feature set containing feature primitive feature time step feature set size 3n described section 11 winnow naive bayes performed much better features produced featuremine parity experiments mined features dramatically improved performance classifiers experiments mined features improved accuracy classifiers significant amount often 20 table 2 shows number features evaluated number returned several problems largest random parity problem featuremine evaluated 7 million features selected 200 fact 100 million possible features 50 booleans features giving rise 100 featurevalue pairs searched depth 4 since rejected implicitly pruning rules table 3 shows impact b pruning rule described section 22 mining time results one data set domain slightly higher values max l maxw experiments pruning rule improve mining time cases made tremendous difference fire world prob lems event descriptors often appear together without b pruning fire world problems essentially unsolvable featuremine finds 20 million frequent sequences 4 related work great deal work done featuresubset selection motivated observation classifiers perform worse feature set f f 0 f eg 4 algorithms explore exponentially large space subsets given feature set contrast explore exponentially large sets potential features evaluate feature independently featuresubset approach seems infeasible problems consider contain hundreds thousands millions potential features 10 applied winnowbased algorithm contextsensitive spelling correction use sets 10000 40000 features either use features prune based classification accuracy individual features obtain higher accuracy approach however involves ensemble winnows combined majority weighting took care choosing good parameters specific task goal demonstrate features produced featuremine improve classification performance data mining algorithms often applied task classification 2 build decision lists patterns found association mining nonsequential version sequence mining additionally previous work explored new methods combining association rules build classifiers thrust work leverage augment standard classification algorithms pruning rules resemble ones used 1 also employs data mining techniques construct decision lists previous work using data mining classification focused combining highly accurate rules together contrast classification algorithms weigh evidence many features low accuracy order classify new examples work close spirit 12 also constructs set sequential boolean features use classification algorithms employ heuristic search algorithm called fgen incrementally generalizes features cover training examples based classification performance holdout set training data whereas perform exhaustive search depth accept features meet selection crite ria additionally use different feature language tested approaches different classifiers conclusions shown data mining techniques used efficiently select construct features sequential domains dna text web usage data plan execution traces domains challenging exponential number potential subsequence features formed primitives describing item sequence data number features large practically handled todays classification algorithms furthermore feature set contains many irrelevant redundant features reduce classification accuracy approach search set possible features mining ones frequent predictive redundant adapting scalable diskbased data mining algorithms able perform search efficiently however one three domains studied search practical due pruning rules incorporated search algorithms experiments several domains show features produced applying selection criteria significantly improve classification accuracy particular shown construct problems classifiers perform better random guessing using original features perform near perfect accuracy using features produced featuremine furthermore shown features produced featuremine improve performance much 20 simulated fireplanning domain spelling correction data generally work shows apply classification algorithms domains obvious small set features describing examples large space combinations primitive features probably contains useful features future work could involve applying ideas classification example images audio signals r learning decision lists using homogeneous rules integrating classification association rule mining mining audit data build intrusion detection models greedy attribute selection efficient enumeration frequent sequences learning quickly irrelevant attributes abound new linearthreshold algorithm pattern classification scene analysis beyond independence conditions optimality simple bayesian clas sifier predicting explaining success task duration phoenix planner applying winnow contextsensitive spelling correction computational analysis presentday american english feature generation sequence categorization tr ctr xiaonan ji james bailey guozhu dong mining minimal distinguishing subsequence patterns gap constraints knowledge information systems v11 n3 p259286 april 2007 florence duchne catherine garbay vincent rialle learning recurrent behaviors heterogeneous multivariate timeseries arificial intelligence medicine v39 n1 p2547 january 2007 sanyih hwang chihping wei wanshiou yang discovery temporal patterns process instances computers industry v53 n3 p345364 april 2004 sbastien ferr ross king dichotomic search algorithm mining learning domainspecific logics fundamenta informaticae v66 n12 p132 january 2005 mohammed j zaki charu c aggarwal xrules effective algorithm structural classification xml data machine learning v62 n12 p137170 february 2006 chihming chen incremental personalized web page mining utilizing selforganizing hcmac neural network web intelligence agent system v2 n1 p2138 january 2004 chihming chen incremental personalized web page mining utilizing selforganizing hcmac neural network web intelligence agent system v2 n1 p2138 august 2004