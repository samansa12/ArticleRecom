recognizing action units facial expression analysis abstractmost automatic expression analysis systems attempt recognize small set prototypic expressions happiness anger surprise fear prototypic expressions however occur rather infrequently human emotions intentions often communicated changes one discrete facial features paper develop automatic face analysis afa system analyze facial expressions based permanent facial features brows eyes mouth transient facial features deepening facial furrows nearly frontalview face image sequence afa system recognizes finegrained changes facial expression action units aus facial action coding system facs instead prototypic expressions multistate face facial component models proposed tracking modeling various facial features including lips eyes brows cheeks furrows tracking detailed parametric descriptions facial features extracted parameters inputs group action units neutral expression six upper face aus 10 lower face aus recognized whether occur alone combinations system achieved average recognition rates 964 percent 954 percent neutral expressions excluded upper face aus 967 percent 956 percent neutral expressions excluded lower face aus generalizability system tested using independent image databases collected facscoded groundtruth different research teams b introduction recently facial expression analysis attracted attention computer vision literature 3 5 6 9 11 13 17 19 automatic expression analysis systems attempt recognize small set prototypic expressions ie joy surprise anger sadness fear disgust 11 17 everyday life however prototypic expressions occur relatively infrequently instead emotion communicated changes one two discrete facial features tightening lips anger obliquely lowering lip corners sadness 2 change isolated features especially area brows eyelids typical paralinguistic displays instance raising brows signals greeting capture subtlety human emotion paralinguistic communication automated recognition finegrained changes facial expression needed ekman friesen 4 developed facial action coding system facs describing facial expressions facs humanobserverbased system designed describe subtle changes facial features facs consists 44 action units including head eye positions aus anatomically related contraction specific facial muscles occur either singly combinations au combinations may additive case combination change appearance constituents nonadditive case appearance constituents changes analogous coarticulation effects speech action units vary intensity 5point ordinal scale used measure degree muscle contraction although number atomic action units small 7000 combinations action units observed 12 facs provides necessary detail describe facial expression automatic recognition action units difficult problem aus quantitative definitions noted appear complex combinations several researchers tried recognize aus 1 3 9 system lien et al 9 used denseflow feature point tracking edge extraction recognize 6 upper face aus au combinations au12 au14 au4 au5 au6 au7 9 lower face aus au combinations au12 au25 au26 au27 au1225 au2025 au1517 au172324 au917 bartlett et al 1 recognized 6 individual upper face aus au1 au2 au4 au5 au6 none occurred combinations performance featurebased classifier novel 57 new images faces used training rate 853 combining holistic spatial analysis optical flow local features hybrid system bartlett et al increased accuracy 909 correct donato et al 3 compared several techniques recognizing action units including optical flow principal component analysis independent component analysis local feature analysis gabor wavelet representation best performances obtained gabor wavelet representation independent component analysis achieved 95 average recognition rate 6 upper face aus 6 lower face aus report developed featurebased au recognition system system explicitly analyzes appearance changes localized facial features since au associated specific set facial muscles believe accurate geometrical modeling facial features lead better recognition results furthermore knowledge exact facial feature positions could benefit areabased 17 holistic analysis 1 optical flow based 9 classifiers figure 1 depicts overview analysis system first head orientation face position detected subtle changes facial components measured motivated facs action units changes represented collection midlevel feature parameters finally action units classified feeding parameters neural network appearance facial features dependent upon head orientation develop multistate modelbased system tracking facial features different head orientations corresponding variation appearance face components defined separate states state corresponding description one feature extraction methods developed separately represent facial features two parameter groups upper face lower face facial actions upper lower face relatively independent 4 fifteen parameters used describe eye shape motion state brow cheek motion upper face furrows upper face nine parameters used describe lip shape lip motion lip state lower face furrows lower face facial features correctly extracted suitably represented employ neural network recognize upper face aus neutral au1 au2 au4 au5 au6 au7 lower face aus neutral au9 au 10 au 12 au 15 au 17 au 20 au 25 au 26 au 27 au2324 respectively seven basic upper face aus eleven basic lower face aus identified regardless whether occurred singly combinations upper face au recognition compared bartletts 1 results using database system achieves recognition accuracy average recognition rate 95 fewer parameters difficult case aus may occur either individually additive nonadditive combinations lower face au recognition previous attempt similar task 9 recognized 6 lower face aus combinationsau 12 au1225 au2025 au917 au172324 au1517 88 average recognition rate separate hidden markov models action unit action unit combination compared previous results system achieves recognition accuracy average recognition rate 9671 difficult cases aus occur either individually additive nonadditive combinations handled also figure 1 feature based action unit recognition system 2 multistate models face facial components 21 multistate face model head orientation significant factor affects appearance face based head orien tation seven head states defined figure 2 develop robust facial expression recognition system head state considered different head states facial components lips appear differently requiring specific facial component models example facial component models front face include f rontlips f ronteyes left right f rontcheeksleft right nasolabialfurrows nosewrinkles right face includes component models sidelips righteye rightbrow rightcheek current system assume face images nearly front view possible inplane head rotations 22 multistate face component models different face component models must used different states example lip model front face doesnt work profile face give detailed facial component models nearly frontview face permanent components lips eyes brows cheeks transient components furrows considered based different appearances different components different geometric models used model components location shape appearance head state b different facial components used head state figure 2 multiple state face model head state left leftfront front rightfront right b different facial component models used different head states component employs multistate model corresponding different component states example threestate lip model defined describe lip states open closed tightly closed twostate eye model used model open closed eye one state brow cheek present absent use model states transient facial features multistate component models different components described table 1 table 1 multistate facial component models front face component state descriptionfeature opened closed xc yc tightly closed lip corner1 lip corner2 eye open xc yc h2 closed corner2 corner1 brow present cheek present furrow present eyes inner corner line furrows nasolabial absent 3 facial feature extraction contraction facial muscles produces changes direction magnitude motion skin surface appearance permanent transient facial features examples permanent features lips eyes furrows become permanent age transient features include facial lines furrows present rest assume first frame neutral expression initializing templates permanent features first frame permanent transient features tracked detected whole image sequence regardless states facial components tracking results show method robust tracking facial features even large plane head rotation 31 permanent features features threestate lip model used tracking modeling lip features shown table 1 classify mouth states open closed tightly closed different lip templates used obtain lip contours currently use template open closed mouth two parabolic arcs used model position orientation shape lips template open closed lips six parameters lip center xc yc lip shape h1 h2 w lip orientation tightly closed mouth dark mouth line connecting lip corners detected image model position orientation shape tightly closed lips lip template manually located neutral expression first frame lip color obtained modeling gaussian mixture shape location lip template image sequence automatically tracked feature point tracking lip shape color information used determine lip state state transitions detailed lip tracking method found paper 15 eye features eye trackers developed far open eyes simply track eye locations however recognizing facial action units need recognize state eyes whether open closed parameters eye model location radius iris corners height open eye shown table 1 eye model consists open closed iris provides important information eye state eye open part iris normally visible otherwise eye closed different states specific eye templates different algorithms used obtain eye features open eye assume outer contour eye symmetrical perpendicular bisector line connecting two eye corners template illustrated table 1 composed circle three parameters two parabolic arcs six parameters eye template yuilles except two points located center whites 18 closed eye template reduced 4 parameters eye corners default eye state open locating open eye template first frame eyes inner corner tracked accurately feature point tracking found outer corners hard track less stable inner corners assume outer corners line connects inner corners outer corners obtained eye width calculated first frame intensity edge information used detect iris iris provides important information eye state halfcircle iris mask used obtain correct iris edges iris detected eye open iris center iris mask center image sequence eyelid contours tracked open eyes feature point tracking closed eye need track eyelid contours line connects inner outer corners eye used eye boundary detailed eye feature tracking techniques found paper 14 brow cheek features features brow cheek areas also important facial expression analysis brow cheek one state used respectively triangular template six parameters used model position brow cheek brow cheek tracked feature point tracking modified version gradient tracking algorithm 10 used track points whole image sequence permanent facial feature tracking results different expressions shown figure 3 facial feature tracking results found httpwwwcscmueduface 32 transient features facial motion produces transient features wrinkles furrows appear perpendicular motion direction activated muscle transient features provide crucial information recognition action units contraction corrugator muscle instance produces vertical furrows brows coded facs au 4 contraction medial portion frontalis muscle au 1 causes horizontal wrinkling center forehead lines furrows may become permanent age permanent crowsfeet wrinkles around outside corners eyes characteristic au 6 transient common adults infants lines furrows become permanent facial features contraction corresponding muscles produces changes appearance deepening lengthening presence absence furrows face image determined geometric feature analysis 9 8 eigenanalysis 7 16 kwon lobo 8 detect furrows snake classify pictures people different age groups lien 9 detected whole face horizontal vertical diagonal edges face expression recognition system currently detect nasolabial furrows nose wrinkles crows feet wrinkles define two states present absent compared neutral frame wrinkle state present wrinkles appear deepen lengthen otherwise absent obtaining permanent facial features areas furrows related different aus decided permanent facial feature locations define nasolabial furrow area area eyes inner corners line lip corners line nose wrinkle area square two eye inner corners crows feet wrinkle areas beside eye outer corners use canny edge detector detect edge information areas nose wrinkles crows feet wrinkles compare edge pixel numbers e current frame edge pixel numbers e 0 first frame wrinkle areas ee 0 large threshold furrows present otherwise furrows absent nasolabial furrows detect continued diagonal edges nasolabial furrow detection results shown fig 4 4 facial feature representation action unit facs anatomically related contraction specific facial muscle instance au 12 oblique raising lip corners results contraction zygomaticus major muscle au 20 lip stretch contraction risorius muscle au 15 oblique lowering lip corners contraction depressor anguli muscle muscle contractions produce motion overlying skin deform shape location facial components order recognize b c figure 3 permanent feature tracking results different expressions narrowing eyes opened smiled mouth b large open eye blinking large opened mouth c tight closed eye eye blinking 4 tightly closed mouth blinking figure 4 nasolabial furrow detection results subject nasolabial furrow anglebetween nasolabial furrow line connected eye inner corners different different expressions subtle changes face expression represent upper face features lower face features group suitable parameters respectively facial actions upper face little influence facial motion lower face vice versa 4 defining parameters first define basic coordinate system eyes inner corners stable features face relatively insensitive deformation facial expressions define xaxis line connecting two inner corners eyes yaxis perpendicular xaxis order remove effects different size face images different image sequences parameters except wrinkles states calculated ratio scores comparison neutral frame 41 upper face feature representation represent upper face features 15 parameters 12 parameters describe motion shape eyes brows cheeks 2 parameters describe state crows feet wrinkles 1 parameter describes distance brows figure 5 shows coordinate system parameter meanings definitions upper face parameters listed table 2 table 2 upper face feature representation au recognition permanent features left right inner brow outer brow eye height motion r binner motion r bouter r eheight r binner r bouter r eheight r binner 0 r bouter 0 r eheight 0 inner brow outer brow eye height move move increases eye top lid eye bottom lid cheek motion motion r top motion r btm r cheek r top r btm r cheek gamma h2gammah2 0 gamma cgammac 0 r top 0 r btm 0 r cheek 0 eye top lid eye bottom lid cheek move move move features distance left crows right crows brows feet wrinkles feet wrinkles brow brow w lef left crows feet right crows feet wrinkle present wrinkle present figure 5 upper face features hlhl1 height left eye right eye distance brows cl cr motion left cheek right cheek bli bri motion inner part left brow right brow blo bro motion outer part left brow right brow f l fr left right crows feet wrinkle areas 42 lower face feature representation define nine parameters represent lower face features tracked facial features 6 parameters describe permanent features lip shape lip state lip motion 3 parameters describe transient features nasolabial furrows nose wrinkles notice nasolabial furrow present different angles nasolabial furrow xaxis different action units example nasolanial furrow angle au9 au10 larger au12 use angle represent orientation present although nose wrinkles located upper face classify parameter lower face feature related lower face aus definitions lower face parameters listed table 3 feature data affine aligned calculating based line connected two inner corners eyes normalized individual differences facial conformation converting ratio scores parameter meanings shown figure 6 figure 6 lower face features h1 h2 top bottom lip heights w lip width lef distance left lip corner eye inner corners line right distance right lip corner eye inner corners line n1 nose wrinkle area 5 facial action unit definitions ekman friesen 4 developed facial action coding system facs describing facial expressions action units aus au combinations anatomically related table 3 representation lower face features aus recognition permanent features lip height lip width left lip corner r height r width r lef gamma left gammad left0 r height 0 r width 0 r lef 0 lip height lip width left lip corner increases increases move right lip corner top lip motion bottom lip right r right r top r btm right gammad right0 right0 gamma dtop gammad top0 gamma btm gammad btm0 r right 0 r top 0 r btm 0 right lip corner top lip bottom lip move move move transient features left nasolibial right nasolibial state nose furrow angle furrow angle wrinkles ang left nasolibial left nasolibial nosew furrow present furrow present nose wrinkles angle ang lef angle present ang right contraction specific set facial muscles thses 12 upper face lower action units occur either singly combinations action unit combinations may additive au15 case combination change appearance constituents nonadditive case appearance constituents change au14 although number atomic action units small 7000 combinations action units observed 12 facs provides necessary detail describe facial expression table 4 basic upper face action units au combinations inner portion outer portion brows lowered brows brows drawn raised raised together upper eyelids cheeks lower eyelids raised raised raised au 14 au 45 au 12 medial portion brows lowered inner outer brows drawn portions raised pulled together brows raised together upper eyelids raised au 124 au12567 au0neutral brows pulled brow eyelids eyes brow together cheek raised cheek upward relaxed 51 upper face action units table 4 shows definitions 7 individual upper face aus 5 nonadditive combinations involving action units example nonadditive effect au4 appears differently depending whether occurs alone combination au1 au14 au1 occurs alone brows drawn together lowered au14 brows drawn together raised action au 1 another example difficult notice difference static images au2 au12 action au2 pulls inner brow results similar appearance au12 contrast action au1 alone little effect outer brow 52 lower face action units table 5 shows definitions 11 lower face aus au combinations 6 image database 61 image database upper face au recognition use database bartlett et al 1 upper face aus recognition image database obtained 24 caucasian subjects consisting 12 males 12 females image sequence consists 68 frames beginning neutral low magnitude facial actions ending high magnitude facial actions sequence action units coded certified facs coder investigation 236 image sequences 24 subjects processed 99 image sequences contain individual upper face aus 137 image sequences contain upperface au combinations training testing performed initial final two frames image sequence image sequences lighting normalizations performed test algorithm individual aus randomly generate training testing sets image sequences shown table 6 rains3 tests3 ensure subjects appear training testing sets test algorithm individuall aus au combinations generate training set rainc1 testing set estc1 shown table 6 table 5 basic lower face action units au combination infraorbital infraorbital lips triangle triangle lower portion center pushed upwards nasolabial upper lip upper lip furrow pulled pulled upwards raised nose pulled back nose wrinkling wrinkle absent laterally present mouth elongated corner chin boss lip corners lips pushed pulled obliquely pulled upwards au 25 au 26 au27 lips relaxed lips relaxed mouth stretched parted parted open mandible mandible pulled lowered downwards au 2324 neutral lips tightened lips relaxed narrowed closed pressed together table 6 data distribution data set upper face au recognition single au data sets aus au0 au1 au2 au4 au5 au6 au7 total trains1 trains2 76 20 28 228 trains3 52 au combination data sets 62 image database lower face au recognition use data pittcmu aucoded face expression image database lower face au recogni tion database currently includes 1917 image sequences 182 adult subjects varying ethnicity performing multiple tokens 29 primary facs action units subjects sat directly front camera performed series facial expressions included single action units eg au 12 smile combinations action units eg au 61225 expression sequence began neutral face sequence action units coded certified facs coder total 463 image sequences 122 adults 65 female 35 male 85 europeanamerican 15 africanamerican asian ages 35 years processed lower face action unit recognition image sequences action unit combinations au917 au1017 au1225 au151723 au9172324 au172026 image sequence use neutral frame two peak frames 400 image sequences used training data 63 different image sequences used test data training testing data sets shown table 7 table 7 training data set lower face au recognition neutral au9 au10 au12 au15 au17 au20 au25 au26 au27 au2324 total train set 400 38 test 7 face action units recognition 71 upper face action units recognition used threelayer neural networks one hidden layer inputs neural networks parameters shown table 2 three separate neural networks evaluated comparison bartletts results first nn recognizing individual aus second nn recognizing au combinations modeling 7 individual upper face aus third nn recognizing au combinations separately modeling nonadditive au combinations desired number hidden units achieve good recognition also investigated 711 upper face individual au recognition nn outputs 7 individual upper face aus output unit gives estimate probability input image consisting associated action units experiments found 6 hidden units sufficient order recognize individual action units used training testing data include individual aus table 8 shows results nn rains1 tests1 training testing sets 923 recognition rate obtained increase training data using rains2 test using tests2 92 recognition rate obtained detecting systems robustness new faces tested algorithm rains3tests3 trainingtesting sets recognition results shown table 9 average recognition rate 929 zero false alarms misidentifications aus although probability output units labeled au close highest probability treated incorrect result table 8 au recognition single aus rains1 tests1 rows correspond nn outputs columns correspond human labels average recognition rate 923 example obtain probability au1 au2 au1059 au2055 labeled au2 means au2 misidentified au1 tested nn trained single au image sequences data set containing au combinations found recognition rate decreases 787 table 9 au recognition single aus test data come new subjects used training average recognition rate 929 712 upper face au combination recognition modeling 7 individual aus nn similar one used previous section except one output units could fire also restrict output first 7 individual aus additive nonadditive au combinations value given corresponding individual aus training data set example au124 outputs au110 au210 au410 experiments found need increase number hidden units 6 12 table shows results nn rainc1testc1 trainingtesting set 95 average recognition rate achieved false alarm rate 64 higher false alarm rate comes au combination example obtained recognition results au1059 au2055 labeled au2 treated au1au2 means au2 recognized au1 false alarm table 10 au recognition au combinations modeling 7 single aus au correct false missed confused recognition rate total 94 false alarm 64 713 upper face au combination recognition modeling nonadditive combinations nnwe separately model nonadditive au combinations 11 outputs consist 7 individual upper face aus 4 nonadditive au combinations au12 au14 au45 au124 nonadditive au combinations corresponding individual aus strongly depend table 11 shows correlations au1 au2 au4 au5 au12 au124 au14 au45 used training set set values based appearances aus combinations table 12 shows results nn rainc1t estc1 trainingtesting set average recognition rate 937 achieved slightly lower false alarm rate 45 case modeling separately nonadditive combinations improve recognition rate due fact table 11 correlation au1 au2 au4 au5 au12 au124 au14 au45 aus combinations strongly depend table 12 au recognition au combinations modeling nonadditive au combinations separate aus au correct false missed confused recognition rate total 111 104 5 7 937 false alarm 45 72 lower face action units recognition used threelayer neural network one hidden layer recognize lower face action units inputs neural network lower face feature parameters shown table 3 7 parameters used except two parameters nasolabial furrows dont use angles nasolabial furrows varied much different subjects generally use analyze different expressions subject two separate neural networks trained lower face au recognition outputs first nn ignore nonadditive combinations models 11 basic single action units shown table 5 use au 2324 instead au23 au24 almost occur together outputs second one separately models nonadditive combinations au917 au1017 besides basic single action units recognition results modeling basic lower face aus shown table 13 recognition rate 963 recognition results modeling nonadditive au combinations shown table 14 average recognition rate 9671 found separately model nonadditive combinations slightly increase lower action unit recognition accuracy misidentifications come au10 au17 au26 mistakes au26 confused au25 reasonable au25 au26 parted lips au26 mandible lowered use jaw motion information current system mistakes au10 au17 caused image sequences au combination au1017 two combinations au1017 classified au1012 one combination au1017 classified au10 missing au17 combination au 1017 modified single aus appearance neural network needs learn modification training data au 1017 ten examples au1017 1220 training data current system data au1017 collecting future training system able identify action units regardless whether occurred singly combinations system trained large number subjects included africanamericans asians addition europeanamericans thus providing sufficient test well initial training analyses generalized new image sequences evaluating necessity including nonadditive combinations also train neural network using 11 basic lower face action units outputs test data set average recognition rate 963 8 conclusion discussion developed featurebased facial expression recognition system recognize individual aus au combinations localize subtle changes appearance facial features developed multistate method tracking facial features uses convergent methods feature analysis table 13 lower face action unit recognition results modeling basic lower face aus au correct false missed confused recognition rate 9 total table 14 lower face action unit recognition results modeling nonadditive au combinations au correct false missed confused recognition rate 9 26 14 9 5 au25 6429 total high sensitivity specificity subtle differences facial expressions facial features represented group feature parameters network able learn correlations facial feature parameter patterns specific action units although often correlated effects muscle contraction potentially provide unique information facial expression action units 9 10 facs instance closely related expressions disgust produced variant regions muscle shape nasolabial furrow state nose wrinkles distinguishe changes appearance facial features also affect reliability measurements pixel motion face image closing lips blinking eyes produces occlusion confound optical flow estimation unless information motion feature appearance considered accuracy facial expression analysis particular sensitivity subtle differences expression may impaired recognition rate 95 achieved seven basic upper face aus eleven basic lower face action units recognized 9671 action units correctly classified unlike previous methods 9 build separate model au au combination build single model recognizes aus whether occur singly combinations important capability since number possible au combinations large 7000 combination modeled separately using database bartlett et al 1 recognized 6 single upper face action units combinations performance featurebased classifier novel faces 57 new images face used training rate 853 combined holistic spatial analysis feature measures optical flow obtained best performance 909 correct compared system featurebased classifier obtained higher performance rate 925 novel faces new images face used training individual au recognition moreover system works well difficult case aus occur either individually additive nonadditive au combinations 95 upper face aus au combinations correctly classified regardless whether action units occur singly combination disagreements occur nonadditive au combinations au12 au14 au124 au45 au67 result analysis nonadditive au combinations done future experimental results following observations 1 recognition performance facial feature measurements comparable holistic analysis gabor wavelet representation au recognition 2 5 7 hidden units sufficient code 7 individual upper face aus 10 16 hidden units needed aus may occur either singly complex combinations 3 upper face au recognition separately modeling nonadditive au combinations affords increase recognition accuracy contrast separately modeling nonadditive au combinations affords slightly increase recognition accuracy lower face au recognition 4 using sufficient data train nn recognition accuracy stable recognizing aus new faces summary face image analysis system demonstrated concurrent validity manual facs coding multistate model based convergentmeasures approach proved capture subtle changes facial features test set included subjects mixed ethnicity average recognition accuracy 11 basic action units lower face 9671 7 basic action units upper face 95 regardless action units occur singly combinations comparable level interobserver agreement achieved manual facs coding represents advancement existing computervision systems recognize small set prototypic expressions vary many facial regions acknowledgements authors would like thank paul ekman human interaction laboratory university california san francisco providing database authors also thank zara ambadar bethany peters michelle lemenager processing images work supported nimh grant r01 mh51435 r measuring facial expressions computer image analysis facial expression hollywoods portrayal emotion classifying facial actions facial action coding system technique measurement facial movement facial feature point extraction method based combination shape extraction pattern matching application kl procedure characterization human faces age classification facial images interative image registration technique application stereo vision recognition facial expression optical flow handbook methods nonverbal behavior research analysis facial images using physical anatomical models robust lip tracking combining shape face recognition using eigenfaces recognizing human facial expression long image sequences using optical flow feature extraction faces using deformable templates tr ctr chaofa chuang frank shih rapid brief communication recognizing facial action units using independent component analysis support vector machine pattern recognition v39 n9 p17951798 september 2006 jiajun wong siuyeung cho facial emotion recognition adaptive processing tree structures proceedings 2006 acm symposium applied computing april 2327 2006 dijon france robust feature detection facial expression recognition journal image video processing v2007 n2 p55 august 2007 jiatao song zheru chi jilin liu robust eye detection method using combined binary edge intensity information pattern recognition v39 n6 p11101125 june 2006 seong g kong jingu heo besma r abidi joonki paik mongi abidi recent advances visual infrared face recognition review computer vision image understanding v97 n1 p103135 january 2005 matthew turk computer vision interface communications acm v47 n1 january 2004 alice j otoole joshua harms sarah l snow dawn r hurst matthew r pappas janet h ayyad herve abdi video database moving faces people ieee transactions pattern analysis machine intelligence v27 n5 p812816 may 2005 hatice gunes massimo piccardi tony jan face body gesture recognition visionbased multimodal analyzer proceedings pansydney area workshop visual information processing p1928 june 01 2004 muchun su yijwu hsieh deyuan huang simple approach facial expression recognition proceedings 2007 annual conference international conference computer engineering applications p456461 january 1719 2007 gold coast queensland australia dong liang jie yang zhonglong zheng yuchou chang facial expression recognition system based supervised locally linear embedding pattern recognition letters v26 n15 p23742389 november 2005 congyong su li huang spatiotemporal graphicalmodelbased multiple facial feature tracking eurasip journal applied signal processing v2005 n1 p20912100 1 january 2005 yongmian zhang qiang ji active dynamic information fusion facial expression understanding image sequences ieee transactions pattern analysis machine intelligence v27 n5 p699714 may 2005 shyichyi cheng mingyao chen hongyi chang tzuchuan chou semanticbased facial expression recognition using analytical hierarchy process expert systems applications international journal v33 n1 p8695 july 2007 philipp michel rana el kaliouby real time facial expression recognition video using support vector machines proceedings 5th international conference multimodal interfaces november 0507 2003 vancouver british columbia canada benjamn hernndez gustavo olague riad hammoud leonardo trujillo eva romero visual learning texture descriptors facial expression recognition thermal imagery computer vision image understanding v106 n23 p258269 may 2007 iain matthews jing xiao simon baker 2d vs 3d deformable face models representational power construction realtime fitting international journal computer vision v75 n1 p93113 october 2007 yan tong yang wang zhiwei zhu qiang ji robust facial feature tracking varying face pose facial expression pattern recognition v40 n11 p31953208 november 2007 maria shugrina margrit betke john collomosse empathic painting interactive stylization observed emotional state proceedings 4th international symposium nonphotorealistic animation rendering june 0507 2006 annecy france haisong gu yongmian zhang qiang ji task oriented facial behavior recognition selective sensing computer vision image understanding v100 n3 p385415 december 2005 yanxi liu karen l schmidt jeffrey f cohn sinjini mitra facial asymmetry quantification expression invariant human identification computer vision image understanding v91 n12 p138159 july tao xiang shaogang gong model selection unsupervised learning visual context international journal computer vision v69 n2 p181201 august 2006 chuang christoph bregler mood swings expressive speech animation acm transactions graphics tog v24 n2 p331347 april 2005 zhang zicheng liu dennis adler michael f cohen erik hanson ying robust rapid generation animated faces video images modelbased modeling approach international journal computer vision v58 n2 p93119 july 2004 r w picard papert w bender b blumberg c breazeal cavallo machover resnick roy c strohecker affective learning manifesto bt technology journal v22 n4 p253269 october 2004 aleix martnez recognizing imprecisely localized partially occluded expression variant faces single sample per class ieee transactions pattern analysis machine intelligence v24 n6 p748763 june 2002 scott brave clifford nass emotion humancomputer interaction humancomputer interaction handbook fundamentals evolving technologies emerging applications lawrence erlbaum associates inc mahwah nj 2002