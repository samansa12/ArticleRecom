dimensionality reduction unsupervised learning conditional gaussian networks abstractthis paper introduces novel enhancement unsupervised learning conditional gaussian networks benefits feature selection proposal based assumption absence labels reflecting cluster membership case database features exhibit low correlation rest features considered irrelevant learning process thus suggest performing process using relevant features every irrelevant feature added learned model obtain explanatory model original database primary goal simple thus efficient measure assess relevance features learning process presented additionally form measure allows us calculate relevance threshold automatically identify relevant features experimental results reported synthetic realworld databases show ability proposal distinguish relevant irrelevant features accelerate learning however still obtaining good explanatory models original database b introduction one basic problems arises great variety elds including pattern recognition machine learning statistics socalled data clustering problem 1 2 10 11 18 22 despite dierent interpretations expectations gives rise generic data clustering problem involves assumption addition observed variables also referred predictive attributes simply features hidden variable last unobserved variable would ect cluster membership every case database thus data clustering problem also referred example learning incomplete data due existence hidden variable incomplete data represents special case missing data missing entries concentrated single variable hidden cluster variable refer given database incomplete cases unlabelled point view adopted paper data clustering problem may dened inference generalized joint probability density function given database concretely focus learning conditional gaussian networks data clustering 25 26 27 36 37 roughly speaking conditional gaussian network graphical model encodes conditional gaussian distribution 25 26 27 variables domain applied data clustering encodes multivariate normal distribution observed variables conditioned state cluster variable aim automatically recover generalized joint probability density function given incomplete database learning conditional gaussian network paper concerned understanding data clustering description task rather prediction task thus order encode description original database learnt model must involve original features instead subset unsupervised learning algorithms focus prediction tasks feature selection proven valuable technique increase predictive ability elicited models paper demonstrate even focusing description feature selection also known dimensionality reduction protable tool improve performance unsupervised learning general framework propose show unsupervised learning conditional gaussian networks benet feature selection straightforward consists three steps identication relevant features learning ii unsupervised learning conditional gaussian network database restricted relevant features iii addition irrelevant features learnt network obtain explanatory model original database according framework feature selection considered preprocessing step accompanied postprocessing step fulll objective postprocessing step consists addition every irrelevant feature learnt model nal model encodes generalized joint probability density function original data completely dene framework one decide automatic dimensionality reduction scheme identify relevant features learning paper introduces simple relevance measure assess relevance features learning process order select subset containing salient ones additionally propose heuristic method automatically qualify every feature completely relevant irrelevant learning process carried automatic calculation relevance threshold features relevance measure values higher relevance threshold considered relevant learning pro cess whereas rest qualied irrelevant experimental results reported paper show framework depicted provides us good explanatory models original database reducing cost learning process relevant features used process addition eectiveness simplicity automatic dimensionality reduction scheme propose represents valuable advantage allows framework reduce dimensionality database perform learning eciently besides scheme tied particular learning algorithm therefore adapted remainder paper organized follows section 2 introduce conditional gaussian networks data clustering section 3 dedicated explain detail automatic dimensionality reduction scheme present new relevance measure well automatically discover relevant irrelevant features calculation relevance threshold section also presents proposal unsupervised learning conditional gaussian networks framework already outlined experimental results showing ability proposal identify relevant features accelerate learning process compiled section 4 finally draw conclusions section 5 conditional gaussian networks data cluster ing section starts introducing notation used throughout paper give formal denition conditional gaussian networks also present bayesian structural em algorithm 13 used explanatory purposes well experiments presented section 4 due good performance unsupervised learning conditional gaussian networks 21 notation follow usual convention denoting variables uppercase letters states letters lowercase use letter letters boldface uppercase designate set variables boldface lowercase letter letters denote assignment state variable given set generalized joint probability density function x represented x additionally generalized conditional probability density function x given variables x discrete joint probability mass function x thus denotes conditional probability mass function x given hand variables x continuous joint probability density function x thus fx j denotes conditional probability density function x given 22 conditional gaussian networks already mentioned facing data clustering problem assume existence random variable x partitioned ndimensional continuous variable unidimensional discrete hidden cluster variable c x said follow conditional gaussian distribution 25 26 27 distribution conditioned state c multivariate normal distribution c ndimensional mean vector c n n variance matrix positive denite dene conditional gaussian network cgn x graphical model encodes conditional gaussian distribution x 25 26 27 36 37 essentially cgns belong class mixed graphical models introduced rst time lauritzen wermuth 27 developed 25 26 class groups models discrete continuous variables present conditional distribution continuous variables given discrete variables restricted multivariate gaussian recently cgns successfully applied data clustering 36 37 concretely cgn dened directed acyclic graph model structure determining conditional independencies among variables set local probability density functions multinomial distribution variable c model structure yields factorization generalized joint probability density function x follows denotes conguration parents consistent x local probability density functions multinomial distribution previous equation assume depend nite set parameters therefore equation 2 rewritten follows model structure multinomial distribution local probability density functions c1 c1 c1 c2 c2 c2 figure 1 structure local probability density functions multinomial distribution cgn three continuous variables one binary cluster variable c denotes parameters local probability density functions h denotes hypothesis conditional independence assertions implied hold true generalized joint probability density function x obtain equation 3 order encode conditional gaussian distribution x local probability density function cgn linearregression model thus normal distribution mean standard deviation 0 given form missing arc j implies b c linearregression model local parameters c b c column vector loop 1 run em algorithm compute map parameters b l given 2 perform search model structures evaluating model structure l l 3 let l1 model structure highest score among encountered search 4 scores l l figure 2 schematic bsem algorithm interpretation components local parameters c follows given unconditional mean v c conditional variance given pas b c linear coecient ecting strength relationship j see figure 1 example cgn three continuous variables one binary cluster variable note model structure independent value cluster variable c thus model structure values c however parameters local probability density functions depend value c may dier distinct values variable c 23 learning conditional gaussian networks incomplete data one methods learning cgns incomplete data wellknown bayesian structural em bsem algorithm developed friedman 13 due good performance algorithm received special attention literature motivated several variants 32 34 35 41 use bsem algorithm explanatory purposes well experiments presented section 4 applying bsem algorithm data clustering problem assume database n cases every case represented assignment n observed variables involved problem domain n 1n random variables describe database let denote set observed variables nn variables assigned values similarly let h denote set hidden unobserved variables n variables ect unknown cluster membership case learning cgns incomplete data bsem algorithm performs search space cgns based wellknown em algorithm 7 29 direct optimization bayesian score shown figure 2 bsem algorithm comprised two steps optimization cgn parameters structural search model selection concretely bsem algorithm alternates step nds maximum posteriori map parameters current cgn structure usually means em algorithm step searches cgn structures iteration bsem algorithm attempts maximize expected bayesian score instead true bayesian score interested solving data clustering problems considerable size direct application bsem algorithm appears figure 2 may unrealistic inecient solution opinion reason possible ineciency computation scores l implies huge computational expense takes account every possible completion database common use relaxed version presented bsem algorithm considers likely completion database compute scores l instead considering every possible completion thus relaxed version bsem algorithm comprised iteration parametric optimization current model structural search database completed likely completion using best estimate generalized joint probability density function data far current model posterior probability distribution cluster variable c case database l calculated case assigned cluster maximum posterior probability distribution c reached use relaxed version experiments section 4 completely specify bsem algorithm decide structural search procedure step 2 figure 2 usual approach perform greedy hillclimbing search cgn structures considering possible additions removals reversals single arc point search structural search procedure desirable exploits decomposition properties cgns factorization properties bayesian score complete data however structural search procedure exploits properties used log marginal likelihood expected complete data log j h usually chosen score guide structural search make use experiments according 15 assumptions database restricted cluster variable c c multinomial sample ii database complete iii parameters multinomial distribution c independent follow dirichlet distribution c database restricted continuous variables cases set values cluster variable c take term pd corresponds marginal likelihood trivial bayesian network single node c calculated closed form reasonable assumptions according 5 moreover term form f c c j h c 2 v alc represents marginal likelihood domain containing continuous variables assumption continuous data sampled multivariate normal distribution terms evaluated factorable closed form reasonable assumptions according 15 16 19 3 automatic dimensionality reduction unsupervised learning conditional gaussian networks section devoted detailed presentation new automatic dimensionality reduction scheme applied unsupervised learning cgns section starts introductory revision general problem feature selection brief discussion problems appear adapting supervised feature selection unsupervised paradigm 31 supervised unsupervised feature selection many data analysis applications size data large largeness due excessive number features huge number instances learning algorithms work eciently even sometimes eectively one may need reduce data size feature selection proven valuable technique achieve reduction dimensionality data selecting subset features focus attention subsequent learning process general form feature selection considered problem searching optimal subset original features according certain criterion 3 23 28 criterion species details measuring goodness feature subsets well relevance feature choice criterion uenced purpose feature selection however shared dierent purposes desire improving performance subsequent learning algorithm usually terms speed learning predictive ability learnt models andor comprehensibility learnt models roughly speaking feature selection involves algorithm explore space potential feature subsets evaluation function measure quality feature subsets since space feature subsets n features size 2 n feature selection mechanisms typically perform nonexhaustive search one popular techniques use simple hillclimbing search known sequential selection may either forward backward 3 23 28 former search starts empty set selected features time adds best feature among unselected ones according evaluation function process stops improvement made similarly backward sequential selection begins full set features time removes worst feature based evaluation function improvement found addressed doak 9 feature selection mechanisms based sequential selection require great deal processing time databases large number features also complex eective search algorithms used explore space potential feature subsets main advantage algorithms sequential selection avoid getting stuck local maxima means randomness however approaches usually involve huge computational eort one recent works eld reported 20 paper authors propose exploring space feature subsets according evolutionary populationbased randomized search algorithm represents instance estimation distribution algorithm eda approach 24 23 authors distinguish two approaches evaluation function feature selection wrapper lter wrapper approach implies search optimal feature subset tailored performance function subsequent learning algorithm considers feedback performance function particular subsequent learning algorithm part function evaluate feature subsets hand lter approach relies intrinsic properties data presumed aect performance learning algorithm direct function performance lter approach tries assess merits dierent feature subsets data ignoring subsequent learning algorithm applied supervised learning main objective feature selection improvement classication accuracy class label predictive accuracy models elicited subsequent learning algorithm considering relevant features task independently approach used lter wrapper approaches require class labels present data order carry feature selection filter approaches evaluate feature subsets usually assessing correlation every feature class label using dierent measures 3 28 hand wrapper approaches rely performance learning algorithm measuring classication accuracy validation set evaluate goodness dierent feature subsets 3 23 28 evidence supervised feature selection research wrapper approaches outperform lter approaches 21 although feature selection central problem data analysis suggested growing amount research area vast majority research carried supervised learning paradigm supervised feature selection paying little attention unsupervised learning unsupervised feature selection works exist addressing latter problem 6 authors present method rank features according unsupervised entropy measure algorithm works lter approach plus backward sequential selection search devaney ram 8 proposes wrapper approach combined either forward backward sequential selection search perform conceptual clustering 39 talavera introduces lter approach combined search one step wrapper approach combined either forward backward sequential selection search feature selection mechanisms hierarchical clustering symbolic data lter approach uses feature dependence measure dened fisher 11 whereas performance criterion considered 39 multiple predictive accuracy measured average accuracy predicting values feature present testing data 40 applies mechanism comprised lter approach search one step presented 39 feature selection conceptual clustering symbolic data considering class label predictive accuracy performance criterion opinion two main problems translate supervised feature selection unsupervised feature selection firstly absence class labels ecting membership every case database inherent unsupervised paradigm makes impossible use evaluation functions supervised feature selection secondly standard accepted performance task unsupervised learning due lack unied performance criterion meaning optimal feature subset may vary task task natural solution problems proposed 39 interpreting performance task unsupervised learning multiple predictive accuracy seems reasonable approach extends standard accepted performance task supervised learning unsupervised learning whereas former learning comprises prediction one feature class knowledge many latter aims prediction many features knowledge many 12 hand 6 8 40 evaluate unsupervised feature selection mechanisms measuring class label predictive accuracy learnt models cases testing set performed learning training set class labels masked speed learning comprehensibility learnt models also studied 8 39 although considered less important performance criteria 32 learning conditional gaussian networks data clustering benets feature selection motivation perform unsupervised feature selection diers motivation previously referred papers due distinct point view data clustering problem learnt models data clustering primarily evaluated regarding multiple class label predictive accuracy occurs 6 8 39 40 feature selection proven valuable technique reduce dimensionality database perform learning usually pursues improvement performance learnt models considering relevant features task however main goal data clustering happens paper description rather prediction learnt models must involve features original database order encode description database wellknown unsupervised learning cgns solve data clustering problems dicult time consuming task even focusing description original features usually considered learning process aim solve handicaps propose framework learning cgns data clustering benets feature selection framework straightforward consists three steps identication relevant features learning ii unsupervised learning cgn database restricted relevant features iii addition irrelevant features learnt cgn obtain explanatory model original database thus feature selection considered preprocessing step accompanied postprocessing step achieve objective postprocessing step consists addition every irrelevant feature elicited model conditional independent rest given cluster variable make framework applicable unsupervised learning cgns dene relevance however meaning relevance depends particular purpose dimensionality reduction due lack unied performance criterion data clustering concrete case objective reducing dimensionality databases learning cgns data clustering decrease cost learning process still obtaining good explanatory models original data achievement goal assessed comparing terms explanatory power runtime learning process cgn learnt given original database cgn elicited using dimensionality reduction learning process assessment achievement objective leads us make following assumption consideration feature either relevant irrelevant learning process absence labels ecting cluster membership case database features exhibit low correlation rest features considered irrelevant learning process implicitly assumption denes relevance according purpose perform dimensionality reduction important note assumption independent clustering data readily applied without requiring previous clustering database justication previous assumption straightforward features low correlated rest likely remain conditionally independent rest features given cluster variable learning cgn original database thus cgn elicited original database restricted features highly correlated rest likely encode set conditional dependence assertions cgn learnt original database parameters local probability density functions features appear cgns similar well furthermore low correlated features added cgn elicited restricted database conditional independent rest given cluster variable nal cgn likely encode set conditional dependence independence assertions cgn learnt original data thus explanatory power cgns almost models likely similar works successfully made use similar assumption 11 39 40 although three works present assumption general form validate conceptual clustering symbolic data paper rst knowledge veries continuous domains 321 relevance measure order assess relevance evaluating following simple thus ecient relevance measure ijjrest n number features database n number cases database r ijjrest sample partial correlation j adjusted remainder variables last quantity expressed terms maximum likelihood estimates elements inverse variance matrix r ijjrest relevance measure value feature average likelihood ratio test statistic excluding edge feature graphical gaussian model 38 means features likely remain conditional independent rest given cluster variable learning progresses receive low relevance measure values thus measure shows reasonable behavior according denition relevance 322 relevance threshold calculated relevance measure value every feature database decreasing relevance ranking features obtained would like know many needed perform learning appropriately would like identify relevance ranking relevant features learning process knew k features needed could simply choose rst k features relevance ranking namely k features highest relevance measure values however kind knowledge usual propose novel automatic solution problem relevance measure value feature interpreted average value likelihood ratio test statistic excluding single edge feature graphical gaussian model thus propose following evaluate relevance measure feature calculate relevance threshold let rel feature subset containing relevant features loop 1 run em algorithm compute map parameters b rel l rel l given 2 perform search model structures evaluating model structure l rel l l rel l l 3 let rel l1 model structure highest score among encountered search 4 exit loop scores rel l l let final nal model obtained adding irrelevant features rel l calculate map parameters b final final return final b figure 3 schematic automatic dimensionality reduction scheme bsem algorithm framework presented heuristic relevance threshold calculated rejection region boundary edge exclusion test graphical gaussian model likelihood ratio test statistic see 38 details heuristic agrees purpose perform dimensionality reduction qualies irrelevant features likely remain conditional independent rest given cluster variable learning progresses shown 38 distribution function likelihood ratio test statistic follows distribution function x 2 1 random variable thus 5 test rejection region boundary considered relevance threshold given resolution following equation simple manipulation resolution previous equation turns nding root equation newtonraphson method used experiments example suitable methods solving equation features exhibit relevance measure values higher relevance threshold qualied relevant learning process rest features treated irrelevant 323 fitting automatic dimensionality reduction learning subsection present automatic dimensionality reduction scheme bsem algorithm general framework previously introduced however noticed scheme coupled particular learning algorithm could adapted figure 3 shows preprocessing step consists automatic dimensionality reduction scheme bsem algorithm applied usual restricting original database relevant features rel hidden cluster variable c database perform learning consists n cases figure 4 example tanb model structure seven predictive attributes g every case represented assignment relevant features r 1n random variables describe database r number relevant features denote set observed variables restricted relevant features set hidden variables rel jo rel respectively obviously figure 3 rel l represents model structure relevant features considered learning pro cess rel l denotes hypothesis conditional independence assertions implied rel l hold true joint probability density function rel learning ends postprocessing step comprises addition every irrelevant feature model returned bsem algorithm conditional independent rest given cluster variable results explanatory model original database local parameters nodes nal model associated irrelevant features easily estimated completing original database last completion restricted database rel 4 experimental evaluation section dedicated show ability proposal perform automatic dimensionality reduction accelerates unsupervised learning cgns without degrading explanatory power nal models order reach conclusion perform 2 sorts experiments synthetic realworld databases rst evaluates relevance measure introduced section 321 means assess relevance features learning process second evaluates ability relevance threshold calculated appears section 322 automatically distinguish relevant irrelevant features learning addressed use bsem algorithm unsupervised learning algorithm current experiments limit bsem algorithm learn tree augmented naive bayes tanb models 14 30 36 sensible usual decision reduce otherwise large search space cgns moreover allows solve eciently data clustering problems considerable size wellknown diculty involved learning densely connected cgns large databases painfully slow probabilistic inference working tanb models constitute class compromise cgns dened following con dition predictive attributes may one predictive attribute parent figure 4 shows example tanb model structure tanb models cgns interesting tradeo eciency eectiveness achieved balance cost learning process quality learnt cgns 36 41 databases involved 2 synthetic 2 realworld databases involved experimental evaluation knowledge cgns used generate synthetic databases allows us assess accurately achievement objectives besides realworld databases provide us realistic evaluation framework obtain 2 synthetic databases constructed 2 tanb models dierent complexity sampled rst tanb model involved 25 predictive continuous attributes 1 3valued cluster variable rst 15 25 predictive attributes relevant rest irrelevant 14 arcs relevant attributes randomly chosen unconditional mean every relevant attribute xed 0 rst value cluster variable 4 second 8 third linear coecients randomly generated interval 1 1 conditional variances xed 1 see equation 5 multinomial distribution cluster variable c uniform every irrelevant attribute followed univariate normal distribution mean 0 variance 1 3 values cluster variable second tanb model involved predictive continuous attributes 1 3valued cluster variable rst 15 predictive attributes relevant rest irrelevant 14 arcs relevant attributes randomly chosen unconditional mean every relevant attribute xed 0 rst value cluster variable 4 second 8 third linear coecients randomly generated interval 1 1 conditional variances xed 2 see equation 5 multinomial distribution cluster variable c uniform every irrelevant attribute followed univariate normal distribution mean 0 variance 5 3 values cluster variable second model considered complex rst due higher degree overlapping probability density functions clusters higher number irrelevant attributes 2 tanb models sampled 4000 cases learning databases 1000 cases testing databases forthcoming learning databases sampled 2 tanb models referred synthetic1 synthetic2 respectively obviously discarded entries corresponding cluster variable 2 learning databases 2 testing databases another source data evaluation consisted 2 wellknown realworld databases uci repository machine learning databases 33 waveform articial database consisting 40 predictive features last 19 predictive attributes noise attributes turn irrelevant describing underlying 3 clusters used data set generator uci repository obtain 4000 cases learning 1000 cases testing pima real database containing 768 cases 8 predictive features 2 clusters used rst 700 cases learning last 68 cases testing rst database chosen due interest working databases considerable size thousands cases tens features addition represented opportunity evaluate eectiveness approach true irrelevant features known beforehand second database considerably shorter number cases number features chosen get feedback scalability dimensionality reduction scheme obviously deleted cluster entries 2 learning databases 2 testing databases 42 performance criteria exist 2 essential purposes focus explanatory power generalizability learnt models rst purpose summarize given databases learnt models second purpose elicit models able predict unseen instances 28 thus explanatory power learnt cgns assessed evaluating achievement purposes log marginal likelihood sc nal multiple predictive accuracy ltest learnt cgns seem sensible performance measures rst second purpose respectively multiple predictive accuracy measured logarithmic scoring rule good 17 jd test j y2d test log fy test set test cases jd test j number test cases higher value criterion higher multiple predictive accuracy learnt cgns note ltest primary performance measure 1 2 measures assess explanatory power learnt cgns focusing description ltest extremely necessary detect models suering overtting high sc nal values although able generalize learning data unseen instances noted equation 10 represents kind probabilistic approach standard multiple predictive accuracy understanding latter average accuracy predicting value feature present testing data data clustering problem considered inference generalized joint probability density function learning data via unsupervised learning cgn probabilistic approach presented equation 10 appropriate standard multiple predictive accuracy illustrated simple example let us imagine 2 dierent cgns exhibit standard multiple predictive accuracy dierent multiple predictive accuracy measured logarithmic scoring rule good would ect generalized joint probability density functions encoded 2 cgns dierent moreover would imply 1 2 cgns generalizes learning data unseen instances better ie likelihood unseen instances higher although standard multiple predictive accuracy thus standard multiple predictive accuracy would appropriate performance criterion context would unable distinguish models works made use logarithmic scoring rule good assess multiple predictive accuracy 31 34 36 37 41 runtime overall learning process runtime also considered valuable information every runtime reported includes runtimes preprocessing step dimensionality reduction learning algorithm postprocessing step addition irrelevant features results reported averages 10 independent runs synthetic1 synthetic2 waveform databases 50 independent runs pima database due shorter size experiments run pentium 366 mhz computer features synthetic12216104relevance503010features synthetic2251791 relevance3010features waveform31197relevance1062features pima7531 figure 5 relevance measure values features databases used dashed lines correspond relevance thresholds 43 results relevance ranking figure 5 plots relevance measure values features 4 databases considered additionally shows relevance threshold dashed line database case synthetic databases 10 true irrelevant features synthetic1 database 15 synthetic2 database clearly appear lowest relevance measure values case waveform database may interesting compare graph figure 5 graphs reported 4 40 42 database caution used detailed comparison advisable due fact relevance dened dierent ways depending particular purpose works moreover work talavera 40 limited conceptual clustering symbolic data original waveform database previously discretized however noticeable 19 true irrelevant features appear plotted low relevance values 4 graphs although shape graphs restricted 21 relevant features varies 3 works reported 4 40 42 agree graph consider rst last relevant features less important rest 21 shape graph slightly closer appear 4 42 one plotted 40 conclude relevance measure proposed exhibits desirable behavior databases true irrelevant features known clearly assigns low relevance values following subsection evaluates values low enough automatically distinguish relevant irrelevant features calculation relevance threshold figure 6 shows log marginal likelihood sc nal multiple predictive accuracy number selected features synthetic12216104scfinal 74000 76000 78000 80000 82000 84000 86000 88000 number selected features synthetic12216104ltest number selected features synthetic2251791 scfinal 122000 132000 number selected features synthetic2251791 number selected features waveform31197scfinal 122000 number selected features waveform31197ltest number selected features pima7531 scfinal 9000 number selected features pima7531 figure log marginal likelihood sc nal multiple predictive accuracy ltest nal cgns databases used functions number features selected relevant decreasing relevance ranking ltest nal cgns 4 databases considered functions number features selected relevant learning addition figure 7 reports runtime needed learn nal cgns function number features selected number selected features synthetic12216104runtime number selected features synthetic2251791 runtime seconds1006020number selected features waveform31197runtime seconds2000 number selected features pima7531 runtime figure 7 runtime needed learn nal cgns databases used function number features selected relevant decreasing relevance ranking relevant learning selection k features relevant means selection k rst features decreasing relevance ranking obtained features concrete database according relevance measure values thus rst part experimental evaluation perform automatic dimensionality reduction instead aim study performance function number features involved learning allows us evaluate ability relevance measure assess relevance features learning process general terms figure 6 conrms relevance measure able induce eective decreasing relevance ranking features database considered addition features low relevance measure values last features rankings imply signicant increase quality nal models even cases hurts explanatory power thus gure conrms assumption low correlated features irrelevant learning process works well continuous domains considered hand addition irrelevant features tends increase cost learning process measured runtime see figure 7 particularly interesting results synthetic databases original models known selection true irrelevant features take part learning produce better models increases runtime learning process also known last 19 40 features waveform database true irrelevant features according relevance measure values features waveform database see figure 5 19 true irrelevant features would appear last 21 positions decreasing relevance ranking furthermore appreciated figure 6 addition 19 irrelevant features signicantly increase table 1 comparison performance achieved learning cgns original databases automatic dimensionality reduction scheme applied features original dimensionality dimensionality reduction database original relevant sc nal ltest runtime sc nal ltest runtime synthetic2 explanatory power nal cgns results obtained pima database knowledge existence true irrelevant features share fact using features learning process degrades quality nal models well making learning process slower thus explanatory power nal cgns appears monotonic respect addition features relevant learning hence need automatic tools discovering irrelevant features may degrade eectiveness enlarge runtime learning 44 results automatic dimensionality reduction figure 5 shows relevance threshold dashed line calculated shown section 322 databases considered features exhibit relevance measure values higher relevance threshold qualied relevant rest features considered irrelevant learning interesting notice 2 synthetic databases true irrelevant features identied independently complexity sampled model remembered synthetic2 database sampled model complex one used generate synthetic1 database results obtained waveform database also specially appealing 19 true irrelevant features correctly identied moreover scheme considers 8 features remainder 21 features also irrelevant appears sensible decision 8 features correspond rst 4 last 4 21 relevant features remember 4 40 42 agree point rst last 21 relevant features less important rest relevant features table 1 compares 4 databases considered performance achieved dimensionality reduction carried performance achieved automatic dimensionality reduction scheme applied learn cgns column relevant indicates number relevant features automatically identied scheme database see figure 5 clearly appears table scheme able automatically set relevance threshold induces saving runtime still obtaining good explanatory models application scheme preprocessing step bsem algorithm figure provides us saving runtime original bsem algorithm achieves 22 synthetic1 database synthetic2 database moreover explanatory power cgns elicited original synthetic databases cgns obtained using automatic dimensionality reduction scheme exactly waveform database automatic dimensionality reduction scheme proposes reduction number features 68 13 40 original features considered relevant reduction induces gain terms runtime 58 whereas scheme signicantly hurt quality learnt models hand cgns learnt help automatic dimensionality reduction scheme pima database exhibit average desirable behavior cgns elicited original pima database higher log marginal likelihood multiple predictive accuracy whereas runtime learning process shortened conclusions main contribution paper twofold first proposal novel automatic scheme perform unsupervised dimensionality reduction comprised simple ecient measure assess relevance every feature learning process ii heuristic calculate relevance threshold automatically distinguish relevant irrelevant features second present framework unsupervised learning cgns benets proposed scheme order obtain models describe original databases framework proposes performing learning taking account relevant features identied automatic dimensionality reduction scheme presented every irrelevant feature incorporated learnt model order obtain explanatory cgn original database experimental results synthetic realworld domains suggested great advantages derived use automatic dimensionality reduction scheme unsupervised learning cgns huge decrease runtime learning process achievement nal models appear good sometimes even better models obtained using features learning process addi tionally experimental results proven assumption made relevance dened according purpose perform dimensionality reduction works fairly well continuous domains considered paper primarily focused gain eciency without degrading explanatory power nal models derived use referred scheme preprocessing learning process however worth noticing identica tion relevant irrelevant features learning process allows us reach better comprehensibility readability problem domains elicited models works addressed problem unsupervised feature selection preprocessing step 6 8 39 40 however dier work whereas focus description original database 6 8 40 interested class label predictive accuracy 39 multiple predictive accuracy impossibilities fair comparison dierent approaches moreover automatic dimensionality reduction scheme oers series advantages existing mechanisms addition simplicity eciency scheme coupled particular learning algorithm could adapted hand existing unsupervised feature selection mechanisms based wrapper approaches tailored performance criterion particular subsequent learning algorithm see 8 39 thus usually require great deal processing time large databases furthermore 6 40 propose feature selection mechanisms based l ter approaches provide user ranking features leaving open problem determining many features used perform proper learning scheme able automatically distinguish relevant irrelevant features relevance ranking one line future research could extension current contribution categorical data order overcome problem determining number features used subsequent learning algorithm aware contribution presented paper unable deal properly domains redundant features exist ie features whose values exactly determined rest features reason relevance measure introduced section 321 scores feature separately instead groups features thus redundant features would considered relevant although would provide learning process additional information true relevant fea tures detect features necessary eect runtime learning process one lines research currently exploring concerned extension general framework depicted paper case redundant features exist current work focused derivation new relevance measure assess gain relevance feature relation features considered relevant far acknowledgments jm pena wishes thank dr steve ellacott interest work useful comments also made possible visit school computing mathematical sciences university brighton brighton united kingdom authors would also like thank two anonymous reviewers whose useful comments previous version paper helped us improve manuscript work supported spanish ministry education culture min isterio de educacion cultura ap97 44673053 grant r analysis applications pattern classi clustering algorithms finding groups data estimation distribution algorithms feature selection knowledge discovery data mining em algorithm extensions uci repository machine learning databases tr ctr martin h c law mario figueiredo anil k jain simultaneous feature selection clustering using mixture models ieee transactions pattern analysis machine intelligence v26 n9 p11541166 september 2004 lance parsons ehtesham haque huan liu subspace clustering high dimensional data review acm sigkdd explorations newsletter v6 n1 p90105 june 2004 j pea j lozano p larraaga globally multimodal problem optimization via estimation distribution algorithm based unsupervised learning bayesian networks evolutionary computation v13 n1 p4366 january 2005