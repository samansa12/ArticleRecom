supernode transformation minimized total running time abstractwith objective minimizing total execution time parallel program distributed memory parallel computer paper discusses find optimal supernode size optimal supernode relative side lengths supernode transformation also known tiling identify three parameters supernode transformation supernode size relative side lengths cutting hyperplane directions algorithms perfectly nested loops uniform dependencies sufficiently large supernodes number processors case multiple supernodes mapped single processor give order n polynomial whose real positive roots include optimal supernode size two special cases 1 twodimensional algorithm problems 2 ndimensional algorithm problems communication cost dominated startup penalty therefore approximated constant give closed form expression optimal supernode size independent supernode relative side lengths cutting hyperplanes case algorithm iteration index space supernodes hyperrectangular give closed form expressions optimal supernode relative side lengths experiment shows good match closed form expressions experimental data b introduction supernode partitioning transformation technique groups number iterations nested loop order reduce communication startup cost paper addresses problem finding optimal grain size shape supernode transformation total running time sum communication time computation time minimized problem distributed memory parallel systems communication startup cost time takes message reach transmission media moment research supported part national science foundation grant ccr9502889 clare boothe luce assistant professorship henry luce foundation supported hal computer systems 1315 dell ave campbell ca 95008 initiation communication startup cost usually orders magnitude greater time transmit message across transmission media compute data message supernode transformation studied 4 5 6 7 8 9 15 reduce number messages sent processors grouping multiple iterations supernodes 1 supernode transformation several iterations grouped one supernode supernode assigned processor unit execution data iterations supernode need sent another processor grouped single message number communication startups reduced number iterations supernode one supernode transformation characterized hyperplanes slice iteration index space parallelepiped supernodes grain size supernode relative lengths sides supernode three factors mentioned affect total running time larger grain size reduces communication startup cost may delay computation processors waiting message also square supernode may good rectangular supernode grain size paper find optimal grain size optimal relative side length vector optimal shape supernode addressed algorithms considered paper nested loops uniform dependences 12 algorithm described iteration index space consisting iteration index vectors loop nest dependence matrix consisting uniform dependence vectors columns two communication models considered one parameter communication model communication cost approximated constant startup penalty two parameter model communication cost function startup penalty message size first model used case startup penalty dominates communication cost second model case message size large ignored approach paper follows unlike related work supernode transformation specified n side lengths parallelepiped supernode n partitioning hyperplanes supernode transformation paper specified grain size g supernode relative side length vector r describes side lengths supernode relative supernode size n partitioning hyperplanes described matrix h contains normal vectors n independent hyperplanes rows approach allows us given partitioning hyperplanes find optimal grain size optimal shape separately formulation based formulation derived closed form analytical expression optimal supernode size one parameter model two parameter model doubly nested loops closed form expression optimal relative length vector also provided one parameter model constant bounded loop iteration space rest paper organized follows section 2 presents necessary defi nitions assumptions terminology models section 3 discusses general model transformation conceptually similar lsgp locally sequential globally parallel partitioning 4 7 8 9 11 15 group iterations assigned single processing element sequential execution different processing elements execute respectively assigned computations parallel total running time components depend different parameters section 4 briefly presents results find optimal grain size shape supernode transformation assuming one parameter communication model section 5 discusses find optimal supernode size assuming two parameter communication model communication cost modeled affine function amount data transferred section 6 briefly describes related work contribution work compared previous work section 7 concludes paper basic definitions models assumptions distributed memory parallel computer processor access local memory capable communicating processors passing messages cost sending message modeled startup time b amount data transmitted transmission rate ie transmission time per unit data computation speed single processor distributed memory parallel computer characterized time takes compute single iteration nested loop parameter denoted c consider algorithms consist single nested loop uniform dependences 12 algorithms described pair j j iteration index space n theta dependence matrix column dependence matrix represents dependence vector assume n matrix full rank equal number loop nests n determinant smith normal form equal one discussed 13 assumptions satisfied iteration index space j contains independent components partitioned several independent subalgorithms assumptions satisfied furthermore assume true loop carried dependences 14 included matrix since dependences cause communication types dependences eliminated using known transformation techniques eg variable renaming supernode transformation iteration space sliced n independent families parallel equidistant hyperplanes hyperplanes partition iteration index space ndimensional parallelepiped supernodes hyperplanes defined normal vectors orthogonal hyperplanes n theta n matrix containing n normal vectors rows denoted h full rank hyperplanes independent supernodes defined matrix h distances adjacent parallel hyperplanes let l distance two adjacent hyperplanes normal vector h supernode side length vector grain size supernode volume denoted g defined number iterations one supernode supernode volume length l related follows l depends angles hyperplanes example supernodes cubes 1 also define supernode relative side length vector clearly vector r describes side lengths supernode relative supernode size example supernode square supernode transformation completely specified h r g denoted h r g supernode transformation obtain new dependence structure j node supernode index space j supernode supernode dependence matrix general different discussed 6 partitioning hyperplanes defined matrix h satisfy hd 0 ie entry product matrix greater equal zero order j computable dependence vectors contained convex cone matrix matrix extreme vectors dependence vectors words dependence vector expressed nonnegative linear combination columns e also n columns e n vectors collinear n sides parallelepiped supernode algorithm j linear schedule 12 defined oe disp jg linear schedule assigns node j 2 j execution step dependence relations respected length linear schedule defined note j 1 oe j 1 maximum always extreme points iteration index space execution algorithm j follows apply supernode transformation h r g obtain j optimal linear schedule found execution based linear schedule alternates computation phases communication phases step assign supernodes oe available processors processor finishes computations supernode processors communicate pass messages communication done go step i1 total amount data transferred communication phase single processor denoted v g r hence total running time algorithm depends following j h g r total running time problem finding optimal supernode transformation optimization problem finding parameters h g r total running time j h figure 1 iteration index space supernode transformations minimized paper addresses problems find optimal grain size g optimal supernode relative length vector r find optimal linear schedule j found 1 12 find optimal h general remains open discussed 6 7 supernode partitioning parameters chosen optimal number processors system determined maximum number independent supernodes computation phase maximum number supernodes linear schedule oe assigns value constant assigned linear schedule oe supernodes phase j supernode index space example 21 illustrate notions introduced show example supernode transformation applied algorithm affects algorithms total running time lets assume 01s consider algorithm j algorithm consists two loops three dependence vectors optimal linear schedule vector algorithm 1 length schedule figure 1 shows iteration index space linear schedule wave fronts supernode consists one iteration execution algorithm 299 computation phases 298 communication phases one computation phase takes 10s assuming iteration produces one eight byte data item 50 bytes header overhead communication time takes 3058s figure 2 supernode index space supernode square containing four iterations total running time processors sequential running time 200ms consider supernode transformation applied algorithm let hyperplane matrix matrix h defines two families lines parallel two axes matrix h two lines orthogonal let length vector supernode square containing four iterations figure 1 shows one supernode outlined dashed square supernode index space dependence matrix j shown figure 2 optimal linear schedule vector j schedule length figure shows supernode index space linear schedule wave fronts computations one supernode take gt 40s communication one phase takes 30001 theta assuming processor send two data items neighboring processor total running time 50 processors simple example speedup 1 2 close 2 note supernode transformation parameters chosen arbitrarily optimal later shown total running time improved optimal solution 3 total running time section discusses total running time model components depend grain size shape supernode transformation ct theta theta theta schedule vector p q distance size shape size shape shape size figure 3 dependences total running time components supernode size shape total running time sum computation time communication time multiples number phases execution linear schedule length defined previous section corresponds number communication phases execution number computation phases usually one number communication phases supernode transformations often generate supernodes containing fewer iterations boundary index space thus first andor last computation phases often shorter computation phases reason assume number computation phases number communication phases equal equal linear schedule length denoted p total running time sum computation time comp communication time comm one phase multiplied number phases figure 3 shows components total running time depend supernode grain size shape computation time comp depends supernode size g communication time comm single communication phase depends c number different neighbors processor send message v g r total amount data transmitted single processor communication phase number messages c depends h algorithm supernodes assigned processors v general depends supernode size shape scheduling length p function schedule vector point j point j distance p q proven later section p q depend shape words two supernode transformations relative points supernode index space possible different distances p q consider algorithm space j dependence matrix obtained applying supernode transformation h r g algorithm according 6 valid supernode transformation hyperplanes h dependence vectors 2 contained convex cone n extreme directions forming parallelepiped supernode hence supernode grain size reasonably large components 2 reasonably small dependence vectors 2 originating extreme point convex cone parallelepiped supernode contained inside parallelepiped supernode therefore reasonable assume paper components dependence matrix 0 1 gamma1 following lemma gives sufficient conditions assumption true lemma 31 components dependence matrix transformed algorithm jth component vector hd consider two supernode transformations h r h r different supernode sizes g 1 g let j s1 j corresponding supernode index spaces lemma 32 shows index space changes supernode size changes lemma 32 let j bg supernode index set g 1 j supernode index set g n number loop nests original algorithm dependence matrix change supernode size changes g 1 g follows assumption components dependence vectors transformed algorithm take values f0 1 gamma1g fact supernode shape defined h r change following lemma shows optimal linear schedule change supernode size g changes lemma 33 optimal linear schedule vectors two supernode transformations differ supernode size identical proofs three lemmas found 2 4 one parameter communication model section briefly summarize find optimal grain size g relative length vector r one parameter communication model used model applies cases message startup time much larger data transmission time data transmission overlapped useful processing case total running becomes ct proofs theorems detailed derivation found 2 theorem 41 algorithm j supernode transformation h r optimal supernode size ct shape supernodes defined two supernode transformation parameters h r given h optimal grain size g problem finding optimal r general cases formulated nonlinear programming problem 2 optimal relative length vector r optimal linear schedule vector given following special case index set j constantbounded loop bounds constant partitioning hyperplane matrix identity matrix ie index space supernodes hyperrectangles also let 1 vector whose components one theorem 42 consider algorithm j supernode transformation h r g transformed algorithm j identity matrix optimal linear schedule vector supernode relative length vector given theorem implies optimal supernode shape similar shape original index set j resulting supernode index set j hypercube equal sides derived 2 optimal grain size algorithm example 21 optimal relative length vector 2 table shows total running time varies different supernode grain sizes fixed square supernode shape total running time shortest optimal grain size improvement total running time achieved optimal relative side length vector used shown table 2 total running time computed different supernode relative vectors supernode size close optimal note values supernode size supernode side lengths computed based theorems 41 42 may integral choose approximate integral values supernode side lengths l close optimal values volume resulting supernode close optimal grain size simple heuristic use approximate values l volume resulting supernode greater equal g total running time increases faster values g g slower values g g alternatively total running time evaluated different approximate values best approximation used l processors time table 1 total running time different supernode sizes square supernode shape close square l processors time 28 4 table 2 total running time different supernode shapes supernode sizes close 30 5 two parameter communication model section discusses find optimal grain size shape two parameter communication model used model communication cost modeled affine function amount data transferred single processor communication phase amount data transferred v g r general complicated function supernode size supernode shape 7 simplify problem consider amount data transferred function supernode size intuitively neighboring supernodes may need data surface supernode general amount data proportional area surfaces dimensionalities supernode thus use following expression amount data transferred ffs constant hence amount data transferred depends supernode size total running time supernode transformation h r g ct according lemmas 31 32 33 section 3 number phases p p g1 scheduling length supernode transformation h r g 1 find optimal supernode size solve equation ct g g ct g ct ct ct substituting ct ct real positive root equation 6 power n give optimal supernode size case two dimensional algorithm 2 optimal supernode size becomes ct example 51 algorithm example 21 assuming v g optimal supernode size g taking approximate supernode size 5 get supernode computation time 309s total running time need 20 processors table 3 shows total running time computed different supernode sizes shows total running time shorter supernode sizes closer optimal value since assumed data amount transferred depend supernode size optimal supernode shape computed using nonlinear program discussed case constant communication time nonlinear program derivation given 2 l processors time 343 table 3 total running time different supernode sizes square supernode shape close square 6 related work section give brief overview previous related work irigoin triolet 4 proposed supernode partitioning technique multiprocessors 1988 idea combine multiple loop iterations order provide vector statements parallel tasks data reference locality ramanujam sadayappan 6 studied tiling multidimensional iteration spaces multiprocessors showed equivalence problem finding partitioning hyperplane matrix h problem finding cone given set dependence vectors ie finding matrix extreme vectors e presented approach determining partitioning hyperplanes minimize communication volume also discussed method finding optimal supernode size reference 7 discusses choice cutting hyperplanes supernode shape goal minimizing communication volume scalable environment includes good description tiling technique 5 optimal tile size studied different model assumptions assumed n 1 theta theta n n hypercube index space mapped p 1 theta theta p processor space optimal side lengths hypercube tile given n certain kind dependence structure 8 approach optimizing tile size shape two dimensional algorithms based spacetime mapping used systolic synthesis 2 give detailed analysis optimal supernode size shape model constant communication time compared related work optimization criterion minimize total running time rather communication volume ratio communication computation volume used different approach specify supernode transformation grain size supernodes relative side length vector r n partitioning hyperplanes three variables become independent optimal values interdependent general though hence method applied find optimal grain size uniform dependence algorithm partitioning hyperplanes optimal grain size determine optimal shape partitioning hyperplanes 7 conclusion paper find optimal supernode size shape studied context supernode transformations goal minimizing total running time general model total running time described derived closed form analytical expressions optimal supernode size one parameter model two parameter model doubly nested loops closed form expression optimal relative length vector also provided one parameter model constant bounded loop iteration space nonlinear program formulation solved numeric methods provided general cases results used parallelizing compiler distributed computer system decide grain size shape breaking task subtasks r linear scheduling close optimality modeling optimal granularity adapting systolic algorithms transputer based supercomputers supernode partitioning optimal tile size adjustment compiling general doacross loop nests tiling multidimensional iteration spaces multicom puters penultimate tiling optimal tiling automatic blocking nested loops evaluating compiler optimizations fortran new jersey time optimal linear schedules algorithms uniform dependencies independent partitioning algorithms uniform depen dencies supercompilers parallel vector computers iteration space tiling tr ctr panayiotis tsanakas nectarios koziris george papakonstantinou chain grouping method partitioning loops onto meshconnected processor arrays ieee transactions parallel distributed systems v11 n9 p941955 september 2000 jingling xue wentong cai timeminimal tiling rise larger zero parallel computing v28 n6 p915939 june 2002 maria athanasaki aristidis sotiropoulos georgios tsoukalas nectarios koziris pipelined scheduling tiled nested loops onto clusters smps using memory mapped network interfaces proceedings 2002 acmieee conference supercomputing p113 november 16 2002 baltimore maryland n koziris sotiropoulos g goumas pipelined schedule minimize completion time loop tiling computation communication overlapping journal parallel distributed computing v63 n11 p11381151 november georgios goumas nikolaos drosinos maria athanasaki nectarios koziris automatic parallel code generation tiled nested loops proceedings 2004 acm symposium applied computing march 1417 2004 nicosia cyprus r andonov balev rajopadhye n yanev optimal semioblique tiling proceedings thirteenth annual acm symposium parallel algorithms architectures p153162 july 2001 crete island greece rashmi bajaj dharma p agrawal improving scheduling tasks heterogeneous environment ieee transactions parallel distributed systems v15 n2 p107118 february 2004 georgios goumas nikolaos drosinos maria athanasaki nectarios koziris messagepassing code generation nonrectangular tiling transformations parallel computing v32 n10 p711732 november 2006 edin hodzic weijia shang time optimal supernode shape ieee transactions parallel distributed systems v13 n12 p12201233 december 2002 maria athanasaki aristidis sotiropoulos georgios tsoukalas nectarios koziris panayiotis tsanakas hyperplane grouping pipelined schedules execute tiled loops fast clusters smps journal supercomputing v33 n3 p197226 september 2005 peizong lee zvi meir kedem automatic data computation decomposition distributed memory parallel computers acm transactions programming languages systems toplas v24 n1 p150 january 2002