distributed memory parallel architecture based modular linear arrays 2d separable transforms computation framework mapping systematically 2dimensional 2d separable transforms parallel architecture consisting fully pipelined linear array stages presented resulting model architecture characterized generality high degree modularity high throughput exclusive use distributed memory control central shared memory block facilitate transposition intermediate results commonly case rowcolumn image processing architectures avoiding shared central memory positive implications speed area power dissipation scalability architecture architecture presented may used realize separable 2d transform changing coefficients stored processing elements pipelined linear arrays computing 2d discrete fourier transform 2d separable convolution presented examples performance evaluated b introduction separable transforms play fundamental role digital signal image processing nearly every problem dsp based transformation time space domain signal alternative spaces better suited efficient storage transmission interpretation estimation commonly employed 2dimensional 2d signal transforms discrete cosine fourier sine transforms dct dft dst discrete hough radon transforms dht drt known separable due special type symmetry kernel 1 separable transforms computationally less expensive nonseparable ones time complexity om per output data element opposed om 2 nonseparable forms applied size theta input image majority available vlsi implementations separable transforms based popular rowcolumn approach see example rao yip 2 guo et al 3 yp lee et al 4 bhaskaran konstantinides 5 gertner shamash 6 chakrabarti jaja 7 2d transform performed three steps 1d transformation input rows followed ii intermediate result transposition usually implemented transposition memory iii 1d transformation intermediate result columns illustrated fig 1 figure throughout paper assume image inputs become available rasterscan order continuous sequence row vectors arrays accept produce data rasterscan avoid expensive host interface memory buffering 1d row column processing blocks fig 1 may realized either highly modular structures standard pipelined arrays identical processing elements pes type architecture optimized targeted transform however one main shortcomings conventional rowcolumn architecture central memory block severely limits modularity overall structure despite possible modularity 1 row column arrays addition central memory requires moderately complex address generation circuitry row column decoders sense amplifiers depending size control coordinating concurrent access two row column architectures shared memory large central memories also negative implications lowpower design paper present method synthesizing fully pipelined modular arrays 2d separable transforms derived computational structures use smallsize fifo memories local pes require neither address generation central control memorytoarray routing faster clock speed smaller area low power achieved architectures regular set simple pes distributed memory control reducing size memory structures shown reduce power consumption 8 largely due reduced effective bus capacitances smaller memories absence sense amplifiers architectures synthesized general sense separable transform realized programming pes memory bank transposition address row processing row column raster scan input b logic column processing figure 1 conventional architecture rasterscan 2d separable transforms relying shared central memory bank transposition intermediate results b proposed alternative architecture based two linear arrays central memory bank replaced fifo queues distributed pes appropriate set kernel coefficients benefits eliminating memory transposition recognized lim swartzlander 9 wang chang 10 9 authors treat dft problem context multidimensional systolic arrays method distinct 9 primarily restricted single input si linear arrays using rasterscan io fisher kung 11 shown linear arrays bounded clock skew regardless size whereas higher 2d arrays arbitrary size may technologically feasible see lee kedem 12 discussion general aspects mapping algorithms linear arrays although dct arrays presented 10 avoid transposition means local pe storage resulting architectures require 2 multipliers theta input many cases may prohibitive arrays developed paper require multipliers another important characteristic proposed architecture uses localized communications interconnection complexity om theta separable 2d transforms best knowledge parallel architectures may use less om pes also require om inputoutput ports may exhibit nonlocal communications among pes ie interconnection complexity may grow faster om furthermore reduction number pes becomes possible exploiting specific coefficient symmetries ie architectures may compute desirable 2d separable transform something architecture developed capable addition general computational structure also derive examples arrays 2d dft 2d separable convolution although purely computational efficiency point view dft expensive fast fourier transform fft vlsi implementation point view significant reasons dft may preferable fft particularly small number coefficients ffts complex data routing limits overall speed expensive terms chip area see swartzlander 13 thompson 14 paper use 1d dft array beraldin et al 15 derive modular array structure computing 2d dft without transposition memory 2d separable convolution studied extensively many years see example dudgeon mersereau 16 used basis constructing general transfer functions abramatic et al 17 used separable fir systems constructing nonseparable iir filters treitel shanks 18 used parallel separable filters approximating nonseparable ones despite limited range frequency responses separable filters attractive due low computational cost compared nonseparable systems addi tion separable convolution plays central role discrete wavelet transform dwt 19 currently basis used 2d dwt separable rest paper organized follows section 2 fix notation define general separable 2d transforms discuss alternative views conventional rowcolumn 2d processing allow us formulate equivalent algorithms suitable raster scan io section 3 derived algorithms transformed systematically fully pipelined linear array structures distributed memory control using appropriate linear spacetime mapping operators sections 4 5 apply general method developed section 3 derive modular linear arrays 2d dft 2d separable convolution respectively transforms definitions notation 2d transform theta signal xi given notice general 2dimensional 2d transform 2 2d distinct orthonormal basis functions spanning transform space namely g g denotes complex conjugate g 1 20 separable symmetric transforms property lm hence one express yl l order formulate transforms matrix form adopt following notation let theta k matrix elements indexed zero interchangeably also denoted ai k ik furthermore let column vectors theta 1 denoted lowercase letters superscript denotes transposition adopt golub van loans 21 matlablike notation denote submatrices au v u v integer column vectors pick rows columns defining submatrix also index vectors specified colon notation implying l example 2 theta 5 submatrix containing rows 2 3 columns 5 9 matrix entire rows columns specified single colon k corresponds kth column ai denoting ith row index vectors nonunit increments specified denoting count p r lengthq increments let x transform input transform coefficient output theta matrices respectively theta matrix g separable transformation kernel whose columns 1d distinct basis vectors gi matrix notation place eqn 2 rewritten consequently separable transform computed two consecutive matrixmatrix products first matrix product written viewed operating matrix g rows input x produce rows intermediate result z second matrix product viewed operating matrix g columns z produce columns interpretation forms basis traditional rowcolumn processing method necessitates transposition z section 3 show obtained processing directly rows z order become available thus eliminating need transposition give definition raster scan ordering definition 21 given set matrix elements g say element xi precedes raster scan order element xi 0 either 0 raster scan induces total ordering input set equivalent rowmajor ordering let us also define vectors x z x j z j column vector views matrices x z respectively suitable raster scan processing means p th row z derived multiplying p th row x times g ie zp set equations expressed compactly using kronecker products 20 22 multiplying 1 theta 2 input vector x times momega g obtain z momega g equivalently matrixvector product eqn 6 alternative view eqn 4 allows us derive algorithm dimension two ie algorithm index space dimension two row processing rather dimension three implied matrixmatrix product form eqn 4 reformulating algorithms lower dimensional index spaces effective technique simplifying mapping algorithms high dimension simple efficient monodimensional linear array structures avoiding complexities multiprojection 23 intermediate result column vector z eqn 6 viewed either composed consecutive vectors zi corresponding rows matrix z vectors interspersed stride factor 21 corresponding columns matrix z given 1g adopting latter view column 1d transform eqn 5 expressed z 7 vector defined similarly x z 1 3 array synthesis framework separable transforms section elaborate space time mapping two components general 2d separable transform array one row one column processing generality treat 1d transformation matrix vector product make use additional computational savings might possible due kernel symmetries rather write three dimensional single assignment form saf 23 2 algorithm row processing suggested eqn 4 con ventional theta matrixmatrix product work eqn 6 matrixvector product nested loop algorithm eqn 6 dimension two trivially mapped linear array processing inputs rasterscan order due way constructed vectors x z 1 check recall alternate definition separable transform basis matrix size 2 theta 2 written kronecker product basis theta matrix g times see akansu 1 substituting eqn 6 eqn 7 get tomega im x using property aomega bcomega acomega bd 1 20 algorithm saf one variables assigned single value throughout entire execution statements form xi xi b allowed define g eqn 6 given algorithm 31 inputs xi initially z z k outputs z algorithm 31 defined 2d index space fi g using linear spacetime mapping methods space transformation algorithm transformed array processing elements pes efficiency 50 due block diagonal sparse structure g applying transformation index variable defined cm obtain equivalent algorithm rectangular constant bounded index space cbis 27 given 0 g new algorithm mapped linear array fully utilized 100 efficient pes localization broadcast variables complex rich problem 23 28 29 maintaining focus address details required writing algorithm 31 localized form variables common domain support rather give algorithm 32 one possible localized algorithm suitable raster scan processing note variables zi localized equivalents xk g zk respectively algorithm 32 inputs xk gi outputs zb k pictorially localized algorithm 32 represented dependence graph dg 23 shown fig 2 4 inputs xk appear along horizontal kaxis shown corresponding elements matrix x operations modeled dg node shown fig 2 b figure arrows represent data dependencies seen local among dg nodes variables x z shown figure dependence vectors associated variable g horizontal length note coefficients matrix g appear dg replicated times one term dg section using linear space map dg fig 2 projected along axis giving rise linear array fig 2 c linear time map schedule used consistent raster scan processing input stream raster scan fed pe 0 one data point per schedule time period output stream becomes available initial delay three time units also raster scan example available pe 0 note although outputs become available single pe every time instant one pe produces z value consequently outputs collected means bus connecting pe outputs 3 local pe control needed realize output partial results z algorithm 32 ie recognize interrow boundaries k 2 dg fig 2 implemented simply attaching bittag called decode bit fig 2 input data token x bit set last element every row matrix x pes accumulate locally result multiplyaccumulate operation z bit inputs set pe finds input x arrives decode bit set places result output link rather internal z register clears internal z register column transform coefficients g stored bank registers fed multiplier sequence wraparound next derive saf algorithm transformation viewed either matrix g operating column vectors z k produce column vectors ii matrix z operating row vectors g produce row vectors using latter interpretation turn equal expression follows nestedloop saf algorithm given similar algorithm 31 except fact basic multiplyadd operation type scalar times vector plus vector 4 algorithm 33 alternatively outputs also systolically pipelined along inputs x become available single pem gamma1 see beraldin et al 15 discussion output pipelining systolization dft 4 socalled saxpy operation 21 z x c row array 2bit cntr z x output internal pe structure column array input row dg b dg node figure 2 dependence graph dg transformation rows x node operations c resulting row array applying linear spacetime mapping operators along io processortime schedule internal structure processing element pe 2 inputs matrices z g initially outputs scalar multiplications one multiplication scalar g1 0 vector z1 resulting vector added vector 0 0 important aspect formulation allows us operate directly rows intermediate matrix z thereby exploiting raster scan order z becoming available row array result avoiding intermediate matrix transposition furthermore although eqn 5 matrixmatrix product defined 3d index space constructed algorithm 33 matrixvector product 2d index space variables k using vector data objects instead scalars similarly algorithm 32 algorithm 33 localized propagating variable zk along iaxis accumulating output along kaxis dg localized algorithm 33 shown fig 3 basic dg node operation consist scalar multiplications scalar additions data dependencies among dg nodes consist parallel vectors depicted one thick arc fig 3 b shows operation modeled dg node saxpy map dg shown fig 3 using space transformation applied dg algorithm 32 namely resulting column linear array shown fig 3 c scheduling dg accomplished means function mapping computation within dg node k schedule time period index picks lth element row zk schedule vector used row array example dg node depicted fig 3 b computation scheduled execution pe 1 first computation dg node namely scheduled pe 2 note timing function two resolution levels one assigns nodes dg macro time steps one assigns individual multiplyadd operations within dg node actual time steps clock cycles regularity scheduling allows simple implementation additional local pe control importantly widthm data dependencies zk pointing upwards column dg fig 3 map physical links rather single physical link suffices propagate inputs z since sequence z values produced rate one scalar token per schedule time period row array addition z tokens consumed vector array rate produced column dg b dg node i0 i1 i2 i3 2bit cntr clock4 z output pe structure c column array z row array dependencies map single physical link fifo fifo figure 3 dependence graph dg transformation dg columns z nodes represent scalarvector multiply vectorvector add c resulting column array applying linear spacetime mapping operators along io processortime schedule internal structure processing element pe 2 array 5 identical structure row array fig 2 c shown fig 3 c along io schedule difference amount memory required column array holding entire rows z rather single elements fig 3 show internal pe structure see io registers x z row array replaced lengthm fifo queues z column array respectively 31 performance characterization calculate latency pipelining period 23 proposed array fig 4 shows complete io schedule arrays row column transform size 4 two consecutive input matrices x 0 x 1 figure see arrays perfectly space time matched output stream z generated row array immediately consumed column array exact rate production consequence intermediate buffering required problem latency total time single separable transform input x 0 calculated follows assuming row array starts computation know schedule time first intermediate result produced ie 4 point one intermediate result becomes available per schedule time period hence column array requires 28 time steps processing entire stream z tokens time accepting z0 producing 3 3 general mt4 time units since first input column array becomes available latency ie total time required parallel architecture compute 2d separable transform single image considering continuous stream problems input matrices fx relevant measure performance block pipelining period ie number time steps initiations two consecutive problems 23 referring fig 4 note first last data token first input enter row array x 0 figure thus time first element next input x 1 0 may enter row array next time column array may start accepting inputs intermediate result z 1 problem instance assuming row array accept x 1 0 0 earliest possible time units later first data token second input stream becomes available column array exactly column time column array ready accept consequently resulting block pipelining period loosely speaking name second array column array vector array remind us produces result column array implementing conventional rowcolumn approach figure 1 note however array z provided raster scan order ie sequence rows columns x03 x10 x13 x20 x23 x30 x33 y03 y10 y13 y20 y23 y30 y33 x0 inputs row array x03 x10 x13 x20 x23 x30 x33 x1 inputs row array intermediate results z0 intermediate results z1 column array outputs y0 time last x0 input 1first intermediate result z1 column array figure 4 overall time schedule arrays considering two successive inputs x 0 x 1 example since single input line row array thus one token fed per time step theoretically minimum fi maximum throughput attained efficiency given ratio total number computations product latency times number processors single input efficiency 50 fully pipelined array exactly 100 implying one output becomes available every schedule time period since performance metrics depend particular schedule chosen number available tradeoffs explored instance nonsystolic schedule used column array timing function leads reduced latency 2 block pipelining period remains minimal however case outputs column becoming available simultaneously every time step collected parallel single input parallel output sipo architecture 4 arrays 2d discrete fourier transform section derive parallel architecture 2d discrete fourier transform dft special case computational structures derived section 3 therefore arrays presented similar shown figs 2 3 except using dft algorithm known goertzels dft 15 may reduce kernel coefficient storage simplify control complexity 41 goertzels dft dft 1d msample sequence xk given set coefficients dft 2d theta msample sequence xi k given clearly kernel g lm li dft separable 1d kernel g l li 1d dft array beraldin et al 15 based socalled goertzel algorithm derived applying horners polynomial multiplication rule following recursive equation equivalent 1d dft zm k denotes th dft coefficient k th recursive step 0 consider example computation dft coefficient z2 according eqn 9 given 4 x3 unraveling recursion eqn 11 z2 z2 4 x0 indeed equal dft coefficient z2 since w gamma6 4 w gamma8 1 goertzels algorithm dft coefficient computed using single twiddle factor w gammam rather twiddle factors direct application eqn 9 42 modular dft array derivation let x z input intermediate result 2 theta 1 vectors representing corresponding raster scan matrices x z section 2 examination eqn 11 reveals together definitions x z algorithm similar localized saf algorithm 32 see section formulated w gammai plays role gi k statement updating z addition takes place multiplication hence localized saf algorithm eqn 11 given algorithm 41 inputs xk w gammai 6 equation frequency index k time index outputs zb k xi k gi zi localized equivalents variables xk w gammai zk spectively dg algorithm similar structure one shown fig 2 except instead 2 g coefficients propagated lengthm horizontal dependencies along k direction complex twiddle factors propagated lengthone dependencies along kdirection also order internal pe operations reversed algorithm 41 mapped space transformation resulting row array shown fig 5 shown fig 5 b internal pe structure dft row array simpler array general transforms single complex twiddle factor always used multiplicand sort simplification exploited designing specialized multiplier particular twiddle factor example depending value w gammai using approximations w gammai factors powers two fast shiftadd accumulator used rather array multiplier dg dft column array vector version eqn 11 constructed similarly one general case shown fig 3 dg node shown fig 3 b vector dg goertzels dft column algorithm similar general one fig 3 exception twiddle factors propagating kdirection operations pes exchanged order illustrate different solution better latency performance arrays section 3 schedule column dft dg function 0 mapping computation l dg node k time period instead mapping vector dg 0 results column array shown fig 5 c internal pe structure shown fig 5 column array using schedule 0 need input fifo queues holding intermediate results z eliminated resulting total latency memory nearly one half general version section 3 schedule 0 also applicable general arrays section 3 specific neither dft goertzels formulation however realize performance gains parallel output ports needed order collect parallel one full column results every clock cycle z output c column array z x output b row array fifo figure 5 parallel architecture 2d dft 4 row array processortime io schedule bthe internal structure pe 2 c column array processortime io schedule internal pe structure 43 performance characterization latency overall architecture schedules row 0 column arrays 2 faster general architecture developed section 3 latency 2m 2 almost factor two due broadcasting intermediate results zi pes column array availability parallel output ports result entire columns final result become available simultaneously shown fig5 c times pipelining period identical array section 3 ie however smaller latency obtained broadcasting comes expense potentially larger duration time step ie reduced maximum rate run clock increases 5 arrays 2d separable convolution 51 2d convolution linear 2d convolution theta k input xi given 1 notice contrast general transform case eqn 1 summation limits convolution range region depends support functions gdelta delta xdelta delta since deal case kernel g smaller support input x summation limits defined kernels support region 7 separable filters property gi substantial difference complexity nonseparable separable filters former requiring l l k multiplications l additions per output sample latter requiring multiplications additions per sample eqn 13 implies input xi first processed rasterscan order performing convolutions rows resulting intermediate result zi final result obtained performing k 1d columnwise convolutions zi k k simplicity exposition without loss generality assume 7 method restricted small convolution kernels focus vlsi arrays relatively small number pes figure 6 shows example dg rowwise convolution rasterscan input xi k 3 figure omitted data dependence vector arrowheads indicating data flow direction broadcast variables x g included pointing upwards localized accumulation variables z contrast general transform case section 3 see fig 2 convolution slight spillover outputs row interface boundaries proposed solution let input rows separated slots enter array result need introduce control mechanism row interface management typically inefficiency incurred leaving empty computations rows schedule small order 1 8 let linear spacetime mapping operators ta given indexing processors denoting schedule time periods resulting array along io schedule shown fig 6 b every input token made simultaneously available pes arrows io schedule indicate progression partial results lead computation output z0 2 row interface control needed long padded end input row example time instants following input x0 5 two zeros inserted input stream clearing filter subsequent insertion row x1 k using notation introduced section 2 write convolution based saxpy row operations vector level dg fig 7 accepts row inputs produces row outputs filter coefficients gi 0 still scalar values internal dg node structure shown fig 7 b k one accepting arguments element row filter coefficient gi 0 subsequently adding product one partial result propagating bottom top leading computation corresponding element fig 7 c show complete 2d convolution pipelined architecture consisting two linear array stages first shown left called row array processing dg fig 6 followed another linear array stage called column array processing vector dg fig 7 pe 8 alternative spacing rows slots introduce control tags carried last first elements every row resulting perfect row pipelining however efficiency improvement perfect row pipelining solution presented negligible row 0 row 1 row 2 b figure rowwise 1d convolution xi resulting zi k 3 note rows spaced slots input b array rowwise convolution processortime snapshots array operation yi0 yi1 yi7 b fifo fifo figure 7 column 1d convolution dg vector form dark nodes represent scalar computations thick edges represent data dependencies among vector variables b internal structure dg node 0 c parallel architecture computing 2d separable convolutions first stage linear array performs row second column processing respectivelyd internal row column array pe structure identified two indices p q q indicates whether pe row processing column processing space transformation column array timing function mapping computation schedule time period given choice timing function implies pe 0 1 serialize execution k scalar operations modeled dg node 0 following 2g two arrays perfectly space time matched order corresponds exactly order outputs produced pe 00 row array see fig 6 result intermediate output broadcasted column array pes via bus soon produced need introducing additional control memory interface two pipelined array stages figure 7 shows internal structure pes arrays every schedule time period pe pq performs following three operations multiplication value input bus local filter coefficient b addition product incoming partial result c propagation result pe pgamma1q regards partial result accumulation column array since every dg node produces outputs also number registers fifo queues required every pair pes see fig 7 c note however buses broadcasting well links moving partial results yi fifos carry scalar quantities bitwidth one word depend k l 52 performance characterization latency memory total computation time single input problem instance xi k follows row array computation time 9 column array computation time since communication structures perfectly pipelined delay one time unit producing first output y0 0 result total computation time vector 1 example consider typical numbers image processing result 966 efficiency loss due imperfect row pipelining efficiency 983 11 relatively small efficiency loss result arrays virtually control circuitry alternatively efficiency loss rows requiring processing time mk lk gamma 1 insertion zeros rows cause steps total number computations since 2l processors efficiency given could reduced introducing control pelevel managing interfaces rows row array opposed insertion zeros block pipelining period input sequence fx implying complete output yi k available every fi time steps substantially improving efficiency efficiency case 1024 next prove proposition lower bound memory architecture processes inputs raster scan order show 2d convolution array proposed nearly attains lower bound proposition shows array require larger memory storage conventional transposition memory solutions result holding also structures presented sections 3 4 following io model draw boundary around circuit consideration assume input stream xi k crosses boundary raster scan rowmajor order 12 input entered circuit held local storage used computation last time discarded assumed input xi k appeared input stream circuit cannot request xi k host manage throughout lifetime analysis similar 30 proposition 51 lower bound memory 2d convolution separable ltap nonseparable filters rowscan input proof consider output yi k eqn 12 nonseparable eqn 13 separable know computation yi requires set l 2 input elements l1g rowscan order imposed def 21 last element set enter circuit boundary xi k minimum time xi spends inside boundary calculated looking last output element requires xi computation shown yi 1 making assumption circuit operating perfect pipelining hence one output element available every single time period minimum time needs spend inside boundary rows finally time least k new input elements entered boundary therefore minimum memory lower bounded storage elements 2 total memory requirements proposed convolution array fig 7 c dominated column fifos accounting total k memory registers meeting exactly lower memory bound proposition 51 addition total 2l registers locally storing filter coefficients could reduced l corresponding pes 12 derivation also holds substitute columnmajor rowmajor order long consistent array share coefficient l registers accumulation zi results row array array proposed section 4 also meets lower memory bound case global transform ie dft equal support regions kernel input proposition 51 reduces trivial case storage entire theta frame required 6 conclusions paper shown basis constructing modular architectures 2d separable transforms array 1d transform resulting overall architecture reflect modularity array restriction required 1d transform array accepts input generates output raster scan order regular architecture consisting two fully pipelined linear array stages shared memory derived based key idea dg first linear array performing row processing structurally equivalent dg second linear array column processing difference latter dg operates coarser level granularity vector instead scalar computations idea exploited even separable transforms complex computation structure since principle avoiding transposition shared memory depend 1d transform structural details rather depends raster scan order 1d transform ii change abstraction level scalar vector operations 2d dft 2d convolution used case studies demonstrate generality effectiveness proposed architecture conjecture method used derive pipelined array structures multidimensional separable transforms dimension also applied methodology discrete wavelet transform timescale transform represents finite energy signals set basis functions shifts translates single prototype mother wavelet modular arrays computing 1d dwt proposed literature 24 25 26 2d solutions usually rely transposition form complex data routing mechanism destroys modularity since current applications research wavelets based separable fir wavelet filters able apply results section 5 1d arrays presented 24 25 dependence graphs spacetime transformation pairs derived supplied input rapid prototyping tool dg2vhdl developed group 31 32 generate automatically high quality hardware description language models individual pes overall processor array architectures generated vhdl models used perform behavioral simulation array synthesized using industrial strength behavioral compilers synopsys 35 rapidly generate hardware implementations algorithm targeting variety devices including increasingly popular field programmable gate arrays 33 34 application specific integrated circuits asics using ideas presented modular linear array cores 2d discrete cosine transform dct supporting variety different io patterns synthesized compared 36 r multiresolution signal decomposition discrete cosine transform algorithms advantages applica tions novel cordicbased array architecture multidimensional discrete hartley transform costeffective architecture 8x8 twodimensional dctidct using direct method image video compression standars vlsi architectures multidimensional fourier transform processing vlsi architectures multidimensional transforms lowpower video encoderdecoder using wavelettsvq conditional replenishment multidimensional systolic arrays multidimensional dfts highly parallel vlsi architectures 2d dct idct computations synchronizing large systolic arrays synthesizing linear array algorithms nested loop algo rithms vlsi signal processing systems fourier transforms vlsi efficient onedimensional systolic array realization discrete fourier transform multidimensional digital signal processing design 2d recursive filters separable denominator transfer functions design multistage separable planar filters parallel algorithms architectures discrete wavelet transforms fundamentals digital image processing matrix computations algorithms discrete fourier transform convolution vlsi array processors synthesis regular vlsi architectures 1d discrete wavelet transform distributed memory control vlsi architectures 1d discrete wavelet transform 1d discrete wavelet transform data dependence analysis synthesis distributed memory control array architectures time mapping uniform dependence algorithms lower dimensional processor arrays systolic array implemenation nested loop pro grams synthesizing systolic arrays control signals recurrence equa tions computational aspects vlsi dg2vhdl tool facilitate high level synthesis parallel processing array architectures dg2vhdl tool facilitate synthesis parallel vlsi architectures using dg2vhdl synthesize fpga implementation 1d discrete wavelet transform synthesis array architectures block matching motion estimation design exploration using tool dg2vhdl behavioral synthesis design synthesis maximum throughput parallel array architectures realtime image transforms tr vlsi array processors vlsi architectures multidimensional fourier transform processing synthesizing linear array algorithms nested loop algorithms fundamentals digital image processing discrete cosine transform algorithms advantages applications vlsi architectures multidimensional transforms behavioral synthesis vlsi signal processing systems image video compression standards multiresolution signal decomposition multidimensional digital signal processing time mapping uniform dependence algorithms lower dimensional processor arrays synchronizing large vlsi processor arrays parallel architectures algorithms discrete wavelet transforms