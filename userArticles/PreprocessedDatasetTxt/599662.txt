dual formulation regularized linear systems convex risks paper study general formulation linear prediction algorithms including number known methods special cases describe convex duality class methods propose numerical algorithms solve derived dual learning problem show dual formulation closely related online learning algorithms furthermore using duality show new learning methods obtained numerical examples given illustrate various aspects newly proposed algorithms b introduction consider statistical learning problem find parameter random observations minimize expected loss risk given loss function lff x n observations independently drawn fixed unknown distribution want find ff minimizes expected loss x z assumption underlying distribution x natural method solving 1 using limited number observations empirical risk minimization erm method cf 13 choose parameter ff minimizes observed riskn specifically consider following type regularized linear system convex loss shall assume f g convex functions appropriately chosen positive regularization parameter balance two terms typically choose gw 0 function penalizes large w formulation naturally arises many statistical learning applications example order apply formulation linear regression let however formulations also interests example robust estimation one interested using order apply formulation purpose training linear classifiers choose f decreasing function fdelta 0 examples include support vector machines logistic regression one interesting theoretical result vc analysis concerning primal formulation dimensional independent generalization error obtained 14 vc analysis extended 16 however turns careful nonvc analysis 17 actually suitable particular problem primal form dual form introduce later paper paper study numerical learning aspects dual form 3 certain attractive properties compared primal formulation duality certain learning problems general gametheoretical sense also lead discovery new learning methodologies paper organized follows section 2 derive dual formulation 3 propose relaxation algorithm solve dual problem section 3 generalizes derivation include equality constraints section 4 provides applications proposed algorithms machine learning section 5 provide numerical examples section 6 study learning aspect dual formulation conclusions final remarks made section 7 dual formulation since 3 convex programming problem involving linear transformations primal variable w dual form obtained introducing auxiliary variables data point sup n kdelta dual transform fdelta assume f lower semicontinuous see 11 well known k convex switching order inf w sup valid minimax convexconcave programming problem proof interchangeability ie strong duality given appendix obtain n w minimizes 5 fixed n use rg denote gradient g respect w note main body paper assume gradient convex function exists required case constraints need dealt introducing appropriate lagrangian multipliers section 3 main reason treatment numerical considerations however mathematical proof strong duality appendix direct numerical consequence use generalized definition convex functions duality 11 without assuming differentiability example general case constraint convex function cz regarded modification cz cz 1 z satisfies constraint substituting 6 5 obtain n st rgw n simplify notations consider dual transform gdelta hdelta also convex function follows thus rewrite 7 following dual formulation n optimal solution w primal problem obtained n system nondegenerate also see section 3 gives following solution n comparison also following equation 4 f differentiable note derived equation 8 using two legendre transforms k dual f h dual g write x matrix data x row legendre transformation point view dual transform xw respect f k dual transform w 1 respect g h consequently regard x interaction dual variables w interaction linear interaction simplest possible interaction w paper propose following generic relaxation algorithm solve dual problem 8 denote dimension w algorithm 1 dual gaussseidel find delta approximately minimizing update v n delta update inner iteration algorithm 1 algorithm essentially fixes dual variables j j 6 find perturbation delta dual variable reduce objective function 8 since objective function reduced step algorithm always converge exact optimization used converges true optimal solution 8 also hessian 8 exists wellbehaved around optimal solution 8 locally approximated quadratic function follows asymptotic convergence rate algorithm 1 linear minimized exactly 4 rate convergence depends spectrum hessian matrix 8 optimal solution main attractive feature algorithm 1 simplicity one might also consider preconditioned conjugate gradient acceleration 4 algorithm 1 preconditioner however many interesting cases kdelta nonsmooth containing simple bounds hence cg might help interesting aspect dual formulation dual variable corresponds data point x algorithm 1 step one looks one data point similar online update algorithms difference algorithm 1 keep dual variable computed far typical online algorithm information kept actually difficult convert algorithm 1 online learning method setting use appropriate scaling factor n step minimax style mistake bounds also derived accordingly however fullscale study related issues left another report paper shall provide simple analysis mistake bounds empirical risk estimator appendix b illustrates basic idea demonstrates theoretical connection dual batch formulation online learning note primal form 3 dual form 8 striking similarity besides connection online algorithms several reasons interested studying dual form example later see paper g quadratic regularization function algorithm 1 especially simple form another reason f nonsmooth g smooth onedimensional optimization problem dual gaussseidel algorithm easier onedimensional optimization problem primal gaussseidel algorithm furthermore even primal problem infinite dimensional infinity dual problem still finite dimensional assuming finite sample size elimination primal dimensionality important generalization performance consequences machine learning see section 6 section 7 finally dual form 8 relationship 9 generalizes kernel form svm originally investigated vapnik 14 constraints order dual formulation 8 valid equation 9 solution however certain circumstances possible 8 singular hence solution right hand side lies range rgw w 2 r imposes constraint x x matrix row consisted data x simplicity shall discuss case equality constraint inequality constraints handled similarly since original problem convex optimization problem therefore dual problem also convex optimization problem implies equality constraint dual variable must linear constraint situation usually arises gw flat linear transformation w c matrix vector since crgw j therefore 9 corresponding constraint imposed dual variable following form equation 10 modified n order preserve structure dual formulation 8 employ gaussseidel update algorithm 1 propose use augmented lagrangian method 3 method modifies dual hv primal regularization term gw 8 lagrangian multiplier vector corresponding constraint 13 14 small positive penalty parameter following algorithm solves 8 constraint 13 utilizing modified dual h cf 3 page 292 algorithm 2 dual augmented lagrangian solve 8 hdelta replaced h delta using algorithm 1 use current v initial point update v else deltas one nice property augmented lagrangian approach augmented dual problem solved algorithm 1 sufficiently high accuracy step 4 executed finitely number times means upon termination bounded away zero next would like briefly discuss situation primal problem 3 convex constraint cw 0 component cdelta convex function let corresponding lagrangian multiplier primal problem rewritten well known 0 3 component c j w 0 indicates cw convex thus regarded regularization term similar gw select appropriate lagrangian multiplier essentially solve problem regularization term gw replaced gw learning problems exact value lagrangian either known example entropy regularized probability distributions investigated later noncrucial since appropriate way determine lagrangian regularization parameter kind crossvalidation anyway also many cases constraint primal variable w becomes flat segment dual variable cases apply unmodified algorithm 1 compute relationship obtain w modified accordingly regard primal inequality constraint regularization term observe parameter increases corresponding c j w decreases optimal solution thus also possible use algorithms suggested paper inner solution engine adjust j appropriately examining c j w optimal solution idea similar modified lagrangian method propose deal dual constraint 13 although modification augmented lagrangian method applied many cases usually recommended due variety reasons subtle methods employed however due limitation space also issue noncrucial learning problems shall skip discussions 41 useful dual pairs section list examples convex legendre dualities relevant learning problems use pu denote primal function primal variable u use qv denote dual function dual variable v 1 assume k symmetric positive definite operator 2 dual pairs 1p 3 dual pairs 1p dual pairs 4 set positive prior 5 note pu flat dimension pu vector identical components therefore v satisfies constraint dual transformation contains free lagrangian parameter 7 0 nonnegative lagrangian parameter 8 9 dual pairs 1p note many examples contain constraints dual also primal variables case also ignore constraint consider corresponding function constraint satisfied last examples interests relevant classification problems also useful note dual linear transform either primal variable primal convex function easily computed assume nonsingular linear transformation 42 regularization term section briefly discuss regularization conditions 3 interests square regularization one important regularization conditions square penalty k symmetric positive definite operator many applications one choose identity matrix case hence algorithm 1 replaced minimizing note system 16 particularly simple actually reduced small problem constant size solved constant time inner product b computed large problems computation b actually precomputed dominates pnorm regularization let interests data qnorm bounded since x w bounded generalization performance also dimensional independent 16 17 case approximate newtons method employed solve newtons iteration lambda usually one may use one newtons iteration need compute derivative part second derivative part ij many cases computation costly evaluation b square regularization formulation however tricks employed alleviate problem since accurate estimate second derivative part ij less important safely use good upperbound second derivative note essentially constraint v order obtain w equation 10 modified similarly essentially constraint regularization term given interests data 1norm entropy bounded typically let large number case approximate newtons method required derivative part requires evaluation addition second derivative part requires evaluation ij entropy regularization usually interests either data infinitenorm bounded weight vector w gives probability distribution case unlike using 1norm regularization condition leads generalization performance degrading logarithmically dimension generalization performance entropy regularization dimensional independent 16 17 first consider normalized entropy probability distributions prior distribution dual choose lagrangian parameter derivative term hv second derivative term hv n n therefore two terms correspond expectation variance x distribution w note second derivative variance vanishes small positive number used regularize solution consider nonnormalized entropy positive weight w one easily deal general situation adding negative weight part w dual newtons approximation straightforward remarks practice absolutely reason need choose regularization term based simple primal form perfectly reasonable highly recommended design learning algorithm based simple choice dual function hv whether dual gw complicated irrelevant far dual algorithms concerned example consider dual hubers function simple form hv 2 v 2 jvj 1 solved relatively easily regularization condition gw good replacement 1norm regularization general one choose hv piecewise linear quadratic function would like w concentrated around 0 appropriate form hv also shape concentrated around 0 however uncertainty principal freedom dual dimension inversely proportional freedom primal dimension easily seen case square regularization since kernel k gamma1 dual problem inverse kernel k primal problem hand would like design algorithm w biased toward nonzero prior case entropy regularization one needs use hv monotone increasing function easily constructed using piecewise quadratic functions 43 loss term regression first consider standard squareloss regression regularization term 15 line search step algorithm 1 solved analytically robust estimation interested case regularization term 15 line search step algorithm 1 solved analytically robustness naturally explained dual update since bounded therefore contribution one data point final weight w well control principle used directly design dual form robust regression without resorting primal form distribution estimation consider modification maximumentropy method given normalized entropy regularization 19 formulation becomes maximumentropy method 0 however conjecture modification advantage standard maximumentropy method reason softmargin svm formulation often preferred optimalseparating hyperplane method practice sense choosing nonzero regularization parameter svm theoretical advantage appropriate nonzero regularization parameter demonstrated 17 even linearly separable classification problems possible achieve exponential rates convergence appropriate nonzero regularization param eters similar results known optimalseparating hyperplane method fact conjectured 17 generalization performance optimalseparating hyperplane method slower exponential worst case case modified maximumentropy method newtons update corresponding line search step algorithm 1 given note 0 standard maximumentropy method update fails reason quadratic penalty method becomes illconditioned penalty parameter close zero easy remedy use modified lagrangian method already discussed likelihood consider mixture model estimation problem want find distribution w loglikelihood maximized case normalized entropy regularization 19 note initial value set zero anymore since shall thus start positive value 1 newtons update corresponding line search step algorithm 1 given w corresponds nicely behaved probability density vector random variable r n instead entropy regularization 2norm based regularization appropriate kernel fisher kernel also utilized next consider logistic regression classification problems derived maximumlikelihood estimate given 15 case newton step corresponding algorithm 1 becomes start nonzero initial value 2 0 1 example 05 good choice binary classification binary classification typically choose nonnegative decreasing f f0 0 one example logistic regression investigated section examine examples let choose gw 15 case exact analytical solution obtained fu replace square regularization condition normalized entropy regularization newtons update becomes shall chosen based class label maximize margin see section 5 x shall data vector inclass member negative data vector outofclass member algorithm corresponds normalized winnow exponentiated gradient update 9 positive weights way support vector machine corresponds perceptron update 10 better understand observe perceptron update corresponds square norm regularization winnow update corresponds entropy regularization example see proofs methods 5 comparison exponentiated gradient versus gradient descent 8 optimal margin svm linearly separable problems modifies perceptron minimizing 2norm margin constraint corresponds minimization entropy margin constraint winnow exponentiated gradient family algorithms softmargin svm modifies optimal margin method introducing decreasing loss function f square regularization 3 case winnow algorithm translates choice entropy regularization soft margin svmlike loss function f update rule addition use forms fu standard one used standard svm formulation discuss shortly algorithmic point view relationship svm perceptron algorithm mentioned 12 chapter 12 platt compared svm dual update rule related 29 perceptron update algorithm analogy 28 winnow exponentiated gradient family algorithms readily observed due normalized exponential form rhdelta shall noted many discussions exponentiated gradient algorithms emphasized proper matching loss functions see 6 example formulation careful choice matching loss replaced proper regularization condition combined loss function also notice nonnormalized entropy used regularization condition obtain algorithm corresponds nonnormalized exponentiated gradient update svm choose f let gw given 15 update exact solution mentioned platt derived update rule 29 12 however derivation employs bregmans technique 1 suitable specific problems particular technique convex duality adopted paper generalizes bregmans approach also platts comment 29 maximize margin true fact found statistically significant difference method standard svm applications although special care taken avoid potential zerodenominator problem see discussion end section 5 implementation considerations shall also interesting mention 7 authors already adopted update 29 applications worthwhile mention standard svm shift b non regularized absorb b last component w appending constant feature typically 1 regularization condition gw symmetry gw c vector one last component zero elsewhere case algorithm 2 employed corresponding dual constraint 13 exact solution modified introduction nonregularized shift b standard svm formulation created complexity optimization although significantly illustrated section 5 also seems adverse effect generalization analysis logn factor note expected generalization performance o1n full regularization see section 6 17 cannot achieved nonregularized dimension well known nonregularized dimension vcdimension 1 contributes logarithmic factor generalization performance term o1n worst case tight example see 14 however increased variance may compensated potentially reduced bias practical difference using nonregularized shift versus regularized shift unclear 5 experiments goal section illustrate proposed algorithms examples reader develop feeling convergence rate proposed algorithms context existing algorithms due broad scope paper theoretical nature illuminating provide examples every specific instance learning formulations mentioned section 4 thus provide two examples one algorithm 1 one algorithm 2 instances algorithms similar behaviors since research interests mainly natural language processing shall illustrate algorithms text categorization examples standard data set comparing text categorization algorithms reuters set news stories publicly available httpwwwresearchattcomlewisreuters21578html use reuters21578 modapte split partition documents training validation sets data contains 118 classes 9603 documents training set documents test set experiments use word stemming without stopword removal also use information gain criterion 15 select 1000 informative wordcount features use binary values selected features indicates word either appears appears document text categorization performance usually measured precision recall rather classification error positive true positive positive theta 100 positive true positive negative theta 100 since linear classifier contains threshold parameter adjusted tradeoff precision recall conventional report breakeven point precision equals recall since document reuters dataset multiply categorized common study dataset separate binary classification problems corresponding category overall performance measured microaveraged precision recall breakeven point computed overall confusion matrix defined sum individual confusion matrices corresponding categories following examples use top ten categories remaining categories typically small experiments done pentium ii 400 pc linux timings include training using training data feature selection testings test data parsing documents winnow versus regularized winnow example study performances regularized winnow corresponding update 28 standard winnow algorithm positive weights use learning rate 0001 standard winnow algorithm approximately optimal example prediction wrong shrink weight expgamma0001x data x class multiply weight exp0001x data x class prediction rule w x implies outofclass w inclass w normalized predefined parameter regularized winnow algorithm 28 let outofclass data sign data reversed training phase ff parameter attempts maximize decision margin inclass outofclass data fixed 01 examples illustration intuitively corresponds let starting point weight update inclass data ff starting point weight update outofclass data goal achieve margin size 2ff regularization parameter 28 fixed 10 gamma4 run algorithms 100 iterations data comparison note kwk data x f0 1g components thus w x 1 also taken sparsity data account reasonable us pick 03 reflect good range threshold choice 01 microaveraged breakeven point winnow 830 cpu time 23 seconds microaveraged breakeven point regularized winnow 863 cpu time 26 seconds 03 microaveraged breakeven point winnow 816 cpu time 23 seconds microaveraged breakeven point regularized winnow 850 cpu time 26 seconds better feeling reported timing one notes c45 decision tree inducer even 500 features 1000 features cannot handled take hours finish training partly sparse structure cannot utilized smo algorithm svm one fast text categorization algorithms 2 requires three minutes table 1 shows breakeven points algorithms ten categories 01 regularized winnow consistently superior difference statistically sig nificant comparable difference svm perceptron algorithms indicated table 2 category winnow regularized winnow acq 828 854 moneyfx 587 628 grain 784 847 crude 781 787 trade 653 711 interest 617 725 ship 637 750 wheat 764 831 corn 655 804 microaverage 830 863 table 1 breakeven points winnow versus regularized winnow although results good svm achieves state art performance text categorization comparison unfair since allow positive weights implementation winnow style algorithms indicates try find indicative words particular topic ignore words even though appearances document may strongly suggest document belong topic utilize additional features need regularized version standard winnow algorithm positive negative weights conversion obtained vector version duality formulation weight w matrix dual data point vector since extension investigated work shall thus skip comparison perceptron versus svms example compare perceptron algorithm proposed svm method 30 smo method svm described chapter 12 12 currently preferred method solving svm problem textcategorization 2 regularization parameter svms fixed 10 gamma3 learning rate perceptron 0001 prediction wrong update weight data x class 0001x data x class prediction rule w x 0 implies outofclass w x 0 implies inclass also normalize weight kwk end iteration data enhances performance total running time perceptron 100 iterations training data 13 seconds faster speed compared winnow algorithm indicates winnow algorithms spend time normalization step 1 proposed formulation 30 exactly implemented described use 50 iterations call algorithm 1 4 iterations algorithm 2 therefore totally use 200 iterations training data running time 33 seconds smo algorithm tricky implement made best judgment tradeoffs among internal parameters suggestions given 12 starting point running time particular implementation 191 seconds table 2 includes comparison breakeven points three algorithms ten categorizes microaveraged breakeven points observed svms consistently better perceptron algorithm however statistically insignificant random discrepancies among svms due different convergence criteria shall also useful point j order 10 gamma6 end algorithm 2 ten categories implies dual constraint largely satisfied results similar svms microaveraged breakeven 91 obtained using 27 100 iterations algorithm 1 total run time 19 seconds case constant feature value 1 appended data point category perceptron smo algorithm 2 acq 924 955 950 moneyfx 687 698 754 grain 879 913 886 crude 810 815 841 trade 686 735 735 interest 634 748 771 ship 820 820 809 wheat 778 831 847 corn 772 875 839 microaverage 885 910 912 table 2 breakeven points perceptron versus svms would like mention although examples faithfully implemented suggested algorithms important stick specific formulations given paper example magic reasons starting value 50 decrease constraint value update 4 algorithm 2 also recommended use update value delta smaller quantity implementation winnows normalized frequently inner iterations avoid numerical instability suggested newtons method gain better numerical stability example observed algorithm 1 27 29 useful use small fraction exact deltaupdates given equations especially important 29 since denominator x zero causes large change one replace regularized version 1 smaller increment alleviates problem heavy oscillation phenomenon less problem 30 since small gives large denominator 1 6 learning consequences differentiable convex function h define distance function h w w 0 also called bregman divergence 1 property h w w strictly convex h w w consider n samples x n 1 x let partition n samples batches subsamples batch xi containing n data let denote optimal solution 8 1 let denotes solution approximate minimizing 8 ith batch subsamples denotes empirical expectation respect ith batch n samples 8 ffl positive approximation error controlled practice checking duality gap primal problem 3 let restraint ith batch subsamples n h h h note first order condition 8 optimal solution implies following estimation equation another form 11 thus obtain following inequality similar analysis primal problem 17 shall ignore contribution simplicity thus obtain following fundamental inequality dual shall define v x 33 bounds convergence v v terms hbregman divergence convergence v also interesting compare inequality primal form 17 approximate empirical risk minimizer w n primal problem satisfies g w w using duality bregman divergence 17 relationship 11 note 33 gives better constants fact asymptotically tight constants compared asymptotic estimates given 16 17 although learning bounds primal formulation investigated 17 insightful study implications terms dual variable one bound given vapnik term number supportvectors ie number nonzero dual variables see 14 although vapniks bound interesting two fundamental drawbacks one drawback bound asymptotically correct sense gives expected generalization error bound slower o1n n samples since number support vectors usually grow unbounded however demonstrated 17 expected generalization error grow rate o1n general case furthermore misclassification error training linear classifier problem linearly separable exponential n note later situation clearly vapniks bound intended therefore bound asymptotically far inferior correct rate drawback vapniks bound handles situation number support vectors small although indication number support vectors large generalization performance becomes poor bound tends lead statistical learning community towards thinking obtain good generalization performance somewhat desirable reduce number support vectors example clearly case design support vector machines however analysis leads correct large sample convergence rate indicates minimizing number support vectors important addition drawbacks support vector concept also characterization learning problems predictably lead small number support vectors therefore nontrivialness bound solely relies empirical evidences entirely satisfactory theory goal section study learning aspect dual problem carefully characterize generalization performance terms dual variables analysis complements primal analysis studied 17 let n 1 33 x n 1 chosen way approximates underlying distribution let denote optimal solution limit regarded random variable respect shall assume solution continuous version 8 exists assume n simplicity denote n taking expectations ex nover n randomly chosen data x n 1 obtain denotes empirical estimation data x n 1 n denotes approximate solution 8 error ffl 31 random variable usually wellbehaved continuous case estimation equation 32 given relationship 11 derivative exists subgradient otherwise see appendix w optimal solution primal problem note right hand side 34 becomes convergence empirical expectation random variable mean estimated standard probability techniques illustrated 17 typical h squarelike right hand side variancelike thus converges rate o1n detailed case studies given 17 shall repeat addition expected hbregman divergence also obtain expected exponential lim using independence assumption x n 1 taking limit obtain 8ff 0 used obtain large deviation type exponential probability bound moment bounds also similarly obtained see 17 examples 34 36 obtain expected generalization error bound large deviation style probability bound using techniques 17 pointed earlier primal estimates dual estimates rather similar constant factor 2 shall regard nonsignificant purpose deriving generalization bounds therefore shall repeat analysis examples given 17 order see expected error bound 33 cannot improved consider 34 linear loss quadratic regularization case solving primal problem obtain kex dual problem j b becomes equality since locally g usually approximately quadratic regularization f linear loss b x dependent shall intentionally remove quadratical term f expansion corresponds dropped k term 33 therefore asymptotically 33 tight 7 concluding remarks paper introduced dual formulation class regularized linear learning methods convex risks new formulation related online learning algorithms stochastic gradient descent methods dual form also leads generalization kernel formulation support vector machines kernel formulation one usually substitute primaldual relationship 10 primal formulation 3 approach emphasizes completely different dual risk 8 dual formulation leads new learning algorithms well new insights certain learning problems intermediate primaldual formulation related minimax online mistake bound framework however pose problem pac style batch setting relationship thus bridges online learning mistake bound analysis pac analysis numerical point view able obtain new batch learning methods dual formulation importantly methods derived systematic way mentioned possible transform derived batch learning methods online learning algorithms shall leave another report appendix b demonstrates basic ideas obtaining mistake bounds using 33 natural ask general duality machine learning primaldual formulation views learning problem gametheoretical setting learner chooses primal weight variable w minimize certain risk opponent chooses dual variable controls random sample behavior maximize risk although strong duality convexconcave programming difficult extend general problems gametheoretical point view still adopted conjecture even strong duality violated may still possible design dual formulation appropriate learning method however many open issues area requiring future study also studied learning aspect dual formulation 8 specifically able obtain generalization bounds better asymptotically tight constants primal analysis 17 interesting consequence dual analysis evidence number nonzero dual variables support vectors significance may surprising since number stable even slight perturbation loss function generalization performance bound based number support vectors asymptotically suboptimal especially linearly separable classification problems bound intended dual formulation also provides valuable insights learning problems exam ple since number dual variables always independent dimensionality primal problem therefore appropriate regularity assumptions dimensionality primal problem appear learning bounds suggests frequently mentioned notion curse dimensionality really issue many learning problems might seem surprising first understand implications consider high dimensional density estimation problem standard argument curse dimensionality high dimension order approximate density function one needs fill number points box exponential dimensionality however reasoning partially valid appropriate way measure whether two distributions similar comparing expectations bounded function respect two distributions therefore instead pointwise convergence criterion shall consider convergence weak topology closeness two densities measured closeness actions set test functions implies proper question ask given fixed number data many test functions utilize obtain stable density estimate methodology used maximum entropy method note appropriate regularity conditions number test functions use dimensional independent however since choose weak topology density estimate need determine topology test functions given family test functions need find appropriate structure hierarchy approximate family proper norm always defined test functions dimensional independent partition obtainable induced dual metric density used measure convergence density estimate example typical partition bounded test function based logexponential criterion induces entropy metric density space see analysis 17 duality test functions density weights essentially duality investigated paper furthermore method measuring complexity learning problem dual vc point view directly measures complexity parameter space example density function estimated advantage dual point view rely specific parametric function family one chooses importantly many learning problems complexity measured convergence rates random test functions means test space see 17 indicates wellestablished probability tools applied obtain complexity measurement dimensional independent proof strong duality would like show valid interchange order inf w sup primaldual formulation 4 assume gw well functions involved analysis take value 1 equivalent dataindependent constraint also assume solution primal problem exists unique simplicity notational purposes let goal show exists sup wellknown example see 11 duality gap sup 37 valid find w case w called saddle point following demonstrate existence saddle point construction consider w minimizes primal problem known 11 page 264 exist subgradients generalization gradient concept convex functions see definitions 11 section 23 denoted delta 2 thatn readers familiar convex analysis equation becomes following estimation equation empirical risk differentiable wn n relationship subgradients duality see 11 page 218 w achieves minimum n also definition gamma subgradient fdelta relationship subgradients duality 11 page 218 achieves maximum finishes proof conventionally subgradient set denoted delta however notational convenience use delta denote member subgradient set proof online mistake bounds consider following scenario given samples primal variable w weight obtained exact solution dual problem 8 v dual v projection v onto first v projection v onto ith sample case 33 proof 33 inequality fundamental bridge online learning formulation batch dual formulation referred paper see inequality important note h square like distance usually true right hand side order o1k 2 implies kd h v small standard online learning telescoping technique applied derive mistake bounds case done fashion similar parallel examples studied 17 however general case treated another dedicated report following shall merely provide illustration using square regularization term 15 draw conclusion v k w v k make assumption bregman divergence l square like true smooth loss functions using estimation equation 32 v k summing v k gives typical mistake bound correct growth order logn standard assumptions example assume c c vs k gamma1 norm bounded b well known logarithmic growth optimal see consider onedimensional easy verify let x large k w hence growth rate logn achieved simply summing equality k note logarithmic factor indicates correct batch learning rate o1n cannot obtained typical randomization technique example see 8 modifying online algorithms mistake bounds batch algorithms also important note matching loss function concept cf 6 important analysis allows us analyze problems loss function role matching loss function corresponds proper choice regularization term analysis data dependent rather loss function dependent r relaxation method finding common point convex sets application solution problems convex programming inductive learning algorithms representations text categorization practical methods optimization matrix computations general convergence results linear discriminant updates relative loss bounds single neurons discriminative framework detecting remote protein homologies additive versus exponentiated gradient updates linear prediction learning quickly irrelevant attributes abound new linearthreshold algorithm mit press convex analysis smola editors advances kernel methods estimation dependences based empirical data statistical learning theory comparative study feature selection text categoriza tion analysis regularized linear functions classification problems primal formulation regularized linear systems convex risks tr ctr fred j damerau tong zhang sholom weiss nitin indurkhya text categorization comprehensive timedependent benchmark information processing management international journal v40 n2 p209221 march 2004 tong zhang fred damerau david johnson text chunking based generalization winnow journal machine learning research 2 312002 zhang yue pan tong zhang focused named entity recognition using machine learning proceedings 27th annual international acm sigir conference research development information retrieval july 2529 2004 sheffield united kingdom tong zhang leaveoneout bounds kernel methods neural computation v15 n6 p13971437 june ron meir tong zhang generalization error bounds bayesian mixture algorithms journal machine learning research 4 1212003 tong zhang covering number bounds certain regularized linear function classes journal machine learning research 2 p527550 312002 qiang wu yiming ying dingxuan zhou multikernel regularized classifiers journal complexity v23 n1 p108134 february 2007 ron meir gunnar rtsch introduction boosting leveraging advanced lectures machine learning springerverlag new york inc new york ny