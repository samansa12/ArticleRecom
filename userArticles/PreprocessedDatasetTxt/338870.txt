timing analysis data wraparound fill caches contributions paper twofold first automatic toolbased approach described bound worstcase data cache performance approach works fully optimized code performs analysis entire control flow program detects exploits spatial temporal locality within data references produces results typically within seconds results obtained running system representative programs presented indicate timing analysis data cache behavior usually results significantly tighter worstcase performance predictions second method deal realistic cache filling approaches namely wraparoundfilling cache misses presented extension pipeline analysis results indicate worstcase timing predictions become significantly tighter wraparoundfill analysis performed overall contribution paper comprehensive report methods results worstcase timing analysis data caches wraparound caches approach taken unique provides considerable step toward realistic worstcase execution time prediction contemporary architectures use schedulability analysis hard realtime systems b introduction realtime systems rely assumption worstcase execution time wcet hard realtime tasks known ensure deadlines tasks met otherwise safety controlled system jeopardized 18 3 static compiler information control flow configurations idcache interface user analyzer timing timing predictions addr info relative data decls virtual address information cache simulator categorizations idcaching user timing requests address calculator source files dependent machine information figure 1 framework timing predictions analysis program segments corresponding tasks provides analytical approach determine wcet contemporary architectures complexity modern processors requires toolbased approach since ad hoc testing methods may exhibit worstcase behavior program paper presents system tools perform timing prediction statically analyzing optimized code without requiring interaction user work presented addresses bounding wcet data caches wraparoundfill mechanisms handling cache misses thus presents approach include common features contemporary architectures static prediction wcet overall work fills another gap realistic wcet prediction contemporary architectures use schedulability analysis hard realtime systems framework wcet prediction uses set tools depicted figure 1 vpo optimizing compiler 4 modified emit controlflow information data information calling structure functions addition regular object code generation static cache simulator uses controlflow information calling structure conjunction cache configuration produce instruction data categorizations describe caching behavior instruction data reference respectively timing analyzer uses categorizations controlflow information perform path analysis program analysis includes evaluation architectural characteristics pipelining wraparoundfilling cache misses description caching behavior supplied static cache simulator used timing analyzer predict temporal effect cache hits misses overlapped temporal behavior pipelining timing analyzer produces wcet predictions user selected segments program entire program 2 related work past years research area predicting wcet programs intensified conventional methods static analysis extended unoptimized programs simple cisc processors 23 20 9 22 optimized programs pipelined risc processors 30 17 11 uncached architectures instruction caches 2 15 13 data caches 24 14 16 related work analyzing data caching previous work wraparoundfill caches context wcet prediction knowledge rawat 24 used graph coloring technique bound data caching performance however live ranges local scalar variables within single function analyzed fairly uncommon references since local scalar variables allocated registers optimizing compilers kim et al 14 recently published work bounding data cache performance calculated references caused load store instructions referencing addressing change dynamically technique uses version pigeonhole principle loop determine maximum number references dynamic loadstore instruction also determine maximum number distinct locations memory referenced instructions difference two values number data cache hits loop given conflicting references technique efficiently detects temporal locality within loops data references within loop fit cache size data reference size cache line technique time detect spatial locality ie line size greater size data reference elements accessed contiguously detects temporal locality across different loop nests fur thermore approach currently deal compiler optimizations alter correspondence assembly instructions source code compiler optimizations make calculating ranges relative addresses significantly challenging et al 16 described framework integrate data caching integer linear programming ilp approach timing prediction implementation performs dataflow analysis find conflicting blocks however linear constraints describing range addresses data reference currently calculated hand also require separate constraint every element calculated reference causing scalability problems large arrays wcet results data caches reported however ilp approach facilitate integrating additional userprovided constraints analysis 3 data caches obtaining tight wcets presence data caches quite challenging unlike instruction caching addresses data references change execution program reference item within activation record could different addresses depending sequence calls associated invocation function data references indexing array dynamically calculated vary time data reference occurs pointer variables languages like c may assigned addresses different variables address dynamically calculated heap initially may appear obtaining reasonable bound worstcase data cache performance simply feasible however problem far hope less since addresses many data references statically calculated static global scalar data references retain addresses throughout execution program runtime stack scalar data references often statically determined set addresses depending upon sequence calls associated invocation function pattern addresses associated many calculated references eg array indexing often resolved statically prediction wcet programs data caches achieved automatically analyzing range addresses data references deriving relative virtual addresses ranges categorizing data references according cache behavior data cache behavior integrated pipeline analysis yield worstcase execution time predictions program segments 31 calculation relative addresses vpo compiler 4 attempts calculate relative addresses data reference associated load store instructions compiler optimizations performed see figure 1 compiler optimizations move instructions basic blocks outside loops expansion registers used address calculations becomes difficult analysis described similar data dependence analysis performed vectorizing parallelizing compilers 5 6 7 21 28 29 however data dependence analysis typically performed highlevel representation analysis performed lowlevel representation code generation optimizations applied calculation relative addresses involves following steps 1 compiler determines loop set induction variables initial values strides loopinvariant registers 1 2 expansion actual parameter information performed order able resolve possible address parameters later 3 expansion addresses used loads stores performed expansion accomplished examining preceding instruction represented register transfer list rtl replacing registers used source values address source rtl setting register induction variables associated loop expanded loop invariant values expanded proceeding end preheader block loop expansion addresses scalar references runtime stack eg local variables trivial expansion references static data eg global variables often requires expanding loopinvariant registers since addresses constructed instructions may moved loop expansion calculated address references eg array indexing requires knowledge loop induction variables approach expanding addresses provides ability handle nonstandard induction variables limited simple induction variables simple loops updated head loop consider c source code rtls sparc assembly instructions figure 2 simple initialize function code initialize goes elements 1 51 array array b initializes random integers note although delay slots actually filled compiler filled compiling code figures paper order simplify examples reader 2 7 add l2loai3 8 add l4lobi4 9 add i4204i0 r14svr1496 1 save sp96sp 2 sethi hibl4 5 sethi hi10200i2 st st o0l4 store aij store bij int bmaxmax int amaxmax define max 50 int j i1 imax j1 jmax j l37 figure 2 example c function rtls sparc assembly function initialize first memory address r20 instruction 22 store aij second memory address r20 instruction 23 store bij register r20 induction register inner loop instructions 19 27 thus cannot expanded initial value stride maximum minimum number iterations associated computed stored earlier compilation process 3 initial value r20 consists first element accessed base address b plus 4 plus offset comes computing row location induction variable outer loop r25 stride 4 minimum maximum number iterations 50 initial value stride number iterations available enough information compute sequence addresses accessed store bij knowing references stride compiler used index reduction avoid use another induction register address computation aij since shares loop control variables b memory address r20 r21 aij includes address b r20 plus difference two arrays r21 seen following sequence expansions simplifications remember register r20 cannot immediately expanded since induction register inner loop expansion continues register r21 follows note register r25 expanded either since induction variable outer loop instructions 1331 1 load 22 2 r20r21r22 17 3 r20r21r25r28 4 r20r25r27r25r28 15 5 r20r25r27r25r20lo b 8 6 r20r25r18lo ar25r20lo b 7 7 r20r25hi alo ar25r20lo b 3 8 r20r25hi alo ar25hi blo b 2 effect expansion simplified following steps 9 r20r25 ar25 b eliminate hi lo 10 r20r25 ar25 b remove unnecessary 11 remove distribute 12 remove negating terms thus left induction register r20 plus difference two arrays simplified address expression string written file containing data declarations relative address information address calculator attempts resolve string actual virtual address use initial value r20 r25 b4 bs cancel r25 b4 b note gives initial address row array range relative addresses example depicted algorithmically shown figure 3 details statically determining address information fully optimized code see 26 addressa r25r20 figure 3 algorithmic range relative addresses load figure 2 startup code program code segment static data runtime program stack initial stack figure 4 virtual address space sunos 32 calculation virtual addresses calculating addresses relative beginning global variable activation record accomplished within compiler since much data flow information required analysis also used compiler optimizations however calculating virtual addresses cannot done compiler since analysis call graph data declarations across multiple files required thus address calculator see figure 1 uses relative address information conjunction controlflow information obtain virtual addresses figure 4 shows general organization virtual address space process executing sunos startup code preceding instructions associated compiled program following program code segment static data aligned page boundary runtime stack starts high addresses grows toward low addresses part memory runtime stack static data heap depicted figure since addresses heap could calculated statically environment static data consists global variables static variables nonscalar constants eg strings floatingpoint constants general unix linker ld places static data order declarations appeared within assembly files also static data within one file precede static data another file specified later list files linked exceptions rules depending upon data statically initialized addition padding variables sometimes occurs instance variables declared int double sparc aligned word doubleword boundaries respectively addition first static global variable declared source files comprising program aligned doubleword boundary runtime stack data includes temporaries local variables allocated registers examples temporaries include parameters beyond sixth word passed function memory used move values integer floatingpoint registers since movement cannot accomplished directly sparc address activation record function vary depending upon actual sequence calls associated activation virtual address activation record containing local variable determined sum sizes activation records associated sequence calls along initial runtime stack address address calculator along static simulator timing analyzer distinguishes different function instances evaluates instance separately static data names activation records functions associated virtual addresses relative address ranges converted virtual address ranges virtual addresses calculated far guarantee virtual address actual physical address used access cache memory machines paper assume system page size integer multiple data cache size often case instance microsparc 4kb page size 2kb data cache thus virtual corresponding physical address would relative offset within page would map line within data cache 33 static simulation produce data reference categorizations method static cache simulation used statically categorize caching behavior data reference program specified cache configuration see figure 1 program controlflow graph constructed includes control flow within function function instance graph uniquely identifies function instance sequence call sites required invocation program controlflow graph analyzed determine possible data lines data cache entry exit basic block within program 19 iterative algorithm used static instruction cache simulation 2 19 sufficient static data cache simulation problem calculated references access range possible addresses point data access occurs data lines associated addresses may may brought cache depending upon many iterations loop performed point deal problem new state created indicate whether particular data line could potentially data cache due change basic block instance b input lines input immed pred p b input stateb output statep calc input stateb output statep p another loop input stateb calc output statep data linesremaining loop output data reference b scalar reference output stateb data lined output stateb data linesd conflicts calc output stateb data lined calc output stateb data linesconflicts output stateb data linesd could conflict calc output stateb data linesd could access calc output stateb data linesd could conflict figure 5 algorithm calculate data cache states calculated references immediate predecessor block different loop transition predecessor block current block exits loop data lines associated calculated references loop guaranteed still cache unioned input cache state block iterative algorithm figure 5 used calculate input output cache states basic block program control flow cache state vectors produced used determine whether memory references within bounded virtual address range associated data reference cache static cache simulator needs produce categorization data reference program four worstcase categories caching behavior used past static instruction cache simulation 2 follows 1 always miss reference guaranteed cache 2 always hit h reference guaranteed always cache 3 first miss fm reference guaranteed cache first time accessed time loop entered guaranteed thereafter 4 first hit fh reference guaranteed cache first time accessed time loop entered guaranteed thereafter categorizations still used scalar data references int a100100 int a100100 row order sum main column order sum int j sum aij sum aij row detecting spatial locality int j sum ai ai ref 1 ai bj ai ref 2 bi ref 3 ref 1 c 13 h h h h h h h h h h h h h 2 h h h h h due temporal locality across loops ref 3 c 13 13 h h h h h h first execution inner loop h h h h h successive executions b detecting temporal locality across within loops figure 6 examples spatial temporal locality obtain accuracy worstcase categorization calculated data reference iteration loop could determined example categorizations data reference loop 20 iterations might follows detailed information timing analyzer could accurately determine worstcase path iteration loop however consider loop 100000 iterations approach would inefficient space stor ing categorizations time analyzing loop iteration separately authors decided use new categorization called calculated c would also indicate maximum number data cache misses could occur loop level data reference nested previous data reference categorization string would represented follows since one loop level order access cache state vectors used detect cache hits within calculated references due spatial locality consider two code segments figure 6a sum elements two dimensional array two code segments equivalent except left code segment accesses array row order right code segment uses column order ie statements reversed assume scalar variables j sum allocated registers also assume size directmapped data cache 256 bytes lines containing 16 bytes thus single row array requiring 400 bytes cannot fit cache static cache simulator able detect load array element left code segment one miss array elements part data line accomplished inspecting order array accessed detecting conflicting lines accessed loops categorizations load data reference two segments given figure note case array happens aligned line boundary specification single categorization calculated reference accomplished two steps loop level cache states calculated first number references iterations performed loop retrieved next maximum number misses could occur reference loop determined instance 25 misses occur innermost loop left code segment static cache simulator determined loads right code segment would result cache misses data caching behavior simply viewed always miss thus range 10000 different addresses referenced load collapsed single categorization c 25 2500 calculated reference 25 misses innermost level 2500 misses outer level left code segment always miss right code segment likewise cache hits calculated references due temporal locality across within loops also detected consider code segment figure 6b assume cache configuration lines 512 byte cache arrays b requiring 400 bytes total 200 fit cache also assume scalar variables allocated registers accesses elements array first loop categorized h always hit static simulator since data lines associated array cache state first loop exited shows detection temporal locality across loops first complete execution inner loop elements b cache references remaining executions inner loop also categorized hits thus categorization c 13 13 given relative innermost loop 13 misses due bringing b cache first complete execution inner loop also 13 misses relative outermost loop since b completely cache iteration first thus temporal locality also detected within loops current implementation static data cache simulator timing alyzer imposes restrictions first directmapped data caches supported obtaining categorizations setassociative data caches accomplished manner similar described work instruction caches 27 second recursive calls allowed since would complicate generation unique function instances third indirect calls allowed since explicit call graph must generated statically 34 timing analysis timing analyzer see figure 1 utilizes pipeline path analysis estimate wcet sequence instructions representing paths loops functions pipeline information instruction type obtained machinedependent data file information specific instructions path obtained controlflow information files instruction added separately pipeline state information timing analyzer uses data caching categorizations determine whether mem data memory access stage treated cache hit miss worstcase loop analysis algorithm modified appropriately handle calculated data reference categorizations timing analyzer conservatively assume misses current loop level calculated reference occur hits level addition timing analyzer unable assume penalty misses overlap long running instructions since analyzer may evaluate misses exact iterations occur thus calculated reference miss always viewed hit within pipeline path analysis maximum number cycles associated data cache miss penalty added total time path strategy permits efficient loop analysis algorithm potentially small overestimations data cache miss penalty could overlapped stalls worstcase loop analysis algorithm given figure 7 additions previously published algorithm 11 handle calculated references shown boldface let n maximum number iterations associated given loop loop terminates number processed iterations reaches n first misses first hits calculated references encountered misses hits misses respectively loop iterate minimum n 1 p r times p number paths r number calculated reference load instructions loop algorithm selects longest path loop iteration 11 10 order demonstrate correctness algorithm one must show path given iteration loop produce longer time calculated algorithm since pipeline effects paths unioned remains shown caching effects treated properly categorizations treated identically repeated references except first misses first hits calculated references assuming data references categorized correctly loop pipeline analysis correct remains shown first misses first hits calculated references interpreted appropriately loop iteration correctness argument interpretation first hits first misses given 2 total pipeline first misses encountered first hits encountered null find longest continue path first misses encountered first misses misses path first hits encountered first hits hits path first miss first hit encountered path curr iter 1 subtract 1 remaining misses calculated reference path concatenate pipeline info union info paths total cycles additional cycles required union else calculated reference encountered path miss minimum number remaining misses calculated reference path nonzero curr iter min misses subtract min misses remaining misses calc ref path concatenate pipeline info union info paths min misses times total cycles min misses additional cycles required union break concatenate pipeline info union pipeline info paths n 1 curr iter times total cycles n 1 curr iter additional cycles required union set exit paths transition unique exit block find longest exit path set first misses encountered first misses misses path first hits encountered first hits hits path concatenate pipeline info union info exit paths set total cycles additional cycles required exit union store information exit block loop figure 7 worstcase loop analysis algorithm loop subtract one calculated reference miss count current loop longest path chosen iteration whenever first misses first hits encountered misses hits respectively first misses first hits encountered longest path path remain longest path long set calculated references encountered misses continue encountered misses since caching behavior references treated thus pipeline effects longest path efficiently replicated number iterations associated minimum number remaining misses calculated references nonzero within longest path loop first misses first hits calculated references longest path encountered hits misses hits respectively unioned pipeline effects loop change since caching behavior references treated thus pipeline effects path efficiently replicated one remaining iterations last iteration loop treated separately since longest exit path may shorter longest continue path correctness argument interpretation calculated references needs show calculated references treated misses appropriate number times algorithm treats calculated reference miss specified number calculated misses loop exhausted ifthen portion loop subtracts one calculated reference miss count since single iteration analyzed calculated reference miss given loop iteration elseifthen portion loop subtracts minimum misses remaining calculated reference path number iterations remaining loop number iterations analyzed number misses subtracted calculated reference since misses calculated references evaluated hits interpretation calculated references underestimate actual number calculated misses given data references categorized correctly example given figure 8 illustrate algorithm statement condition contrived force worstcase paths taken executed assume data cache line size 8 bytes enough lines hold three arrays cache figure also shows iterations element three arrays referenced whether references hit miss two different paths taken loop iteration shown integer pipeline diagram figure 8 note pipeline diagrams reflect loads array elements found cache miss penalty calculated reference misses simply added total cycles path directly reflected pipeline information since misses may occur exact iterations assumed timing analyzer table 1 shows steps timing analyzer uses algorithm given figure 7 estimate wcet loop example shown figure 8 longest path detected first step path contains references ki ci pipeline time required 20 cycles misses two calculated references ki ci required cycles note miss penalty assumed require 9 cycles since first misses timing analyzer determines minimum number remaining misses two calculated references 13 thus path replicated additional 12 times overlap iterations determined 4 cycles note 4 subtracted first iteration since overlap would calculated determining worstcase execution time path main function total time first 13 iterations 446 longest path detected step 2 also path time references ci hits 37 remaining misses ki total time iterations 14 50 925 cycles longest path detected step 3 path b 25 remaining misses si results 550 additional cycles iterations 51 75 step 3 worstcase loop analysis exited loop algorithm step char c100 short s100 int k100 int sum sum kici else sum si path b blocks 2 4 5 path blocks 23 5 paths loop load si load ci load ki ld l0o1 instructions 1 11 instructions 12 15 block 4 instructions 23 28 instructions 29 block 5 block 6 data line 0 data line 1 data line 2 data line 3 data line 50 data line 51 data line 76 data line 77 k missmiss hit miss miss hit miss miss result accessed iteration elements array data lines c ki c 50 h h h si c 25 h h h h h h h h h c 13 h h h h h 26 28 26 28 26 28 26 28 id cycle stage121213141515 22 2223 2323 28 28 26 28 pipeline diagram path b instructions 1215 2128 blocks 245 id cycle 19 232324252526272828 pipeline diagram path instructions 1220 2328 blocks 235 figure 8 example illustrate worstcase loop analysis algorithm 4 calculates 384 cycles next 24 iterations 7699 step 5 calculates last iteration require 16 cycles timing analyzer calculates last iteration separately since longest exit path may shorter paths loop total number cycles calculated timing analyzer example identical number obtained execution simulation timing analysis tree constructed predict worstcase performance node tree represents either loop function function instance graph function instance uniquely identified sequence calls resulting invocation nodes representing outer level function instances treated loops iterate worstcase time node calculated time immediate child nodes known table 1 timing analysis steps loop figure 8 start longest total step iter path cycles min misses iters additional cycles cycles instance consider example shown figure 8 table 1 timing analyzer would calculate worstcase time loop use information next calculate time path main contains loop block 1 loop block 6 construction processing timing analysis tree occurs similar manner described 2 11 35 results measurements obtained code generated sparc architecture vpo optimizing compiler 4 machinedependent information contained pipeline characteristics microsparc processor 25 directmapped data cache containing 16 lines 32 bytes total 512 bytes used microsparc uses writethroughnoallocate data caching 25 static simulator able categorize store data references categorizations ignored timing analyzer since stores always accessed memory hit miss associated store data reference effect performance instruction fetches assumed hits order isolate effects data caching instruction caching table 2 shows test programs used assess timing analyzers effectiveness bounding worstcase data cache performance note programs restricted specific classes data references include dynamic allocation heap two versions used last five test programs version size arrays used previous studies 2 11 b version program used smaller arrays would totally fit 512 byte cache number bytes reported table total number bytes variables program note bytes static data area others runtime stack amount data changed program des since encryption algorithm based using large static arrays preinitialized values table 3 depicts dynamic results executing test programs hit ratios obtained data cache execution simulation sort high data cache hit ratios due many repeated references array elements observed cycles obtained using execution simulator modified 8 simulate data cache pipeline affects count number cycles estimated cycles obtained timing analyzer discussed table 2 test programs data caching name bytes description emphasis des 1346 encrypts decrypts 64 bits matcnta 40060 count sum values 100x100 int matrix matcntb 460 count sum values 10x10 int matrix matmula 30044 multiply 2 50x50 matrices 50x50 int matrix matmulb 344 multiply 2 5x5 matrices 5x5 int matrix matsuma 40044 sum values 100x100 int matrix matsumb 444 sum values 10x10 int matrix sorta 2044 bubblesort 500 int array 444 bubblesort 100 integer array section 34 estimated ratio quotient two values naive ratio calculated assuming data cache references misses dividing cycles observed cycles timing analyzer able tightly predict worstcase number cycles required pipelining data caching test programs fact five prediction exact less onetenth one percent inner loop function within sort sorted values varying number iterations depends upon counter outer loop number iterations performed overrepresented average factor two inner loop strategy treating calculated reference miss hit pipeline adding maximum number cycles associated miss penalty total time path caused overestimations statsa statsb programs floatingpoint intensive programs test set often delays due longrunning floatingpoint operations could overlapped data cache miss penalty cycles matmula overestimation table 3 dynamic results data caching hit observed estimated estobs naive name ratio cycles cycles ratio ratio des 7571 155340 191564 123 145 matcnta 7186 1143014 1143023 100 115 matcntb 7073 12189 12189 100 115 matmula 6281 7245830 7952807 110 124 matmulb 8940 11396 11396 100 133 matsuma 7186 1122944 1122953 100 115 matsumb 6998 11919 11919 100 115 sorta 9706 4768228 9826909 206 288 average 8075 na na 124 155 10 whereas smaller data version matmulb exact matmul program repeated references elements three different arrays references would miss first time encountered would cache smaller matmulb accessed since arrays fit entirely cache arrays fit cache interference however fit cache static simulator conservatively assumes possible interference must result cache miss therefore categorizations conservative overestimation larger finally des program several references element statically initialized array used index another array simple method determine value used index therefore must assume element array may accessed time data reference occurs program forces conflicting lines deleted cache state resulting categorizations conservative des program also overestimations due data dependencies longer path deemed feasible timing analyzer could taken function due value variable despite relatively small overestimations detailed results show certain restrictions possible tightly predict much data caching behavior many programs difference naive estimated ratios shows benefits performing data cache analysis predicting worstcase execution times benefit worstcase performance data caching significant benefit obtained instruction caching 2 11 instruction fetch occurs instruction executed performance benefit writethroughnoallocate data cache occurs data reference load instruction determined timing analyzer cache load instructions comprised average 1428 total executed instructions test programs however results show performing data cache analysis predicting worstcase execution time still result substantially tighter predictions fact programs test set prediction improvement averages 30 performance overhead associated predicting wcets data caching using method comes primarily static cache simulation time required static simulation increases linearly size data however even large arrays given test programs time rather small average time static simulation produce data reference categorizations 11 programs given table 3 289 seconds overhead timing analyzer averages 105 seconds 4 wraparoundfilling instruction cache misses several timing tools exist address hitmiss behavior instruction cache modern instruction caches often employ various sophisticated approaches decrease miss rate reduce miss penalty 12 one approach reduce miss penalty instruction cache wraparound fill processor employing feature load cache line one word time starting instruction table 4 order fill loading words cache line first requested word miss delay word within cache line caused cache miss word program line loaded cache associated instruction cannot fetched word loaded motivation wraparound fill let cpu proceed instruction allow pipelined execution continue subsequent instructions loaded cache thus benefit necessary wait entire cache line loaded proceeding execution fetched instruction however feature complicates timing analysis since introduce dead cycles pipeline analysis cycles instruction loaded cache 25 wraparound fill used several recent architectures including alpha axp 21064 mips r10000 ibm 620 table 4 shows words loaded cache microsparc processor 25 instruction cache line eight words hence eight instructions rows table distinguished word w within cache line requested entire line found cache leftmost column shows words 07 miss become first word respective program line loaded cache takes seven cycles requested instruction reach instruction cache eighth cycle word w even w 1 w odd gets loaded cache pair words loaded cache dead cycle word written table 6 indicates microsparc dead cycles ninth twelfth fifteenth cycles miss occurs takes seventeen cycles entire program line requested memory completely loaded cache note microsparc additional requirement entire program line must completely loaded cache different program line accessed whether program line already cache wraparoundfill analysis information stored path loop includes program line number cycles words loops first last program lines loaded cache cycles called available times timing analyzer calculates beginning ending available times path particular loop loop analysis set beginning ending information propagated along worstcase paths pipeline requirements data hazard information keeping track words program line available cache analogous determining particular pipeline stage last occupied detecting value register available via hardware forwarding available times used carry wraparoundfill analysis paths loops analysis detects delays associated dead cycles cases delays overlapped pipeline stalls produce accurate wcet prediction 41 wraparoundfill delays within path analysis single path instructions necessary know individual words cache line loaded appropriate instructions timing analyzer processes instruction categorized miss automatically determine instructions program line loaded cache according order fill given machinedependent information timing analyzer stores program line number relative word number line every instruction program analysis path timing analyzer update information program line arriving cache words line available fetched without delay point timing analyzer finished examining path store information associated first last program lines referenced path including cycles words lines become available cache plus amount delay caused latencies filling cache lines information useful path evaluated larger context namely first iteration loop path function entered called another part program figure 9 shows algorithm used determine number cycles associated instruction fetch analyzing path cycles words become available last line fetched calculated miss order demonstrate correctness algorithm one must show required number cycles calculated wraparoundfill delay instruction fetch three possible cases first case instruction fetched last line fetched means instruction fetch must hit line 4 algorithm uses arrival time associated word containing instruction determine extra cycles needed stage second third cases reference last line fetched instruction fetch could hit miss respectively either case cycles stage instruction include delay complete loading last line calculated line 6 line 8 calculates additional cycles stage required miss load requested word line lines 910 establish arrival times words line miss three cases handled thus algorithm correct given arrival times current line preceding first instruction path accurate techniques determine arrival times point path entered described following sections matrix containing information table 4 const int waf delaywords per linewords per line indicates word last line fetched become available int availablewords per line const int max delay delay required load last word line 1 curr word inst word num words per line 2 first cycle first vacancy stage 3 instruction last line fetched 4 cycles first cycle 5 else cycles previous line delay max0last word avail first cycle 7 reference miss 8 cycles waf delaycurr word numcurr word num previous line delay last word previous line delay 12 last line fetched line current instruction 13 cycles 1 figure 9 algorithm calculate waf delay within path 42 delays upon entering loop function path analysis timing analyzer encounters loop function call path child determines first instruction child lies different program line instruction executed immediately entering loop function first instruction child must delayed fetched program line containing last instruction executed child still loading cache two instructions lie program line necessary ensure instructions belonging first program line child available fetched often available times corresponding dead cycle delays already calculated child likewise available times could calculated child encountered earlier current path ie situation path calls two functions share program line figure shows small program containing loop ten iterations comprises instructions 59 first instruction loop instruction 5 memory instruction located program line 0 instructions 04 beginning program execution instruction 0 misses cache causes program line 0 containing instructions 07 load cache first iteration loop timing analyzer detects instruction 5 needs spend 1 cycle stage dead cycle associated instruction 5 even though program line 0 process still fetched cache first iteration categorizations fmfm prog line000011135713 word number id cycle stage pipeline instructions inst 0 save sp104sp inst 1 mov g0o3 inst 2 add sp1ao0 inst 3 mov o0o4 inst 4 add o040o5 inst inst 7 cmp o4o5 inst 8 bl l16 inst 10 ret inst 9 add o31o3 inst 11 restore g0g0o0 inst 5 st o3o4 c source code int a10 return 0 figure 10 example program pipeline diagram first 2 loop iterations 43 delays loop iterations loop analysis algorithm important detect delay may result program line loaded cache late previous iteration causes subsequent iteration delayed example consider program figure 10 instruction cache activity inferred long various instructions occupy stage timing analysis begins static cache simulator 19 determined instruction 8 first miss pipeline behavior first two iterations loop given pipeline diagram figure 10 instruction cache activity often inferred long various instructions occupy stage first iteration instruction 6 delayed stage cycle 16 dead cycle occurs program line loaded cache note instruction 6 delayed would later victim structural hazard instruction 5 occupies mem stage cycles 1819 4 thus dead cycle delay overlaps potential pipeline stall later first iteration instruction 8 miss must spend total 8 cycles stage program line 1 containing instructions 811 1215 existed takes cycle 19 cycle 36 completely loaded time instruction 8 referenced program line finishes loading cycle 36 instruction 5 second iteration cannot fetched cycle 37 situation delay due jumping new program line takes place loop iterations 44 results results evaluating test programs section 35 shown table 5 fifth column table gives ratio estimated cycles observed cycles timing analyzer executed wraparoundfill analysis enabled sixth column shows similar ratio estimated observed cycles time wraparoundfill analysis disabled mode timing analyzer assumes constant penalty 17 cycles instruction cache miss maximum miss penalty instruction fetch would incur fetch delay actually attains maximum consecutive misses case call function instruction located delay slot call first instruction function misses case second miss incur 17 cycle penalty entire program line containing delay slot instruction must completely loaded first data cache accesses assumed hits timing analyzer simulator experiments table 5 results test programs hit observed estimated ratio ratio without name ratio cycles cycles waf analysis waf analysis des 8616 154791 171929 111 138 matcnta 8186 2158038 2161172 100 112 matmula 9893 4544944 4547299 100 101 matsuma 9399 1131964 1132178 100 113 sorta 7606 14371854 30714661 214 252 average 8758 na na 121 138 wcet programs wraparoundfill analysis enabled significantly tighter wraparound fill considered des sorta overestimations reasons described section 35 small overestimations remaining programs primarily result timing analyzers conservative approach first misstofirst miss categorization transitions slight overestimations also occurred timing analysis assumed constant miss penalty 11 situation occurs infrequently approach resulted small overestimations overhead executing timing analysis quite small even wraparoundfill analysis average time required produce wcet programs table 5 127 seconds 5 future work several areas timing analysis investigated hardware features write buffers branch target buffers could modeled timing analysis best case timing bounds various types caches hardware features may also investigated eventual goal research integrate timing analysis instruction data caches obtain timing predictions complete machine actual machine measurements using logic analyzer could used gauge accuracy simulator effectiveness entire timing analysis environment 6 conclusion two general contributions paper first approach bounding worstcase data caching performance presented uses data flow analysis within compiler determine bounded range relative addresses data reference address calculator converts relative ranges virtual address ranges examining order data declarations call graph program categorizations data references produced static simulator timing analyzer uses categorizations performing pipeline path analysis predict worstcase performance loop function program results far indicate approach valid result significantly tighter worstcase performance predictions second technique wcet prediction wraparoundfill caches presented processing path instructions timing analyzer computes instruction entire program line loaded cache based instruction categorizations indicate instruction fetches could result cache misses timing analyzer uses information determine much delay fetched instruction suffer due wraparound fill analyzing larger program constructs loops function instances wraparoundfill information associated path used detect delays beyond scope single path results indicate wcet bounds significantly tighter timing analyzer conservatively assumes constant miss penalty paper contributes comprehensive report methods results worstcase timing analysis data caches wraparound caches approach taken unique provides considerable step toward realistic worstcase execution time prediction contemporary architectures use schedulability analysis hard realtime systems acknowledgments authors thank anonymous referees comments helped improve quality paper robert arnold providing timing analysis platform research research article based supported part office naval research contract number n00014941006 national science foundation cooperative agreement number hrd 9707076 notes 1 basic loop induction variable assignments form v v sigma c v variable register c integer constant nonbasic induction variables also incremented decremented constant value loop iteration get values either directly indirectly basic induction variables variety forms assignment nonbasic induction variables allowed loop invariant values change execution loop discussion induction variables loop invariant values identified found elsewhere 1 2 annulled branches sparc actually access memory update registers instructions delay slot branch taken simple feature causes host complications load store annulled delay slot however approach correctly handle data reference 3 earlier computation expansion initial value string induction register proceeds basically manner already discussed except loop invariant registers expanded well 4 microsparc st instruction required spend two cycles mem stage r bounding worstcase instruction cache performance fixed priority preemptive scheduling historical perspective portable global optimizer linker programming vienna fortran high performance fortran without templates extending hpf advanced data parallel applica tions design environment addressing architecture compiler interactions retargetable technique predicting execution time bounding pipeline instruction cache performance integrating timing analysis pipelining instruction caching computer architecture quantitative approach worst case timing analysis risc processors r3000r3010 case study efficient worst case timing analysis data caching efficient microarchitecture modeling path analysis realtime software cache modeling realtime software beyond direct mapped instruction caches accurate worst case timing analysis risc processors scheduling algorithms multiprogramming hard realtime environment static cache simulation applications predicting program execution times analyzing static dynamic program paths parallel programming compilers zeitanalyse von echtzeitprogrammen calculating maximum execution time realtime programs static analysis cache analysis realtime programming integrated sparc processor bounding worstcase data cache performance timing analysis data caches setassociative caches optimizing supercompilers supercomputers high performance compilers parallel computing pipelined processors worst case execution times tr ctr wegener frank mueller comparison static analysis evolutionary testing verification timing constraints realtime systems v21 n3 p241268 november 2001 wankang zhao william kreahling david whalley christopher healy frank mueller improving wcet applying worstcase path optimizations realtime systems v34 n2 p129152 october 2006 kiran seth aravindh anantaraman frank mueller eric rotenberg fast frequencyaware static timing analysis acm transactions embedded computing systems tecs v5 n1 p200224 february 2006 kaustubh patil kiran seth frank mueller compositional static instruction cache simulation acm sigplan notices v39 n7 july 2004 yudong tan vincent mooney timing analysis preemptive multitasking realtime systems caches acm transactions embedded computing systems tecs v6 n1 february 2007 wankang zhao david whalley christopher healy frank mueller improving wcet applying wc codepositioning optimization acm transactions architecture code optimization taco v2 n4 p335365 december 2005 aravindh anantaraman kiran seth kaustubh patil eric rotenberg frank mueller virtual simple architecture visa exceeding complexity limit safe realtime systems acm sigarch computer architecture news v31 n2 may