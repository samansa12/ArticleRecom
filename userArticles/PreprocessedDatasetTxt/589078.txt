degenerate nonlinear programming quadratic growth condition show quadratic growth condition mangasarianfromovitz constraint qualification mfcq imply local minima nonlinear programs isolated stationary points result started sufficiently close points linfty exact penalty sequential quadratic programming algorithm induce least rlinear convergence iterates local minimum construct example degenerate nonlinear program unique local minimum satisfying quadratic growth mfcq positive semidefinite augmented lagrangian exists present numerical results obtained using several nonlinear programming packages example discuss implications algorithms b introduction recently renewed interest analyzing modifying sequential quadratic programming sqp algorithms constrained nonlinear optimization cases traditional regularity conditions hold 1413 2025 research motivated fact largescale nonlinear programming problems tend almost degenerate large condition numbers jacobian active constraints therefore important establish extent convergence properties sqp methods dependent illconditioning constraints work term degenerate nonlinear programs nlps gradients active constraints linearly dependent case may several feasible lagrange multipliers many previous analysis rate convergence results degenerate nlp based validity secondorder conditions essentially equivalent condition unconstrained optimization critical point function fx local minimum f xx 0 necessary condition f xx 0 sufficient condition positive semidefinite ordering place f xx constrained optimization taken conditions l xx hessian lagrangian required positive definite critical cone one lagrange multipliers 621 department mathematics university pittsburgh work completed author wilkinson fellow mathematics computer science division argonne national laboratory 9700 south cass avenue argonne il 60439 work supported mathematical information computational sciences division subprogram office advanced scientific computing us department energy contract w31109eng38 work differs previous approaches assume 1 local solution x constrained nonlinear program firstorder mangasarianfromowitz constraint qualification holds 2 quadratic growth condition qg 416 satisfied oe 0 x feasible neighborhood x 3 data problem twice continuously differentiable assumptions equivalent weaker form secondorder sufficient conditions 154 require positive semidefinitenes hessian lagrangian entire critical cone prove conditions guarantee x local stationary point 3 nonlinear program important issue guarantees descentlike algorithms stop arbitrarily close x except x extends result 21 required secondorder sufficient conditions satisfied multipliers particular work implies mfcq holds secondorder sufficient conditions hold one multiplier x strict local minimum isolated stationary point also show assumptions l1 exact penalty sequential quadratic program sqp induces least q linear convergence 19 penalized objective fx rlinear convergence iterates finally provide example nonlinear program satisfies assumptions possible construct augmented lagrangian x unconstrained local minimum may present adverse case algorithms based assumption lagrange multiplier methods however show possible construct nondifferentiable function x minimum namely l1 penalty function also inferred results 4 describe computational experience several nonlinear programming packages applied example discuss expected observed behavior lagrangian multiplier methods convergence analysis l1 exact penalty function suggests possible construct convergence theory much general secondorder conditions may result algorithms superior robustness properties depend significantly fewer assumptions 11 previous work framework notations deal nlp problem fx subject gx 0 2 twice continuously differentiable call x stationary point following conditions hold 2 ir degenerate nonlinear programming quadratic growth condition 3 l lagrangian function certain regularity conditions hold discussed local solution x 2 stationary point case 3 referred kkt karushkuhntucker conditions since analysis limited neighborhood point x strict minimum assume constraints active x gx situation obtained simply dropping constraints 0 since relationship holds entire neighborhood x reduce generality results simplifies notation refer separately active set regularity condition constraint qualification ensures linear approximation feasible set neighborhood x captures geometry feasible set often local convergence analysis constrained optimization algorithms assumed constraint gradients rg linearly independent lagrange multiplier 3 unique assume instead mangasarianfromowitz constraint qualification mfcq n 5 well known 9 mfcq equivalent boundedness set mx lagrange multipliers satisfy 3 note mx certainly polyhedral case critical cone x 622 briefly review secondorder conditions literature although assumption analysis basis com parison framework 6 secondorder sufficient conditions x isolated local solution 2 9 2 mx conditions hold x quadratic growth condition satisfied irrespective validity firstorder constraint qualification 67 however imply x isolated stationary point shown simple example 21 may prevent optimization algorithm uses first derivative information reaching x even started arbitrarily close x 21 shown mfcq holds relation 8 satisfied 2 mx x isolated stationary point minimum 2 also conditions exact solution lipschitz stable respect perturbations compactness mx choose oe independently case 1 proven assumptions l1 exact penalty sqp converge qlinearly fx descent direction computed qp using firstorder information refinement secondorder conditions introduced 15 presence mfcq conditions require analysis shows presence mfcq conditions necessary sufficient quadratic growth condition hold 4151622 also exact solution lipschitz stable respect certain classes perturbations 22 though perturbation see example 10 p308 paper assume quadratic growth condition mfcq thus use perturbation results condition 9 holds 8 positive semidefinite augmented lagrangian show example interesting aspect since invalidates usual working assumption lagrange multiplier methods 3 finally review facts concerning l1 nondifferentiable exact penalty function looking unconstrained minimum function c oe sufficiently large constant descent directions oex point x obtained solving following quadratic program qp 3 subject g j x h positive definite matrix g 0 paper analysis restricted case although results apply positive definite matrix current point x k iterative procedure attempts determine x qp 11 generates descent direction k next iterate x obtained line search procedure usual stepsize rules minimization rule limited minimization rule armijo rule 3 rules limit point fx k g stationary point oex descent procedure therefore globally convergent sense 3 addition degenerate nonlinear programming quadratic growth condition 5 2 mx x stationary point oex 2 suitable value c oe available early stages algorithm good estimate found via update scheme 2 assume c constant 2 mx fl prescribed safety factor consider quadratic program subject g j x denote unique solution program dx set multipliers mx x 14 multiplier set 2 denoted mx since mfcq satisfied x qp feasible neighborhood x kkt conditions qp require notations dx qp 14 unconstrained solution would name descentlike algorithm sequential quadratic program solves instances qp x qp 14 satisfies mfcq secondorder sufficient con ditions 21 exists c neighborhood x exists 2 mx therefore definition c oe exists neighborhood x multipliers 2 mx x verified inspection solution 11 2 p 195 therefore concentrate qp 14 c oe large enough sufficiently close x generates descent direction 11 thus sharing global convergence property function h ir k denote c 1h c 2h bounds depending first second derivatives h positive negative parts hx h componentwise notation 6 mihai anitescu 2 stationary points nlps satisfying mfcq section assume x sufficiently small neighborhood x whose size properties specified following results particular standing assumptions hold neighborhoods considered p one vectors satisfying 5 suitable neighborhood x lemma 1 exist neighborhood w x p x usual l1 penalty function 10 proof taylors theorem choose 0 ff ff p claim follows choosing c proof following lemma inferred 4 include completeness lemma 2 exists c oe x neighborhood x proof let r 0 bx r ae w x choose r 2 always possible x 2 bx thus x r intermediate value theorem implying turn rg degenerate nonlinear programming quadratic growth condition 7 take x infeasible ff 1 0 exists g applies give x feasible ff previous bound still applies quadratic growth assumption 1 feasibility x ff 1 p must 23 taylors theorem c p choose c p c p 23 c p c p c p c p using 25 24 26 get conclusion follows cauchyschwartz inequality assume c oe previous lemma satisfies 17 otherwise replace righthand side 17 conclusion lemma still holds new c oe prove following results use results 12 concerning sets defined linear inequalities set denote dx p distance point x 2 ir n set p also denote dp x maximum value infeasibility exists number p 0 equality constraints recast two inequality constraints following lemma uses fact mx polyhedral thus expressed form 27 lemma 3 let index set exists multiplier lambda exists constant c 8 2 mx exists 2 mx jj gamma jj c jj jj 1 vector denoted restriction vector index set proof let x set 2 mx 0 0 32 assumptions x empty eventually rescaling x space assume without loss generality vectors defining equality constraints 30 norm 1 otherwise entries 0 remove row feasible set remains unchanged described alternative way 0 35 0 36 0 37 row described unit vector puts set form 27 thus 12 exists 0 dmi 38 however since 2 mx valid multiplier set constraints 0 35 violated thus conclusion follows 38 taking c proof complete define iaef1mg c feasible x lemma 4 exists neighborhood w x 8x 2 w 2 mx implies exists 2 mx 0 degenerate nonlinear programming quadratic growth condition 9 proof assume contrary exists sequence x k x exists k 2 mx index set 0 6 0 finite set index sets extract infinite subsequence happens fixed set extracting another subsequence assume k convergent 16 fact mx compact k 2 mx 0 contradiction use extensively h twice continuously differen tiable continuous function 3h indeed taylors theorem exist continuous functions 1 3h 3h 3h 3h relation 40 follows comparing last two equations theorem 1 exists constant c oe 0 neighborhood x solution qp 14 proof 16 exists 2 mx jj gamma jj c jjx gamma x jj let set indices jj jj c jjx gamma x jj 39 lemmas 3 4 exists 2 mx result important consequence fact using complementarity relations 15 gamma indeed equalities 0 lemma 2 solution 15 also used 40 employ identity ab 42 taylors theorem rgx get continuing previous equation denote sufficiently close x using 1 40 43 42 get x using bound 52 together gammad degenerate nonlinear programming quadratic growth condition 11 choose sufficiently small neighborhood x jjx gamma x jj oeand subtract last term last relation lower bound take new notation get treat jjx gamma x jj variable using formulas quadratic equation get oe using arithmeticquadratic mean inequality get choosing oe c oe g 67 prove claim corollary 1 x isolated stationary point proof let x another stationary point nlp neighborhood x theorem holds therefore exists 2 mx satisfying 3 hence solution 15 unique solution strictly convex qp 14 since feasible 14 p x complementarity conditions 15 get previous theorem get proves claim corollary 2 secondorder sufficient condition 8 satisfied one multiplier mfcq holds x x isolated stationary point proof since x satisfies quadratic growth condition 1 assumptions 67 mfcq holds corollary 1 applies 3 example without locally convex augmented lagrangian consider matrix take u 1 since vector corresponds positive eigenvalue u angle 6 u 0 u qu 1jjujj 2 consider rotation matrix since q 0 q 2 axes symmetry eigenvalues switched also u 2 ir 2 exists k u q k u 1jjujj 2 since wide cones centered axis positive eigenvalues q k sweep entire ir 2 consider optimization problem minz subject z previous observation z 1x 2 2 feasible set thus z 0 clearly solution problem 0 0 0 since z z 2 4 z 1x 2 2 z 2 x therefore x quadratic growth condition satisfied nlp constant 1 obviously mfcq holds 0 0 0 simple calculation shows multiplier 70 particular least one multiplier positive also 0 0 0 constraints active gradients 0 0 gamma1 result linear constraints 8 become either z 0 z 0 least one therefore critical cone x 0g also 3 2 mx assume choice 2 mx l xx hessian lagrangian positive semidefinite critical cone za 0 8x z z 0 71 equivalent x since construction invariant rotations u positive semidefiniteness holds circular permutation oe multiplier set x denote c 4 set circular permutations four elements since set positive definite matrices convex cone must impossible therefore l xx cannot positive semidefinite critical cone choice 2 mx hence secondorder conditions 621 hold choice multipliers degenerate nonlinear programming quadratic growth condition 13 31 augmented lagrangian approaches discuss expected behavior augmented lagrangian techniques applied example methods inequalities nlp 2 converted equalities 35 feasible set represented 5 nlp replaced boundconstrained optimization problem equality constraints incorporated objective function based estimate multipliers penalty term subject 0 barrier parameter objective function 76 augmented lagrangian problem subjected additional trustregion constraint 5 enforce global convergence desired outcome bounded away zero trustregion inactive approaches mx solution problem approaches x happens example continuity argument following lower boundedness solution 76 appropriate choice since 76 linearly independent gradients constraints first second order necessary conditions must hold 7 first order necessary condition results components 0 multipliers associated variables result 2 mx second order necessary conditions require 4 positive semidefinite least subspace ffix ffi results proved last matrix cannot positive semidefinite example thus get contradiction shows either trust region active arbitrarily close x 0 14 mihai anitescu also shows hessian augmented lagrangian equality constrained problem f xx x positive semidefinite thus augmented lagrangian equality constrained problem cannot locally convex 4 linear convergence sqp nondifferentiable exact penalty p x points x considered thus subsection assumed sufficiently close x notation 2 mx refer solutions 14 15 also p x l1 penalty function 10 41 proof technical results lemma 5 proof since feasible point 14 rg x gammag x 8i 2 mg taylors remainder theorem hence completes proof lemma 6 exist ff proof writing kkt conditions 14 obtain hence rfx degenerate nonlinear programming quadratic growth condition 15 since complementarity conditions satisfied solution 14 rgx therefore since g gammad gammad 10 17 taylors remainder theorem hence ff 2 0 1 ffgammad 79 lemma 5 therefore ff 2 0 1 result statement follows choosing lemma 7 exists constant c 5 8 2 mx proof 15 definition lagrangian 4 follows using tay lors theorem sufficiently small neighborhood x g also 16 choose 2 mx thus therefore conclusion lemma follows choosing c 1g 42 nondifferentiable exact penalty algorithms linear convergence theorem linearization algorithm 3 p372 following form 1 2 compute k 11 3 choose ff k line search procedure set x 4 return step 2 stepsize ff k chosen one following procedures 3 p372 minimization rule ff k chosen b limited minimization rule fixed scalar 0 selected ff k chosen c armijo rule fixed scalars oe 0 2 0 1 oe 2 0 1 chosen set ff first nonnegative integer shown armijo rule yields stepsize finite number iterations following theorem establishes convergence properties linearization algorithm global convergence properties established 2 prop 433 also stated completeness theorem 2 let x k sequence generated linearization algorithm stepsize ff k chosen minimization rule limited minimization rule armijo rule accumulation point sequence x k stationary point strict local minimum problem 2 satisfying local quadratic growth 1 mangasarianfromowitz constraint qualification 5 oex k qlinearly x k x rlinearly proof first part immediate consequence 2 prop 433 prove linear convergence statement armijo rule proof similar stepsize selection mechanisms lemma 6 ff 2 0 ff since k smallest integer degenerate nonlinear programming quadratic growth condition 17 follows ff therefore ensures stepsize least ff k sufficiently large result lemma 6 hand lemma 7 theorem 1 previous relation follows exists c c6 using lemma 6 obvious manipulation follows proves qlinear convergence 19 sequence oex k oex linear rate ffigamma1 therefore lim sup lemma 2 therefore lim sup proves rlinear convergence 19 0 sequence x proof complete following techniques 1 extend result case matrix h qp changes iteration iteration condition sequence strictly convex h k uniformly upper lower bounded iteration oex k gammaoex 9 400 table 1 rates convergence l1 penalty algorithm iteration new penalty parameter trust region radius jjjj 1 43 1e4 11 e02 268 1e14 193 283 1e16 441 e02 336 stop table 2 reduction penalty parameter lancelot 5 numerical experiments degenerate nlp experimented several nonlinear programming packages example section 3 certainly comparing behavior nlp algorithms unique degenerate example cannot result complete characterization nev ertheless may interest determine whether methods using augmented lagrangians really encounter problems solving example without positive semidefinite augmented lagrangian also desire validate theoretical conclusions preceding sections shifted origin example avoid one step convergence algorithms start 0 0 0 default algebraic form example minz analysis w minimum satisfying quadratic growth condition 1 z gamma 0 feasible x feasible set described figure 5 lateral view quadratic growth 1 1 0 fairly obvious curvature ridges appear intersection two constraints shape feasible set also clear 1 1 0 unique stationary point nlp among solvers used minos 17 snopt 11 use quasinewton methods require secondorder derivatives constraints degenerate nonlinear programming quadratic growth condition 19 feasible set view center 110 lateral view fig 1 feasible set nonlinear program 89 110 local minimum satisfying quadratic growth condition 1 jagged edges lateral view meshing effect also use augmented lagrangian merit function donlp2 23 solves linear system instead quadratic program iteration uses l 1 penalty function lancelot 5 uses augmented lagrangian technique conjunction trustregion filtersqp 8 also uses trust region approach special classification relative merits iterates instead nonlinear solver jjx final gamma x jj 2 iterations message termination filtersqp 526e09 28 convergence lancelot 865e07 336 step size small linf 105e08 28 step size small loqo 160e07 200 iteration limit loqo 550e07 1000 iteration limit minos 476e06 27 current point cannot improved snopt 337e07 3 optimal solution found table 3 runs various nonlinear solvers problem 89 penalty merit function loqo 24 interiorpoint approach finally linf ad hoc matlab implementation l1 exact penalty function described preceding section armijo rule latter algorithm started 0 0 0 runs except l1 penalty filtersqp algorithms done neos server 18 additional documentation found solvers small example time execution relevant comparing behavior solvers since solution problem known chose criteria comparison best achievable solution set relevant tolerances 1e gamma 16 via ampl interface neos smaller tolerances may interfere machine precision though solvers gave comparable answers even tolerances set 1e gamma 20 larger tolerances 1e gamma 12 resulted similar results whenever allowed also changed limiting parameters intrinsic stopping decision issued exception donlp2 converged digits mantissa default settings table 1 shows ratios oex k gammaoex various iterations implementation linf close 400 consistent qlinear convergence claim oex table 2 shows lancelot decreases succesively value penalty parameter 16 orders magnitude stops message step size small indeed one alternatives allowed analysis subsection 31 0 undesirable outcome since subproblems may become harder solve results runs illustrated table 3 seen solvers use augmented lagrangians minos snopt lancelot exhibit error least one order magnitude larger compared algo rithms however one would expect snopt minos would least good behavior linf would use different merit function since nature qp solved similar 14 increasing iteration limit loqo result better outcome interesting note outcome filtersqp linf differ factor 2 number iterations though filtersqp uses secondorder information whereas linf linf filtersqp solve quadratic programs degenerate nonlinear programming quadratic growth condition 21 iteration donlp2 remarkable behavior though investigation necessary determine whether general implications impossible draw general conclusion one example however seems adverse bias methods using augmented lagrangians degenerate nlps one advocating use linf general nlp since similarity steepest descent makes sensitive illconditioning fact gives outcome comparable one solvers using secondorder information shows better results different way incorporating secondorder derivatives may necessary 6 conclusions work analyze behavior nonlinear programs presence constraint degeneracy linear dependence gradients active constrains problems interest exhibit minima quadratic growth property satisfy mangasarianfromowitz constraint qualification novelty approach studying sqp convergence properties assume positive semidefiniteness hessian lagrangian critical cone feasible lagrange multipliers conditions equivalent weak secondorder sufficient condition 1522 prove assumptions data problem twice continuously differentiable target minimum isolated stationary point nlp also show started sufficiently close minimum l1 exact penalty sqps induce qlinear convergence values penalized objective rlinear convergence iterates shows methods robust respect constraint degeneracy give example nonlinear program unique minimum satisfies conditions hessian lagrangian positive semidefinite critical cone feasible choice multipliers direct consequence fact augmented lagrangian positive semidefinite solution therefore lagrange multipliers algorithms drive penalty parameter zero examples unless trust region active even convergence provide computational experience small nonlinear program criteria comparison used best achievable solution obtained tuning parameters algorithms observed example algorithms use augmented lagrangians resulted errors one order magnitude larger compared approaches lagrange multiplier package used lancelot 5 confined decrease substantially value penalty parameter 16 orders magnitude one outcomes allowed analysis linear convergence results concerning l1 penalty function also validated experiments 22 mihai anitescu undoubtedly small experiment insufficient draw conclusions especially approaches theory assump tions interiorpoint algorithms however theory experiments appear methods use augmented lagrangians less robust respect constraint degeneracy compared sqp believe attempting develop convergence theory absence usual secondorder conditions interesting may result algorithms robust virtue fact properties depend fewer assumptions however improve current results especially define reliable variants newton method possible case subject future research acknowledgments thanks stephen wright jorge danny ralph many discussions subject david gay sven leyffer chijen lin kindly provided information support numerical examples r rate convergence sequential quadratic programming nondifferentiable exact penalty function presence constraint degeneracy new york secondorder sufficiency quadratic growth nonisolated minima lancelot fortran package largescale nonlinear optimization introduction sensitivity stability analysis practical methods optimization nonlinear programming without penalty function necessary sufficient regularity condition bounded multipliers nonconvex programming differential stability nonlinear programming users guide snopt 53 fortran package largescale nonlinear programming relaxation method solving systems linear inequalities stabilized sequential quadratic programming stability presence degeneracy error estima tion necessary sufficient conditions local minimum3 second order conditions augmented duality sensitivity analysis nonlinear programs banach spaces approach via composite unconstrained optimization neos guide iterative solutions nonlinear equations several vari ables superlinear convergence interiorpoint method despite dependent constraints generalized equations solutions part ii applications nonlinear programming sensitivity analysis nonlinear programs differentiability properties metric projections sqp method general nonlinear programs using equality constrained subproblems interiorpoint code quadratic programming superlinear convergence stabilized sqp method degenerate lution tr ctr jinbao jian superlinearly convergent implicit smooth sqp algorithm mathematical programs nonlinear complementarity constraints computational optimization applications v31 n3 p335361 july 2005