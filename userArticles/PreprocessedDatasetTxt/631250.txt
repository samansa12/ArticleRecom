quantitative analysis faults failures complex software system abstractthe dearth published empirical data major industrial systems one reasons software engineering failed establish proper scientific basis paper hope provide small contribution body empirical knowledge describe number results quantitative study faults failures two releases major commercial system tested range basic software engineering hypotheses relating pareto principle distribution faults failures use early fault data predict later fault failure data metrics fault prediction benchmarking fault data example found strong evidence small number modules contain faults discovered prerelease testing small number modules contain faults discovered operation however neither case explained size complexity modules found evidence support previous claims relating module size fault density find evidence popular complexity metrics good predictors either faultprone failureprone modules confirmed number faults discovered prerelease testing order magnitude greater number discovered 12 months operational use also discovered fairly stable numbers faults discovered corresponding testing phases surprising important result strong evidence counterintuitive relationship pre postrelease faults modules faultprone prerelease among least faultprone postrelease conversely modules faultprone postrelease among least faultprone prerelease observation serious ramifications commonly used fault density measure misleading use surrogate quality measure previous extensive use metrics studies shown flawed results provide datapoints building empirical picture software development process however even strong results observed generally valid software engineering laws fail take account basic explanatory data notably testing effort operational usage module tested used reveal faults irrespective size complexity factor b introduction despite heroic efforts small number research centres individuals see example carman et al 1995 kaaniche kanoun 1996 khoshgoftaar et al 1996 ohlsson n alberg 1996 shen et al 1985 continues dearth published empirical data relating quality reliability realistic commercial software systems two best important studies adams 1984 basili perricone 1984 12 years old adams study revealed great proportion latent software faults lead rare failures practice vast majority observed failures caused tiny proportion latent faults adams observed remarkably similar distribution fault sizes across nine different major commercial systems one conclusion adams study removing large numbers faults may negligible effect reliability small proportion large faults removed reliability improve significantly basili pericone looked number factors influencing fault failure proneness modules one notable results larger modules tended lower fault density smaller ones fault density number faults discovered predefined phase testing operation divided measure module size normally kloc fault density measure numerous weaknesses quality measure see fenton pfleeger 1996 indepth discussion result nevertheless surprising appears contradict basic hypotheses underpin notions structured modular programming curiously result rediscovered systems moeller paulish 1995 recently hatton provided extensive review similar empirical studies came conclusion compelling empirical evidence disparate sources implies software system larger components proportionally reliable smaller components hatton 1997 thus various empirical studies thrown results counterintuitive basic popular software engineering beliefs studies warning software engineering research community importance establishing wide empirical basis yet warnings clearly heeded fenton et al 1994 commented almost total absence empirical research evaluating effectiveness different software development testing methods also continues almost total absence published benchmarking data paper hope provide small contribution body empirical knowledge describing number results quantitative study faults failures two releases major commercial system section 2 describe background study basic data collected section 3 provide pieces evidence one day reasonable number similar studies published may help us test basic software engineering hypotheses particular present range results examine extent provide evidence following hypotheses hypotheses relating pareto principle distribution faults failures 1a small number modules contain faults discovered prerelease 1b small number modules contain faults discovered prerelease testing simply modules constitute code size 2a small number modules contain faults cause failures 2b small number modules contain operational faults simply modules constitute code size hypotheses relating use early fault data predict later fault failure data module level higher incidence faults function testing implies higher incidence faults system testing higher incidence faults prerelease testing implies higher incidence failures operation tested hypotheses absolute normalised fault perspective hypotheses metrics fault prediction loc good predictors fault failure prone modules complexity metrics better predictors simple size metrics fault failureprone modules hypotheses relating benchmarking figures quality terms defect densities fault densities corresponding phases testing operation remain roughly constant subsequent major releases software system systems produced similar environments broadly similar fault densities similar testing operational phases particular system studied provide strong evidence hypotheses also explain previous studies looked hypotheses flawed hypotheses 1a 2a strongly supported 1b 2b strongly rejected hypothesis 3 weakly supported curiously hypothesis 4 strongly rejected hypothesis 5 partly supported hypotheses 6 weakly rejected popular complexity metrics however certain complexity metrics extracted early design specifications shown reasonable fault predictors hypothesis 7 partly supported 8 tested properly organisations publish analogous results discuss results depth section 4 2 basic data data presented paper based two major consecutive releases large legacy project developing telecommunication switching systems refer earlier releases release n later release release n1 study 140 246 modules respectively release n n1 selected randomly analysis set modules either new modified modules ranged size approximately 1000 6000 loc shown table 1 releases approximately total system size table 1 distribution modules sizeloc release n release n1 1000 23 26 10012000 58 85 20013000 37 73 30014000 15 38 total 140 246 21 dependent variable dependent variable study number faults faults traced unique modules fault data collected four different phases function test system test st first 26 weeks number site tests si first year approx operation op therefore module four corresponding instances dependent variable testing process environment used project well established within company developed maintained taught applied number years team separated design implementation organisation develop test cases based early function specifications throughout paper refer combination ft st faults collectively testing faults refer combination si op faults collectively operational faults shall also refer times failures formally failure observed deviation operational system behaviour specified expected behaviour failures traced back unique operational fault module observation distinct failures traced fault counted separately means example 20 op faults recorded module x 20 unique faults caused set failures observed traced back faults module x first year operation company classified fault found phase according following fault already corrected b fault corrected c fault requires action ie treated fault fault due installation problems paper considered faults classified b internal investigations shown documentation faults classification according categories reliable summary number faults discovered testing phase system release shown table 2 prerelease faults postrelease faults release function test system test site test operation sample size 140 modules n1 sample size 246 modules table 2 distribution faults per testing phase 22 independent variables various metrics collected module included lines code loc main size measure mccabes cyclomatic complexity various metrics based communication modelled signals modules within module specification phase number new modified signals similar messages module specified notably metric sigff count number new modified signals metric also used measure interphase complexity ohlsson alberg 1996 provides full details metrics computation complexity metrics collected automatically actual design documents using tool erimet ohlsson 1993 automation possible module designed using fctool tool formal description language fdl related sdls process diagrams turner 1993 metrics extracted direct fdl graphs fact metrics computed artefacts available design stage important point often asserted computing metrics design documents far valuable metrics source code heitkoetter et al 1990 however published attempts kitchenham et al 1990 reported using design metrics based henry kafuras information flow metrics 1981 1984 outlier analysis khoshgoftaar et al 1996 used subset metrics could collected design documentation metrics extracted code numerous studies ebert liedtke 1995 munson khoshgoftaar 1992 reported using metrics extracted source code reported promising prediction results based design metrics 3 hypotheses tested results since data collected analysed retrospectively possibility setting controlled experiments however sheer extent quality data could use test number popular software engineering hypotheses relating distribution prediction faults failures section group hypotheses four categories section 31 look hypotheses relating pareto principle distribution faults failures widely believed example small number modules system likely contain majority total system faults often referred 2080 rule sense 80 faults contained 20 modules show strong evidence support two commonly cited pareto principles assumption pareto principle faults led many practitioners seek methods predicting faultprone modules earliest possible development testing phases methods seem fall two categories 1 use early fault data predict later fault failure data 2 use product metrics predict fault failure data given evidence support pareto principle therefore test number hypotheses relate methods early prediction faultprone modules section 32 test hypotheses concerned section 33 test hypotheses concerned 2 finally section 34 test hypotheses relating benchmarking fault data time provide data valuable future benchmarking studies 31 hypotheses relating pareto principle distribution faults failures main part total cost quality deficiency often found caused faults fault types bergman klefsjo 1991 pareto principle juran 1964 also called 2080 rule summarises notion pareto principle used concentrate efforts vital instead trivial many number examples pareto principle software engineering gained widespread acceptance notion given software system faults lie small proportion software modules adams 1984 demonstrated small number faults responsible large number failures munson et al 1992 motivated discriminative analysis referring 2080 rule even though data demonstrated rule zuse 1991 used pareto techniques identify common types faults found function testing finally schulmeyer macmanus 1987 described principle supports defect identification inspection applied statistical techniques investigated four related pareto hypotheses hypothesis 1a small number modules contain faults discovered prerelease testing phases ft st hypothesis 1b small number modules contain faults discovered prerelease testing simply modules constitute code size hypothesis 2a small number modules contain operational faults meaning failures defined observed phases si op hypothesis 2b small number modules contain operational faults simply modules constitute code size examine turn 311 hypothesis 1a small number modules contain faults discovered testing phases ft st figure 1 illustrates 20 modules responsible nearly 60 faults found testing release n almost identical result obtained release n1 shown also almost identical result earlier work faults testing operation considered ohlsson et al 1996 together results munson et al 1992 provides strong support hypothesis 1a even suggests specific pareto distribution area 2060 2060 finding strong one observed compton withrow 1990 found 12 modules referred packages accounted 75 faults system integration test nevertheless important2060100 modules faults figure 1 pareto diagram showing percentage modules versus percentage faults release n 312 hypothesis 1b small number modules contain faults discovered prerelease testing simply modules constitute code size since found strong support hypothesis 1a makes sense test hypothesis 1b popularly believed hypothesis 1a easily explained away fact small proportion modules causing faults actually constitute system size example compton withdraw 1990 found 12 modules accounting 75 faults accounted 63 loc study found evidence support hypotheses 1b release n 20 modules account 60 faults discussed hypothesis 1a actually make 30 system size result release n1 almost identical 313 hypothesis 2a small number modules contain operational faults meaning failures defined namely phases cu op discovered support pareto distribution much exaggerated one hypothesis 1a figure 2 illustrates pareto effect release n 10 modules responsible 100 failures found result release n1 remarkable nevertheless still quite striking 10 modules responsible 80 failures2060100 failures modules figure 2 pareto diagram showing percentage modules versus percentage failures release n 314 hypothesis 2b small number modules contain operational faults simply modules constitute code size hypothesis 1a popularly believed hypothesis 2a easily explained away fact small proportion modules causing failures actually constitute system size fact find evidence hypothesis 2 discovered strong evidence favour converse hypothesis operational faults caused faults small proportion code release n 100 operational faults contained modules make 12 entire system size release n1 60 operational faults contained modules make 6 entire system size 78 operational faults contained modules make 10 entire system size 32 hypotheses relating use early fault data predict later fault failure data given likelihood hypotheses 1a 2a strong case trying predict faultprone modules early possible development next subsection test hypotheses relating methods precisely first look use fault data collected early means predicting subsequent faults failures specifically test hypotheses hypothesis 3 higher incidence faults function testing ft implies higher incidence faults system testing st hypothesis 4 higher incidence faults prerelease testing ft st implies higher incidence faults postrelease operation si op tested hypotheses absolute normalised fault perspective examine results 321 hypothesis 3 higher incidence faults function testing ft implies higher incidence faults system testing st results associated hypothesis strong release n see figure 3 50 faults system test occurred modules responsible 37 faults function test 0 20 40 80 100 15 30 45 60 75 90 ft modules accumalated faults st figure 3 accumulated percentage absolute number faults system test modules ordered respect number faults system test function test release n prediction perspective figures indicate faultprone modules function test extent also faultprone system test however 10 faultprone modules system test responsible 38 faults system test 10 faultprone modules function test responsible 17 faults system test persistent 75 modules means nearly 20 faults system test need explained another way pattern found using normalised data faultsloc instead absolute even though percentages general lower prediction bit poorer results slightly different release n1 found 50 faults system test occurred modules responsible 25 faults function test 10 faultprone modules system test responsible 46 faults system test 10 faultprone modules function test responsible 24 faults system test results also using normalised data instead absolute similar result release n 322 hypothesis 4 higher incidence faults prerelease testing ft st implies higher incidence faults postrelease operation si op rationale behind hypothesis 4 relatively small proportion modules system account faults likely faultprone pre post release modules somehow intrinsically complex generally poorly built want find faults lie look found past common popular maxim example compton withrow 1990 found much six times greater post delivery defect density analysing modules faults discovered prior delivery many respects results study relating hypothesis remarkable evidence support hypothesis strong evidence support converse hypothesis release n release n1 almost faults discovered prerelease testing appear modules subsequently reveal almost operation faults specifically found release n see figure 4 93 faults prerelease testing occur modules subsequent operational faults 75 total thus 100 75 failures operation occur modules account 7 faults discovered prerelease testing postrelease faults261014 prerelease faults figure 4 scatter plot prerelease faults postrelease faults version n dot represents module release n1 observed much greater number operational faults similar phenomenon release n see figure 5 77 prerelease faults occur modules postrelease faults thus 100 366 failures operation occur modules account 23 faults discovered function system test remarkable results also exciting closely related adams phenomenon results major ramifications one commonly used software measures fault density specifically appears modules high fault density prerelease likely low faultdensity postrelease vice versa discuss implications length section 4 33 hypotheses metrics fault prediction previous subsection concerned using early fault counts predict subsequent fault prone modules absence early fault data widely proposed software metrics automatically computed module designs code used predict fault prone modules fact widely considered major benefit metrics fenton pfleeger 1997 therefore attempted test basic hypotheses underpin assumptions specifically tested hypothesis 5 size metrics loc good predictors fault failure prone modules hypothesis complexity metrics better predictors simple size metrics especially predicting faultprone modules 331 hypothesis 5 size metrics loc good predictors fault failure prone modules strictly speaking test several different closely related hypotheses hypothesis 5a smaller modules less likely failure prone larger ones hypothesis 5b size metrics loc good predictors number prerelease faults module hypothesis 5c size metrics loc good predictors number postrelease faults module postrelease faults5152535 prerelease faults figure 5 scatter plot prerelease faults postrelease faults version dot represents module hypothesis 5d size metrics loc good predictors modules prerelease faultdensity hypothesis 5e size metrics loc good predictors modules postrelease faultdensity hypothesis 5a underpins many respects principles behind modern programming methods modular structured objected oriented general idea smaller modules easier develop test maintain thereby leading fewer operational faults hand also accepted modules made small complexity pushed interfacecommunication mechanisms size guidelines decomposing system modules therefore desirable organisations turns small number relevant empirical studies produced counterintuitive results relationship size operational fault density basili pericone 1984 reported fault density appeared decrease module size explanation large number interface faults spread equally across modules relatively high proportion small modules also offered explanation authors moeller paulish 1995 observed similar trend suggested larger modules tended better configuration management smaller ones tended produced fly fact study reveal similar trend believe strong results previous studies may due inappropriate analyses begin results replication key part basili pericone 1984 study table 3 compare basili perricones table iii shows number modules certain number faults table also displays figures different types modules percentages data set analysed paper comparison basili pericone 1984 lower proportion modules faults proportion new modules lower subsequent analysis new modules excluded modules also generally larger basili pericone 1984 believe introduces bias scatter plots figure 6 lines code versus number pre postrelease faults reveal strong evidence trends release n1 neither could strong trends observed line code versus total number faults graphed figure 7 results release n reasonably similar20601001400 2000 4000 6000 8000 10000 postrelease faults faults lines code lines code figure scatterplots loc pre postrelease faults forrelease n1 dot represents modulerelease n release n1 fault mod new percent modified modules mod new splitted percent modified modules 11 15 21 26 31 36 40 table 3 number modules affected fault release n 140 modules 1815 faults release n1 246 modules 3795 faults faults lines code figure 7 scatterplots loc faults release n1 dotrepresents modulewhen basili pericone could see trend calculated number faults per 1000 executable lines code table 4 compares table vii basili pericone 1984 shows results study release n release n1 module size frequency faults1000 lines frequency faults1000 lines 1000 15 477 17 6 2000 3000 22 574 37 5 table 4 faults1000 lines code release n n1 superficially results table 4 release n1 appear support basili pericone finding release n1 clear smallest modules highest fault density however fault density similar groups release n result opposite reported basili perricone approach grouping data done basili perricone 1984 highly misleading basili pericone failed show simple plot fault density module size done figure 9 release n1 even though grouped data release appeared support basili pericone findings graph shows high variation small modules evidence module size significant impact faultdensity clearly explanatory factors design inspection testing effort per module important lines code figure 8 scatter plot module fault density size release n1 scatter plots assumes data belong interval ratio scale prediction perspective always necessary fact number studies built pareto principle often require ordinal data tests hypothesis used technique based ordinal data called alberg diagrams ohlsson alberg 1996 evaluate independent variables ability rank dependent variable loc ranking ability assessed figure 9 diagram reveals even though previous analysis indicate predictability loc quite good ranking faultprone modules fault pronemodules 20 percent much better previous ones 0 20 40 80 100 10 20 40 60 80 100 faults accumalated faults modules figure 9 accumulated percentage absolute number faults modules ordered respect loc release n1 332 hypothesis complexity metrics better predictors simple size metrics fault failureprone modules complexity metrics rather misleading term used describe class measures extracted directly source code structural model like flowgraph representation occasionally beneficially complexity metrics extracted code produced detailed designs represented graphical language like sdl case system study archetypal complexity metric mccabes cyclomatic number mccabe 1976 fact many dozens published zuse 1991 details also limitations complexity metrics extensively documented see fenton pfleeger 1996 wish revisit issues concerned underlying assumption complexity metrics useful easy extract indicators faults lie system example munson khosghoftaar asserted clear intuitive basis believing complex programs faults simple programs munson khosghoftaar 1992 implicit assumption complexity metrics better simple size measures respect little motivation use already seen section 331 size reasonable predictor number faults although fault density investigate case complexity metrics cyclomatic number demonstrated testing last hypothesis problem comparing average figures different size intervals instead replicating relevant analysis basili pericone 1984 calculating average cyclomatic number module size class plotting results generated scatter plots alberg diagrams cyclomatic complexity pre postrelease faults graphed release n1 figure 10 observed number interesting trends complex modules appear faultprone prerelease appear nearly faults postrelease faultprone modules postrelease appear less complex modules could explained test effort distributed modules modules appear complex treated extra care simpler ones analysing retrospect earlier graphs size versus faults reveal similar pattern scatter plot cyclomatic complexity total number faults figure 11 shows small indication correlation alberg diagrams similar size used20601001400 1000 2000 3000 faults cyclomatic complexity figure 11 scatterplot cyclomatic complexity faults release n1 dot represents module explore relations scatter plots also graphed normalised data figure 12 result showed even clearly mostfault prone modules prerelease nearly postrelease faults20601001400 1000 2000 3000 postrelease faults faults cyclomatic complexity cyclomatic complexity figure 10 scatterplots cyclomatic complexity number preand postrelease faults release n1 dot represents module order determine whether large modules less dense complex smaller modules basili perricone 1984 plotted cyclomatic complexity versus module size following pattern earlier analysis failed see trends therefore analysed relation grouping modules according size illustrated misleading instead graphed scatter plots relation calculated correlation figure 13 relation may linear however good linear correlation cyclomatic complexity loc 2 earlier studies ohlsson alberg 1996 suggested design metrics could used combination explain faultproneness therefore analysis using sigff measure instead cyclomatic complexity 0000 prerelease 0000 0004 0006 postrelease cyclomatic complexity cyclomatic complexity figure 12 scatterplots cyclomatic complexity fault density preand postrelease release n1 dot represents module10003000500070009000 cyclomatic complexity figure 13 complexity versus module size scatterplots using absolute numbers figure 14 normalised data indicate new trends earlier work product cyclomatic complexity sigff shown good predictor faultproneness evaluate ccsigff predictability alberg diagram graphed figure 15 combined metrics appear better sigff cyclomatic complexity also better size metric 0 20 40 80 100 20 40 60 10 80 100 faults dec accumulated faults modules figure 15 accumulated percentage absolute number faults modules ordered respect loc release n1 results paint glowing report usefulness complexity metrics argued good predictor fault density appropriate validation criteria complexity metrics discussed section 4 nevertheless positive aspects combined metric ccsigff shown reasonable predictor faultprone modules also measures like sigff unlike loc available early stage software development fact correlates closely final loc good predictor total number faults major benefit 34 hypotheses relating benchmarking one major benefits collecting publicising kind data discussed paper enable intra intercompany comparisons despite incredibly vast20601001400 100 200 300 400 postrelease faults faults interphase complexity interphase complexity figure 14 scatterplots sigff number preand postrelease faults release n1 dot represents module volumes software operation throughout world consensus constitutes example good bad average fault density certain fixed conditions measurement seem unreasonable assume information might known example commercial c programs faults defined operational faults sense paper first 12 months use typical user although individual companies may know kind data systems almost nothing ever published grey literature referenced example pfleeger hatton 1997 seems suggest crude unsubstantiated guidelines following fault density first 12 months typical operational use less 1 fault per kloc good typically achieved companies using stateoftheart development testing methods 4 8 faults per kloc typical greater 12 faults per kloc bad prerelease faults considered notion 1030 faults per kloc typical function system integration testing combined reasons discussed already high values prerelease fault density indicative poor quality may fact suggest opposite therefore would churlish talk terms good bad densities already stressed figures may explained key factors effort spent testing study consider following hypothesis hypothesis 7 fault densities corresponding phases testing operation remain roughly constant subsequent major releases software system since data successive releases results present based one system represents single datapoint nevertheless believe may also valuable researchers similar vein consider hypothesis 8 software systems produced similar environments broadly similar fault densities similar testing operational phases really hoping build idea range fault densities reasonably expected compare results published data 341 fault densities corresponding phases testing operation remain roughly constant subsequent major releases software system ft st si op rel n 349 260 007 020 rel n1 415 182 043 020 table 5 fault densities four phases testing operation table 5 shows support hypothesis faultdensity remains roughly subsequent releases exceptional phase si well providing support hypothesis result suggests development process stable repeatable respect faultdensity interesting implications software process improvement movement epitomised capability maturity model cmm general assumption cmm stable repeatable process necessary prerequisite continuous process improvement immature organisation level assumed take many years reach level cmms terminology companies kind stable repeatable process indicated figures level 3 yet like almost every software producing organisation world organisation case study project level 3 results reflects stability repeatability according cmm case question cmms underlying assumption constitutes organisation stable repeatable process 342 software systems produced similar environments broadly similar fault densities similar testing operational phases test hypothesis compared results case study published data simplicity restricted analysis two distinct phases 1 prerelease fault density 2 postrelease fault density first compare two results two separate releases cases study table 6 prerelease postrelease rel n 609 027 636 rel n1 597 063 660 table densities preand postrelease case study system overall fault densities similar reported range systems hatton 1995 agresti evanco 1992 reported similar ballpark figures study ada programs 30 55 faultskloc postrelease fault densities seem roughly line reported studies best practice interesting difference pre postrelease fault densities versions prerelease fault density order magnitude higher postrelease density published studies reveal difference pre postrelease fault density pfleeger hatton 1997 also report 10 times many faults prerelease although overall fault density lower kitchenham et al 1986 reports higher ratio prerelease postrelease study investigation impact inspections combining inspected noninspected code together reveals prerelease fault density approx per kloc postrelease fault density approximately 03 per kloc however likely operational time long thus small amount evidence conclude appears 1030 times many faults prerelease post release 4 discussion conclusions apart usual quality control angle important perceived benefit collecting fault data different testing phases able move toward statistical process control software development example basis software factory approach proposed japanese companies hitachi yasuda koga 1995 build fault profiles enable claim accurate fault failure prediction another important motivation collecting various fault data enable us evaluate effectiveness different testing strategies paper used extensive example fault failure data test range popular software engineering hypotheses results presented come two releases major system developed single organisation may therefore tempting observers dismiss relevance broader software engineering community attitude would dangerous given rigour extensiveness datacollection also strength observations evidence found support two pareto principles 1a 2a least surprising seem inevitable small number modules system contain large proportion prerelease faults small proportion modules contain large proportion postrelease faults however popularly believed explanations two phenomena appear quite wrong case size explains significant way number faults many people seem believe hypotheses 1b 2b reason small proportion modules account faults simply faultprone modules disproportionately large therefore account system size shown assumption false system case complexity least complexity measured complexity metrics explains faultprone behaviour hypothesis 6 fact complexity significantly better predicting fault failure prone modules simple size measures also case set modules especially faultprone prerelease going roughly set modules especially faultprone postrelease hypothesis 4 yet view seems widely accepted partly assumption certain modules intrinsically difficult throughout testing operational life strong rejection hypothesis 4 important observation many believe first place look modules likely faultprone operation modules fault prone testing fact results relating hypothesis 4 suggest exactly opposite testing strategy effective want find modules likely faultprone operation ignore modules faultprone testing reality danger assuming given data provides evidence causal relationship data observed explained fact modules faults discovered testing may simply tested properly modules reveal large numbers faults testing may genuinely well tested sense faults really tested key missing explanatory data case course testing effort results hypothesis 4 also bring question entire rationale way software complexity metrics used validated ultimate aim complexity metrics predict modules faultprone postrelease yet found relationship modules faultprone prerelease modules faultprone postrelease previous validation studies complexity metrics deemed metric valid correlates prerelease fault density results suggest valid metrics may therefore inherently poor predicting supposed predict results hypothesis 4 also highlight dangers using fault density defacto measure user perceived software quality fault density measured terms prerelease faults common module level measure tells us worse nothing quality module high value likely indicator extensive testing poor quality analysis value complexity metrics mixed confirmed previous studies results popular complexity metrics closely correlated size metrics like loc loc hence also complexity metrics reasonable predictors absolute number faults poor predictors fault density really however complexity metrics like sigff unlike loc available early stage software development process fact correlates closely final loc therefore useful moreover argued fenton pfleeger 1996 good predictor faultproneness may appropriate test validity complexity metric reasonable expect complexity metrics good predictors module attributes comprehensibility maintainability investigated extent benchmarking type data could provide insights software quality testing hypotheses 7 8 showed fault densities roughly constant subsequent major releases data indicates 10 times many prerelease faults postrelease faults even readers uninterested software engineering hypotheses 16 surely value publication figures future comparisons benchmarking believe software engineering laws always possible construct system environment contradicts law example studies summarised hatton 1997 suggest larger modules lower fault density smaller ones apart fact found clear evidence hypothesis 5 also found weaknesses studies would dangerous state law software engineering need change amount testing buck law test use module observe faults failures associated association size fault density causal one kind reason recommend complete models enable us augment empirical observations explanatory factors notably testing effort operational usage sense results justify recent work building causal models software quality using bayesian belief networks rather traditional statistical methods patently inappropriate defects predictionneil fenton 1996 case study system described paper datacollection activity considered part routine configuration management quality assurance used data shed light number issues central software engineering discipline companies shared kind data software engineering discipline could quickly establish empirical scientific basis sorely lacks acknowledgements indebted martin neil valuable input work pierrejacques courtois karama kanoun jeanclaude laprie stuart mitchell valuable review comments work supported part epsrcfunded project impress espritfunded projects deva serene swedish national board industrial technical development ericsson utvecklings ab r project software defects analyzing ada designs estimating fault content software suing fixonfix model prediction control ada software defects integrated approach criticality prediction rigorous practical approach 2nd edition tapping wheels software design metrics aids automatic collection software structure metrics based information flow reliability telecommunications system software reliability analysis three successive generations switching system software dependability telephone switching system experience software reliability data collection quantitative evaluation early quality prediction case study telecommunications effects inspections software quality productivity evaluation design metrics predicting software quality using bayesian belief networks predicting errorprone software modules telephone switches using formal description techniques introduction estelle research structured programming empiricists evaluation product development quality software factory analysis several software defect models software complexity measures methods tr ctr wang hassan guedem w abdelmoez k gosevapopstojanova h ammar architectural level risk assessment tool based uml specifications proceedings 25th international conference software engineering may 0310 2003 portland oregon norman fenton paul krause martin neil software measurement uncertainty causal modeling ieee software v19 n4 p116122 july 2002 mechelle gittens hanan lutfiyya michael bauer david godwin yong woo kim pramod gupta empirical evaluation system regression testing proceedings 2002 conference centre advanced studies collaborative research p3 september 30october 03 2002 toronto ontario canada thomas j ostrand elaine j weyuker distribution faults large industrial software system acm sigsoft software engineering notes v27 n4 july 2002 parastoo mohagheghi reidar conradi ole killi henrik schwarz empirical study software reuse vs defectdensity stability proceedings 26th international conference software engineering p282292 may 2328 2004 thomas j ostrand elaine j weyuker robert bell predicting location number faults large software systems ieee transactions software engineering v31 n4 p340355 april 2005 thomas j ostrand elaine j weyuker robert bell bugs acm sigsoft software engineering notes v29 n4 july 2004 piotr tomaszewski lars lundberg hkan grahn improving fault detection modified code study telecommunication industry journal computer science technology v22 n3 p397409 may 2007 anneliese andrews catherine stringfellow quantitative analysis development defects guide testing case study software quality control v9 n3 p195214 november 2001 patrick knab martin pinzger abraham bernstein predicting defect densities source code files decision tree learners proceedings 2006 international workshop mining software repositories may 2223 2006 shanghai china gerard j holzmann economics software verification proceedings 2001 acm sigplansigsoft workshop program analysis software tools engineering p8089 june 2001 snowbird utah united states giovanni denaro mauro pezz empirical evaluation faultproneness models proceedings 24th international conference software engineering may 1925 2002 orlando florida robert bell thomas j ostrand elaine j weyuker looking bugs right places proceedings 2006 international symposium software testing analysis july 1720 2006 portland maine usa robyn r lutz ines carmen mikulski operational anomalies cause safetycritical requirements evolution journal systems software v65 n2 p155161 15 february austen rainer dorota jagielska tracy hall software engineering practice versus evidencebased software engineering research acm sigsoft software engineering notes v30 n4 july 2005 elaine j weyuker using operational distributions judge testing progress proceedings acm symposium applied computing march 0912 2003 melbourne florida nachiappan nagappan thomas ball use relative code churn measures predict system defect density proceedings 27th international conference software engineering may 1521 2005 st louis mo usa sajjad mahmood richard lai complexity measure uml componentbased system specification softwarepractice experience v38 n2 p117134 february 2008 gne koru jeff tian empirical comparison characterization high defect high complexity modules journal systems software v67 n3 p153163 15 september philip j boland harshinder singh bojan cukic comparing partition random testing via majorization schur functions ieee transactions software engineering v29 n1 p8894 january thomas j ostrand elaine j weyuker robert bell automating algorithms identification faultprone files proceedings 2007 international symposium software testing analysis july 0912 2007 london united kingdom adrian schrter thomas zimmermann andreas zeller predicting component failures design time proceedings 2006 acmieee international symposium international symposium empirical software engineering september 2122 2006 rio de janeiro brazil elaine j weyuker thomas j ostrand robert bell using developer information factor fault prediction proceedings third international workshop predictor models software engineering p8 may 2026 2007 p ware f g wilkie shapcott application product measures directing software maintenance activity journal software maintenance evolution research practice v19 n2 p133154 march 2007 gwendolyn h walton robert patton douglas j parsons usage testing military simulation systems proceedings 33nd conference winter simulation december 0912 2001 arlington virginia andy chou junfeng yang benjamin chelf seth hallem dawson engler empirical study operating systems errors acm sigops operating systems review v35 n5 dec 2001 yong woo kim efficient use code coverage largescale software development proceedings conference centre advanced studies collaborative research p145155 october 0609 2003 toronto ontario canada mohammad alshayeb wei li empirical study system design instability metric design evolution agile software process journal systems software v74 n3 p269274 february 2005 wei li raed shatnawi empirical study bad smells class error probability postrelease objectoriented system evolution journal systems software v80 n7 p11201128 july 2007 gunes koru dongsong zhang hongfang liu modeling effect size defect proneness opensource software proceedings third international workshop predictor models software engineering p10 may 2026 2007 norman fenton william marsh martin neil patrick cates simon forey manesh tailor making resource decisions software projects proceedings 26th international conference software engineering p397406 may 2328 2004 piotr tomaszewski jim hkansson hkan grahn lars lundberg statistical models vs expert estimation fault prediction modified code industrial case study journal systems software v80 n8 p12271238 august 2007 salah bouktif houari sahraoui giuliano antoniol simulated annealing improving software quality prediction proceedings 8th annual conference genetic evolutionary computation july 0812 2006 seattle washington usa gunes koru jeff jianhui tian comparing highchange modules modules highest measurement values two largescale opensource products ieee transactions software engineering v31 n8 p625642 august 2005 zhenmin li lin tan xuanhui wang lu yuanyuan zhou chengxiang zhai things changed empirical study bug characteristics modern open source software proceedings 1st workshop architectural system support improving software dependability p2533 october 2121 2006 san jose california raimund moser barbara russo giancarlo succi empirical analysis correlation gcc compiler warnings revision numbers source files five industrial software projects empirical software engineering v12 n3 p295310 june 2007 matthew harrison gwendolyn h walton identifying high maintenance legacy software journal software maintenance research practice v14 n6 p429446 november 2002 michael ellims james bridges darrel c ince economics unit testing empirical software engineering v11 n1 p531 march 2006 khaled el emam sada benlarbi nishith goel walcelio melo hakim lounis shesh n rai optimal class size objectoriented software ieee transactions software engineering v28 n5 p494509 may 2002 lenin singaravelu calton pu hermann hrtig christian helmuth reducing tcb complexity securitysensitive applications three case studies acm sigops operating systems review v40 n4 october 2006 avner engel mark last modeling software testing costs risks using fuzzy logic paradigm journal systems software v80 n6 p817835 june 2007 norman e fenton martin neil software metrics roadmap proceedings conference future software engineering p357370 june 0411 2000 limerick ireland mohammad alshayeb wei li empirical validation objectoriented metrics two different iterative software processes ieee transactions software engineering v29 n11 p10431049 november yair wiseman advanced nondistributed operating systems course acm sigcse bulletin v37 n2 june 2005 norman e fenton martin neil critique software defect prediction models ieee transactions software engineering v25 n5 p675689 september 1999 kalhed el emam sada benlarbi nishith goel shesh n rai confounding effect class size validity objectoriented metrics ieee transactions software engineering v27 n7 p630650 july 2001 niels veerman ernstjan verhoeven cobol minefield detection softwarepractice experience v36 n14 p16051642 november 2006 khaled elemam objectoriented metrics review theory practice advances software engineering springerverlag new york inc new york ny 2002