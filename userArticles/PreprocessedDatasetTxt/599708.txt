classifiers approximate functions classifier system xcsf introduced prediction estimation mechanism used learn approximations functions addition weight vectors classifiers allows piecewiselinear approximation classifiers prediction calculated instead fixed scalar weight vector classifiers condition coadapt results functions six dimensions show high accuracy idea calculating prediction leads concept generalized classifier payoff prediction approximates environmental payoff function subspace defined classifier condition action restriction specified classifier permitting continuousvalued actions b introduction considerable recent research properties performance learning classifier system xcs wilson 1995 butz wilson 2001 areas interest include data mining eg bernado llora garrell 2001 generalization inputs butz pelikan 2001 selfadaptation learning parameters hurst bull 2001 learning nonmarkov environments lanzi wilson 2000 among others one area received attention function approximation xcss accuracybased fitness makes pos sible paper demonstrates xcs function approximator shows somewhat surprisingly leads generalized classifier structure embraces traditional classifier formats first time permits classifiers capable continuousvalued actions xcss classifiers estimate payoff rules classifiers evolved xcs keep statistical estimate payoff reward reinforcement expected classifiers condition satisfied action executed system moreover classifiers form quite accurate payoff estimates predictions since classifiers fitnesses genetic algorithm depend prediction accuracies effect xcs approximates mapping set possible inputs systems set available actions typically finite discrete p set possible payoffs attention restricted single action 2 mapping form x theta p function input vectors x scalar payoffs thus system approximates separate function reinforcement learning contexts classifier systems typically used reason forming payoff function approximations permit system choose x best highestpaying action however contexts output desired learning system discrete action continuous quantity instance predicting continuous time series output might future series value control context output might vector continuous quantities angles thrusts apart classifier systems based fuzzy logic valenzuelarendon 1991 bonarini 2000 none produce realvalued outputs hypothesis payoff function approximation ability xcs could adapted produce realvalued outputs well used function approximation general applications test adapted xcs learn approximations functions form real x vector integer components x results demonstrated approximation high accuracy together evolution classifiers tended distribute efficiently input domain research fed back however basic classifier concepts realized thinking classifier function approximator implied new generalized classifier syntax covered existing classifier formats included discreteaction classifiers ones continuous actions next section describes modifications xcs function approximation section 3 results simple piecewiseconstant approximation section 4 introduce new classifier structure permits piecewiselinear approximations results simple functions shown section 5 section 6 demonstrate accurate approximation sixdimensional function section 7 presents generalized classifier syntax examines use final section conclusions suggestions future work 2 modification xcs xcs modified two respects later third first adapt program integer instead binary input vectors second simple make programs payoff predictions directly accessible output restrict system single dummy action resulting program called xcsf omit description basic xcs refer reader wilson 1995 wilson 1998 updated formal description butz wilson 2001 xcsf follows closely changes xcs integer inputs follows drawn wilson 2001 classifier condition changed string f01g concatenation interval predicates int integers classifier matches input x attributes x l x u x crossover twopoint xcsf operates direct analogy crossover xcs crossover point occur two alleles ie within interval predicate pred icates also ends condition action involved crossover mu tation however different best method appears mutate allele adding amount sigmarandm 0 0 fixed integer rand picks integer uniform randomly 0 0 sign chosen uniform randomly new value l less minimum possible input value present case 0 new value set 0 new value greater u set equal u corresponding rule holds mutations u condition covering classifier classifier formed existing classifier matches input components l limited minimum possible input value u limited maximum possible input value rand 1 picks random integer 0 r 0 r 0 fixed integer subsumption deletion operations defined subsumption one classifier another occur every interval predicate first classifiers condition subsumes corresponding predicate second classifiers condition interval predicate subsumes another one l less equal u greater equal purposes actionset subsumption classifier general another generality greater generality defined sum widths u interval predicates divided maximum possible value sum 3 piecewiseconstant approximation simplest way approximate function xcsf let x input payoff sufficient sampling input space system prediction fitnessweighted average matching classifiers predictions given x less accurately predict corresponding xcslike systems fitness classifier depends accuracy prediction xcsf converge population classifiers respective input ranges predict payoff well closeness approximation controllable error threshold ffl 0 follows classifier prediction error ffl 1 higher accuracy classifier error ffl 2 wilson 2001 since classifier fitness depends accuracy classifiers lower errors win evolutionary competition however definition classifiers errors less ffl 0 constant fitness fitness pressure applies thus ffl 0 limit closeness approximation important besides evolving accurate classifiers system employ classifiers efficiently input domain slowlychanging lowgradient regions function would hope evolve classifiers relatively large interval predicates function value thus given classifiers error would change relatively little regions xcs tendency toward accurate maximally general conditions wilson 1995 cause interval predicates expand conversely expect classifiers small interval predicates function changing rapidly also hope efficient distribution classifiers domain sense tiling minimizes overlaps classifier predicates figure shows typical results experiment xcsf approximated function interval 0 x 100 experiments reported input value range 099 plotted xcsfs prediction possible x well function system learned dataset consisting 1000 x pairs x chosen randomly corresponding function value experiment pair drawn randomly dataset x value presented xcsf input value used reward reinforcement xcsf formed match set classifiers matching x calculated system prediction possible action usual way since one dummy action one system prediction calculated became systems output action set formed consisting classifiers dummy action ie classifiers predictions classifiers adjusted using reward usual way classifier parameters also adjusted genetic algorithm run called cycle repeated 50000 times x prediction figure 1 piecewiseconstant approximation parabola function plot figure 1 obtained sweeping possible values x recording resulting system predictions parameter settings experiment follows using notation butz population size learning rate fitness power probability deletion threshold del 50 fitness fraction accelerated deletion addition mutation increment covering interval r subsumption enabled time threshold several aspects figure 1 interest prediction curve staircase appearance typical piecewiseconstant approximation height major steps varies 1000 2000 examination individual steps indicates average error roughly consistent value ffl 0 suggesting ffl 0 controlling closeness approximation width steps roughly wider flatter part function narrower steep part finally significant prediction curve indeed takes form staircase instead smoother long flat steps suggest one set classifiers control forms next step another set takes turn suggests tendency toward efficient distribution classifier resources domain figure another perspective listing macroclassifiers population end experiment 15 shown classifiers condition prediction error fitness numerosity note classifiers substantial fitnesses errors less ffl 0 special graphic notation used represent condition since x one component condition contains one interval predicate possible range x 099 divided 20 equal subranges interval predicate indicated cluster os covers range interval predicate entirely covers subrange eg 3539 placed ranges position interval predicate covers less whole subrange small put notation found perspicuous using raw numbers note consistent figure 1 classifier conditions larger toward beginning domain function slope lower also interesting classifiers higher fitnesses numerosities cover domain without great deal overlap classifiers dominate calculation system prediction since latter condition pred err fitn num 0oo 8992 283796 31 1oo 7876 322906 2ooo 6162 503564 20 3ooo 5990 570188 9 4oooo 5988 569028 1 5oooo 5668 413075 3 6ooo 5534 281090 3 8ooooo 5053 524002 1 9oooo 4861 465054 5 10oooo 3854 468862 12ooooo 1359 377358 13 oooooooooo 821 378372 15 14 ooooooooo 820 377371 15 figure 2 classifiers experiment figure 1 prediction error fitness numerosity fitnessweighted average predictions matching classifiers dom inate prediction curve takes form staircase apart presence remaining lower fitness classifiers distribution resources domain thus relatively efficient sum xcsf succeeds approximating function accordance stated error criterion confirmed additional values ffl 0 classifiers employed reasonably well still piecewiseconstant approximation primitive compared approximation approximating segments closely follow functions contour simplest approach piecewiselinear approximation could piecewiselinear approximation done classifier system 4 piecewiselinear approximation traditionally classifiers prediction number intended apply inputs x satisfy condition however function approximation would desirable prediction could vary conditions domain since function approximated generally varies effect prediction function simplest form would linear polynomial input components call hx function hx would substitute classifiers traditional scalar prediction p given input x matching classifier would calculate prediction computing hx approximating onedimensional function fx hx would twoterm polynomial case w 1 thought slope approximating straight line w 0 intercept ndimensional fx weight vector w input vector x augmented constant case hx computes hyperplane approximation fx classifiers would different weight vectors w since general domains conditions differ course classifiers weight vectors must adapted classifiers predict given accuracy coefficients w weight vectors must appropriate one approach use evolutionary algorithm weight vector would evolved along classifier condition w could concatenated interval predicates condition whole thing evolved unit might preferable use separate processes example evolutionsstrategie might suitable ga weight vector optimizing weights linear network unimodal es generally better unimodal tasks present work however use evolutionary technique weight vector instead adapted using modification delta rule mitchell 1997 delta rule given w x ith components w x 0 respectively quantity output present case classifier prediction target case correct value according amount prediction corrected negative classifiers instantaneous error finally j correction rate delta rule says change weight proportionally product input value correction notice correcting w effect changes output jx 0 j 2 factored difficult choose j get wellcontrolled overall rate correction j large results weights fluctuating converging j small convergence unnecessarily slow experimentation issue noticed original use widrow hoff 1988 correction rate selected entire error corrected one step possible however input vector binary absolute value constant problem reliable onestep correction would possible modified delta rule employed total correction would strictly proportional gamma could reliably controlled j instance would give onestep correction widrow hoff experiments follow used modified delta rule various values use delta rule requires selection appropriate value x 0 constant augments input vector tests found x 0 small weight vectors would learn right slope would tend point toward originie w x factor equation deltaw x 0 small compared x adjustments w 0 tend swamped adjustments w keeping w 0 small choosing x 100the order magnitude x appeared solve problem piecewiselinear approximation changes necessary xcsf except addition weight vectors classifiers provision calculation predictions application modified delta rule action set classifiers every timestep classifier created covering weight vector randomly initialized weights offspring classifiers inherited parents weight vectors policies yielded performance improvements initializations experiments parameter settings given section 3 differences noted settings new parameter j given x prediction 2line figure 3 piecewiselinear approximation piecewiselinear function 2line ffl condition pred err fitn num 0ooooooooooo 4705 19158 62 1oooooooooo 4670 0706 274 2oooooooooo 4670 0044 4 ooooooooooo 3057 16005 6 5 ooooooooooo 3050 0676 247 6 oooooooooo 3050 0377 164 figure 4 classifiers experiment figure 3 5 tests simple functions preliminary testing carried approximating functions linear piecewise linear example tests done function 2line defined parameters experiment previously except 04 approximation obtained figure close plots prediction function difficult distinguish visually figure 4 shows seven classifiers end run 1 clearly divided one group upper segment function another lower segment dominant classifier upper group 1 error zero predicate covering interval 5299 inclusive lower group dominant classifier 5 also error zero covers interval 050 raising error threshold caused approximation deteriorate figure 5 ffl prediction curve seems ignore break reflecting change slope gradually classifier list showed several bridged break point predictates 30 70 evidently larger error threshold system forced figure 3 evolve classifiers corresponded closely two function segments 1 prediction values represent recent weightvector calculation particularly significant section 3 following experiments run 50000 input cycles x prediction 2line figure 5 approximation 2line ffl x prediction figure piecewiselinear approximation parabola ffl figures 6 12 show typical results parabola sine functions parameters 2line experiments except insignificant difference values ffl 0 given captions two values chosen function equivalent 5 1 functions ranges highlight dominant classifiers save space classifier lists include highestfitness classifiers full populations three four times larger parabola figures show good linear approximations small number classifiers somewhat surprising thatunlike piecewiseconstant casethe sizes interval predicates seem reflect functions slope perhaps piecewiselinear approximation different analysis order predicate length may related curve straightness steepness sinewaves approximation overshoots peaks ffl 0 large figure 10 effect disappears smaller values curve quite nicely matched figure 12 figure 11 suggests system divides approximation classifiers beginning middle end curve condition pred err fitn num 0ooooooooooo 9375 246286 151 1oooooooooooooo 9014 431122 54 2ooooooooooooooo 8985 432176 80 3ooooooooooooooo 8906 456037 37 4oooooooooooooooo 8744 456035 33 5 ooooooooooooooo 4184 379077 35 6 oooooooooooooo 3756 247258 122 7 oooooooooooooo 3564 256066 8 ooooooooooooo 3193 262035 9 ooooooooooo 2250 204211 125 figure 7 high fitness classifiers experiment figure 62000600010000 x prediction figure 8 piecewiselinear approximation parabola ffl condition pred err fitn num 0ooooooo 9591 77823 212 1ooooooo 7734 75081 21 2oooooooo 7687 89035 9 3oooooooo 4438 79731 184 4oooooo 3149 26123 41 6ooooooo 1621 61072 13 7 ooooooo 967 104607 219 8 ooooooo 749 85074 figure 9 high fitness classifiers experiment figure 8 x prediction figure 10 piecewiselinear approximation sine function ffl condition pred err fitn num 1 oooooo 118 10083 22 2 ooooo 111 7572 158 3ooooo 66 5032 15 4ooooo20 4792 173 5ooooooo23 12101 78 6ooooooooooo130 8888 234 figure 11 high fitness classifiers experiment figure 10 x prediction figure 12 piecewiselinear approximation sine function ffl 2 instances system figure 13 system error100 population size3200 approximation sixdimensional rms function 1 6 multidimensional input xcsf initially tested functions one variable letting linear function ie hyperplane function system rapidly evolved solutions one classifiers arbitrary accuracy expected since classifiers weight vectors effectively linear functions test xcsf multidimensional nonlinear function chose somewhat arbitrarily rms function experiments went well parameters previously except 10 error threshold ffl range contrast previous experiments instances chosen randomly fixed data set instances picked randomly domain figure 13 plots system error population size starting initially high system error moving average absolute difference xcsfs prediction actual function value fell rapidly less 1 01 plotted graph population sizein macroclassifiersrose quickly 2400 stayed system seemed little difficulty approximating function within though quite classifiers required 7 generalized classifiers 71 definition xcs classifier systems classifier prediction scalar system adapts classifier conditions prediction scalars find accurate classifiers general possible xcsf prediction replaced weight vector computing linear function leading powerful subtle coadaptation condition prediction extreme instructive example xcsf approximate highdimensional linear function o1 classifiers far less required using scalar predictions results xcsf suggest classifier architecture essential novelty xcsf prediction calculated instead fixed scalar prediction linear function x input vector precisely prediction function approximates desired output function input subdomain defined classifiers condition classifiers approximate parts desired output function taken together classifiers evolve approximate within specified error criterion inputoutput mapping defined introduction noted xcs approximates mapping x theta p includes actions well inputs another way saying desired output payoff prediction function input x action taken system certainly imagine approximating linear function x define generalized classifier expresses ideas generalized classifier says within subdomain x theta defined truth function tx action restriction ra payoff approximated prediction function px components 4 follows truth function tx defines subdomain x within classifier applies matches traditional terminology example binary input space tx could simply 010 meaning course set strings f0110 0100g tx could sexpression x 1 x 3 lanzi 1999 could also representation neural network bull ohara 2001 whatever specific form classifiers tx determines given x whether classifier applies action restriction ra defines range effector values example ra might specify range rudder angles 10 34 degrees generally one effector eg rudder aileron throttle ra expression defines subdomain effector space set effector vectors taking values eg rudder aileron angles throttle positions simple special case ra would name action turn left occurs traditional classifier systems whatever specific form classifiers ra specifies allowed set effector values system decide act according classifier described finally prediction function px serves calculate classifiers payoff prediction case tx satisfied system intends take action generally vector ra according formeg linear expression x combination basis functionspx calculates prediction course approximation expected payoff within given error criterion xcs course px simply constant consider however robot taking fixedlength steps plane able turn angle robot supposed learn orient move toward goal environmental payoff would probably depend maybe x prediction function px would depend least generally prediction function given classifier would depend input effector variables 72 operation would generalized classifiers used systems performance reinforcement discovery cycles performance main step would calculate payoff prediction classifier matches input x since prediction function px also depends value used answer system exploit mode ie seeking act best information maximize payoff choose classifiers payoff maximized thus a2ra chosen action x current input many situations lengthy calculation instance px linear x eg form correspond separate effectors maximum obtained choosing associated term maximized calculated associated classifier match set system would carry action whose prediction highest hand system explore mode seeking try new actions hopes gaining new information environmental payoff classifier would selected random ra according exploration regime system would randomly choose one resulting actions execution environment reinforcement update cycle would consist updating classifier whose action taken often updating occurs explore mode update would occur exactly xcsf prediction function would adjusted according difference prediction actual payoff received error fitness values would adjusted xcs discovery cycle would also occur xcsf except genetic operators besides acting truth function tx would also act action restriction ra syntax ra must course amenable genetic operators example action restriction form v encoded concatenating genetic operators would apply exactly xcsf 73 special case illustrate generality 4 interesting see boolean multiplexer problem wilson 1995 might represented quite well known typical learning experiment 6multiplexer xcs evolves pairs classifiers accurate maximally general ie cant generalized without losing accuracy two classifiers condition two possible actions 1 0result different payoffs xcs evolves accurate classifiers whether correct payoff 1000 wrong payoff 0 generalized representation however pair would covered single classifier ra meaning take either 1 0 px linear expression 1000a seen substituting 1 0 px results correct payoff action thus example binaryinput discrete action stepwise payoff case fully captured generalized representation 74 continuous actions sense generalized classifier 4 lead system capable continuous actions discussed sec 72 classifier principle capable advocating action value permitted ra practice system exploit mode classifier choose action maximizes prediction thus given state classifier system certain discreteness actions maintained however degree discreteness depends quality prediction function approximations better approximations less discretized action space since classifier cover smaller portion space therefore one perhaps say generalized classifier permits continuous actions limit continuous actions desirable first place desirable system able choose actions maximize payoff available environment continuous environments payoffmaximizing actions precise angles turns etc difficult system designer anticipate important system choose within limits resourceseg quality approximation permitted population size general better predefined set fixed actions however point actions chosen generalized classifier system finegrained enough resolutionor perfect continuitywill add significantly payoff returns 8 summary conclusions paper introduced classifier system xcsf designed learn approximations func tions prediction estimation mechanism used form approximations given input vector x value function approximated treated payoff learned first incarnation xcsf produced piecewiseconstant approximations advanced version added weight vector classifier permitting approximation piecewiselinear tests simple onedimensional functions yielded arbitrarily close approximations according setting error parameter system tended evolve classifiers distributed reasonably efficiently functions main though overlap occurred together presence moderate number redundant lowfitness classifiers limited tests sixdimensional nonlinear function xcsf rapidly formed highly accurate approximations though number classifiers required much larger onedimensional functions future work continue multidimensional functions determine tech niques general viability estimate complexity terms learning time resources classifiers required since xcsf approximates linear functions effortlessly regardless dimensionality likely complexity relate degree smoothness flatness hyperspace function exhibits comparisons made fuzzy classifier systems appear quite different concept output fuzzy system computed jointly one classifier whereas xcsf accurate output principal computed one function approximation xcsf could useful online learning function mapping vector input values output value example would financial timeseries prediction future price presumably approximable function known prices quantities earlier times series piecewiselinear function approximation xcsf based idea calculating classifiers prediction leads concept generalized classifier condition truth function tx input x prediction approximation function px depends x action classifier would apply subspace x theta p mapping defined tx action restriction ra specified classifier given x best action take would determined maximizing restrictions range architecture would appear permit system evolve classifiers take actions optimally match environments payoff landscape instead generally suboptimal actions prespecified finite set since presented generalized classifiers conceptually next step experiments especially domains would appear advantages first advantage course continuous vs discrete actions tests continuous robotic environ ments either actual simulated order second advantage lies potential generalization power generalized classifiers payoff function relates x way readily handled piecewiselinear approximation generalization advantage expected compared conventional xcs systems means savings space complexity population size well increased transparency systems model furthermore approximation bases linear tried promising acknowledgement work supported part nutech solutions inc r xcs gale comparative study two learning classifier systems six learning algorithms classification tasks introduction learning fuzzy classifier systems neural rule representation learning classifier systems analyzing evolutionary pressures xcs extending representation classifier conditions part ii messy coding sexpressions toward optimal classifier system performance nonmarkov environments machine learning fuzzy classifier system classifier system continuously varying variables adaptive switching circuits classifier fitness based accuracy generalization xcs classifier system genetic programming mining oblique data xcs tr ctr lashon b booker adaptive value function approximations classifier systems proceedings 2005 workshops genetic evolutionary computation june 2526 2005 washington dc amin nikanjam adel rahmani anticipatory approach improve xcsf proceedings 8th annual conference genetic evolutionary computation july 0812 2006 seattle washington usa daniele loiacono pier luca lanzi improving generalization xcsf classifier system using linear leastsquares proceedings 2005 workshops genetic evolutionary computation june 2526 2005 washington dc pier luca lanzi daniele loiacono stewart w wilson david e goldberg xcs computed prediction multistep environments proceedings 2005 conference genetic evolutionary computation june 2529 2005 washington dc usa hau trung tran cdric sanza yves duthen thuc dinh nguyen xcsf computed continuous action proceedings 9th annual conference genetic evolutionary computation july 0711 2007 london england pier luca lanzi daniele loiacono stewart w wilson david e goldberg classifier prediction based tile coding proceedings 8th annual conference genetic evolutionary computation july 0812 2006 seattle washington usa pier luca lanzi daniele loiacono stewart w wilson david e goldberg prediction update algorithms xcsf rls kalman filter gain adaptation proceedings 8th annual conference genetic evolutionary computation july 0812 2006 seattle washington usa pier luca lanzi daniele loiacono stewart w wilson david e goldberg extending xcsf beyond linear approximation proceedings 2005 conference genetic evolutionary computation june 2529 2005 washington dc usa pier luca lanzi stewart w wilson using convex hulls represent classifier conditions proceedings 8th annual conference genetic evolutionary computation july 0812 2006 seattle washington usa martin v butz pier luca lanzi stewart w wilson hyperellipsoidal conditions xcs rotation linear approximation solution structure proceedings 8th annual conference genetic evolutionary computation july 0812 2006 seattle washington usa pier luca lanzi daniele loiacono classifier systems compute action mappings proceedings 9th annual conference genetic evolutionary computation july 0711 2007 london england daniele loiacono andrea marelli pier luca lanzi support vector regression classifier prediction proceedings 9th annual conference genetic evolutionary computation july 0711 2007 london england pier luca lanzi daniele loiacono standard averaging reinforcement learning xcs proceedings 8th annual conference genetic evolutionary computation july 0812 2006 seattle washington usa martin v butz kernelbased ellipsoidal conditions realvalued xcs classifier system proceedings 2005 conference genetic evolutionary computation june 2529 2005 washington dc usa martin v butz martin pelikan studying xcsboa learning boolean functions structure encoding random boolean functions proceedings 8th annual conference genetic evolutionary computation july 0812 2006 seattle washington usa pier luca lanzi daniele loiacono stewart w wilson david e goldberg generalization xcsf classifier system analysis improvement extension evolutionary computation v15 n2 p133168 summer 2007 jan drugowitsch alwyn barry formal framework extensions function approximation learning classifier systems machine learning v70 n1 p4588 january 2008 jorge muruzbal probabilistic classifier system application data mining evolutionary computation v14 n2 p183221 june 2006