feasible direction decomposition algorithms training support vector machines article presents general view class decomposition algorithms training support vector machines svm motivated method feasible directions first algorithm pattern recognition svm proposed joachims 1999 schlkopf et al eds advances kernel methodssupport vector learning pp 185208 mit press extension regression svmthe maximal inconsistency algorithmhas recently presented author laskov 2000 solla leen mller eds advances neural information processing systems 12 pp 484490 mit press detailed account algorithms carried complemented theoretical investigation relationship two algorithms proved two algorithms equivalent pattern recognition svm feasible direction interpretation maximal inconsistency algorithm given regression svm experimental results demonstrate order magnitude decrease training time comparison training without decomposition importantly provide experimental evidence linear convergence rate feasible direction decomposition algorithms b introduction computational complexity svm training algorithms attracting increasing interest applications svm extend problems larger larger size requests recently made algorithms capable handling problems containing examples 14 basic training algorithm 16 involves solution quadratic programming problem size depends size training data set training data set size l pattern recognition svm quadratic program l variables nonnegativity constraints l inequality constraints one equality constraint regression quadratic program 2l variables nonnegativity constraints 2l inequality constraints one equality constraint addition basic formulation svm training algorithm requires storage kernel matrix size l theta l 2l theta 2l respectively thus running time storage kernel matrix two main optimizationrelated bottlenecks svm training algorithms 1 another possible bottleneck computation kernel matrix real dimension data points large time may become comparable training time however first svm implementations used general purpose optimization packages minos loqo others designed problems size soon discovered packages suitable solutions problems involving hundred examples early specialpurpose methods proposed speedup training brought much relief chunking 15 prescribes iteration training data accumulating support vectors adding chunk new data changes solution occur main problem method percentage support vectors high essentially solves problem almost full size another method proposed kaufmann 4 modifies traditional optimization algorithms combination newtons conjugate gradient methods yield overall complexity os 3 per iteration priori unknown number support vectors significant improvement ol 3 however number support vectors guaranteed small decomposition first practical method solving largescale svm training problems originally proposed pattern recognition svm 9 subsequently extended regression svm 8 key idea decomposition freeze small number optimization variables solve sequence constantsize problems set variables optimized current iterations denoted working set working set reoptimized value objective function improved itera tion provided working set optimal reoptimization iteration stopped termination criteria derived karushkuhntucker kkt conditions satisfied required precision selection working set important issue decomposition algorithms first provision working set must suboptimal reoptimizaiton crucial prevent algorithm cycling therefore important criteria test suboptimality given working set second working set selection affects rate convergence algorithm suboptimal working sets selected less random algorithm converges slowly finally working set selection imporant theoretical analysis decomposition algorithms particular recent convergence proof chang et al 2 original decomposition algorithm essentially addressed first issuethe design termination criteria pathological cases prevents composition already optimal working sets implementation featured unpublished heuristics provided reasonable convergence speed ob viously formal framework working set selection highly desirable one framework method feasible directions proposed optimization theory 1960 zoutendijk 18 connection method working set selection problem first discovered joachims paper assume rest article kernel computation main factor contributing complexity training drawn wide attention 3 however joachims algorithm limited pattern recognition case uses fact labels sigma1 main goal current article provide unified treatment working set selection problem within framework method feasible directions specifically first two issues raised addressed common way pattern recognition regression svm criterion proposed identify suboptimal working sets heuristic shares motivation optimality criterion proposed approximately optimal working set selection new algorithm termed maximal inconsistency algorithm applicable pattern recognition regression svm reveal machinery new algorithm shown pattern recognition svm new algorithm equivalent joachims algorithm regression svm similar algorithm exists based feasible direction princi ple similar pattern recognition case maximal inconsistency algorithm satisfies theoretical requirements crucial proof convergence relationships allow classify algorithms feasible direction decomposition algorithms article organized follows section 2 provides basic formulations decomposition algorithms pattern recognition regression svm section 3 presents method feasible directions sections 4 5 cover issues related feasible direction decomposition algorithms pattern recognition regression svm respectively exposition sections carried different fashion section 4 presented less chronological order starts joachims algorithm fully motivated feasible direction method criterion testing optimality working set presented maximal inconsistency working set selection rule derived criterion finally equivalence maximal inconsistency algorithm joachims algorithm proven order presentation reversed regression svm section 5 maximal inconsistency algorithm introduced first followed interpretation feasible direction decomposition algorithm experimental results regression svm presented section 6 classical problem training svm given data fx hence kernel matrix expressed following quadratic program maximize w ff ff subject c interpretation symbolilc components 1 varies pattern recognition regression cases pattern recognition svm regression svm ff ff gammay k gammak gammak k details formulation svm training problems found 16 17 11 13 mentioned introduction main idea decomposition allow subset optimization variables change weights current iteration iteration process repeated termination conditions met let ff b denote variable included current working set fixed size q let ff n denote rest variables corresponding parts vectors c also bear subscripts n b matrix partitioned nb dnn required regression svm ff ff either included omitted working set 2 optimization working set turns also quadratic program seen rearranging terms objective function equality constraint 1 dropping terms independent ff b objective resulting quadratic program subproblem formulated follows maximize wb subject c termination conditions decomposition algorithm derived kkt conditions 9 8 best consider separately two types svm pattern recognition svm let l b threshold svm computed 3 l strictly speaking rule required decomposition however facilitates formulation subproblems solved iteration point satisfies kkt provided regression svm let l l point satisfies kkt conditions provided conditions 7 10 checked individual points given iteration one violated point point exchanged point current working set thus new working set suboptimal strict improvement overall objective function achieved problem using conditions 7 10 require knowledge threshold b also lagrange multiplier equality constraint svm problem formulas given 6 9 require set sv unbounded support vectors nonempty usually assumption true however cannot guaranteed simple trick rectify problem pattern recognition svm letting similar trick doesnt work regression svm overcoming problem initial motivation new termination condition proposed article instead individual points optimality entire working set considered new condition described detail section 42 intelligent selection working sets possible utilizing ideas method feasible directions introduced next 3 method feasible directions letomega feasible region general constrained optimization problem vector said feasible direction point ff 2omega exists ff 2omega 0 main idea method feasible direction find path initial feasible solution optimal solution making steps along feasible directions iteration feasible direction algorithm proceeds follows find optimal feasible directionthat feasible direction providing largest rate increase objective function determine step length along feasible direction maximizes objective function line search algorithm terminates feasible direction found improves objective function general constrained optimization problem form maximize fff subject aff b optimal feasible direction found solving direction finding linear program maximize rf subject ad 0 method feasible directions applied directly svm training 8 case respective optimal feasible direction problem stated follows maximize g 13 subject c definitions g c vary two svm formulations turns solving linear problem full size iteration expensive overall performance method svm training inferior traditional optimization methods however slight modification approximate solution optimal feasible direction problem obtained linear time solution provides powerful guidance working set selection approximate solution lies core feasible direction decomposition algorithms described ensuing sections 4 feasible direction decomposition pattern recognition svm 41 joachims decomposition algorithm key observation joachims adding requirement q components nonzero provides straightforward working set selection rule variables corresponding nonzero components included new working set unlike zoutendijks method optimal feasible direction vector followed exactly reoptimization assumed cheap one afford finding optimal solution entire subspace spanned nonzero components instead line search strictly along unfor tumately additional constraint optimal feasible direction problem becomes intractable therefore one seek approximate solution one solution realized joachims algorithm rest section detailed account solution presentedin order provide insight underline ideas used extension algorithm regression svm approximate solution obtained changing normalization constraint 17 normalization order satisfy equality constraint 14 recall pattern recognition svm equality constraint suffices number elements sign matches equal number elements sign mismatches obviously working set size q condition holds number equal q2 therefore equality constraint 14 enforced performing two passes data forward pass selects q2 elements sign mismatches backward passq2 elements sign matches 3 directions determined elements recall goal maximize objective function 13 subject constraints absense constraints maximum objective function would achieved selecting q points highest values jg j assigning directions lets consider largest contribution fl k objective function provided point k subject equality constraints forward pass signs k k must different therefore gammag hence combining subscripts 3 motivation names forward backward clear shortly likewise backward pass signs k k must therefore gammag hence combining subscripts thus quantity g reflects elements contribution objective function subject equality constraint working set composition rule stated follows sort data elements g increasing order 4 select q2 elements front list hence forward pass q2 elements back list hence backward pass finally account inequality constraints 15 16 points may skipped violate one constraints points direction leading improvement objective function optimal feasible direction problem infeasible joachims algorithm summarized algorithm 1 algorithm 1 joachims svm decomposition algorithm let list samples termination conditions 7 met increasing order select q2 samples front 15 16 forward pass select q2 samples back 15 16 backward pass reoptimize working set 42 optimality working set mentioned earlier pointwise termination criteria osunas joachims algorithms require knowledge threshold b svm threshold difficult calculate especially regression svm section alternative termination condition presented allows determine practice sorting takes log n operations replaced heapbased algorithms yielding complexity log q oq log n depending heap built whether entire working set suboptimal hence suitable optimization new conditions based examination kkt conditioins standard form quadratic program exposition section concentrates pattern recognition svm whereas similar result regression svm presented section 51 consider quadratic problem 12 pattern recongition svm standard form quadratic program obtained transforming constraints either quality nonnegativity constraints adding slack variables necessary particular cast problem 12 vector slack variables added every ff c constraint slack variable 0 value added represent requirement equality constraint satisified notational purposes following matrices vectors introduced matrix notation 18 constraints original problem 12 compactly expressed z 0 19 karushkuhntucker theorem 1 p 36 stated follows theorem 1 karushkuhntucker theorem primal vector z solves quadratic problem 1 satisfies 19 exists dual vector upsilon 0 21 follows karushkuhntucker theorem u satisfying conditions 21 22 system inequalities 20 inconsistent solution problem 1 optimal since subproblem 4 obtained merely rearranging terms objective function constraints initial problem 1 conditions guarantee subproblem 4 optimal thus main strategy identifying suboptimal working sets enforce inconsistency system 20 satisfying conditions notice constant terms 20 represent negative gradient vector thus inequality 20 written follows consider three cases according values ff take case complementarity condition 22 implies gammag 2 ff case complementarity condition 22 implies ae inequality 23 becomes gammag 3 case complementarity condition 22 implies inequality 23 becomes gammag easily seen enforcing complementarity constraint 22 causes become free variable system 20 point restricts certain interval real line intervals denoted sets rest article rules computation sets summarized follows gammag ff gammag development section summarized following theorem theorem 2 vector ff solves quadratic program 1 2 intersection sets computed 27 nonempty also follows expressions 27 least one ff strictly bounds optimal solution intersection sets nonempty kuhntucker theorem single point set consistent known property svm optimal solution intersection sets optimal solution nonempty non single point set variables bounds case point intersection sets taken b particular value suggested 11 43 maximal inconsistency algorithm inconsistency working set iteration guarantees convergence decomposition rate convergence quite slow arbitrary inconsistent working sets chosen natural heuristic select maximally inconsistent working sets hope choice would provide greatest improvement objective function notion maximal inconsistency easy define let gap smallest right boundary largest left boundary sets elements training set 0il l 0il r l left right boundaries respectively possibly minus plus infinity set convenient require largest possible inconsistency gap maintained pairs points comprising working set obvious implementation strategy select q2 elements largest values l q2 elements smallest values r one feature joachims method needs retained rejection zoutendijk infeasible points cf 15 16 inclusion points working set doesnt make sense anyway values ff change optimization pattern recognition case set constraints capable encoding feasibility 5 since notion direction explicitly maintained maximal inconsistency algorithm feasibility test needs tobe modified slightly point infeasible ff maximal inconsistency strategy summarized algorithm 2 algorithm 2 maximal inconsistency algorithm pattern recognition svm let list samples intersection sets empty according rules 27 elements select q2 feasible samples largest values l left pass select q2 feasible samples smallest values r right pass reoptimize working set 44 equivalence joachims algorithm maximal inconsistency algorithm far motivation maximal inconsistency algorithm purely heuristic inconsistency gap indeed provide good measure optimality 5 regression case working set given iteration affirmative answer developed section showing equivalence joachims algorithm maximal inconsistency algorithm show algorithms equivalent prove produce identical working sets iteration 6 termination conditions equivalent two propositions handle claim proposition 1 working set joachims algorithm identical working set maximal inconsistency algorithm iteration proof statement proved half working set namely set elements selected forward pass joachims algorithm identical set elements selected right pass maximal inconsistency algorithm similar argument allows establish equivalence half working sets let f set feasible samples iteration let r j p denote rank position sample p array obtained sorting f r p rank sample p array obtained sorting f r let set h q2g half working set selected forward pass joachims algorithm let p sample whose r j q2 ie element h j largest value key let h r p r pg going prove h j j h sample p selected forward pass gammay considering possible values ff conclude p 2 see order r p preserved mapping h j 7 h therefore prove set equivalence remains shown p 2 h p 2 h j suppose way contradiction case ie exists sample p r p r p r p equal three possible cases covered 28 r case r p r p r j p r j p contradicts previous conclusion r j p r j p remaining two cases 27 contradicts assumption r p r p thus conclude h j j h proposition 2 termination conditions joachims algorithm satisfied termination conditions maximal inconsistency algorithm satisfied 6 since algorithms use identical feasibility check every sample obvious infeasible samples never included working set algorithms proof maximal inconsistency algorithm terminates system 20 consistent time conditions 21 22 enforced hence kkt conditions satisfied consequently algorithm terminates optimal solution found likewise termination conditions relationship kkt conditions hence optimality solution except solution contains variables strictly bounds 11 used calculation b latter case however condition 11 satisfies kkt conditions primal svm training problem problem 1 2 dual follows dorns duality theorem 6 p 124 solution dual problem also optimal hence algorithms terminate point solution space 5 feasible direction decomposition regression 51 maximal inconsistency algorithm turn attention maximal inconsistency algorithm regression svm recall quadratic program latter given equations 1 3 derivation progress way pattern recognition case consist stating karushkuhntucker theorem standard form qp b derivation rules computation sets c defining inconsistency gap used working set selection algorithm standard form qp regression svm defined following matrices terms constraints expressed way pattern recognition case z 0 30 statement karushkuhntucker theorem well use test optimality working set remain see theorem 1 ensuing discussion regression svm inequality 20 one following forms l considering possible values ff 1 case inequality 31 becomes 2 ff becomes 3 inequality 31 becomes similar reasoning ff inequality 32 yields following results 1 ff 2 ff 3 finally taking account regression svm ff ff rules computation sets regression svm following regression svm new termination condition stated following theorem algorithm 3 maximal inconsistency algorithm regression svm let list samples intersection sets empty according rules 34 elements select q2 feasible samples largest values l left pass select q2 feasible samples smallest values r right pass reoptimize working set theorem 3 vector ff solves quadratic program 1 3 intersection sets computed 34 nonempty maximal inconsistency algorithm regression svm summarized algorithm 3 feasibility samples tested following rule left pass skip samples ff right pass skip samples justification rule given lemma 1 section 52 52 interpretation maximal inconsistency algorithm feasible direction framework shown section 44 maximal inconsistency algorithm equivalent joachims algorithm motivated zoutendijks feasible direction problem section demonstrated maximal inconsistency algorithm regression svm also interpreted feasible direction algorithm recall svm optimal feasible direction problem stated 13 17 problemspecific components c following expressions regression svm oe defined 33 addition feasible direction algorithm must satisfy constraint develop equivalent feasible direction algorithm construct mapping phi ff maps state maximal inconsistency algorithm direction vector normalization nd similar joachims normalization replaces 17 construction possess following properties 1 iteration phi ff solution optimal feasible direction problem normalization nd 2 termination condition maximal inconsistency algorithm holds solution optimal feasible direction problem zero direction intuitively first property shows working sets selected maximal inconsistency algorithm selected feasible direction algorithm using normalization nd second property ensures algorithms terminate time consider normalization mapping phi 0 gamma1 whichever feasible left pass 0 1 whichever feasible right pass 0 0 sample infeasible reached sake brevity optimal feasible direction problem comprising equations denoted feasible direction problem first need make sure phi ff ambiguous e one nonzero directions suggested 39 feasible lemma 1 mapping phi ff ambiguous proof let us denote directions 1 0 0 gamma1 gamma1 0 0 1 ia ib iia iib type directions assigned left pass type ii directions right pass table 1 shows feasibility different directions depending values optimization variables infeasibility directions marked table 1 feasibility directions optimization variables feasible infeasible due special property regression svm ff ff follows table pass one direction feasible lemma justifies feasibility test maximal inconsistency algo rithm clear table 1 left pass feasible direction samples ff right pass feasible direction samples ff next two lemmas show phi ff provides solution feasible direction problem lemma 2 phi ff satisfies constraints feasible direction problem proof equality constraint optimal feasible dirction problem regression svm form l l number selected elements directions f1 0gfgamma1 0g f0 gamma1g f0 1g respectively l l selection policy follows hence phi ff satisfies equality constraint optimal feasible direction problem inequality constraints cardinality constraint trivially satisfied construction phi ff lemma 3 phi ff provides optimal value objective function feasible direction problem proof let b l b r denote halves working set selected left right passes respectively suppose way contradiction exists feasible sample g element k considered left pass k 0 gamma1 feasible therefore contradicts hypothesis likewise element k considered right pass k 0 1 feasible therefore gammag contradicts hypothesis lemma 4 intersection sets nonempty feasible direction problem zero solution proof theorem 3 nonempty intersection sets implies optimality solution quadratic program 1 3 rules existence nonzero feasible direction would otherwise led new optimal solution hand optimal solution feasible direction problem zero implies feasible directions negative projections gradient vector hence decrease value objective function follows solution quadratic program 1 3 optimal hence intersection sets nonempty prove two properties mapping phi ff normalization nd claimed earlier section 6 experimental results aim section provide insight properties feasible direction decomposition algorithms might explain behaviour different situations particular following issues addressed scaling factors traditional way analyzing performance svm training algorithms introduced platt 10 joachims 3 perform least qualitative comparison results similar evaluation performed maximal inconsistency algorithm experimental convergence rates traditional optimization literature algorithms often evaluated terms convergence rates since decomposition algorithms borrow core ideas optimization theory iterative natural attempt establish rates con vergence shown maximal inconsistency algorithm seems linear rate convergence consistent known linear convergence rates gradient descent methods profiled scaling factors decreasing number iterations highly desirable must achieved cost significantly increasing cost iteration therefore important investigate profile one iteration decomposition algorithm experimental evaluation new algorithm performed modified kdd cup 1998 data set original data set available httpwwwicsuciedukdddatabaseskddcup98kddcup98html following modifications made obtain pure regression problem 75 character fields eliminated fields controln odatedw tcode dob elimi tated remaining 400 features labels scaled 0 1 initial subsets training database different sizes selected evaluation scaling properties new algorithm experiments run sun4u400 ultra450 workstation 300mhz clock 2048m ram rbf kernel cache size 300m used value box constraint c 1 working set size used two sets experiments performed one using full set 400 features another one using first 50 features reduced set turns second problem constrained larger proportion bounded support vectors also full set features kernel computation dominates overall training time 61 overall scaling factors training times without decomposition different samples sizes displayed tables 2 3 full reduced sets features respectively scaling factors computed plotting training times versus sample sizes loglog scale fitting straight lines svscaling factors obtained fashion using number unbounded support vectors instead sample size abscissa actual plots shown figure 1 easily seen decomposition improves running time order magnitude scaling factors also significantly better scaling factors consistent scaling factors presented platt 10 joachims 3 pattern recognition svm number interesting findings made results first easily seen training decomposition produce identical solution training without solutions differ number support vectors especially constrained problem reduced set features difference due fact termination conditions 7 order results conceptually compatible joachims used old pointwise termination conditions table 2 training time sec number svs kdd cup problem examples dcmp dcmp time total sv bsv time total sv bsv 1000 91193 454 0 24907 429 0 2000 665566 932 2 118131 894 2 5000 107854 2305 7 914359 2213 7 10000 926847 4598 28 495826 4454 26 scaling factor 303 229 svscaling factor 286 213 table 3 training time sec number svs kdd cup problem reduced feature set examples dcmp dcmp time total sv bsv time total sv bsv 500 16958 114 29 14175 128 26 1000 129591 242 2000 998191 445 114 178752 656 104 5000 107596 977 323 929523 1667 255 10000 793232 1750 633 278235 3383 491 scaling factor 280 176 svscaling factor 312 160 logt scaling factors svm training without decomposition logt scaling factors svm training without decomposition b fig 1 scaling factor fits full set features table 2 b reduced set features table decomposition algorithm require kkt conditions satisfied given numerical precision thus decomposition algorithm disadvantage producing approximate solution seen tables 4 5 tables display values objective functions attained training without decomposition ratio latter shows relative turns solution constrained problem reduced feature set roughly 10 times worse solution less constrained problem fulls feature set however deviation accuracy sample size observed another important observation scaling factors svscaling factors vary among different problems two particular problems possible explanation might fixed termination accuracy fact looser constrained problem thereby producing less accurate solution taking less time general however results demonstrate scaling factors produce rather crude measure performance decomposition algorithms 8 table 4 objective function values kdd cup problem examples dcmp dcmp ratio 1000 043501 043488 099970 2000 127695 127662 099974 5000 319561 319414 099953 10000 674862 674507 099947 table 5 objective function values kdd cup problem reduced feature set examples dcmp dcmp ratio 500 111129 110823 099724 1000 325665 324853 099750 2000 641472 639787 099456 5000 145490 145164 099776 10000 269715 269119 099779 8 scaling factors also vary results joachims 3 62 convergence rates optimization literature common performance measure iterative algorithms convergence rate notion convergence rate generally defined numeric sequence purpose analysis decomposition algorithms concerned sequence objective function values following definitions taken 7 let x k sequence ir n converges x convergence said qlinear constant r 2 0 1 sufficiently large prefix q stands quotient quotient successive distances limit point considered likewise convergence said qsuperlinear lim said order p p 1 quadratic k sufficiently large positive constant necessarily less 1 easily seen qconvergent sequence order strictly greater 1 converges qsuperliearly 9 convergence rates observed experimentally recording values objective function course iteration plotting respective ratios versus iteration number sample plots shown figures 2 3 problems training sample size 10000 limit points obtained training without decomposition similar plots observed experiments evident convergence plots decomposition algorithm converges linearly superlinearly quadratically 10 plots also reveal training problem illconditioned ratio stays close 1 results consistent known linear convergence rates gradient descent methods unconstrained optimization noteworthy particular semblance full feature set problem variables stay away upper bounds thus resemble unconstrained case another important message convergence analysis experimental theoretical special importance decomposition algorithms unlike scaling factors reveals effects conditioning algorithms performance 9 prefix q omitted rest presentation section difficult see plot reduced feature set tiny margin 00001 seprates ratios 1 0992099409960998iteration linear ratio linear 1000 2000 3000 4000 5000 6000 7000 8000 9000096102106 iteration ratio 1000 2000 3000 4000 5000 6000 7000 8000 9000100200300 iteration quadratic ratio c quadratic fig 2 convergence rates full feature space 1000 2000 3000 4000 5000 6000 700009910993099509970999 iteration linear ratio linear 1000 2000 3000 4000 5000 6000 7000096098101103 iteration ratio 1000 2000 3000 4000 5000 6000 70001216iteration quadratic ratio c quadratic fig 3 convergence rates reduced feature space 63 profiled scaling factors overall processing performed iteration feasible direction decomposition algorithm broken 5 main steps 1 optimization optimization proper calculation support vectors 2 update gradients 3 kernel evaluation requests dot products modules system notice caching kernel evaluations operation equally distributed across iterations beginning takes longer kernels must computed towards end kernels end cache 11 4 selection computation maximal violation kkt conditions left right passes 5 evaluation computation objective function threshold following section scaling factors per iteration established five factors overall scaling factors kernel evaluation optimization scaling factors values obtained selection scaling factors 0237 full feature space 0176 reduced feature space however quality fits low expected behavior constanttime per iteration working sets constant size perhaps larger data sets conditioning subproblems deteriorates slightly thus increasing number iterations optimizer 12 fits displayed figure 4 22 24 26 28 32 34 36 38 4 36 34 32 26 24 22 log logt opt optimization scaling factor full set features 22 24 26 28 32 34 36 38 4 36 34 32 26 24 22 log logt opt optimization scaling factor b reduced set features fig 4 optimization scaling factor fits 11 experiments enough memory allocated kernel cache hold values kernels support vectors 12 current implementation uses minos solve optimization problems update scaling factors values obtained update scaling factors 1060 full feature space 1064 reduced feature space coinsides theoretical expectations linear growth order update op eration fits displayed figure 5 22 24 26 28 32 34 36 38 4 26 24 22 14 12 log logt update update scaling factor full set features 22 24 26 28 32 34 36 38 4 26 24 22 14 12 log logt update update scaling factor b reduced set features fig 5 update scaling factor fits kernel scaling factors kernel scaling factors computed based timing accumulated entire run obtained values 2098 2067 full reduced sets features respectively coincides expected quadratic order growth fits displayed figure 6 logt kernel kernel scaling factor full set features logt kernel kernel scaling factor b reduced set features fig 6 kernel scaling factor fits selection scaling factors values obtained selection scaling factors 1149 full feature space 1124 reduced feature space close theoretical expectations linear growth order selection operation 13 fits displayed figure 7 22 24 26 28 3 32 34 36 38 4 26 24 22 14 12 full set features 22 24 26 28 32 34 36 38 4 26 24 22 14 12 log logt selection selection scaling factor b reduced set features fig 7 selection scaling factor fits evaluation scaling factors values obtained evaluation scaling factors 1104 full feature space 1118 reduced feature space close theoretical expectations linear growth order evaluation operation fits displayed figure 8 conclusions unified treatment working set selection decomposition algorithms presented article provides general view decomposition methods based principle feasible direction regardless particular svm formula tion implementation maximal inconsistency strategy straightforward pattern recognition regression svm either termination conditions used formal justification maximal inconsistency strategy provides useful insight mechanism working set selection experimental results demonstrate similar pattern recognition case significant decrease training time achieved using decomposition algorithm scaling factors decomposition algorithms significantly better straightforward optimization word 13 implementation uses heapbased method whose theoretical running time order log q logarithmic factor feature scaling factor q assumed constant 24 26 28 32 34 36 38 4 log logt evaluation evaluation scaling factor full set features 22 24 26 28 32 34 36 38 4 log logt evaluation evaluation scaling factor b reduced set features fig 8 evaluation scaling factor fits caution needs said regard constants seen profiled experiments worstcase growth orders given periteration basis number iterations depends convergence rate required precision adds another dimension running time analysis linear convergence rate observed experiments suggests progress towards optimal solution slow additional investigation impact problem conditioning necessary number open questions remain regarding svm decomposition al gorithms linear convergence rate established theoretically superlinear quadratic convergence rate achieved different algo rithm finally extremely farreaching results borne investigation conditioning training problem since latter byproduct number factors choice kernel box constraint risk functional etc conditioning optimization problem might useful guide choice svm parameters r quadratic programming analysis decomposition methods support vector machines making largescale support vector machine learning practical solving quadratic problem arising support vector classi fication improved decomposition algorithm regression support vector machines nonlinear programming numerical optimization support vector machines training applications improved training algorithm support vector machines fast training support vector machines using sequential minimal optimization support vector learning advances kernel methods support vector learning learning kernels tutorial support vector regression estimation dependences based empirical data nature statistical learning theory statistical learning theory methods feasible directions tr ctr shuopeng liao hsuantien lin chihjen lin note decomposition methods support vector regression neural computation v14 n6 p12671281 june 2002 chihchung chang chihjen lin training vsupport vector regression theory algorithms neural computation v14 n8 p19591977 august 2002 chihwei hsu chihjen lin simple decomposition method support vector machines machine learning v46 n13 p291314 2002 rameswar debnath masakazu muramatsu haruhisa takahashi efficient support vector machine learning method secondorder cone programming largescale problems applied intelligence v23 n3 p219239 december 2005 nikolas list hans ulrich simon general polynomial time decomposition algorithms journal machine learning research 8 p303321 512007 pavel laskov christian gehl stefan krger klausrobert mller incremental support vector learning analysis implementation applications journal machine learning research 7 p19091936 1212006 hush patrick kelly clint scovel ingo steinwart qp algorithms guaranteed accuracy run time support vector machines journal machine learning research 7 p733769 1212006 tatjana eitrich bruno lang optimal working set size serial parallel support vector machine learning decomposition algorithm proceedings fifth australasian conference data mining analystics p121128 november 2930 2006 sydney australia hyunjung shin sungzoon cho neighborhood propertybased pattern selection support vector machines neural computation v19 n3 p816855 march 2007