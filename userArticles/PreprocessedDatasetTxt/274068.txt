online learning versus offline learning present offline variant mistakebound model learning intermediate model online learning model littlestone 1988 littlestone 1989 selfdirected learning model goldman rivest schapire 1993 goldman sloan 1994 like two models learner offline model learn unknown concept sequence elements instance space makes guess test trials models aim learner make mistakes possible difference models online model set possible elements known offline model sequence elements ie identity elements well order presented known learner advance hand learner weaker selfdirected learner allowed choose adaptively sequence elements presented himwe study fundamental properties offline model particular compare number mistakes made offline learner certain concept classes made online selfdirected learners give bounds possible gaps various models show examples prove bounds tightanother contribution paper extension combinatorial tool labeled trees unified approach captures various mistake bound measures models discussed believe tool prove useful study models incremental learning b introduction mistakebound model learning introduced littlestone l88 l89 attracted considerable amount attention eg l88 l89 lw89 b90a b90b m91 cm92 hll92 grs93 gs94 recognized one central models computational learning theory basically models process incremental learning learner discovers labels instances one one given stage learning process learner predict label next instance based current knowledge ie labels previous instances already seen quantity learner would like minimize number mistakes makes along process two variants model considered allowing learner different degrees freedom choosing instances presented ffl online model l88 l89 sequence instances chosen adversary instances presented learner onebyone ffl selfdirected model grs93 gs94 learner one chooses sequence instances moreover may make choices adaptively ie instance chosen seeing labels previous instances online model learner faced two kinds uncertainties first function target function functions concept class consistent data second instances would challenged future first uncertainty common almost learning model second particular online learning model central aim research focus uncertainty regarding target function trying neutralize uncertainty involved knowing future elements understand effect uncertainty mistakebound learning model one way allow learner full control sequence instances done selfdirected model approach different define offline learning model one learner knows sequence elements advance since difference online learner offline learner uncertainty regarding order instances comparison gives insight information knowing sequence define offline cost sequence elements also define best sequence sequence optimal learner knowing sequence makes fewest mistakes worst sequence sequence optimal learner knowing sequence makes mistakes best compared online selfdirected models think following way ffl worst sequence offline model one sequence instances chosen adversary whole sequence without labels presented learner prediction process starts ffl best sequence offline model one whole sequence instances chosen learner prediction process starts denote ongammaline c mworst c best c sd c number mistakes made best learning algorithm online worst sequence best sequence selfdirected model respec tively worst target concept concept class c obviously c mworst best main issue consider degree measures differ one another emphasize complexity measure number mistakes complexity measures computational complexity learning algorithms ignored known certain cases sd strictly smaller ongammaline example consider class monotone monomials n variables seen class addition give examples show certain concept classes sd may smaller best multiplicative factor olog n hence showing power adaptiveness following example shows also gaps best mworst given n points interval 0 1 consider class functions suffix interval ie functions f x 1 x 0 otherwise n points n1 possible concepts therefore halving algorithm l88 l89 guaranteed make olog n mistakes ie ongammaline olog n class best sequence would receiving points increasing order case learner makes one mistake ie best 1 hand show worst sequence forces mworst thetalog n mistakes interesting question optimal strategy given sequence show simple optimal strategy prove number mistakes exactly rank search tree insert points order appear sequence generalize example show concept class exact number mistakes rank certain tree corresponding concept class particular sequence tree based consistent extensions formalization number mistakes spirit littlestones formalization online case l88 l89 provides powerful combinatorial characterization examples raise question large gaps prove gaps demonstrated mentioned examples essentially largest possible precisely show ongammaline greater sd factor log jx j x instance space eg monomials example x set 2 n boolean assignments result implies particular ratio number mistakes best sequence worst sequence olog n n length sequence 1 also show mworst omegagamma log ongammaline implies either constant nonconstant finally show examples ongammaline 3 mworst showing mworst 6 ongammaline cases able derive better bounds cases mworst mworst 2 show simple online algorithms 1 3 mistakes respectively one way view relationships among models model experts cfhhsw93 fmg92 mf93 sequence oe expert e oe makes predictions assumption sequence oe let oe sequence chosen adversary related result b90a implies efficiency constraints imposed model cases orders easy others computationally hard expert e oe makes mworst mistakes online learner know sequence oe advance question close get best expert e oe problem number experts overwhelming initially n experts although number experts consistent elements sequence oe seen far decreases element presented therefore previous results experts apply rest paper organized follows section 2 give formal definitions model measures performance discussed paper followed simple properties definitions section 3 give definition rank tree well related definitions prove properties section 4 present various gaps characterize offline complexity completeness present section 421 characterization nature based l88 l89 online complexity selfdirected complexity gs94 use characterizations obtain basic results finally section 5 use characterizations study gap online complexity offline complexity 2 model 21 basic definitions section formally present versions mistake bound learning model subject work general framework similar online learning model defined littlestone l88 l89 let x set let c collection boolean functions defined set x ie refer x instance space c concept class let finite subset x online learning algorithm respect concept class c algorithm given advance input works steps follows ith step algorithm presented new element 2 outputs prediction p 2 f0 1g response gets true value c c 2 c denotes target function prediction p may depend set values seen far course concept class c process continues elements presented denote order according elements presented learning algorithm denote mas oe c number mistakes made algorithm sequence oe target function c 2 c algorithm given advance ie number elements p mistake bound algorithm fixed mas 4 finally let original definitions l88 l89 obtained least finite x considering offline learning algorithm algorithm given advance set also actual sequence oe input learning process remains unchanged except prediction p depend actual sequence oe set elements denote maoe c number mistakes made offline algorithm sequence oe target c define maoe sequence oe define given interested best worst sequences denote best c smallest value moe c oe ordering let oe best sequence achieves minimum several sequences pick one arbitrarily similarly worst c maximal value moe c oe worst sequence moe worst worst c selfdirected learning algorithm one chooses sequence adaptively hence sequence may depend classifications previous instances ie target function denote sd c number mistakes made selfdirected algorithm target function c 2 c algorithm given advance set allowed pick queries define sd 4 following simple consequence definitions lemma 1 x c finite x best c worst c online c 22 relations equivalence query models well known online learning model basically equivalent equivalence query model l89 hard realize versions online scenario give rise corresponding variants eq model need following definitions ffl equivalencequery learning algorithm respect concept class c algorithm given advance input works steps follows ith step algorithm outputs hypothesis h response gets counterexample ie element x denotes target function process goes ffl let f denote function chooses counterexamples x denote eqas f c number counterexamples x presented f algorithm course learning process target c knows advance know f finally let eqs c note original definitions angluin a89 obtained considering following well known easy prove fact 1 every one aspect last definition considers worst case performance learner possible choices f counterexamples hypotheses turns relaxing definition learner confronted f certain type one gets eq models match various offline learning measures presented previous subsection ffl let oe denote ordering set let f oe following strategy choosing coun terexamples given hypothesis h counterexample f oe h minimal element according ordering oe ffl let eqoe c 4 ffl let eq best c 4 worst c 4 variant equivalence query model minimal counterexample provided algorithm studied eg pf88 2 every x c x every ordering oe eqoe proof given eq algorithm c oe construct offline algorithm predicting element value current hypothesis eq algorithm h k assigns whenever teachers response indicates prediction wrong present element counterexample eq algorithm replace current hypothesis revised hypothesis direction given offline algorithm define stage learning process hypothesis assigning unseen element 2 value algorithm would guessed got responses indicating made mistakes along sequence oe current stage element whenever counterexample presented pass offline algorithm fact erred element update hypothesis according new guesses corollary 1 every 1 eq best best c 2 eq worst worst c 3 labeled trees central tool employ quantitative analysis work notion ranks trees shall consider certain classes labeled trees depending upon classes learned type learning wish analyze following section introduces technical notions basic combinatorial properties 31 rank trees subsection define notion rank binary tree see eg clr90 eh89 b92 plays central role paper prove simple properties definition tree empty rankt left subtree tr right subtree example rank leaf 0 let r following lemma standard fact rank lemma 2 depth rank r tree r leaves proof induction r exactly one leaf two least common ancestor rank 1 one leaf special case r 0 two leaves case r must 1 cases claim holds induction step let depth rank r tree tr depth definition rank worst case one rank r hence induction hypothesis number leaves bounded r r r completes proof r small relative may convenient use weaker r r number leaves subtree tree subset nodes ordered order induced lemma 3 rank binary tree least k iff subtree 0 complete binary tree depth k proof mark nodes rank increases nodes 0 marked node rank children rank hence marked descendant binary tree direction note rank tree least rank subtrees complete binary tree depth k rank k lemma 4 let complete binary tree depth k let l partition leaves disjoint subsets subtree induced leaves l tree nodes member l exists proof proof induction hence claim obvious consider nodes depth bktc two cases nodes belong trees trees contains complete subtree depth bktc lemma 3 rank least bktc b exists node v depth bktc belong trees consider subtree 0 whose root v consists nodes v definition v leaves tree 0 belong gamma 1 sets l addition depth 0 least tgamma1k hence induction hypothesis one subtrees 0 0 rank least finally note 0 subtree hence desired rank let us mention lower bound rank induced subtrees essentially best possible example take complete binary tree leaf corresponds path root leaf call edge path left right edge goes node left right son let l 0 l 1 set leaves left right edges verified rankt 0 32 labeled trees let x denote domain set x c f0 1g x labeled tree pair f binary tree f function mapping internal nodes x furthermore impose following restriction f ancestor f ffl branch tree path root leaf follows mapping f one one branches ffl branch realizes function 1 son hf 1 note branch realize one function hand branch realizes single function labeled tree ctree set functions realized branches exactly denote set ctrees ffl sequence elements x let c oe denote maximal tree c every node v kth level labeled f note using notation class c shatters set elements sequence oe c oe complete binary tree recall class c shatters set fs every exists function f 2 c therefore conclude class c oe complete binary treeg shall usually omit superscript c clear context 4 gaps complexity measures lemma 1 provides basic inequalities concerning learning complexity different models section turn quantitative analysis sizes possible gaps measures begin presenting section 41 examples concept classes exist large gaps learning complexity different models section 43 prove upper bounds possible sizes gaps bounds show examples section 41 obtain maximal possible gap sizes useful tool analysis characterization various complexity measures ranks certain trees characterization given section 42 let us begin quantitative analysis stating basic upper bound learning complexity demanding model students point view namely online c given instance space x concept class c set define c projection functions c set note several functions c may collide single function c using halving algorithm l88 l89 get theorem 2 x c online c log jc j 41 examples first example demonstrates best c may much smaller worst c online c example already mentioned introduction appears details example 1 let x unit interval 0 1 0 1 define function f x 0 x 1 x let c 4 1g words concept class c consists intervals form 1 0 1 let g theorem 2 easy see example online c logn 1 would like understand offline algorithm performs case clearly every sequence oe adversary always force mistake first element sequence hence best c 1 see best sequence following strategy makes 1 mistake predict 0 mistake made elements 1s function worst sequence consider figure 1 concept class example 2 may seen adversary force learning algorithm make one mistake sets hence total mistakes worst possible performance already granted online algorithm discussed lemma 1 offline algorithm always match online performance next example shows n exist sets class c sd c 2 best c n loss generality assume value dg concept class c see figure 1 consists 2 delta 2 functions function f defined follows f single x assigned 1 hence viewed indicator corresponding function f elements partitioned 2 blocks size blocks 2 functions f get 2 possible combinations values figure 1 functions defined similarly switching roles xs ys precisely g serves indicator corresponding function g elements partitioned blocks size blocks 2 functions get 2 possible combinations values see sd c 2 describe selfdirected learner concept class learner first asks z predicts arbitrary way whether right wrong answer indicates whether target functions one f one g case learner looks corresponding indicator c asks xs one one predicting 0 mistake x ie c immediately implies target function f mistakes made anymore symmetrically c learner asks ys one one predicting 0 mistake ie c implies target function g mistakes made anymore case total number mistakes 2 prove best c n 2 idea learner must choose sequence advance know whether looks one f one g formally let oe best sequence chosen learner describe strategy adversary choose target function way forces learner least d4 omegagamma363 n mistakes let oe 0 prefix oe length 2 adversary considers number x queried oe 0 versus number assume without loss generality number x oe 0 smaller number oe 0 adversary restricts choosing one f moreover eliminates functions f whose corresponding element x appears oe 0 still least 2 2 possible functions choose consider ys queried oe 0 construction c partition elements groups size 2 every function f j gives value elements group least 2 2 elements ys queried oe 0 belong groups simple counting estimate number possible behaviors groups follows originally 2 behaviors groups possible hence eliminate one behaviors elements one needs eliminate 2 dgamma functions eliminated 2 2 functions number eliminated behaviors most2 words least 12 behaviors elements hand guaranteed make r mistakes follows theorem 6 lemma 2 number functions r must least 2 d4 n 42 characterizing moe c using rank main goal section characterize measure moe c byproduct present optimal offline prediction algorithm ie algorithm every sequence oe next theorem provides characterization moe c terms rank tree c oe concept class c sequence oe similar characterization proved littlestone l88 l89 online case see section 421 theorem 3 proof show moe c rankt oe present appropriate algorithm predicting 1 algorithm considers tree oe defined whose root 1 denote left subtree tr right subtree rankt predicts 0 predicts 1 otherwise rankt l predict arbitrarily recall case rankt l definition rank rankt l rankt r smaller rankt oe therefore step algorithm uses prediction subtree oe consistent values seen far conclude step algorithm made mistake rank decreased least 1 rankt oe mistakes made show algorithm better present strategy adversary choosing target c guarantee given algorithm makes least rankt oe mistakes adversary constructs tree oe step holds subtree whose root node halving algorithm online c therefore also best c olog n marked consistent values already gave classification getting prediction adversary decides true values follows one subtrees either tr rank rank chooses value according subtree note definition rank one subtrees may property well defined case possible guessed correct value example algorithm described rank subtree used adversary 1th step decreased second possible case definition rank rank tr smaller 1 rank case adversary chooses negation prediction hence step makes mistake rank decreased 1 therefore adversary force total rankt oe mistakes theorem immediately implies corollary 4 worst oe ordering best oe ordering remark 1 worth noting sauers lemma s72 concept class c v c dimension size c oe bounded n n usual length oe follows c small v c tree small therefore consistency checked efficiently construction tree efficient turn implies efficiency generic optimal offline algorithm proof classes small vc dimension example 3 consider concept class example 1 note case tree oe exactly binary search tree corresponding sequence namely oe tree constructed starting empty tree performing sequence operations eg clr90 hence moe c exactly rank search tree 421 characterizing online selfdirected learning complete picture one would certainly like combinatorial characterization online c well characterization given littlestone l88 l89 reformulate characterization terms ranks trees proof remains similar one given littlestone l88 l89 provide completeness theorem 5 proof show online c max use adversary argument similar one used proof theorem 3 adversary uses tree gives maximum expression choose sequence classification elements time rank decreased 1 prediction algorithm makes mistake show online c present appropriate algorithm similar one presented proof theorem 3 predicting 2 first define c 0 functions c consistent functions c consistent 1 algorithm compares max predicts according larger one crucial point least one two values must strictly smaller otherwise tree c whose rank prediction continues way maximal rank decreased mistake finally following characterization implicit gs94 theorem proof consider tree whose rank minimal one c show sd c rank present appropriate algorithm makes use tree point learner asks instance current node tree addition predicts according subtree current node whose rank higher arbitrarily ranks two subtrees equal true classification determines child current node learner needs proceed follows definition rank whenever algorithm makes mistake remaining subtree rank strictly smaller previous one direction given strategy learner makes sd c mistakes construct tree describes strategy namely point instances learner ask next stage given possible classifications current instance determine two children current node rank sd c gives adversary strategy fool learner node classify current instance according subtree higher rank ranks subtrees equal answer algorithm adversary says opposite definition rank gives rankt mistakes hence rankt sd c certainly minimum trees smaller 43 bound size gap natural question large gaps various complexity measures example maximum ratio worst c best c example 1 best 1 worst log n easily generalized k versus thetak log n following theorem shows gap smallest measure sd c largest measure online cost cannot exceed olog n particular implies similar bound gap oe best oe worst example 1 bound tight ie cases achieve gap similarly gap sd c best c exhibited example 2 also optimal theorem 7 size n online c sd c delta log n shall present two quite simple different proofs theorem first proof employs tool labeled trees gives slightly weaker result second information theoretic argument proof using labeled trees consider tree gives minimum theorem 6 depth n rank theorem 6 c lemma 2 tree contains leaves jc theorem 2 online c log proof information theoretic argument let c projection functions c set note several functions c may collide single function c consider number bits required specify function c one hand least log jc j bits required hand selfdirected learning algorithm learns class yields natural coding scheme answer queries asked algorithm according function c 2 c coding consists list names elements prediction algorithm wrong information enough uniquely identify c follows sd c delta log n bits enough hence log finally halving algorithm l88 l89 known online c log jc j theorem follows corollary 8 worst best c delta log n proof combine theorem 7 lemma 1 worst c vs online c section discuss question much learner benefit knowing learning sequence advance terminology model issue determining possible values gap online c worst c show section 52 one two measures nonconstant quantitatively online algorithm makes k mistakes offline algorithm makes omegagamma log mistakes oe special cases worst c either 1 2 prove section 51 online c 1 3 respectively 51 simple algorithms section present two simple online algorithms e1 e2 case offline algorithm bounded one two mistakes respectively sequence let set elements x every sequence oe permutation offline learning algorithm makes one mistake show online algorithm e1 makes one mistake without knowing actual order advance algorithm e1 uses guaranteed offline algorithm works follows ffl given element x 2 choose sequence oe starts x predict according prediction oe ie aoe mistake made x aoe made mistake make mistakes sequence oe hence use aoec x get true values elements sequence aoec x denote predictions makes sequence oe getting value c x words 2 unique value consistent value c x 6 aoe otherwise aoe make another mistake therefore e1 make one mistake case sequence offline learning algorithm makes two mistakes present online algorithm e2 makes three mistakes optimal due call element x bivalent respect exist sequences oe 0 oe 1 start xy oe 0 online algorithm predicts c online algorithm predicts c 1 otherwise x univalent respect say x 1univalent respect prediction always 1 0univalent prediction always 0 online procedure e2 input x works follows ffl far made mistakes x 1univalent respect predict c predict ffl far made one mistake positive w made mistake predicted c implies w 1univalent respect particular respect x w either 0univalent bivalent cases sequence oe wxoe 0 aoe predicts c makes mistake use prediction x aoe1 denotes prediction makes sequence oe getting value c case another mistake sequence already made two mistakes make mistakes namely use aoe1 b get value elements ffl far made one mistake negative w w either 1univalent respect x bivalent respect x similar previous case difficulty time also possibility w 0univalent respect x however case made mistake means predicted implies exists w 1univalent respect consider sequence definition aoe predicts c therefore makes first mistake w denote prediction wrong elements sequence uniquely determined namely unique function f 1 consistent c b hand b indeed true value denote prediction x wrong unique function f 2 consistent c therefore predict c x case made mistake second mistake know sure possible functions f 1 f 2 fact lucky f 1 done know two functions target need make one mistake 3 total 52 general bound section discuss gap measures online c worst c show online makes k mistakes offline algorithm makes omegagamma log oe proof makes use properties proved section 31 characterizations online offline mistake bounds ranks trees proved section 4 precisely take tree c maximum rank rank theorem 5 exactly characterizes number mistakes made online algorithm use construct tree rank small nodes level labeled element tree form oe sequence oe lemma 5 given complete labeled binary tree depth k f sequence oe elements tree c oe log k proof construct appropriate sequence oe phases phase starting add 2 new elements oe rank c oe increased least one hence beginning ith phase rank least beginning ith phase collection 2 subtrees rank least k2 oi 2 particular beginning phase 0 single subtree whose rank k subtrees consistent one leaves tree c oe already built previous phases ie subtree consistent assignment elements included oe previous phases moreover corresponding 2 leaves induce complete binary subtree depth ith phase consider 2 subtrees collection subtree 0 add sequence oe element r rank subtree 0 rooted r rankt 0 rank subtrees tr corresponding sons r rankt remark order treat subtrees within ith phase arbitrary elements r already appear oe need add adding new elements oe examine trees tr corresponding subtree partitions leaves tree 2 according possible values hence lemma 4 exists subtrees r respectively rank least consistent one leaves extended c oe 2 i1 subtrees get way form collection trees phase 1 finally note choice elements r added oe get c oe complete binary subtree depth 1 ith phase rank subtrees collection least k ith phase rank subtrees collection least k 2 hence simple induction implies therefore repeat process log phases hence obtaining tree c oe rank log k theorem 9 let c concept class x instance space x set elements worst log online c proof assume online k theorem 5 rank k tree c lemma 3 contains complete binary subtree depth k lemma 5 sequence oe tree c oe log k hence theorem 3 moe c log k major open problem exact relationship online offline mistake bounds largest gap could show multiplicative factor 32 worst proof first give example case space 4 elements c 1 following 8 functions 4 elements f0000 0011 0010 0111 1000 1010 1100 1111g verified inspection online worst 2 general k take k independent copies x 1 c 1 let x k space 4k elements partitioned k sets 4 elements let c k 8 k functions obtained applying one 8 functions c 1 k sets elements let due independence k functions follows online worst 6 discussion work analyze effect various degrees knowledge order elements mistake bound model learning performance ie number mistakes learner remark setting learner deterministic corresponding questions case randomized learners remain future research also analyze quantitatively advantage online algorithm may gain knowing set elements advance without knowing order oe presentation wish compare situation online algorithm knows nothing apriori sequence consists elements x case algorithm knows set elements sequence taken additional information order following example shows knowledge gives advantage learning algorithm consider intervals concept class example 1 instance space x restricted f 1 g proven online x 1 hand every set size showed online 1 therefore small compared ie small compared n number mistakes significantly improved knowledge acknowledgment wish thank moti frances nati linial helpful discussions r equivalence queries approximate fingerprints separating distributionfree mistakebound learning models boolean domain learning boolean functions infinite attribute space rankr decision trees subclass rdecision lists use expert advice online learning rectangles learning decision trees random examples universal prediction individual sequences learning binary relations total orders power selfdirected learning apple tasting nearly onesided learning learning irrelevant attributes abound new linearthreshold algo rithm mistake bounds logarithmic linearthreshold learning algorithms weighted majority algorithm online learning oblivious environment power randomization universal sequential decision schemes individual sequences learning automata ordered examples density families sets tr learning decision trees random examples needed learning mistake bounds logarithmic linearthreshold learning algorithms introduction algorithms equivalence queries approximate fingerprints learning boolean functions infinite attribute space online learning oblivious environment power randomization learning automata ordered examples online learning rectangles rankitalicritalic decision trees subclass italicritalicdecision lists learning binary relations total orders use expert advice weighted majority algorithm power selfdirected learning learning quickly irrelevant attributes abound ctr peter damaschke adaptive versus nonadaptive attributeefficient learning machine learning v41 n2 p197215 november 2000 paul burke sue nguyen penfan sun shelley evenson jeong kim laura wright nabeel ahmed arjun patel writing bok designing networked learning environment college students proceedings 2005 conference designing user experience november 0305 2005 san francisco california