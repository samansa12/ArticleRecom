markov model prediction io requests scientific applications given increasing performance disparity processors storage devices exploiting knowledge spatial temporal io requests critical achieving high performance particularly parallel systems although perfect foreknowledge io requests rarely possible even estimates request patterns potentially yield large performance gains paper evaluates markov models represent spatial patterns io requests scientific codes paper also proposes three algorithms io prefetching evaluation using io traces scientific codes shows highly accurate prediction spatial access patterns resulting reduced execution times possible b introduction disparity processor disk performance continues increase driven market economics reward highperformance processors highcapacity storage devices parallel systems eg clusters increasingly constructed using large numbers commodity components work supported part national science foundation grants nsf asc 9720202 nsf asc 99 75248 nsf eia9972884 department energy contracts llnl b341494 llnl b505214 nsf alliance paci cooperative agreement permission make digital hard copies part work personal classroom use granted without fee provided copies made distributed profit commercial advantage copies bear notice full citation first page copy otherwise republish post servers redistribute lists requires prior specific permission andor fee ics02 june 2226 2002 new york new york usa io become performance bottleneck many scientic applications characterization application io patterns shown many applications complex irregular io patterns 4 18 mediated multilevel io libraries illsuited le system policies optimized simple sequential patterns ie sequential read ahead write behind patterson et al 12 showed large io performance gains possible using hints advise le systems io libraries anticipated io accesses given foreknowledge temporal spatial io patterns one schedule prefetching write behind requests ameliorate eliminate io stalls indeed foreknowledge optimal le cache replacement decisions possible restricted conditions 8 unfortunately perfect foreknowledge rarely possible applications io access stream may data dependent may change due user interactions moreover complex irregular patterns enumeration complete request stream often requires multilevel instrumentation capture record io patterns reuse guiding future executions instrumentation expensive requires storing access pattern rather specifying complex io pattern exactly probabilistic models potentially capture salient features compactly describing likely request sequences although statistical characterization io patterns less accurate enumeration compact shall see used guide caching prefetching write behind decisions hence paper explores techniques constructing compact markov models spatial access pat terns evaluating ecacy predictions using io traces drawn largescale scientic codes remainder paper organized follows x2 outline markov model construction techniques followed x3x4 description implementation test suite x5 describe results tracedriven simulations evaluate performance several prediction algorithms well timing experiments using scientic codes finally x6x7 summarize related work results outline plans future work 2 markov model prediction one lessons experimental analysis io patterns parallel codes behavior far complex originally expected 4 16 14 18 13 unlike sequential io patterns vector systems parallel io 15e0625e0635e0645e060 2000 4000 6000 8000 10000 12000 14000 16000 initial offset bytes request number figure 1 initial continuum io request osets patterns often strided irregular vary widely size 21 io pattern complexity simple approaches le system tuning nblock read ahead work well sequential access patterns inappropriate irregular patterns example figure 1 shows initial le osets requests execution continuum parallel unstructured mesh code whose io behavior captured using pablo io characterization toolkit 4 fewer half io requests follow sequentially previous request complex patterns continuum cannot reduced simple rules ie sequential strided captured powerful expressive meth ods moreover complex patterns common application io requests mediated multilevel io libraries 22 markov models capture nonsequential io patterns chosen model applications io access stream markov model markov model n states fully described transition matrix p value p ij n n matrix represents probability transition state j current state transition probabilities depend current state transition path current state aect future state transitions characteristic known markov property 7 given le system block size create state every block associated le application io requests normally expressed block osets rather byte osets convert request sequence blocks must accessed satisfy application quest hence multiple requests block result block number repeated access stream transition occurs whenever le block accessed models transition matrix created setting p ij fraction times state j accessed immediately state consider two illustrative examples first simple sequential access pattern represented cyclic matrix block accessed succession second figure 2 illustrates simple strided pattern repeats multiple0 1 6 10135791111311from figure 2 strided io pattern transitions times examples highlight key aspect markov transition matrices matrices sparse complex irregular patterns consequently one compactly represent access patterns even large les however markov property allows compact representation also potential error source block access depends solely immediately preceding access history accesses lost example consider following pattern block accesses 0 1 0 2 0 3 0 4 markov model pattern four transitions block zero probability 025 therefore model implies accesses blocks one four equally likely access block zero however block three already accessed next block follow block zero block four 23 prediction strategies given io access pattern markov model used subsequent application execution predict future io accesses several dierent prediction strategies based markov model possible ranging complexity simply following likely sequence single step state transitions greedy prediction complex schemes predict likely nstep path diers implementation complexity execution overhead associated predictive power greedy prediction simplest prediction method chooses sequence le blocks repeatedly nding likely transition current state approach builds directly nstep transition models markov theory hence call algorithm greedyxed sequence predicted blocks stops specied number blocks reached though terminating conditions possible instance sequence might terminated total likelihood sequence drops given threshold results predictions varying length either case greedyxed prediction strategy provides possibly variable length le block sequence used prefetching write behind path prediction greater prediction accuracy may possible choosing sequence highest total likeli hood rather choosing likely sequence single step transitions method treats markov model graph performs depthlimited search beginning current state nd likely path call pathxed prediction choose less likely initial transitions lead high probability sequences amortized prediction io access patterns revisit block many times likely predicted path may visit block however enough alternate paths may visit block cause accessed often application execution explore eectiveness approach propose method called amortized prediction named value predicting block sense amortized entire execution rather next l blocks amortized prediction creates state occupancy probability vectors next l time steps chooses likely state vector prediction time step initial probability vector 0 zero except element 0 set one vectors 1 l generated repeated application kolmogorov equation p models transition matrix block j prediction sequence chosen state greatest probability j 3 assessment infrastructure assess ecacy markov models predicting io access patterns within scientic applications relied three approaches tracedriven simulation cache simula tion experimental measurement within research le system 31 tracedriven assessment constructed markov models using sparse matrix strategy represent markov models transition matrix reduce storage requirements many scientic applications typically regular though sequential le access patterns regularity transitions occur state average number transitions source state bounded small constant size matrix grow linearly rather quadrat ically size le tracedriven simulator accepts sparse matrix markov models prediction strategy application io trace pablo selfdening data format sddf 1 computes prediction accuracies figure 3 details simulators operation 32 cache simulation also developed simple le cache simulator lru replacement xedsize blocks request cache satised either cache hit loading requested block cache prefetches additional blocks specied prediction algorithm application trace file library markov model builder markov model io request algorithm prediction predicted sequences comparison prediction accuracy results figure 3 tracedriven simulator structure2e066e061e0714e07 initial offset bytes request number figure 4 cactus io request pattern detail 33 file system measurement explore thesis performance best maximized tuning le system policies application behavior also developed portable parallel le system ppfs2 consists portable userlevel inputoutput library variety le caching prefetching data policies ppfs2 executes atop linux cluster uses underlying le system io used ppfs2 study temporal access pattern prediction via time series 19 20 conducting experiments used cluster 32 dual processor 933 mhz intel pentium iii systems linked though gigabit ethernet machine contained 1 gb memory ran linux 22 kernel disk requests serviced machine 4 scientific application suite assess accuracy compact markov models io prediction utility le block prefetching chose suite six scientic applications parallel applications subject extensive analysis 4 16 14 chosen large size variety io request characteristics io traces obtained using pablo io characterization toolkit 4 simplicity used io trace data single node parallel application execution model applications requests sin 0request size bytes request number figure 5 continuum request sizes gle le application code brie described paragraphs cactus cactus code 2 modular environment developing highperformance multidimensional simulations particularly numerical relativity test problem used obtain io trace black hole simulation read requests test problem extremely small largest mere sixteen bytes though le request osets span almost gigabyte percent requests sequential almost 98 percent within 50 bytes previous request however remaining requests regions least 1 mb previous request often greater 10 mb away figure 4 shows detailed view rst six hundred requests continuum continuum 1 unstructured mesh continuum mechanics code continuums io requests primarily restricted rst eighth le shown figure 1 however end le accesses occur regular intervals 57 percent accesses nonsequential seek distances within rst eighth le small interestingly figure 5 shows request sizes vary widely ranging less 10 bytes 100000 bytes dyna3d dyna3d explicit niteelement code analyzing transient dynamic response threedimensional solids structures application generates long sequence sequential requests one processor sequentially reads rst 2 mb large input le single byte time hartreefock hartreefock code 5 calculates interactions among atomic nuclei electrons reaction paths trace data continuum hydro codes taken codes use existing serial io library developers analyzed library determined unsuitable parallel environment developers replacement io library expect dierent pattern io new library integrated codes1e063e065e060 50 100 150 200 250 300 350 400 initial offset bytes request number figure hartreefock io request pattern1e063e065e067e060 100 200 300 400 500 600 initial offset bytes request number figure 7 hydro io request pattern storing numerical quadrature data subsequent reuse io requests retrieve 80 kb data le accessed sequentially six times figure 6 shows le osets data reuse hydro hydro 1 blockstructured mesh hydrodynamics code multigroup radiation diusion majority io accesses three widely separated regions le sixtyseven percent accesses follow sequentially previous access seeks almost eight million bytes also occur distribution request sizes ranges one byte almost one million bytes sar sar synthetic aperture radar code produces surface images aircraft satellitemounted radar data figure 8 shows application issues two sequential quests followed seek next portion le request sizes range 370 kb almost 2 mb 5 experimental assessment given simulation infrastructure x3 io traces application suite x4 built markov models request position extent bytes request number figure 8 sar io request pattern associated io patterns analyzed accuracy models comparing model predictions actual io requests also investigated eect le block sizes model size complexity analyzed prediction accuracy single multiple block predictions 51 prediction accuracy markov models lossy representation application io patterns simplest metric prediction accuracy dierence model predictions application request streams approach allows one assess ecacy function spatial io patterns simulator figure 3 generates prediction sequence length l application block request pro cessed accuracy prediction sequence fraction predicted blocks exactly match blocks requested next l timesteps overall accuracy arithmetic mean across predicted request se quences capturing eects le block size prediction length prediction algorithm block size basic choice building markov models io activity choice block size matching models block size le cache disk block size natural choice intuitively larger block sizes reduce models storage requirements provide weaker bounds range bytes requested larger blocks provide implicit prefetching interacts access pattern sequentiality request size figure 9 shows single step ie greedy single block lookahead prediction accuracy io trace function block size continuum hydro cactus dyna3d codes show high accuracy block size accuracy near constant across block sizes contrast prediction accuracy sar code declines sharply block size increases due repeated le seeks large 80 kb requests hartreefock code create nonlinear behavior large blocks prefetch data unused stride boundaries figure 6 crossed generally behavior consequence regularity application request patterns proximity of0550650750850954096 8192 16384 32768 65536 131072 262144 prediction accuracy block size bytes cactus continuum dyna3d hartreefock hydro figure 9 one step accuracy multiple block sizes02061 prediction accuracy prediction length cactus continuum dyna3d hartreefock hydro figure 10 greedy fixed accuracy 64 kb blocks block size request size block size approaches request size state associated markov model single transition next state fewer transitions back side selftransitions interrupt procession one state another frequently factors result model likely state transition low probability compared models suggests block size reasonable multiple application request size amortizes disk io overhead unless application request sizes large prediction length prefetching eective must accurately predict access pattern request sequence predictions io system stage data requests figure 10 shows eect prediction path length 64 kb block size greedyxed prediction strategy surprisingly prediction accuracy generally decreases prediction length increases prominent exception hartreefock code single step prediction accuracy 64 kb blocks low multiplicative eect prediction sequence also low even 4 kb blocks shown hartreefock single step accuracy 90 percent prediction accuracy percent predicting 25 steps ahead prediction algorithm multistep prediction errors sar hartreefock codes illustrate information loss compact markov model memoryless property means certain seek patterns trigger mispredictions greedy lookahead strategies rationale pathxed amortized strategies described x23 tested prediction strategy multiple block sizes prediction lengths 25 steps cases three algorithms resulted similar prediction accuracy declining accuracy sequence length increased however figure 11 shows wide gap performance amortized greedy xed path xed strategies hartreefock code two large block sizes reason striking dierence steps using amortized prediction enough probability mass moved succeeding state cause predicted state change following steps greedy xed lesser degree path xed continue following likely transition account multiple paths another state 52 cache behavior access prediction accuracy one metric markov model utility assess benets markov models predicting io requests also simulated simple client cache compared prefetching using markov model using baseline n block read ahead strategy simulated cache requests prediction length n block request predicted blocks loaded already replacement via standard lru policy predicted blocks already present cache placed newly fetched blocks end lru list conducted experiments using block sizes 1 kb 4 kb 16 kb 64 kb variety le cache sizes prediction horizons 1 10 blocks given relatively strong sequentiality application io access patterns three prediction strategies yield relatively high cache hit ratios figures 1213 show cache hit ratios two typical cases hydro cactus codes greedy amortized prediction strategies always perform better n block read ahead prefetching multiple blocks often perform better predicting one two steps ahead 53 file system measurements true test io prediction strategy performance part parallel le system tests capture interplay application system features hence also conducted experiments using cactus synthetic application similar hartreefock modied use ppfs2 userlevel parallel le system x3 ppfs2 extended support n block read ahead markov model prediction methods use policies block prefetching decisions assess benets markov model prediction standard prefetch policies ran experiments using cactus code 50x50x50 grid 2000 iterations results 2 million block read requests distributed across 2 gb data le congured ppfs2 testbed use le cache 4 kb 8 kb blocks lru cache0960981 hit rate prefetch depth nblock readahead greedyfixed amortized figure 12 hydro cache hit ratios0960981 hit rate prefetch depth nblock readahead greedyfixed amortized figure 13 cactus cache hit ratios replacement strategy prefetch depth ranged blocks read requests cached write requests sent directly underlying le system purposes single local disk markov model predictions computed using greedyxed simplest prediction strategy figure 14 shows results experiments variety prefetch distances experiments using markov model prediction showed performance improvement tested prefetch depths compared n block readahead prefetching total application execution time decreased 10 prefetching case using 8 kb blocks 3 using 4 kb blocks n block readahead yielded substantially poorer performance either two methods conrms assertion le system policies designed sequential access patterns inappropriate scientic codes data figure 14 also show overhead using markov model small overhead repaid improved prefetch predictions figure 15 shows cache hit rates recorded tests block sizes markov model prediction achieves prediction accuracy prediction sequence length greedyfixed pathfixed prediction accuracy prediction sequence length greedyfixed pathfixed amortizedfixed 64 kb block size b 256 kb block size figure 11 hartreefock strategy comparison2006001000 time prefetch depth blocks nblock nblock 4kb prefetching 8kb prefetching 4kb markov markov 4kb figure 14 cactus execution time hit rate 9999 n block readahead achieves hit rate 98 large performance dierence two methods indicates n block readahead prefetching many blocks go unused quantify overhead markov models wrote synthetic code issues requests hartreefock pattern x4 synthetic code reads 264 mb input le sequentially six times via 396 requests 80 kb request followed loop iterates approximately one millisecond simulating compute cycle pattern ideally suited n block readahead large overhead markov model approach apparent figure shows results experiment 1 mb cache 8 kb blocks markov model prediction performs well n block readahead request pattern two algorithms also result decreased execution time compared prefetching case time savings reaching 13 prefetching 10 blocks time0975098509951 2 3 cache hit rate prefetch depth blocks markov 4kb markov nblock 4kb nblock figure 15 cactus cache hit rate 54 experiment summary results shown high prediction accuracy across large range block sizes look ahead lengths using sim ple greedy prediction strategies application io patterns however amortized strategy yielded results equal greedy strategies cases substantially better performance others believe additional complexity nongreedy strategies justi ed addition timing experiments using userlevel le system modied markov models demonstrate approach noticeably reduce execution time sample scientic application also demonstrate overhead low 6 related work many groups examined spatial temporal io patterns scientic applications smirni et al 17 16 15 characterized io accesses suite scientic ap08 time prefetch depth blocks prefetching nblock markov figure hartreefock execution time plications drawn scalable io initiative 4 characterization showed applications exhibited access patterns ranging simple sequential accesses interleaved patterns across processors although simple descriptions sucient describe patterns introduction higherlevel io libraries flexio 11 led even complicated patterns patterson et al 12 used applicationprovided hints guide prefetching caching decisions integrated caching prefetching system used costbenet analysis decide prefetch blocks replace cache vellanki chervenak 21 used approach similar pattersons replaced applicationprovided hints automaticallygenerated probabilistic model access stream built used prefetch tree using lempelziv scheme duke 22 6 addition reducing size access stream tree allowed prediction future accesses based likely paths tree restricting width tree allowed decrease storage requirements cost additional inaccuracy predicted accesses use markov models builds many ideas emphasizes congurability compatibility temporal prediction schemes 19 articial neural networks anns used previously io pattern characterization madhyastha reed 10 9 used anns hidden markov models assign request pattern simple qualitative classication sequential variablystrided markov models also used cortes labarta 3 identify predict strided access patterns work diers providing explicit prediction blocks expect next represent arbitrary access patterns 7 conclusions futures outlined approach access pattern prediction based markov models several associated prediction strategies using io traces large scientic ap plications experiments explored eects varying block size used build model length predicted io stream algorithm used generate predictions experimental data suggests irregular io patterns found todays scientic applications markov models strike eective balance implementation complexity predictive power reasonable block sizes performance largely independent block size allowing le system designers choose set defaults achieving high performance allows users models choose block size appropriate tasks without affecting prediction accuracy large degree finally amortized path scheme generally eective predicting access patterns simple greedy schemes several opportunities exist future study including extensive experiments scientic applications also current techniques assume one previous runs available generate markov model allowing model created used online may provide benets uninformed prefetching caching even though large parts application pattern yet integrated model finally cases hints provided future io accesses markov models used establish accuracy hints measured would allow adaptive le system decide using hinted sequence prefetching caching likely provide benet 8 r pablo selfde ning data format three dimensional numerical relativity hyperbolic formulation linear aggressive prefetching aay increase performance cooperative caches characterization suite inputoutput intensive applications inputoutput characteristics scalable parallel applications practical prefetching via data compression art computer systems performance analysis tracedriven comparison algorithms parallel prefetching caching automatic classi inputoutput access pattern classi diving deep datamanagement visualization strategies adaptive mesh nement simulations informed prefetching caching scalable inputoutput achieving system balance comparison logical physical parallel io patterns io requirements scienti performance modeling parallel io system application driven approach workload characterization inputoutput intensive parallel applications lessons characterizing inputoutput behavior parallel scienti arima time series modeling forecasting adaptive io prefetching automatic arima time series modeling forecasting adaptive inputoutput prefetching prefetching without hints costbene analysis predicted accesses optimal prefetching via data compression tr practical prefetching via data compression informed prefetching caching inputoutput characteristics scalable parallel applications optimal prefetching via data compression tracedriven comparison algorithms parallel prefetching caching inputoutput access pattern classification using hidden markov models lessons characterizating inputoutput behavior parallel scientific applications series modeling forecasting adaptive io prefetching diving deep linear aggressive prefetching workload characterization inputoutput intensive parallel applications io requirements scientific applications automatic classification inputoutput access patterns automatic arima time series modeling forecasting adaptive inputoutput prefetching ctr nancy tran daniel reed automatic arima time series modeling adaptive io prefetching ieee transactions parallel distributed systems v15 n4 p362377 april 2004 yifeng zhu hong jiang ceft costeffective faulttolerant parallel virtual file system journal parallel distributed computing v66 n2 p291306 february 2006