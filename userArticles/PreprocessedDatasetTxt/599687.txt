sparse regression ensembles infinite finite hypothesis spaces examine methods constructing regression ensembles based linear program lp ensemble regression function consists linear combinations base hypotheses generated boostingtype base learning algorithm unlike classification case regression set possible hypotheses producible base learning algorithm may infinite explicitly tackle issue define solve ensemble regression hypothesis space infinite approach based semiinfinite linear program infinite number constraints finite number variables show regression problem well posed infinite hypothesis spaces primal dual spaces importantly prove exists optimal solution infinite hypothesis space problem consisting finite number hypothesis propose two algorithms solving infinite finite hypothesis problems one uses column generation simplextype algorithm adopts exponential barrier approach furthermore give sufficient conditions base learning algorithm hypothesis set used infinite regression ensembles computational results show methods extremely promising b introduction past years seen strong interest boosting ensemble learning algorithms due success practical classication applications eg drucker et al 1993 lecun et al 1995 maclin opitz 1997 schwenk bengio 1997 bauer kohavi 1999 dietterich 1999 basic idea boosting ensemble learning general iteratively generate sequence fh g t1 functions hypotheses usually combined c 2001 kluwer academic publishers printed netherlands regressiontex 15012001 2026 p1 yoshua bengio dale schuurmans nonekluwer v13 g ratsch demiriz kp bennett hypothesis coecients used hypotheses h elements hypothesis class p index set hypotheses producible base learning algorithm l typically one assumes set hypotheses h nite also consider extensions innite hypothesis sets classication ensemble generates label signf x weighted majority votes regression predicted value f x recent research eld focused better understanding methods extensions concerned robustness issues mason et al 1998 bennett et al 2000 ratsch et al 2000b 2001 shown classication ensemble methods viewed minimizing function classication margin typically performed algorithmically using gradient descent approach function space recently shown soft margin maximization techniques utilized support vector machines readily adapted produce ensembles classication ben nett et al 2000 ratsch et al 2000b algorithms optimize soft margin error measures originally proposed support vector machines certain choices error margin norms problem formulated linear program lp rst glance lp may seem intractable since number variables linear program proportional size hypothesis space exponentially large fact two practical algorithms exist optimizing soft margin ensembles rst uses column generation simplex algorithm bennett et al 2000 second uses barrier functions interiorpoint method ratsch et al 2000 advantage linear programming approaches produce sparse ensembles using fast nite algorithms purpose work tackle regression ensembles using analogous support vector linear programming methodology regression date relatively papers addressed ensembles regression zemel pitassi 2001 one major diculty rigorously dening regression problem innite hypothesis space classication assuming hypothesis nite set possible outputs hypothesis space always nite since nite number ways label nite training set regression even relatively simple hypothesis spaces linear functions constructed using weighted least squares consist uncountable innite set hypothe ses priori clear even express regression problem innite hypothesis space clearly practically consider sparse regression ensembles 3 ensemble functions linear combination nite subset set possible hypotheses work study directly issue innite hypothesis spaces begin section 2 review boosting type algorithms classication regression examine relationship ensemble methods linear programming section 3 review linear program approach sparse regression show easily extendible ensemble regression nite hypothesis case section 32 investigate dual linear program ensemble regression section 33 propose semiinnite linear program formulation boosting innite hypothesis sets rst dual primal space dual problem called semiinnite innite number constraints nite number variables important sparseness property semiinnite regression problem solution consisting nite number hypotheses section 4 propose two dierent algorithms eciently computing optimal ensembles exact implementation algorithms dependent choice base learning algorithms section 43 investigate three possible base learning algorithms result innite nite hypothesis sets computational results presented section 5 notational conventions used paper found table table notational conventions n n counter number patterns counter number hypotheses nite counter number iterations indexset hypotheses space dimensionality x training data input targets training pattern label set base hypotheses element h set linear combinations h element f hypothesis weight vector weighting training set w weight vector linear models indicator function tube size tube parameter determines c regularization complexity parameter weighted classication error k kp pnorm product scalar product feature space 4 g ratsch demiriz kp bennett 2 boostingtype algorithms brie review discuss existing boostingtype algorithms section 21 start classication case describe adaboost closely related arcgv breiman 1997 discuss properties solutions generated boosting show connections linear program lp maximizing margins section 22 brie review recent regression approaches mainly motivated gradientdescent understanding boosting 21 classification boosting lp classication case generally assumed hypotheses class dened base learning algorithm l iteration base learner used select next hypothesis using certain criteria ensemble generates label weighted majority votes signf x note hypothesis class always nite 2 n distinct labelings training data consider adaboost algorithm details see eg freund 1997 main idea adaboost introduce weights n training patterns z g used control importance single pattern learning new hypothesis ie repeatedly running base algorithm training patterns dicult learn misclassied repeatedly become important increasing weight shown adaboost minimizes error function breiman 1997 frean downs 1998 friedman et al 1998 ratsch et al 2001 expressed terms margins namely iteratively solves problem min 0 optimization strategy adaboost also called gradient descent function space mason et al 1999 friedman et al 1998 one eectively optimizes along restricted gradient directions space linearly combined functions f also understood coordinate descent method eg luenberger 1984 minimize g possible weightings hypotheses h ratsch et al 2000 one hypothesis added time weight never changed unless hypothesis added sparse regression ensembles 5 widely believed breiman 1997 freund schapire 1996 schapire et al 1997 ratsch et al 2001 2001 adaboost approximately maximizes smallest margin training set problem solved exactly following linear programming problem complete hypothesis set h cf grove schuurmans 1998 assuming nite number basis n f 1 breiman 1997 proposed modication adaboost arcgv making possible show asymptotic convergence 1 global solution lp 1 grove schuurmans 1998 lp 1 solved using iterative linear programming based approach retrospectively considered column generation algorithm unfortunately neither approach performed well practice margin versions linear program based ideas support vector machines perform well practice theoretically terms generalization bounds ratsch et al 2000b bennett et al 2000 example soft margin version could n f bennett et al 2000 column generation algorithm classication proposed eciently solve lps algorithm closely related ratsch et al 2001 kivinen warmuth 1999 dier gradientboosting idea used motivate boostingtype algorithms mason et al 1999 friedman et al 1998 iteration generated hypothesis weights optimized respect maximum margin error function gradient approach xes hypothesis weights hypotheses generated purpose paper examine extensions approaches regression case 6 g ratsch demiriz kp bennett 22 previous regression approaches several regression boosting methods proposed provide brief description three note rst two described also fisher 1997 g ridgeway 1999 reduce problem series classication tasks thus eliminating consideration innite hypothesis spaces last approach friedman 1999 applied innite hypothesis spaces dene means boost innite hypothesis space 221 adaboostr rst boostingtype algorithm regression adaboostr proposed freund schapire 1994 based reduction classication case algorithm aims nd regression function problem algorithm uses piecewise linear function 0 1 whose number branchpoints increases exponentially number iterations therefore algorithm computationally intractable 222 adaboostr another reduction nding f x 7 0 1 classication case proposed bertoni et al 1997 pattern predicted error less 0 counted correctly classied misclassied otherwise combined regression function given probability weighting training patterns used assumption weighted classication error iteration smaller 1 0 number training patterns jfx n converges quickly zero experience turned choice rather dicult ii selection next hypothesis base learner demanding problem weighted error usually converges quickly 1 2 algorithm stop 223 gradient boosting regression friedman 1999 based understanding boosting gradient descent method regression algorithms proposed eg interesting paper friedman friedman 1999 derivative g cost function g eg squared loss respect output fx n regression function sparse regression ensembles 7 projected gradient direction basis function h 2 h direction true gradient found g idea worked squared loss linear absolute loss hubers loss however gradient direction found 2 optimal squared loss linear absolute loss specialized treeboost algorithm friedman 1999 task nding next hypothesis posed classication problem sign gradient determines class membership algorithm aim maximize correlation gradient output base hypothesis approach similar algorithm proposed section 433 approach works well practice explicitly deal innite hypothesis case like gradient descent algorithms oers convergence limit even nite hypothesis spaces since regularization used potentially overt development good stopping criteria essential next section develop alternative approach based linear programming advantages lp approach include extensibility innite hypothesis case sparse solution guarantee existence sparse nite solutions practical fast nite algorithms 3 linear programs regression section develop nite semiinnite lp formulations sparse ensemble regression begin primal lp nite case investigate dual nite lp extend dual primal innite hypothesis cases 31 finite sparse linear regression r iid training data regression problem often stated nding function f 2 minimizes regularized risk functional vapnik 1995 rf 3 8 g ratsch demiriz kp bennett l loss function p regularization operator c regularization parameter determining tradeo loss complexity ie size function class paper consider wellknown insensitive loss vapnik 1995 scholkopf et al 1999 loss function penalize errors 0 chosen priori shown several nice properties see later cf smola 1998 however principle analysis algorithms also work loss functions cf ratsch 2001 paper consider f space linear combinations base hypotheses another space h socalled base hypothesis space including bias ie f assume h nite number hypotheses j generalized innite hypothesis classes sections 33 34 throughout paper assume h closed complementation hence one may enforce eectively changing f let us consider 1 norm hypothesis coecients regularization using 4 minimizing 3 stated linear program call lpregression problem min n f 5 0 xed constant regularization operator jjjj 1 frequently used sparse favoring approaches eg basis pursuit chen et al 1995 parsimonious least norm approximation bradley et al 1998 roughly speaking reason induced sparseness fact vectors far coordinate axes larger respect 1 norm respect pnorms p 1 example consider vectors 1 1 2 two norm sparse regression ensembles 9 1 norm 2 note using 1 norm regularizer optimal solution always vertex solution expressed tends sparse easily shown cf corollary 4 independent size nite hypothesis space h optimal number hypotheses ensemble greater number samples optimization algorithms proposed section 4 exploit property nice property 6 solution robust respect small changes training data proposition 1 smola et al 1999 using linear programming regression insensitive loss function 4 local movements target values points inside outside ie edge tube uence regression parameter 6 usually dicult control muller et al 1997 scholkopf et al 2000 one usually know beforehand accurately one able curve problem partially resolved following optimization problem smola et al 1999 min n f dierence 6 7 lies fact become positively constrained variable optimization problem core aspect 7 captured proposition stated proposition 2 smola et al 1999 assume 0 following statements hold upper bound fraction errors ie points outside tube ii lower bound fraction points inside ie outside edge tube iii suppose data generated iid distribution p x ically equals fraction points inside tube fraction errors g ratsch demiriz kp bennett summarizing optimization problem 7 two parameters regularization parameter c controls size hypothesis set therefore complexity regression function ii tubeparameter directly controls fraction patterns outside tube indirectly controls size tube 32 dual finite lp formulation section state dual optimization problem 7 introducing lagrangian multipliers n rst constraint computes error target underestimated n error measures target overestimated see linear programming text book specics construct dual lp problem dual problem 7 dd constraint comes reparameterization 2n xed constraints j jhj constraints one hypothesis h 2 h optimality point quantity p n denes error residual complementarity know error zero point underesti point overestimated f thus point within tube p n 0 point falls tube p n 0 point falls tube magnitude p n ects sensitivity objective changes larger change error larger p n quantity constraints ects well hypothesis addressed residual errors positive large size hypothesis likely improve ensemble must suciently large oset penalty increasing kk 1 33 generalization infinite hypotheses consider case innite set possible hypotheses h say select nite subset h 1 h primal dual regression lps h 1 well dened say increase sparse regression ensembles 11 subset size dene h 2 h 1 h relationship optimal ensembles created two subsets solution smaller h 1 lp always primal feasible larger h 2 lp h 1 solution dual feasible larger h 2 lp solution also optimal problem h 2 dual feasibility key issue dene base learning algorithm l xed p dual feasibility violated h p good hypothesis added ensemble solution may optimal thinking h function 9 extend dual problem 8 innite hypotheses case set dual feasible values p equivalent following compact polyhedron dual silpregression problem dd example semiinnite linear program silp class problems extensively studied mathematical program ming problem called semiinnite innite number constraints nite number variables set p known index set set hypotheses producible base learner nite eg fh nite problem exactly equivalent lpregression problem 8 establish several facts semiinnite programming problem using results general linear semiinnite programs summarized excellent review paper hettich kortanek 1993 simplify presentation simplied results hettich kortanek 1993 case silp additional set nite linear constraints results presented easily derived hettich change notation increasing g ratsch demiriz kp bennett index set include additional nite set traditional linear constraints consistent derivation silpregression problem refer problem innitely many constraints dual problem problem innitely many variables primal problem care taken since reverse convention used mathematical programming literature dene generic dual silp compact sets function b r n b function r n r make additional assumption problem always feasible feasible region compact clearly maximum value always obtained since maximizing continuous function compact set ideally would like solution linear program correspond optimal solution semiinnite problem dene necessary condition existence nite linear program whose optimal solution also solves semiinnite program denote generic dual silp restricted nite subset b dpn linear program since nite number constraints rst theorem gives necessary conditions optimal solution generic dual silp equivalent solution nite linear program theorem 42 hettich kortanek 1993 theorem 3 necessary condition nite solution assume following slater condition holds every set n z hap n n qz r exists 1 2 exist multipliers n 0 result immediately applies dual silp regression problem since strictly interior point 1 satises slater condition corollary 4 finite solution regression ensemble prob lem 11 1 exists dpn sparse regression ensembles 13 signicance result exists optimial ensemble consists n hypotheses n number data points true even set possible hypotheses innite 34 primal regression silp next look corresponding primal problem semiinnite case would like semiinnite dual problem equivalent meaningful primal problem simplies original primal nite hypothesis case set nonnegative borel measures b subset r denotes set nonnegative generalized nite sequences primal problem generic silp 12 nite linear programming optimal objective values primal dual problems always equal always true semiinnite case weak duality always holds p must ensure duality gap ie p hettich kortanek 1993 theorem 65 following theorem 5 sucient conditions duality gap let convex cone ap ap closed p primal minimum attained regression problem set base hypotheses evaluated training points obtainable learning algorithm constant thus theorem simplied follows corollary 6 sucient conditions base learner let convex cone 14 g ratsch demiriz kp bennett closed primal minimum attained corollary imposes conditions set possible base hy potheses examples sets base hypothesis would satisfy condition set possible hypotheses nite eg fh pg nite function h continuous respect p two conditions sucient cover base hypotheses considered paper conditions possible 4 lp ensemble optimization algorithms section propose two algorithms optimizing nite innite regression linear programs rst uses column generation execute simplextype algorithm second adopts exponential barrier strategy connections boosting algorithms classication ratsch et al 2000 41 column generation approach basic idea column generation cg construct optimal ensemble restricted subset hypothesis space lp 8 solved nite subset hypotheses called restricted master problem base learner called generate hypothesis assuming base learner nds best hypothesis satisfying condition 9 current ensemble optimal constraints fullled hypothesis added problem corresponds generating column primal lp silp row dual lp silp cgregression algorithm cf algorithm 1 assumes base learner lx p nite p 2 p algorithm 1 special case set silp algorithms known exchange methods methods known converge clearly set hypotheses nite method converge nite number iterations since constraints ever dropped one also prove converges silp cf theorem 72 hettich theorem 7 convergence algorithm 1 algorithm 1 stops nite number steps solution dual regression sparse regression ensembles 15 algorithm 1 cgregression algorithm argumentsample regularization constant c tube parameter 2 0 1 returns linear combination h function cgregx repeat let solution 8 using 1 hypotheses let b dual solution ie solution 7 return silp sequence intermediate solutions least one accumulation point solves dual regression silp theorem holds general set exchange methods algorithm 1 example possible add drop multiple constraints iteration convergence result unchanged practice found column generation algorithm stops optimal solution small number iterations lp silp regression problems 42 barrier algorithm following propose algorithm see also ratsch et al 2000 uses barrier optimization technique bertsekas 1995 frisch 1994 details connection boostingtype algorithms barrier methods see ratsch et al 2000 2001 similar algorithm proposed duy helmbold 2000 developed independently sequel give brief introduction barrier optimization goal barrier optimization nd optimal solution problem min 2s f f convex function nonempty convex set feasible solutions problem solved using called barrier function eg bertsekas 1995 cominetti dussault 1994 mosheyev zibulevsky 1999 censor zenios 1997 exponential barrier particularly useful g ratsch demiriz kp bennett choice purposes exp penalty parameter nding sequence uncon strained minimizers f g 18 using sequence f g minimizers shown converge global solution original problem ie holds min min barrier minimization objective problem 7 using exponential barrier written exp simplicity omitted constraints 0 rst line 20 objective 7 second line corresponds constraints n last line implements constraints n n note setting r e 0 nd minimizing slack variables 20 given b thus problem minimizing 20 greatly simplied 2n variables less optimize section propose algorithm cf algorithm 2 similar column generation approach last section solves sequence optimization problems called restricted master problems iteration algorithm one selects hypothesis solves approximately solves unconstrained optimization problem variables hypothesis coecients previous iterations bias b tube size solution restricted master problem respect master problem 1 clearly suboptimal one cannot easily apply 19 however known fast one decrease intermediate 1 full master problem j sparse regression ensembles 17 algorithm 2 barrierregression algorithm argumentsample number iterations regularization constant c tube parameter 2 0 1 constants start 0 returns linear combination h function barregx endfor endfor return solutions suboptimal cf proposition 1 cominetti dussault 1994 ratsch et al 2000 roughly speaking one ensure achieve desired convergence sense 19 gradient taken respect variables base learner needs nd hypothesis large edge hypotheses correspond violated constraints dual problem whereas classication case maximum edge minimized regression edges 1 therefore dene corrected edge respect constraint positive constraint violated consider case base learner nds hypothesis optimal respect corrected edge mean nds hypothesis much worse best hypothesis h ie constant 2 0 1 note correction edge comes regularization term kk 1 get g ratsch demiriz kp bennett lemma 8 running algorithm 2 using base learner satisfying 21 barrier parameter decreased kre k 1 gradient taken respect variables b proof gradient e respect b always zeros unbounded variables minimization line gradient e respect j two cases hypothesis already restricted master problem line get j 0 r j note case r j happen thus gradient projected feasible set 0 always zero hypothesis already included r j last constraint 8 violated j hypothesis h j needs included hypothesis set thus one exploit property 21 base learner upperbound gradient master problem current solution learner returns hypothesis 21 exist another hypothesis edge larger factor assume exists violated constraint line decreased kre k 1 using lemma one gets desired convergence property algorithm 2 theorem 9 assume h nite base learner l satises condition 21 1 output algorithm converges global solution 7 proof let e given 20 proposition 1 cominetti dus sault 1994 see ratsch et al 2000 one knows accumulation point sequence f g satisfying kr e global solution 7 lemma 8 decreased kre k 1 decreased gradient reduced nite number iterations kre k 1 thus 0 sparse regression ensembles 19 similar conditions used prove convergence algorithm 1 case nonoptimal base learners sense 21 barrier methods also applied semiinnite programming problems kaliski et al 1999 similar barrier algorithm using logbarrier used cf also mosheyev zibulevsky 1999 future work rigorously prove algorithm 2 also converges optimal solution hypothesis space innite algorithms proposed incomplete without descriptions base hypothesis space base learner algorithm next section consider choices hypothesis space base learner eect algorithms 43 choice hypothesis space base learner recall algorithms require hypothesis h p solves approximately solves question solve dierent types base learners set base learners compact maximum must exist 431 kernel functions suppose wish construct ensembles functions linear combinations functions eg kernel functions using coecient ie functions form k n kx n set fh g innite hypothesis set unbounded unbounded one restrict consider bounding 1 norm constant eg h fh g problem 22 closed form solution let j maximum absolute sum kernel values weighted p solution 22 p n means boost linear combinations kernel functions bounded 1 norm g ratsch demiriz kp bennett adding exactly one kernel basis function kx j per iteration resulting problem exactly optimizing svm regression lp eg smola et al 1999 rst place dierence dened algorithm optimizing function adding one kernel basis time posed problem semiinnite learning problem exactly equivalent nite svm case set hypotheses boosted individual kernel functions kx bounded using dierent norms would longer true would adding functions sum many kernel functions using 2 norm see ratsch et al 2000a likewise performed active kernel strategy set kernels parameterized set algorithm would change consider problem next section 432 active kernel functions consider case chose set kernel functions parameterized vector argument impose bound k need consider one basis function time case since kernel parameterized set continuous values innite set hypothesis say example wish pick rbf kernel parameters center 2 variance ie chose hypothesis function exp parameters maximize correlation weight p output socalled edge ie reasonable assumptions bounded function p thus results semiinnite case hold several ways eciently nd straightforward way employ standard nonlinear optimization technique maximize 24 however rbf kernels xed variance 2 fast easy implement emlike strategy setting z normalization factor 1 update computing weighted center data weights sparse regression ensembles 21 depend p note given vector q one compute mstep optimal center however q depends one iteratively recompute q estep iteration stopped change anymore objective function local minima one may start random position eg random training point 433 svm classication functions consider case using linear combination classica tion functions whose output 1 form regression function example algorithm treeboost algorithm friedman 1999 absolute error functions treeboost constructs classication tree class point taken sign residual point ie points overestimated assigned class 1 points underestimated assigned class 1 decision tree constructed based projected gradient descent technique exact linesearch point falling leaf node assigned mean value dependent variables training data falling node corresponds dierent node decision tree iteration virtual number hypotheses added sense corresponds number leaf nodes decision tree take simplied view consider one node decision trees decision trees linear combinations data specically decision function node fx b thus iteration algorithm want wb note nitely many ways label n points nite set hypotheses innitely many possible w b produce objective value equivalent boosting algorithm question practically optimize problem clearly upper bound best possible value equation obtained w b solution satisfying sense consider signp n desired class x n frequently may possible construct f x n misclassied penalized exactly jp n j thus think jp n j misclassication cost x n given classes misclassication weights use weight sensitive classication algorithm construct hypothesis 22 g ratsch demiriz kp bennett study used following problem converted lp form construct f signp n hw x becomes parameter problem interesting facts formulation choice controls capacity base learners data xed choice classication functions using relatively xed number w nonzero user determine based experimentation training data eects complexity base hypothesis user may x according desired complexity base hypothesis alternatively weighted variation svms scholkopf et al 2000 could used dynamically chose like treeboost would like allow side linear decision dierent weight describe changes required algorithm 1 allow iteration lp 26 solved nd candidate hypothesis instead adding single column restricted master lp 12 two columns added rst column second column h 0 algorithms stop hypotheses meet criteria given algorithm algorithm terminate 2 call variant algorithm cglp change eect convergence properties 5 experiments section present preliminary results indicating feasibility approaches start section 51 showing basic properties cg barrier algorithms regression show algorithms able produce excellent ts noiseless several noisy toy problems base learners use three proposed section 43 denote cgk cgak cglp cg algorithms using rbf kernels active rbf kernels classication functions base learners respectively likewise bark barak barlp using barrier algorithm possible combinations implemented sparse regression ensembles 23 show competitiveness algorithms performed benchmark comparison section 52 timeseries prediction problems extensively studied past moreover give interesting application problem derived computeraided drugdesign section 53 particular show approach using classication functions base learner well suited datasets dimensionality problem high number samples small 51 experiment toy data illustrate proposed regression algorithm converges optimal ie zero error solution ii capable nding good noisy data signalnoise21 applied toy example frequently used sinc function range 2 2 demonstration cf fig 1 used two base hypothesis spaces rbf kernels way described section 431 ie classication functions described section 433 rst case used cg barrier approch leading algorithms cgk bark latter case included demonstration purposes cglp designed highdimensional data sets perform well low dimensions due severely restricted nature base hypothesis set keep results comparable dierent data sets use normalized measure error q 2 error also called normalized mean squared error dened meaningless since simply predicting mean target value result q 2 value one let us rst consider case rbfkernels noisefree case left panel fig 1 observe expected proposition 2 automatically determined tube size small 00014 kept large 012 high noise case right panel using right tube size one gets almost perfect q noisefree case excellent noisy case q without retuning parameters cglp produced piecewiseconstant function based two classication functions solution produced noisy noisefree cases interestingly noisy g ratsch demiriz kp bennett figure 1 toy example left panel shows sinc function without noise using rbfkernels solid classication functions dashed solid almost perfect q dashed function simple q right panel shows using rbfkernels q noisy data sig 100 tube size automatically adapted algorithm right half patterns lie inside tube case produces almost identical function hypothesis space consists linear classication functions constructed lp 26 set base hypothesis extremely restricted thus high bias low variance behavior expected see later high dimensional datasets cglp perform quite well let us compare convergence speed cg barrier regression controlled setting toy example run algorithms record objective values restricted master problem iteration barrier algorithm one nd minimizing almost minimizing parameters b barrier function e restricted master problem implementation use iterative gradient descent method number gradient steps parameter algorithm result shown fig 2 one observes algorithms converge rather fast optimal objective value dotted line cg algorithm converges faster barrier algorithm barrier parameter usually decreases quick enough compete ecient simplex method however number gradient descent steps large enough eg 20 barrier algorithm produces comparable results number iterations note one one gradient descent step per iteration approach similar algorithm proposed collins et al 2000 uses parallel coordinate descent steps similar jacobi iterations sparse regression ensembles 25 objective value iteration replacements objective value iteration figure 2 convergence toy example convergence objective function cgregression solid barrierregression optimal value dotted number iterations left noise right large normal noise 1 barrierregression 1 dashdotted 20 dashed gradient descent steps iteration respectively used 52 time series benchmarks section would like compare new methods svms rbf networks chose two wellknown data sets frequently used benchmarks timeseries prediction mackeyglass chaotic time series mackey glass 1977 ii data set santa fe competition weigend na gershenfeld eds 1994 x following experimental setup comparison use seven dierent models comparison three models used muller et al 1999 rbf nets svm regression svr linear huber loss four new models cgk cgak bark barak models trained using simple cross validation technique choose model minimum prediction error measured randomly chosen validation set originally taken muller et al 1999 data including experimental results obtained httpidafirstgmdderaetschdatats 521 mackey glass equation rst application highdimensional chaotic system generated mackeyglass delay dierential equation dt 26 g ratsch demiriz kp bennett delay originally introduced model blood cell regulation mackey glass 1977 became quite common articial forecasting benchmark integrating 28 added noise time series obtained training 1000 patterns validation following 194 patterns sets using embedding dimension 6 test set 1000 patterns noiseless measure true prediction error conducted experiments dierent signal noise ratios 2 snr using uniform noise table ii state results given original paper muller et al 1999 svms using insensitive loss hubers robust loss quadraticlinear rbf networks moreover give results cg barrier algorithm using rbf kernels active rbfkernels 3 also applied cg algorithm using classication functions cglp algorithm performed poorly q 2 016 could generate complex enough functions table ii observe four algorithms perform average good best algorithms 11 cases better 13 cases worse 100 step prediction low noise levels rather poor compared svms great higher noise levels note cg barrier algorithm perform significantly dierent cg 5 cases better 7 cases worse shows simple barrier implementation given algorithm 2 achieves high enough accuracy compete sophisticated simplex implementation used cgalgorithms 522 data set santa fe competition data set santa fe competition articial data generated ninedimensional periodically driven dissipative dynamical system asymmetrical fourwell potential slight drift parameters weigend na gershenfeld eds 1994 system property operating one well time switching another well dierent dynamical behavior therefore rst segment time series regimes approximately stationary dynamics accomplished applying annealed competition experts ace method described pawelzik et al 1996 muller et al 1995 assumption number stationary subsystems made moreover order reduce eect continuous dene snr experiment ratio variance noise variance data 3 entries set italic model selection failed completely case selected model manually chosing model 10th percentile test errors tested models sparse regression ensembles 27 drift last 2000 data points training set used segmentation applying ace algorithm data points individually assigned classes dierent dynamical modes select particular class data includes data points end data set training set 4 allows us train models quasistationary data avoid predict average dynamical modes hidden full training set see also pawelzik et al 1996 discus sion however time left rather small training set requiring careful regularization since 327 patterns extracted training set previous section use validation set 50 patterns extracted quasistationary data determine model parameters svms rbf networks cgregression embedding parameters used methods compared table iii table iii shows errors q 2 value 25 step iterated predic tion 5 previous result muller et al 1999 support vector machine ins loss 30 better one achieved pawelzik et al pawelzik et al 1996 current record dataset given quite hard beat record methods perform 4 hereby assume class data generated last points training set one also responsible rst couple steps iterated continuation aim predict 5 iterated prediction means based past predictions original data new prediction computed table ii 1s denotes 1step prediction error q 2 test set 100s 100step iterated autonomous prediction snr ratio variance respective noise underlying time series snr 62 124 186 test error 1s 100s 1s 100s 1s 100s svm ins 00007 00158 00028 00988 00057 04065 28 g ratsch demiriz kp bennett quite well cgak improves result pawelzik et al 1996 28 cgk 26 better 6 close previous result modelselection crucial issue benchmark competition model selected basis best prediction 50 validation patterns turns rather suboptimal thus sophisticated model selection methods needed obtain reliable results table iii comparison competition condi tions 25 step iterated predictions q 2 value data set prior segmentation data according muller et al 1995 pawelzik et al 1996 done preprocessing cg svm neural net cgk cgak ins huber rbf pkm 53 experiments drug data data set taken computeraided drug design goal predict bioreactivity molecules based molecular structure creation quantitative structureactivity relationship models predictive model constructed large databases screened cost eectively desirable chemical prop erties small subset molecules tested using traditional laboratory techniques target dataset lc cka logarithm concentration compound required produce 50 percent inhibition site cholecys tokinin cck molecule cck ccklike molecules serve important roles neurotransmitters andor neuromodulators 66 compounds taken merck cck inhibitor data set dataset originally consisted 323 descriptors taken combination traditional 2d 3d topological properties electron density derived tae transferable atomic equivalent molecular descriptors derived using wavelets brenema et al 2000 data scaled 0 1 data obtained httpwwwrpiedu bennek well known appropriate feature selection dataset others essential good performance qsar models due 6 performed experiments barrier algorithm data since performance expected similar sparse regression ensembles 29 small amount available data known bioreactivity large number potential descriptors see example embrechts et al 1998 unrelated study bennett et al 2001 feature selection done constructing 1 norm linear support vector regression machine like equation 6 features input dimensions produce sparse weighting descriptors descriptors positive weights retained take reduced set 39 descriptors given refer full data set lccka reduced dataset lcckar typical performance measured used evaluate qsar data average sum squared error predicted true target values divided true target variance q 2 dened 27 q 2 less 03 considered good measure perfor mance 6fold cross validation performed report outof sample averaged 6 folds preliminary study modelselection using parameter selection techniques performed models consider cglp cg classication functions cg k cg nonactive kernels described sections 433 431 cgk used three dierent values regularization constant c tubeparameter parameter base learner kernelwidth complexity parameter 26 respectively thus examined 27 dierent parameter combinations cglp used parameter values found work well reduced dataset bennett et al 2001 chose c number hypotheses attributes per hypothesis similar training data research progress repeat studies using appropriate model selection technique leaveoneout cross validation model selection critical performance methods thus ecient model selection techniques important open question needs addressed first tried cgk full data set lccka failed achieve good performance q simple approach cg lp performed quite well 033 cglp able select discriminative features based subsets attributes kernelapproaches get confused uninformative features reduced set lcckar features already pre selected kernel approach improves signicantly q signicantly dierent cglp methods produced sparse ensembles full dataset using parameters cglp used average ensembles containing 22 hypotheses consisting average 101 possible 323 attributes cgk rbfkernel used 45 hypotheses reduced g ratsch demiriz kp bennett dataset using parameters used average ensembles containing 235 hypotheses consisting average 107 attributes cgk approach used average 303 hypotheses 01 slight dierence cglp cgk might explained presence uninformative features summarizing cglp approach seems robust method learn simple regression functions highdimensional spaces automatic feature selection 6 conclusion work examined lp constructing regression ensembles based 1 norm regularized insensitive loss function used support vector machines rst proposed ensembles nite hypothesis sets smola et al 1999 used dual formulation nite regression lp rigorously dene proper extension innite hypothesis case ii derive two ecient algorithms solving shown theoretically empirically even hypothesis space innite small nite set hypotheses needed express optimal solution cf corollary 4 sparseness possible due use 1 norm hypothesis coecient vector acts sparsityregularizer proposed two dierent algorithms eciently computing optimal nite ensembles baselearner acts oracle nd constraints dual semiinnite problem violated rst algorithm cg algorithm regression based simplex method proved convergence innite case cf theorem 7 second algorithm barrier algorithm regression based exponential barrier method connections original adaboost method classication cf ratsch et al 2000 algorithm converges nite hypothesis classes cf theorem 9 using recent results mathematical programming literature eg mosheyev zibulevsky 1999 kaliski et al 1999 claim possible generalize innite case computationally algorithms nd provably optimal solution small number iterations examined three types base learning algorithms one based boosting kernel functions chosen nite dictionary ker nels example nite hypothesis set also consider active kernel methods kernel basis selected innite dictionary kernels finally consider case using nite set linear classication functions constructed using lp sparse regression ensembles 31 limited hypothesis space specically designed work underdetermined highdimensional problems drug design data discussed paper preliminary simulations toy real world data showed proposed algorithms behave well nite innite cases benchmark comparison timeseries prediction problems algorithms perform well current state art regression methods support vector machines regression case data set santa fe competition obtained results good current record svm dataset lp classicationbased approach worked extremely well highdimensional drug design datasets since algorithm inherently performs feature selection essential success datasets primary contribution paper theoretical conceptual study lpbased ensemble regression algorithms nite innite hypothesis spaces future work plan rigorous investigation computational aspects approach one open question best perform selection lp model parame ters another open question involves best algorithmic approaches solving semiinnite linear program work well practice column generation barrier interiorpoint methods described current state art semiinnite linear programming primaldual interior point algorithm may perform even better theoretically empirically especially large datasets lastly ability handle innite hypothesis sets opens possibility many possible types base learning algorithms acknowledgments g ratsch would like thank sebastian mika klausr muller bob williamson manfred warmuth valuable discussions work partially funded dfg contracts ja 37991 ja 37971 mu 98711 national science foundation grant 970923 9979860 r empirical comparison voting classi boosting algorithm regression nonlinear programming parsimonious least norm approximation prediction games arcing algorithms wavelet representations molecular electronic properties applications adme parallel optimization theory atomic decomposition basis pursuit adaboost logistic regression uni stable exponential penalty algorithm superlinear convergence experimental comparison three methods constructing ensembles decision trees bagging computationally intelligent data mining automated design discovery novel pharmaceuticals simple cost function boost ing game theory decisiontheoretic generalization online learning application boosting experiments new boosting algorithm additive logistic regression statistical view boosting greedy function approximation logarithmic potential method convex pro gramming logarithmic barrier decomposition methods semiin nite programming submitted elsevier science boosting entropy projection linear nonlinear programming second edition oscillation chaos physiological control systems empirical evaluation bagging boosting improved generalization explicit optimization margins functional gradient techniques combining hypotheses advances kernel methods boosting margin new explanation e new support vector algorithms adaboosting neural networks gerstner learning kernels nature statistical learning theory time series pre diction forecasting future understanding past gradientbased boosting algorithm regression problems tr ctr gilles blanchard gbor lugosi nicolas vayatis rate convergence regularized boosting classifiers journal machine learning research 4 1212003 pierre geurts louis wehenkel florence dalchbuc gradient boosting kernelized output spaces proceedings 24th international conference machine learning p289296 june 2024 2007 corvalis oregon jinbo bi tong zhang kristin p bennett columngeneration boosting methods mixture kernels proceedings tenth acm sigkdd international conference knowledge discovery data mining august 2225 2004 seattle wa usa kristin p bennett michinari momma mark j embrechts mark boosting algorithm heterogeneous kernel models proceedings eighth acm sigkdd international conference knowledge discovery data mining july 2326 2002 edmonton alberta canada p granitto p f verdes h ceccatto neural network ensembles evaluation aggregation algorithms artificial intelligence v163 n2 p139162 april 2005 peter bhlmann bin yu sparse boosting journal machine learning research 7 p10011024 1212006 gunnar rtsch manfred k warmuth efficient margin maximizing boosting journal machine learning research 6 p21312152 1212005 sren sonnenburg gunnar rtsch christin schfer bernhard schlkopf large scale multiple kernel learning journal machine learning research 7 p15311565 1212006 robust loss functions boosting neural computation v19 n8 p21832244 august 2007 sebastian mika gunnar rtsch jason weston bernhard schlkopf alex smola klausrobert mller constructing descriptive discriminative nonlinear features rayleigh coefficients kernel feature spaces ieee transactions pattern analysis machine intelligence v25 n5 p623633 may gunnar rtsch sebastian mika bernhard schlkopf klausrobert mller constructing boosting algorithms svms application oneclass classification ieee transactions pattern analysis machine intelligence v24 n9 p11841199 september 2002 ron meir gunnar rtsch introduction boosting leveraging advanced lectures machine learning springerverlag new york inc new york ny