controlling application grain size network workstations important challenge area distributed computing automate selection parameters control distributed computation performancecritical parameter grain size computation ie interval successive synchronization points application parameter hard select since depends compile time loop structure data dependences computational complexity run time components speed compute nodes network networks workstations shared users runtime parameters change time result also necessary consider interactions dynamic load balancing needed achieve good performance environment paper present method automatically selecting grain size computation consisting nested loops method based close cooperation compiler runtime system evaluate method using simulation measurements implementation nectar multicomputer b introduction high availability relatively low cost networks workstations often considered platforms applications used relegated dedicated multiprocessors parallel languages parallelizing compilers simplified programming shared distributed memory multiprocessors however modifications tools needed targeted networks workstations independent nature machines higher variable costs communication network research investigates issues context parallelizing compilers generally parallelizing compilers assume specific homogeneous target system dedicated application allocate amount work processor networks worksta tions processors may heterogeneous may performance varies run time due competing users different types networks different loads may encountered result static equal distribution work result good utilization available resources address problem designed system supports dynamic load balancing parallelized code periodically adjusting amount work allocated processor run time 11 12 load balancing point system attempts allocate work units numbers proportional relative processing capabilities processors grain size applicationthe amount computation successive synchronization pointsis important performance parameter parallelized applications affects communication costs load balancing effectiveness communication costs load balancing depend configuration dynamic aspects system appropriate grain size selected using compiletime information cooperation compiler runtime system necessary selection control grain size application distributed network paper investigates impact grain size parallelism communication costs load balancing network workstations presents methods selecting controlling grain size nested loops next section outline parallel compilation model section 3 describe parameters must considered select appropriate grain size loop transformations useful controlling grain size sections 4 5 describe appropriate grain size determined possible controlled different types synchronization analysis simulation used predict performance different grain sizes using simplified models estimate computation communication costs summarize section 6 parallelization model compiler model computations parallelized distributing loop iterations slices aggregate data structures using owner computes rule data slices modified iteration assigned processor iteration compiler modifies loop bounds sequential program processor executes iterations assigned aggregate data structures may distributed along one dimensions although paper discuss applications distributed single dimension load balancing performed adjusting number iterations data slices allocated processor goal parallelization minimize execution time elapsed parallelized application minimum execution time attained application uses computing resources available productively dedicated homogeneous set processors use traditional efficiency measure 8 9 evaluate productive use resources productive available speedup sequential elapsed 1 p number processors sequential time sequential measured dedicated processor paper experiments performed assuming homogeneous set pro cessors cases competing processes use processing resources experiments competing loads modify efficiency formulation account resources used competing processes productive available sequential 2 compete amount resources ie cpu time used competing processes processor 3 grain size section discusses factors influence grain size grain size controlled grain size determined frequently executed synchronizations application nature synchronizations determines easily grain size controlled 31 compile time control grain size synchronizations parallelized application result communication requirements turn determined distribution loop iterations dependences loop structure original sequential code cases amount computation synchronizations ie grain size changed controlled modifying loop structure parallelized code greatest interest loop restructuring transformations strip mining 7 allow grain size parameterized continuum grain size choices transformations loop interchange 1 10 17 19 loop skewing 16 19 also used modify grain size application difficult pa rameterize basic techniques manipulating communication code loop splitting 7 message aggregation 3 5 6 13 14 also necessary techniques effective given application communication patterns parallelized code determine transformations useful controlling grain size dynamic nature computation communication costs network workstations actual selection parameters transformations requires runtime information 32 applications requiring communication simplest loop nests parallelize require com munication matrix multiplication applications interaction processors grain size issue dedicated machines however load balancing added applications communication needed load balancing determines grain size ie load balancing frequency determines frequency communication selection load balancing frequency based several factors including overhead interactions application processes load balancer overheads work movement latter typically limits load balancing frequency practice communication infrequent grain size performance con cern load balancing frequency selection described detail siegell 11 33 applications communication applications communication distinguish two types synchronizations communication resulting pipelining doacross loops causes unidirectional synchronizations processors communication outside distributed loops may cause bidirectional synchronizations processors compute communicate compute unidirectional synchronization bidirectional synchronization figure 1 communication pattern observed one slave processors communication pattern determines synchronization type model applications alternating computation com municationsynchronization phases case unidirectional synchronizations pipelined applications data communicated one direction processors communication phase application communication enforces partial ordering computation processors earlier pipeline must generate data later processors proceed processors early pipeline work ahead processors later pipeline much buffering intermediate data processors allow often allows computation occur parallel communication take advantage flexibility partial ordering control grain size bidirectional case communication phase end message based data sent phase received figure 1b permit much overlap computation communication processors involved communica tion processors involved synchronization ie none processors exit synchronization point processors reached bidirectional synchronization barrier synchronization barrier synchronizations impose total ordering computation phases making control grain size difficult dynamic load balancing even single assignment statement involving distributed data elements result barrier synchronization global communication may needed identify processors owning source destination elements analysis treat bidirectional synchronizations barrier synchronizations even involve subset processors unidirectional synchronizations applications loop carried dependences ie doacross loops parallelism obtained pipelining multiple instances distributed loop two main factors influence efficiency parallelization pipelining time spent communication intermediate values due loop carried dependences time spent filling draining pipeline given application minimum execution time attained grain size compromise communication overhead degree paral lelism begin section discussion grain size controlled applications doacross loops discuss appropriate grain size selected 41 controlling grain size run time applications doacross loops grain size parameterized using techniques strip mining 7 strip mining replaces single loop two nested loops communication moved inner loop message aggregation 3 5 6 13 14 used combine messages common destinations number iterations resulting inner loop block size computation figure 2 demonstrates use techniques successive overrelaxation sor example control grain size run time compiler strip mines loop block size variable eg blocksize figure 2c set run time grain block size related follows iteration iteration longest execution time local portion distributed doacross loop processors loop tiling 14 15 18 complicated approach combines strip mining loop interchange usually localize memory references resulting inner loops loop tiling also used control grain size communication overhead manner similar strip mining 14 however compared strip mining tiling significantly complicates data management system load balancing addressed paper 42 grain size selection figure 3 outlines model pipelined computation distributed loop n iterations distributed across p processors distributed sequential code pid pid pcount1 receiveright blastcol0 n pid pid pcount1 sendright blastcol1i 1 pipelined pid pid pcount1 receiveright blastcol0 n pid pid pcount1 sendright blastcol1i0blocksize blocksize c blocked pipelined figure 2 parallelization options sor simplified version error computation shown code portions affected strip mining message aggregation shaded strip mined loop total iterations divided blocks size block block size b pipelining allows blocks different processors executed parallel staggered fashion due dependences application processors blocks communication phases drain phases fill phases figure 3 pipelined execution distributed loop showing parameters modeling execution time distributed loop n iterations pipelined loop enclosing distributed loop iterations pipelined applications tradeoff parallelism communication costs must considered select appropriate block size strip mined loop communication costs minimized block size made large possible however due time required fill drain pipeline increasing block size reduces parallelism application eg compare figures 4b versus 4a sor example figure 2 two conflicting effects modeled estimate total computation time application using model select block size strip mined loop maximizes performance fill drain fill drain p1222222222 pipelined b blocked pipelined figure 4 pipelined execution single relaxation phase sor example figure 2 421 pipeline fill drain times figure 3 observed elapsed time application ignoring communication costs times time execute one block block given upper bound efficiency homogeneous environment ignoring communication costs 422 communication costs communication boundary values occurs pipeline phases since computation phases phases thus total communication cost shift cost single communication phase communication phase modeled follows incr theta elements fixed fixed overhead sending messages pro cessors incr cost per data element sent elements number data elements must sent communication point equal block size b fixed incr estimated measuring communication costsby measuring times passing messages processorswhen application started 11 measurements repeated periodically eg executions pipelined loop take account changing conditions network 423 selecting optimal block size model total execution time created combining equations 5 8 shift replaced using equation 9 block replaced using equation 6 sequential estimated extrapolating measurements startup time several iterations copy loop body results representation total one unknown number blocks sequential wish select minimize total execution time value minimizes total computed setting derivative total respect equal zero dt total solving shortest execution time highest efficiency attained fixed known block size optimal grain size calculated using equation 4 optimal computed application startup time using known values p estimates sequential fixed incr determined measuring execution time copies small portions computation code collects measurements generated compiler ie without intervention programmer initial measurements typically take several hundred milliseconds throughout execution estimates updated using new measured times thus allowing application adjust changes processors network updating estimates takes less time making initial estimates measurements actual computation used 424 related work fortran compiler performs similar analysis select appropriate block size pipelined computations 6 ap proach optimal block size determined setting derivative model execution time application equal zero however unlike approach estimates computation communication times program determined static performance estimator runs training set kernel routines characterize costs system 4 static performance estimator matches computations given application kernels training set approach requires separate characterization machine configuration might used running application approach flexible since rely runtime measurements automatically adjust specific application nature computation input size machine configuration communication costs number processors processor speeds however delaying characterization costs run time add characterization time cost executing application 43 evaluation implemented sor example figure 2c nectar system 2 highspeed fiber optic network developed carnegie mellon university connecting several sun workstations use implementation evaluate accuracy model effect grain size adjust system application characteristics 431 model evaluation figure 5 compares efficiency parallelization sor example predicted model measured efficiencies nectar 4 slave processors using different block sizes graph peak efficiency predicted modelthe optimal block sizeis marked vertical version sequential fixed incr added delays 8479 sec 1716 msec 48sec added delay 8497 sec 1663 msec 48sec c 50 msec added delay 8500 sec 1524 msec 48sec table 1 measured values used calculating optimal grain size figure 5 line table 1 shows measured times used compute optimal block size figure 5 times used computing efficiencies include time spent characterizing system computing optimal block size see figure 5 close match model measured values two curves reach maximum block size ie model fairly accurately predicts optimal block size show execution model responds different communication costs experiment artificial delays added communication functions results shown figures 5b 5c observe good fit model measured values ie model adjusts well different networks changing network conditions since values sequential fixed incr estimates important question sensitive results errors estimates curves figure 5 relatively flat around maxi mum indicating changing optimal block size much factor two either direction reduce efficiency much addition given shape curves better select block sizes high low estimates sequential based measurements several iterations pipelined loop consistent several measurements eg measurements presented table 1 quite close measurements actual execution time one dedicated processor ie estimates sequential accurate implemen tation estimated sequential dedicated system taking smallest several measurements execution times iterations loop actual execution competing loads one processors actual grain size distributed application larger expected based estimate general expect grain size variations small enough eg within factor two efficiency remains near maximum value addition take account changing loads strip mined loop executed multiple times estimate sequential updated executions loop based measurements include effects competing loads communication costs variable difficult measure predict efficiency compute optimal block size use conservative estimate communication costs 0103050709efficiency blocksize iterations measured predicted upper bound added delays0103050709efficiency blocksize iterations measured predicted upper bound millisecond delay added message0103050709efficiency blocksize iterations measured predicted upper bound c 50 millisecond delay added message figure 5 efficiency pipelined loop sor example 1000x1000 function block size time start first communication end last communication communication phase estimate tends increase fixed reduce incr increase optimal grain size prediction although cost estimate may include time spent computation less conservative estimate measuring communication time point view single processor could result shifting optimal grain size prediction left slope efficiency curve much greater 432 optimal grain size vs fixed grain size show effectiveness considering communication overhead parallelism selecting grain size compared performance version sor fixed grain size 150 milliseconds performance version automatically selected optimal grain size selected using method described previous section figure 6 shows efficiency measurements taken nectar system homogeneous dedicated processors two different problem sizes efficiency fixed grain size approximately automatically selected grain size number processors small number processors increased total execution time problems decreased increasing effect filling draining pipeline automatically selected grain size takes communication costs parallelism account resulted higher efficiency0103050709efficiency processors sequential automatically selected fixed 15 quanta0103050709efficiency processors sequential automatically selected fixed 15 quanta 1000 theta 1000 40 iterations b 2000 theta 2000 10 iterations figure parallel versions sor without load balancing dedicated homogeneous environment fixed grain size vs automatically selected grain size 44 effect competing loads competing load added one processors executing pipelined application intermediate results delayed processors following processor pipeline bubble inactivity idle waiting passes pipeline time competing load given cpu figure 7a dynamic load balancing keep load balanced periodically redistributing work proportion processing capabilities processor siegell 11 exam ple uses measured computation rates specific application loop iterations computed per unit timeto characterize available capabilities processor load balanced processor competing load allocated less work allocation cpu generates enough data keep processors follow busy competing load control cpu figure 7b communication required application aligns processors efficiency affected adversely competing loads pipelined execution continue without stalling true grain size long enough buffer space store intermediate data confirm grain size little effect efficiency pipelined application load balanced environment competing loads simulated interactions scheduling processes operating system communication slave processors model system assumes operating system allocates equal portions cpu time running processes roundrobin fashion fixed time quantum 100 milliseconds simulations consider communication costs model time spent filling draining pipeline figure 8 shows parallelization efficiencies resulting simulating different grain sizes different conditions environments simulated efficiencies stay close predicted upper bound solid line computed using equation 7 regardless grain size systems competing loads efficiency sometimes exceeds predicted upper bound length blocks varies pipeline stage phase difference start competing load start pipeline stage changes may slight degradations efficiency noticeable figure 8d due time spent slaves aligning early stages pipeline real systems process scheduling complicated roundrobin competing loads vary course application slaves may realign ever natural tendency communication align processors prevent efficiency affected adversely time msec cpu allocated competing process cpu allocated application process application process waiting data communication point equal distribution b load balanced figure 7 pipelined execution competing load first processor grain size equal distribution 70 milliseconds round robin scheduling 100 millisecond time quantum ignoring communication costs 5 bidirectional barrier synchronizations barrier synchronizations may caused reduction operations distributed loops shift data processors assignment statements involve data multiple processors bidirectional synchronizations grain size changed using transformations loop splitting 7 loop interchange 1 5 10 17 19 loop skewing 16 19 transformations difficult parameterize continuum grain sizes case efficiency grain size quanta upper bound simulated0103050709efficiency grain size quanta upper bound simulated competing load p0 b 2 competing loads p00103050709efficiency grain size quanta upper bound simulated0103050709efficiency grain size quanta upper bound simulated c 3 competing loads p0 competing loads 1 p0 2 p1 3 p2 4 p3 figure 8 parallelization efficiency determined simulation pipelined execution 4 processor system upper bound included graphs sequential execution time simulated problem 200 time quanta unidirectional synchronizations possible possible generate several versions code select appropriate run time 17 provides limited runtime control grain size limitation investigate options modifying grain size problems bidirectional synchroniza tions analyze overhead bidirectional synchronizations examine effects bidirectional synchronizations program performance presence competing loads 51 synchronization overhead figure 9 shows basic structure parallelized code barrier synchronizations barrier synchronizations critical path application impose total order processors thus communication costs synchronizations add parallelization overhead program also load imbalance synchronizations processors finish computation first remain idle processors finish reducing parallelization efficiency homogeneous system model total execution time program follows theta iteration theta n total number iterations distributed loop number times distributed loop executed sequential version program execution time approximately iteration theta thus efficiency parallelization dedicated homogeneous system sequential iteration theta iteration theta iteration iteration efficiency formulation shows parallelization overhead due barriers ie p theta barrier term denominator efficiency determined cost barrier relative amount computation barriers efficiency improved reducing cost barrier compiler programmer restructure program modeled differently 52 effect competing loads multiple processors competing loads scheduling processes different processors may synchronized application may inactive different processors different times barrier synchronization elapsed time worst times processors barriers cause compute iteration end global synchronization end figure 9 parallelized version doall loop followed global operation skews execution times accumulate increase total execution time even work allocated processors proportion available resources system competing loads application may able use share processing resources productively due interactions grain size scheduling processes operating system effective utilization resources computation assigned processor period barrier synchronizations must correspond amount cpu allocated processor period section identifies grain sizes make match likely 521 model evaluate effects barrier synchronizations performance presence competing loads model scheduling processes using roundrobin scheduling model described section 44 barriers work follows application process enters barrier completing computation phase none process may exit barrier processes entered process must active ie control cpu entering leaving barrier processes must active time model applies systems communication buffered figures 11 show time lines four processor system single competing load one processor different work assignments grain sizes time lines identify different cpu states working waiting inactive respect load balanced ap plication show interactions barrier synchronizations figures thick horizontal lines indicate times application processes enter barrier synchronization arrows indicate times processes exit barrier figure 10 grain size case without load balancing figure 10a 07 time quanta load balancing figure 10b grain size loaded processor p0 04 quanta dedicated processors p1 p2 p3 grain size 08 quanta load balancing still quite bit time spent waiting synchronization points execution time reduced much relative case without load balancing load balancing increases efficiency 609 765 however grain sizes balancing closer multiples time quantum result higher utilization available cpu time occasional small corrections ie small waiting periods needed keep synchronizations scheduling phase figure 11 grain sizes load balancing closer multiples time quantum 08 quanta p0 16 processors time reduction load balancing much greater figure 10 grain size 04 time quanta processor 0 efficiency increases 589 without load balancing 946 load balancing better cpu utilization also results increasing grain size scheduling competing processes causes variation execution times consecutive computation phases causing temporary skews processors resulting idle time variation increases load processors significance variations decreases grain size increased grain size controlled grain size large possibleat least one time quantum loaded processors balancingshould selected minimize effects variations 11 522 simulation show effects varying grain size performance simulated interactions different grain sizes scheduling processes operating system simulations model interactions grain size scheduling manner used figures 10 11 run 1000 synchronizations figure shows parallelization efficiencies attained different grain sizes different load conditions simulation results confirm hypotheses efficiency increases grain size monotonically peaks 100 efficiency occur grain sizes scheduling phase eg grain sizes multiples 175 quanta figure 12a 25 12b 325 12c 1925 12d 11 simulation results also show sensitivity efficiency fluctuations grain size decreases grain size increases actual systems scheduling algorithms complicated roundrobin used normal system activity may cause variations schedule practical control grain size accurately desirable range grain sizes efficiency fluctuates greatly fore grain size large possible applications barrier synchronizations achieve higher efficiency less sensitive changes competing loads time cpu allocated competing process cpu allocated application process application process waiting data communication point equal distribution b load balanced figure 10 application bidirectional synchronizations executing competing load first processor grain size equal distribution case 70 milliseconds round robin scheduling 100 millisecond time quantum ignoring communication costs 6 conclusion selection optimal grain size applications distributed networks workstations requires compiletime runtime information presented evaluated method automatically selecting grain size method based close cooperation compiler runtime system applications unidirectional synchronizations ie doacross loops showed grain size con time cpu allocated competing process cpu allocated application process application process waiting data communication point equal distribution b load balanced figure 11 application bidirectional synchronizations executing competing load first processor grain size equal distribution case 140 milliseconds roundrobin scheduling 100 millisecond time quantum ignoring communication costs trolled showed select optimal grain size considering communication overhead parallelism simulations measurements verified effectiveness method selecting optimal grain size simulations showed applications unidirectional synchronizations interactions grain size scheduling operating system significantly affect performance therefore interactions need considered 0103050709efficiency grain size quanta0103050709efficiency grain size quanta competing load p0 b 2 competing loads p00103050709efficiency grain size quanta0103050709efficiency grain size quanta c 3 competing loads p0 competing loads 1 p0 2 p1 3 p2 4 p3 figure 12 parallelization efficiency varying grain sizes 4 processor system simulation results roundrobin scheduling 100 millisecond time quantum ignoring communication costs selecting grain size applications bidirectional synchronizations grain size difficult control simulations showed reduce undesirable interactions bidirectional synchronizations scheduling operating system grain size made large possible r automatic loop interchange design nec tar network backplane heterogeneous multicomputers compiling programs distributedmemory multiprocessors overview fortran programming system compiling fortran mimd distributedmemory machines evaluation compiler optimizations fortran mimd distributedmemory machines program improvement sourcetosource transformation dynamic scheduling method irregular parallel programs advanced compiler optimizations supercomputers automatic generation parallel programs dynamic load balancing network workstations automatic generation parallel programs dynamic load balancing techniques designing efficient parallel pro grams reducing data communication overhead doacross loop nests loop transformation theory algorithm maximize parallelism loop skewing wavefront method revis ited vector optimization vs vectorization iteration space tiling massive parallelism program restruc turing tr advanced compiler optimizations supercomputers loop skewing wavefront method revisited vector optimization vs vectorization design nectar network backplane heterogeneous multicomputers compiling fortran mimd distributedmemory machines dynamic scheduling method irregular parallel programs evaluation compiler optimizations fortran mimd distributed memory machines reducing data communication overhead doacross loop nests automatic generation parallel programs dynamic load balancing network workstations program improvement sourcetosource transformation automatic loop interchange loop transformation theory algorithm maximize parallelism overview fortran programming system ctr peter steenkiste networkbased multicomputers practical supercomputer architecture ieee transactions parallel distributed systems v7 n8 p861875 august 1996