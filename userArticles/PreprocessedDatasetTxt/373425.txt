robust classification imprecise environments realworld environments usually difficult specify target operating conditions precisely example target misclassification costs uncertainty makes building robust classification systems problematic show possible build hybrid classifier perform least well best available classifier target conditions cases performance hybrid actually surpass best known classifier robust performance extends across wide variety comparison frameworks including optimization metrics accuracy expected cost lift precision recall workforce utilization hybrid also efficient build store update hybrid based method comparison classifier performance robust imprecise class distributions misclassification costs roc convex hull rocch method combines techniques roc analysis decision analysis computational geometry adapts particulars analyzing learned classifiers method efficient incremental minimizes management classifier performance data allows clear visual comparisons sensitivity analyses finally point empirical evidence robust hybrid classifier indeed needed many realworld problems b introduction traditionally classification systems built experimenting many different classifiers comparing performance choosing best experimenting different induction algorithms parameter settings training regimes yields large number classifiers evaluated compared unfortunately comparison often difficult realworld environments key parameters target environment known example optimal costbenefit tradeoffs target class priors seldom known precisely often subject change example fraud detection cannot ignore either cost class distribution assume distribution specifications precise static fawcett provost 1997 need method management comparison application multiple classifiers robust imprecise changing environments describe roc convex hull rocch method combines techniques roc analysis decision analysis computational geometry roc convex hull decouples classifier performance specific class cost distributions may used specify subset methods potentially optimal combination cost assumptions class distribution assumptions rocch method efficient facilitates comparison large number classifiers minimizes management classifier performance data specify exactly classifiers potentially optimal incremental easily incorporating new varied classifiers demonstrate possible desirable avoid complete commitment single best classifier system construction instead rocch used build available classifiers hybrid classification system perform best target costbenefit class distributions target conditions specified run time moreover cases precise information still unavailable system run conditions change dynamically operation hybrid system tuned easily optimally based feedback actual performance paper structured follows first sketch briefly traditional approach building systems order demonstrate brittle types imprecision common realworld problems introduce describe rocch properties comparing visualizing classifier performance imprecise environments following sections formalize notion robust classification system show rocch elegant method constructing one automatically solution elegant resulting hybrid classifier robust wide variety problem formulations including optimization metrics accuracy expected cost lift precision recall workforce utilization efficient build store update show hybrid actually better best known classifier situations finally citing results empirical studies provide evidence type system needed 11 example systemsbuilding team wants create system take large number instances identify action taken instances could potential cases fraudulent account behavior faulty equipment responsive customers interesting science etc consider problems best method classifying ranking instances well defined system builders may consider machine learning methods neural networks casebased systems handcrafted knowledge bases potential classification models ignoring moment issues efficiency foremost question facing system builders available models performs best classification traditionally experimental approach taken answer question distribution instances sampled known priori standard approach estimate error rate model statistically choose model lowest error rate strategy common machine learning pattern recognition data mining expert systems medical diagnosis cases measures cost benefit used well applied statistics provides methods crossvalidation bootstrap estimating model error rates recent studies compared effectiveness different methods salzberg 1997 kohavi 1995 dietterich 1998 unfortunately experimental approach brittle two types imprecision common realworld environments specifically costs benefits usually known precisely target class distributions often known approximately well observation made many authors bradley 1997 catlett 1995 fact concern large subfield decision analysis weinstein fineberg 1980 imprecision also arises environment may change time system conceived time used even used example levels fraud levels customer responsiveness change continually time place place 12 basic terminology paper address twoclass problems formally instance mapped one element set fp ng correct positive negative classes classification model classifier mapping instances predicted classes classification models produce continuous output eg estimate instances class membership probability different thresholds may applied predict class membership distinguish actual class predicted class instance use labels fy ng classifications produced model discussion let cclassification class twoplace error cost function cy n cost false positive error cn p cost false negative error 1 represent class distributions classes prior probabilities pp true positive rate hit rate classifier positives correctly classified total positives false positive rate false alarm rate classifier negatives incorrectly classified total negatives traditional experimental approach brittle chooses one model best respect specific set cost functions class distribution target conditions change system may longer perform optimally even acceptably example assume maximum false positive rate fp must exceeded want find classifier highest possible true positive rate tp exceed fp limit neymanpearson decision criterion egan 1975 three classifiers three fp limits shown figure 1 different classifier best fp limit system built single best classifier brittle fp requirement change 1 paper consider error costs include benefits realized true positive rate false positive rates figure 1 three classifiers three different neymanpearson decision criteria evaluating visualizing classifier performance 21 classifier comparison decision analysis roc analysis prior work building classifiers uses classification accuracy equivalently undifferentiated error rate primary evaluation metric use accuracy assumes class priors target environment constant relatively balanced real world rarely case classifiers often used sift large population normal uninteresting entities order find relatively small number unusual ones example looking defrauded accounts among large population customers screening medical tests rare diseases checking assembly line defective parts unusual interesting class rare among general population class distribution skewed ezawa singh norton 1996 fawcett provost 1997 kubat holte matwin 1998 saitta neri 1998 class distribution becomes skewed evaluation based accuracy breaks consider domain classes appear 9991 ratio simple rulealways classify maximum likelihood classgives 999 accuracy performance may quite difficult induction algorithm beat though simple rule presumably unacceptable nontrivial solution sought skews 10 2 common fraud detection reported applications clearwater stern 1991 evaluation classification accuracy also tacitly assumes equal real world rarely case classifications tacitly involve actions consequences actions diverse cancelling credit card account moving control surface airplane informing patient cancer diagnosis actions consequences sometimes grave performing incorrect action costly rarely costs mistakes equivalent mushroom classification example judging poisonous mushroom edible far worse judging edible mushroom poisonous indeed hard imagine domain learning system may indifferent whether makes false positive false negative error cases accuracy maximization replaced cost minimization problems unequal error costs uneven class distributions related suggested learning highcost instances compensated increasing prevalence instance set breiman friedman olshen stone 1984 unfortunately little work published either problem exist several dozen articles techniques costsensitive learning suggested turney 1996 little done evaluate compare article pazzani et al 1994 exception literature provides even less guidance situations distributions imprecise change model produces estimate ppji posterior probability instances class membership machinelearned models decision analysis gives us way produce costsensitive classifications weinstein fineberg 1980 classifier error frequencies used approximate probabilities pazzani et al 1994 instance decision emit positive classification particular classifier regardless whether classifier produces probabilistic binary classifications normalized cost test set evaluated empirically published work costsensitive classification uses equation rank classifiers given set classifiers set examples precise cost function classifiers cost computed minimumcost classifier chosen however discussed analyses assume distributions precise static true positive rate false positive rate classifier 3 figure 2 roc graph three classifiers general comparisons made receiver operating characteristic roc analysis classic methodology signal detection theory common medical diagnosis recently begun used generally ai classifier work egan 1975 beck schultz 1986 swets 1988 roc graphs depict tradeoffs hit rate false alarm rate use term roc space denote coordinate system used visualizing classifier performance roc space tp represented axis fp represented x axis classifier represented point roc space corresponding models produce continuous output eg posterior probabilities tp fp vary together threshold output varied extremes threshold defines classifier resulting curve called roc curve roc curve illustrates error tradeoffs available given model figure 2 shows graph three typical roc curves fact complete roc curves classifiers shown figure 1 orientation several points roc graph noted lower left point 0 0 represents strategy never alarming upper right point 1 1 represents strategy always alarming point 0 1 represents perfect classification line represents strategy randomly guessing class informally one point roc space better another northwest tp higher fp lower false positive rate true positive rate figure 3 roc graph four classifiers roc graph allows informal visual comparison set classifiers figure 3 curve better curve dominates points roc graphs illustrate behavior classifier without regard class distribution error cost decouple classification performance factors unfortunately roc graph valuable visualization technique poor job aiding choice classifiers one classifier clearly dominates another entire performance space declared better consider classifiers shown figure 3 best answer depends upon performance requirements ie error costs class distributions effect classifiers used take moment convince classifiers figure 3 optimal conditions 22 roc convex hull method section combine decision analysis roc analysis adapt comparing performance set learned classifiers method based three highlevel principles first roc space used separate classification performance class cost distribution information second decisionanalytic information projected onto roc space third convex hull roc space used identify subset methods potentially optimal 221 isoperformance lines separating classification performance class cost distribution assumptions decision goal projected onto roc space neat visualization specifically expected cost applying classifier represented point space therefore two points performance cn ppp equation defines slope isoperformance line ie classifiers corresponding points line expected cost set class cost distributions defines family isoperformance lines lines northwesthaving larger tp interceptare better correspond classifiers lower expected cost realworld cases target distributions known precisely valuable able identify subset classifiers potentially optimal possible set distributions defines family isoperformance lines given family optimal methods lie mostnorthwest iso performance line thus classifier potentially optimal lies northwest boundary ie line convex hull barber dobkin huhdanpaa 1993 set points roc space 2 section 3 provide formal proof roughly one see point lies convex hull exists line point line slope point larger tp intercept thus classifier represented point optimal distribution assumptions corresponding slope point lie convex hull family isoperformance lines another point lies isoperformance line slope larger tp intercept thus classifier cannot optimal 2 convex hull set points smallest convex set contains points false positive rate true positive rate ch figure 4 roc convex hull identifies potentially optimal classifiers call convex hull set points roc space roc convex hull rocch corresponding set classifiers figure 4 shows curves figure 3 roc convex hull drawn ch border shaded unshaded areas clearly optimal surprisingly b never optimal either none points roc curve lies convex hull also remove consideration points c lie hull 222 roc convex hull consider classifiers two distribution scenarios negative examples outnumber positives 51 scenario false positive false negative errors equal cost scenario b false negative 25 times expensive false positive eg missing case fraud much worse false alarm scenario defines family isoperformance lines lines corresponding scenario slope 5 b slope 1 figure 5 shows convex hull two isoperformance lines ff fi line ff best line slope 5 intersects convex hull line fi best line slope 1that intersects convex hull line identifies optimal classifier given distribution figure 6 shows three roc curves initial example convex hull drawn false positive rate true positive rate figure 5 lines ff fi show optimal classifier different sets conditions 223 generating roc convex hull roc convex hull method selects potentially optimal classifiers based roc convex hull iso performance lines 1 classifier plot tp fp roc space continuousoutput classifiers vary threshold output range plot roc curve 2 find convex hull set points representing predictive behavior classifiers interest n classifiers done logn time quickhull algorithm barber et al 1993 3 set class cost distributions interest find slope range slopes corresponding isoperformance lines 4 set class cost distributions optimal classifier point convex hull intersects isoperformance line largest tp intercept ranges slopes specify hull segments figures 4 5 demonstrate subset classifiers potentially optimal identified classifiers compared different cost class distributions demonstrate additional benefits method true positive rate false positive rate classifier 3 convex hull figure curves convex hull 224 comparing variety classifiers roc convex hull method accommodates binary continuous classifiers binary classifiers represented individual points roc space continuous classifiers produce numeric outputs thresholds applied yielding series comprising roc curve point may may contribute roc convex hull figure 7 depicts binary classifiers e f g added previous hull e may optimal circumstances extents convex hull classifiers f g never optimal extend hull new classifiers added incrementally rocch analysis demonstrated figure 7 addition classifiers ef g new classifier either extends existing hull former case hull must updated accordingly latter case new classifier ignored therefore method require saving every classifier saving statistics every classifier reanalysis different conditionsonly points convex hull classifiers ever optimal need saved every classifier lie convex hull must saved false positive rate true positive rate f g figure 7 classifier e may optimal extends roc convex hull f g cannot 225 changing distributions costs class cost distributions change time necessitate reevaluation classifier choice fraud detection costs change based workforce reimbursement issues amount fraud changes monthly roc convex hull method comparing new distribution involves calculating slopes corresponding isoperformance lines intersecting hull shown figure 5 roc convex hull method scales gracefully degree precision specifying cost class distribu tions nothing known distribution roc convex hull shows classifiers may optimal conditions figure 4 showed given classifiers b c figure 3 c ever optimal complete information method identifies optimal classifiers figure 5 saw classifier particular threshold value optimal scenario classifier c optimal scenario b next see less precise information roc convex hull show subset possibly optimal classifiers 226 sensitivity analysis imprecise distribution information defines range slopes isoperformance lines range slopes intersects segment roc convex hull facilitates sensitivity analysis example segment defined false positive rate true positive rate false positive rate true positive rateb false positive rate true positive rate c figure 8 sensitivity analysis using roc convex hull low sensitivity c optimal b high sensitivity e c optimal c nothing optimal strategy range slopes corresponds single point roc space small threshold range single classifier sensitivity distribution assumptions question consider scenario similar b negative examples 5 times prevalent positive ones scenario cost dealing false alarm 10 20 cost missing positive example 200 250 defines range slopes isoperformance lines 1 1 figure 8a depicts range slopes corresponding segment roc convex hull figure shows choice classifier insensitive changes within range fine tuning classifiers threshold necessary figure 8b depicts scenario wider range slopes2 3 figure shows scenario choice classifier sensitive distribution classifiers c e optimal subrange particularly interesting question domain nothing strategy better available classifiers consider figure 8c point 0 0 corresponds nothing ie issuing negative classifications regardless input set cost class distribution assumptions best hull intersecting isoperformance line passes origin eg line ff defines scenario null strategy optimal intuitively figure 8c illustrates false positives expensive negatives prevalent neither c good enough used improvements might allow beat null strategy represented ff modification c likely effect building robust classifiers point concentrated use rocch visualizing evaluating sets classifiers rocch helps delay classifier selection long possible yet provides rich performance comparison however systembuilding incorporates particular classifier problem brittleness resurfaces important delay systembuilding deployment may large many systems must survive years fact many domains precise static specification future costs class distributions unlikely impossible provost fawcett kohavi 1998 address brittleness using rocch produce robust classifiers defined satisfying following target cost class distributions robust classifier perform least well best classifier conditions statements optimality practical best classifier may bayesoptimal classifier least good known classifier stating classifier robust stronger stating optimal specific set conditions robust classifier optimal possible conditions principle classification brittleness could overcome saving possible classifiers neural nets decision trees expert systems probabilistic models etc performing automated runtime comparison desired target conditions however system feasible time space limitationsthere myriad possible classification models arising many different learning methods many different parameter settings storing classifiers practical tuning system comparing classifiers fly different conditions practical fortunately necessary moreover show sometimes possible better classifiers 31 rocchhybrid classifiers show robust hybrid classifiers built using rocch space possible instances let c space sets classification models let hybrid classifier comprise set classification models c 2 c function hybrid classifier takes input instance 2 classification number x 2 output produces classification produced x c things get involved later time consider set cost class distributions defines value x used select predetermined best classifier conditions build hybrid classifier must define set c would like c include models perform optimally conditions class cost distributions since stored system would like general enough apply variety problem formulations models comprising rocch combined form hybrid classifier elegant robust classifier definition 2 rocchhybrid hybrid classifier c set classifiers comprise rocch makes classifications using classifier rocch note moment rocchhybrid defined fp values corresponding rocch vertices 32 robust classification definition robust classifiers intentionally vague means one classifier better another different situations call different comparison frameworks continue minimizing expected cost process proving rocchhybrid minimizes expected cost cost class distributions provides deep understanding rocchhybrid works later generalize wide variety comparison frameworks rocchhybrid seen application multicriteria optimization classifier design construction classifiers rocch edgeworthpareto optimal stadler 1988 respect tp fp objective functions discuss multicriteria optimization used previously machine learning tcheng lambert lu rendell selection inductive bias tcheng lambert lu rendell 1989 alternatively rocch seen application theory games statistical decisions convex sets convex 321 minimizing expected cost expected cost applying classifier particular set cost class distributions slope corresponding isoperformance lines cn ppp 2 every set conditions define ec 0 show rocchhybrid robust problems best classifier classifier minimum expected cost slope rocch important tool argument rocch piecewiselinear concavedown curve therefore x increases slope rocch monotonically nonincreasing k number rocch component classifiers including degenerate classifiers define rocch endpoints confusion use phrases points roc space shorthand cumbersome classifiers corresponding points roc space subsection points rocch refer vertices rocch definition 3 real number 0 point slope rocch one arbitrarily chosen endpoints segment rocch slope segment exists otherwise vertex left adjacent segment slope greater right adjacent segment slope less completeness leftmost endpoint rocch considered attached segment infinite slope rightmost endpoint rocch considered attached segment zero slope note every defines least one point rocch lemma 1 set cost class distributions point rocch minimum expected cost proof contradiction assume conditions exists point c smaller expected cost point rocch equations 1 2 point expected cost point therefore conditions corresponding ec points equal expected cost form isoperformance line roc space slope ec also 1 2 points lines larger yintercept lower expected cost point c rocch either curve curve curve rocch convex set enclosing points contradiction curve iso performance line c also contains point p rocch since points isoperformance line expected cost point c smaller expected cost points rocch also contradiction 2 although necessary purposes shown minimum expected cost classifiers rocch definition 4 isoperformance line slope misoperformance line lemma 2 cost class distributions translate ec point rocch minimum expected cost slope rocch point ec proof contradiction suppose point rocch slope ec point minimum expected cost definition 3 either segment left slope less ec b segment right slope greater ec case consider point n vertex rocch neighbors left consider parallel ec isoperformance lines l l n n n left line connecting slope less ec yintercept l n greater yintercept l means n lower expected cost contradiction argument b analogous symmetric 2 lemma 3 slope rocch point ec point minimum expected cost proof point point slope rocch ec proof follows directly lemma 1 lemma 2 multiple points definition connected ec isoperformance line expected cost proof follows directly lemma 1 lemma 2 2 straightforward show rocchhybrid robust problem minimizing expected cost theorem 4 rocchhybrid minimizes expected cost cost distribution class distribution proof rocchhybrid composed classifiers corresponding points rocch follows directly lemmas 1 2 3 2 shown rocchhybrid robust goal provide minimum expected cost classification result important even accuracy maximization preferred classifier may different different target class distributions rarely taken account experimental comparisons classifiers corollary 5 rocchhybrid minimizes error rate maximizes accuracy target class distribution proof rate minimization cost minimization uniform error costs 2 33 robust classification common metrics showing rocchhybrid robust helps us understanding rocch method generally also shows us rocchhybrid pick best classifier order produce best classifications return later ignore need specify pick best component classifier show rocch applies generally theorem 6 classifier evaluation metric ffp tp f exists point rocch fvalue least high known classifier proof contradiction assume exists classifier c rocch fvalue higher point rocch c either ii rocch case rocch convex set enclosing points contradiction case ii let c represented rocspace c rocch exist points call one rocch tp p tp however restriction partial derivatives point ffp contradiction 2 true positive rate false positive rate classifier 3 hull neymanpearson figure 9 roc convex hull used select classifier neymanpearson criterion two complications general use rocch illustrated decision criterion first example recall neymanpearson criterion specifies maximum acceptable fp rate standard roc analysis selecting best classifier neymanpearson criterion easy plot roc curves draw vertical line desired maximum fp pick roc curve largest tp intersection line minimizing expected cost sufficient rocchhybrid choose vertex rocch ec value problem formulations neymanpearson criterion performance statistics non vertex point rocch may preferable see figure 9 fortunately slight extension rocchhybrid yield classifier performance statistics theorem 7 rocchhybrid achieve tp f p tradeoff represented point rocch vertices proof construction extend x c nonvertex points follows pick point p rocch exactly one let tp x tp value point x tp x rocch vertex use corresponding classifier vertex call left endpoint hull segment c l right endpoint c r let distance c l c r let p distance c l p make classifications follows input instance flip weighted coin choose answer given classifier c l probability p given classifier c r probability straightforward show fp tp classifier x tp x 2 second complication illustrated neymanpearson criterion many practical classifier comparison frameworks include constrained optimization problems discuss frameworks arbitrarily constrained optimizations problematic rocchhybrid given total freedom easy place constraints classifier performance even restriction partial derivatives interior point scores higher acceptable point hull example two linear constraints enclose subset interior exclude entire rocchthere acceptable points rocch however many realistic constraints thwart optimality rocchhybrid theorem 8 classifier evaluation metric ffp tp f constraint classifier performance eliminates point rocch without also eliminating higherscoring interior points rocchhybrid perform least well known classifier proof follows directly theorem 6 theorem 7 2 linear constraints classifiers fp tp performance common realworld problems following useful theorem 9 classifier evaluation metric ffp tp f single constraint classifier performance form b nonnegative rocchhybrid perform least well known classifier proof single constraint eliminates contention points classifiers fall left line nonpositive slope restriction partial derivatives constraint eliminate point rocch without also eliminating interior points higher fvalues thus proof follows directly theorem 8 2 finally following corollary 10 neymanpearson criterion rocchhybrid perform least well known classifier proof neymanpearson criterion evaluation metric higher tp better constraint classifier performance fp fpmax satisfy conditions theorem 9 therefore corollary follows 2 foregoing effort may seem misplaced simple criterion like neymanpearson however many realistic problem formulations example consider decisionsupport problem optimizing workforce utilization workforce available process fixed number cases cases underutilize workforce many cases leave cases unattended expanding workforce usually shortterm solution workforce handle c cases system present best possible set c cases similar neymanpearson criterion absolute cutoff c instead percentage cutoff theorem 11 workforce utilization rocchhybrid provide best set c cases choice c proof construction decision criterion maximize tp subject constraint theorem therefore follows theorem 9 2 fact many screening problems found marketing information retrieval use exactly linear constraint follows maximizing lift berry linoff 1997 precision recall subject absolute percentage cutoffs case presentation rocchhybrid provide best set cases minimizing expected cost imprecision environment forces us favor robust solution comparison frameworks many realworld problems precise desired cutoff unknown change eg fundamental uncertainty variability case difficulty competing responsibilities worse fixed absolute cutoff merely changing size universe cases eg size document corpus may change preferred classifier change constraint line rocchhybrid provides robust solution gives optimal subset cases constraint line example document retrieval rocchhybrid yield best n documents n prior class distribution target corpus target corpus size 34 ranking cases apparent solution problem robust classification use system ranks cases rather one provides classifications work ranked list cutoff implicit however practical situations choosing best ranking model equivalent choosing classifier best cutoff used remember roc curves formed case rankings moving cutoff one extreme different cutoffs implicit explicit different ranking functions perform better exactly problem robust classification solved elegantly rocchhybridthe rocchhybrid comprises set rankers best possible cutoffs example consider two ranking functions r r b r perfect first cases picks randomly thereafter r b randomly chooses first 10 cases ranks perfectly thereafter r preferable cutoff 10 cases r b preferable much larger cutoffs wholecurve metrics situations either target cost distribution class distribution completely unknown researchers advocate choosing classifier maximizes singlenumber metric representing average performance entire curve common wholecurve metric area roc curve auc bradley 1997 auc equivalent probability randomly chosen positive instance rated higher negative instance thereby also estimated wilcoxon test ranks hanley mcneil 1982 criticism auc specific target conditions classifier maximum auc may suboptimal provost et al 1998 criticism may made singlenumber metric fortunately rocchhybrid optimal specific target conditions maximum auc theorem 12 classifier auc larger rocchhybrid proof contradiction assume roc curve another classifier larger area curve would least one point rocspace falls outside area enclosed rocch means convex hull enclose points contradiction 2 36 using rocchhybrid use rocchhybrid classification need translate environmental conditions x values plug c minimizing expected cost equation 2 shows translate conditions ec ec lemma 3 want fp value point slope rocch ec straightforward calculate neymanpearson criterion conditions defined fp values workforce utilization conditions corresponding cutoff c fp value found intersecting line tp rocch argued target conditions misclassification costs class distribution rarely known may confusing seem require exact knowledge conditions rocchhybrid gives us two important capabilities first need precise knowledge target conditions deferred run time second absence precise knowledge even run time system optimized easily minimal feedback using rocchhybrid information target conditions needed train compare classifiers important imprecision caused temporal geographic differences may exist training use example building system realworld problem introduces nontrivial delay time data gathered time learned models used problem exacerbated domains error costs class distributions change time even slow drift brittle model may become suboptimal quickly many scenarios costs class distributions specified respecified run time reasonable precision sampling current population used ensure rocchhybrid always performs optimally cases even run time quantities known exactly benefit rocchhybrid tuned easily yield optimal performance minimal feedback environment conceptually rocchhybrid one knob varies x x c one extreme knob setting rocchhybrid give optimal tp f p tradeoff target conditions corresponding setting turning knob right increases tp turning knob left decreases fp monotonicity rocchhybrid simple hillclimbing guarantee optimal performance example system produces many false alarms turn knob left system presenting cases turn knob right 37 beating component classifiers perhaps surprisingly many realistic situations rocchhybrid system better component classifiers consider neymanpearson decision criterion rocch may intersect fp line highest component roc curve occurs fp line intersects rocch vertices therefore component classifier actually produces particular statistics figure 9 theorem 13 rocchhybrid surpass performance component classifiers neymanpearson problems proof nonvertex hull point xt p x tp x larger tp point theorem 7 rocchhybrid achieve tp hull small number fp values correspond hull vertices 2 holds common problem formulations workforce utilization lift maximization precision maximization recall maximization 38 time space efficiency argued rocchhybrid robust wide variety problem formulations also efficient build store update time efficiency building rocchhybrid depends first efficiency building component models varies widely model type models built machine learning methods built seconds data available handbuilt models take years build however presume work would done anyway rocchhybrid built whatever methods available two two thousand described new classifiers become available rocchhybrid updated incrementally time efficiency depends also efficiency experimental evaluation classifiers presume work would done anyway limitations finally time efficiency rocchhybrid depends efficiency building rocch done log n time using quickhull algorithm barber et al 1993 n number classifiers rocch space efficient comprises classifiers might optimal target conditions theorem 14 minimizing expected cost rocchhybrid comprises classifiers optimal cost class distributions proof follows directly lemmas 13 definitions 3 4 2 number classifiers must stored reduced bounds placed potential target conditions described ranges conditions define segments rocch thus rocchhybrid may need subset c adding new classifiers rocchhybrid also efficient adding classifier rocch either extend hull adding possibly subtracting rocchhybrid ii conclude new classifiers superior existing classifiers portion roc space discarded runtime classification complexity rocchhybrid never worse component classifiers situations runtime complexity crucial rocch constructed without prohibitively expensive classification models find best subset computationally efficient models empirical demonstration need robust classification fundamental interest weakens two strong assumptions availability precise knowledge costs class distributions however might existing classifiers already robust example given classifier optimal one set conditions might optimal beyond scope paper offer indepth experimental study answering question however provide solid evidence answer referring results two prior studies one comprehensive roc analysis medical domains recently conducted andrew bradley 1997 3 published roc analysis uci database domains undertook last year ronny kohavi provost et al 1998 note classifier dominates roc curve completely defines rocch means dominating classifiers robust vice versa therefore exist trivially domains classifier dominates techniques like rocchhybrid essential 3 purpose answer question fortunately published results anyway true positive rate false positive rate bayes knn msc perceptron figure 10 bradleys classifier results heart bleeding data 41 bradleys study bradley studied six medical data sets noting unfortunately rarely know individual misclassification costs plotted roc curves six classifier learning algorithms two neural nets two decision trees two statistical techniques one data sets dominating classifier means domain exist different sets conditions different classifiers preferable fact running example based three best classifiers bradleys results heart bleeding data results full set six classifiers found figure 10 classifiers constructed cleveland heart disease data shown figure 11 bradleys results show clearly many domains classifier maximizes single metricbe accuracy cost area roc curvewill best cost class distributions best others shown rocchhybrid best 42 study study performed ronny kohavi chose ten datasets uci repository contained least 250 instances accuracy decision trees less 95 domain induced classifiers minority class road chose class grass selected several induction algorithms mlc kohavi sommerfield dougherty 1997 decision tree learner mc4 naive bayes discretization nb knearest neighbor several k values ibk baggedmc4 breiman 1996 mc4 similar c45 true positive rate false positive rate bayes knn msc perceptron figure 11 bradleys classifier results cleveland heart disease data quinlan 1993 probabilistic predictions made using laplace correction leaves nb discretizes data based entropy minimization dougherty kohavi sahami 1995 builds naivebayes model domingos pazzani 1997 ibk votes closest k neighbors neighbor votes weight equal one distance test instance roc curves shown figures 12 one vehicle ten domains absolute dominator general 100 runs performed 10 data sets using 10 crossvalidation folds dominating classifiers cases close example adult waveform21 cases curve dominates one area roc space dominated another results also support need methods like rocchhybrid produce robust classifiers examples expectedcostminimizing rocchhybrids would look like internally table 1 shows component classifiers make rocch four uci domains figure 12 example road domain see figure 12 table 1 naive bayes would chosen target conditions corresponding slope less 038 baggedmc4 would chosen slopes greater 038 perform equally well 038 5 limitations future work limitations rocch method presented defined twoclass problems believe extended multiclass problems yet done formally noted dimensionality rochyperspace grows quadratically number classes table 1 locally dominating classifiers four uci domains domain slope range dominator domain slope range dominator vehicle 0 1 baggedmc4 satimage 0 005 nb road 0 038 nb 005 022 baggedmc4 crx 0 003 baggedmc4 260 311 ib3 006 206 baggedmc4 754 3114 ib3 also assumed constant error costs given type error eg false positives cost problems different errors type different costs many cases problem transformed evaluation equivalent problem uniform intratype error costs duplicating instances proportion costs simply modifying counting procedure accordingly also assumed paper estimates classifiers performance statistics fp tp good mentioned much work addressed production good estimates simple performance statistics error rate much less work addressed production good roc curve estimates simpler statistics care taken avoid overfitting training data ensure differences roc curves meaningful one solution use crossvalidation averaging roc curves provost et al 1998 procedure used produce roc curves section 42 knowledge issue open best produce confidence bands appropriate particular problem shown section 42 appropriate neymanpearson decision criterion ie show confidence tp various values fp also addressed predictive performance computational performance concerns choosing classification model comprehensibility important easy answer particular setting rocchhybrid comprehensible underlying model using however answer falls short rocchhybrid interpolating two models one wants understand multiplemodel system whole work fundamentally different recent machine learning work combining multiple models ali 1996 work combines models order boost performance fixed cost class distribution rocchhybrid combines models robustness across different cost class distributions principle methods independentmultiplemodel classifiers candidates extending rocch however may multiplemodel classifiers achieve increased performance specific set conditions interpolating along edges rocch rocch method also complements research costsensitive learning turney 1996 existing costsensitive learning methods brittle respect imprecise cost knowledge thus rocch essential evaluation tool furthermore costsensitive learning may used find better components rocchhybrid searching explicitly classifiers extend rocch 6 conclusion roc convex hull method robust efficient solution problem comparing multiple classifiers imprecise changing environments intuitive compare classifiers general specific distribution assumptions provides crisp visualizations minimizes management classifier performance data selecting exactly classifiers potentially optimal thus need saved preparation changing conditions moreover due incremental nature new classifiers incorporated easily eg trying new parameter setting rocchhybrid performs optimally target conditions many realistic problem formulations including optimization metrics accuracy expected cost lift precision recall workforce utilization efficient build terms time space updated incrementally furthermore sometimes classify better known model therefore conclude elegant robust classification system believe work important implications machine learning applications machine learning research provost et al 1998 applications helps free system designers need choose sometimes arbitrary comparison metrics precise knowledge key evaluation parameters available indeed knowledge may never available yet robust systems still built machine learning research frees researchers need precise class cost distribution information order study important related phenomena particular work costsensitive learning impeded difficulty specifying costs tenuous nature conclusions based single cost metric researchers need held back either costsensitive learning studied generally without specifying costs precisely goes research learning highly skewed distributions methods effective levels distribution skew rocch provide detailed answer note implementation rocch method perl publicly available code related papers may found httpwwwcroftjnetfawcettrocch acknowledgments thank many discussed roc analysis classifier comparison especially rob holte george john ron kohavi ron rymon peter turney thank andrew bradley supplying data analysis r reduction learning multiple descriptions quickhull algorithm convex hull use roc curves test performance evaluation data mining techniques marketing theory games statistical decisions republished dover publications use area roc curve evaluation machine learning algorithms classification regression trees bagging predictors tailoring rulesets misclassificatioin costs approximate statistical tests comparing supervised classification learning algorithms neural computation beyond independence conditions optimality simple bayesian classifier supervised unsupervised discretization continuous features prieditis learning goal oriented bayesian networks telecommunications risk management adaptive fraud detection available httpwww meaning use area receiver operating characteristic roc curve study crossvalidation bootstrap accuracy estimation model selection httprobotics data mining using mlc machine learning detection oil spills satellite radar images reducing misclassification costs case accuracy estimation comparing induction algorithms real world comparing classifiers pitfalls avoid recommended approach measuring accuracy diagnostic systems building robust learning systems combining induction optimization cost sensitive learning bibliography clinical decision analysis tr c45 programs machine learning bagging predictors quickhull algorithm convex hulls reduction learning multiple descriptions optimality simple bayesian classifier zeroone loss learning myampersandldquoreal worldmyampersandrdquo machine learning detection oil spills satellite radar images approximate statistical tests comparing supervised classification learning algorithms activity monitoring metacost explicitly representing expected cost data mining techniques adaptive fraud detection comparing classifiers case accuracy estimation comparing induction algorithms detecting concept drift support vector machines ctr jigang xie zhengding qiu zhenjiang miao yanqiang zhang bootstrap fda counting positives accurately imprecise environments pattern recognition v40 n11 p32923298 november 2007 anna olecka evaluating classifiers performance constrained environment proceedings eighth acm sigkdd international conference knowledge discovery data mining july 2326 2002 edmonton alberta canada tom fawcett peter flach response webb tings application roc analysis predict classification performance varying class distributions machine learning v58 n1 p3338 january 2005 tom fawcett alexandru niculescumizil pav roc convex hull machine learning v68 n1 p97106 july 2007 sven f crone stefan lessmann robert stahlbock utility based data mining time series analysis costsensitive learning neural network predictors proceedings 1st international workshop utilitybased data mining p5968 august 2121 2005 chicago illinois reuven arbel lior rokach classifier evaluation limited resources pattern recognition letters v27 n14 p16191631 15 october 2006 geoffrey webb kai ming ting application roc analysis predict classification performance varying class distributions machine learning v58 n1 p2532 january 2005 lian yan michael fassino patrick baldasare enhancing lift budget constraints application mutual fund industry proceeding eleventh acm sigkdd international conference knowledge discovery data mining august 2124 2005 chicago illinois usa csar ferri peter flach jos hernndezorallo delegating classifiers proceedings twentyfirst international conference machine learning p37 july 0408 2004 banff alberta canada steven n thorsen mark e oxley description competing fusion systems information fusion v7 n4 p346360 december 2006 jos mara gmez hidalgo guillermo cajigas bringas enrique puertas snz francisco carrero garca content based sms spam filtering proceedings 2006 acm symposium document engineering october 1013 2006 amsterdam netherlands jos mara gmez hidalgo evaluating costsensitive unsolicited bulk email categorization proceedings 2002 acm symposium applied computing march 1114 2002 madrid spain tom fawcett roc graphs instancevarying costs pattern recognition letters v27 n8 p882891 june 2006 exploiting auc optimal linear combinations dichotomizers pattern recognition letters v27 n8 p900907 june 2006 zhiqiang zheng balaji padmanabhan haoqiang zheng dea approach model combination proceedings tenth acm sigkdd international conference knowledge discovery data mining august 2225 2004 seattle wa usa csar ferri multiparadigm learning declarative models thesis ai communications v17 n2 p9597 april 2004 rich caruana mohamed elhawary art munson mirek riedewald daria sorokina daniel fink wesley hochachka steve kelling mining citizen science data predict orevalence wild bird species proceedings 12th acm sigkdd international conference knowledge discovery data mining august 2023 2006 philadelphia pa usa david jensen matthew rattigan hannah blau information awareness prospective technical assessment proceedings ninth acm sigkdd international conference knowledge discovery data mining august 2427 2003 washington dc thomas c w landgrebe david j tax pavel paclk robert p w duin interaction classification reject performance distancebased rejectoption classifiers pattern recognition letters v27 n8 p908917 june 2006 katia kermanidis manolis maragoudakis nikos fakotakis george kokkinakis learning greek verb complements addressing class imbalance proceedings 20th international conference computational linguistics p1065es august 2327 2004 geneva switzerland francesco tortorella rocbased reject rule dichotomizers pattern recognition letters v26 n2 p167180 15 january 2005 bianca zadrozny charles elkan learning making decisions costs probabilities unknown proceedings seventh acm sigkdd international conference knowledge discovery data mining p204213 august 2629 2001 san francisco california dirk ourston sara matzner william stump bryan hopkins coordinated internet attacks responding attack complexity journal computer security v12 n2 p165190 may 2004 gerhard widmer discovering simple rules complex data metalearning algorithm surprising musical discoveries artificial intelligence v146 n2 p129148 june chaoton su longsheng chen tailin chiang neural network based information granulation approach shorten cellular phone test process computers industry v57 n5 p412423 june 2006 francis r bach david heckerman eric horvitz considering cost asymmetry learning classifiers journal machine learning research 7 p17131741 1212006 tadeusz pietraszek use roc analysis optimization abstaining classifiers machine learning v68 n2 p137169 august 2007 juan jos garca adeva juan manuel pikatza atxa intrusion detection web applications using text mining engineering applications artificial intelligence v20 n4 p555566 june 2007 jerzy w grzymalabusse linda k goodwin xiaohui zhang increasing sensitivity preterm birth changing rule strengths pattern recognition letters v24 n6 p903910 march nilesh dalvi pedro domingos mausam sumit sanghai deepak verma adversarial classification proceedings tenth acm sigkdd international conference knowledge discovery data mining august 2225 2004 seattle wa usa paul n bennett susan dumais eric horvitz probabilistic combination text classifiers using reliability indicators models results proceedings 25th annual international acm sigir conference research development information retrieval august 1115 2002 tampere finland ashwin srinivasan extracting contextsensitive models inductive logic programming machine learning v44 n3 p301324 september 2001 tom fawcett introduction roc analysis pattern recognition letters v27 n8 p861874 june 2006 stijn viaene bart baesens guido dedene jan vanthienen dirk van den poel proof running two stateoftheart pattern recognition techniques field direct marketing enterprise information systems iv kluwer academic publishers hingham sofus macskassy foster provost intelligent information triage proceedings 24th annual international acm sigir conference research development information retrieval p318326 september 2001 new orleans louisiana united states daniel grossman pedro domingos learning bayesian network classifiers maximizing conditional likelihood proceedings twentyfirst international conference machine learning p46 july 0408 2004 banff alberta canada tom fawcett vivo spam filtering challenge problem kdd acm sigkdd explorations newsletter v5 n2 december tams horvth thomas grtner stefan wrobel cyclic pattern kernels predictive graph mining proceedings tenth acm sigkdd international conference knowledge discovery data mining august 2225 2004 seattle wa usa aleksandar lazarevic vipin kumar feature bagging outlier detection proceeding eleventh acm sigkdd international conference knowledge discovery data mining august 2124 2005 chicago illinois usa aleksander kolcz local sparsity control naive bayes extreme misclassification costs proceeding eleventh acm sigkdd international conference knowledge discovery data mining august 2124 2005 chicago illinois usa roco alaizrodrguez alicia guerrerocurieses jess cidsueiro minimax regret classifier imprecise class distributions journal machine learning research 8 p103130 512007 mansoor j zolghadri eghbal g mansoori weighting fuzzy classification rules using receiver operating characteristics roc analysis information sciences international journal v177 n11 p22962307 june 2007 anneleen assche celine vens hendrik blockeel sao deroski first order random forests learning relational classifiers complex aggregates machine learning v64 n13 p149182 september 2006 thomas grtner john w lloyd peter flach kernels distances structured data machine learning v57 n3 p205232 december 2004 ashwin srinivasan david page rui camacho ross king quantitative pharmacophore models inductive logic programming machine learning v64 n13 p6590 september 2006 shlomo hershkop salvatore j stolfo combining email models false positive reduction proceeding eleventh acm sigkdd international conference knowledge discovery data mining august 2124 2005 chicago illinois usa gavrila munder multicue pedestrian detection tracking moving vehicle international journal computer vision v73 n1 p4159 june 2007 konstantinos koumpis steve renals automatic summarization voicemail messages using lexical prosodic features acm transactions speech language processing tslp v2 n1 p1es february 2005 nada lavra branko kavek peter flach ljupo todorovski subgroup discovery cn2sd journal machine learning research 5 p153188 1212004 clifton phua damminda alahakoon vincent lee minority report fraud detection classification skewed data acm sigkdd explorations newsletter v6 n1 june 2004 jeremy z kolter marcus maloof learning detect malicious executables wild proceedings tenth acm sigkdd international conference knowledge discovery data mining august 2225 2004 seattle wa usa paul n bennett susan dumais eric horvitz combination text classifiers using reliability indicators information retrieval v8 n1 p67100 january 2005 stijn viaene richard derrig guido dedene case study applying boosting naive bayes claim fraud diagnosis ieee transactions knowledge data engineering v16 n5 p612620 may 2004 j zico kolter marcus maloof learning detect classify malicious executables wild journal machine learning research 7 p27212744 1212006 johannes frnkranz peter flach roc n rule learning towards better understanding covering algorithms machine learning v58 n1 p3977 january 2005 foster provost pedro domingos tree induction probabilitybased ranking machine learning v52 n3 p199215 september gary weiss mining rarity unifying framework acm sigkdd explorations newsletter v6 n1 june 2004 nitesh v chawla nathalie japkowicz aleksander kotcz editorial special issue learning imbalanced data sets acm sigkdd explorations newsletter v6 n1 june 2004 estevam r hruschka jr nelson f f ebecken towards efficient variables ordering bayesian networks classifier data knowledge engineering v63 n2 p258269 november 2007 chris drummond robert c holte cost curves improved method visualizing classifier performance machine learning v65 n1 p95130 october 2006 mohammed j zaki charu c aggarwal xrules effective algorithm structural classification xml data machine learning v62 n12 p137170 february 2006 perlich foster provost jeffrey simonoff tree induction vs logistic regression learningcurve analysis journal machine learning research 4 p211255 1212003 mukund deshpande michihiro kuramochi nikil wale george karypis frequent substructurebased approaches classifying chemical compounds ieee transactions knowledge data engineering v17 n8 p10361050 august 2005 minghsuan yang david j kriegman narendra ahuja detecting faces images survey ieee transactions pattern analysis machine intelligence v24 n1 p3458 january 2002 michael berthold david j hand references intelligent data analysis springerverlag new york inc new york ny