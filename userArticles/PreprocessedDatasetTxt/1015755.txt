stylebased inverse kinematics paper presents inverse kinematics system based learned model human poses given set constraints system produce likely pose satisfying constraints realtime training model different input data leads different styles ik model represented probability distribution space possible poses means ik system generate pose prefers poses similar space poses training data represent probability novel model called scaled gaussian process latent variable model parameters model learned automatically manual tuning required learning component system additionally describe novel procedure interpolating stylesour stylebased ik replace conventional ik wherever used computer animation computer vision demonstrate system context number applications interactive character posing trajectory keyframing realtime motion capture missing markers posing 2d image b overview main idea work learn probability distribution function pdf character poses motion data use select new poses ik represent pose 42dimensional vector q consists joint angles position orientation root kinematic chain approach consists following steps feature vectors order provide meaningful features ik convert pose vector feature representation represents character pose velocity local coordinate frame motion capture pose qi corresponding feature vector yi index training poses features include joint angles velocity vertical orientation described detail section 4 sgplvm learning model likelihood motion capture poses using novel model called scaled gaussian process latent variable model sgplvm given features yi set motion capture poses learn parameters sgplvm described section 5 sgplvm denes lowdimensional representation original data every pose qi corresponding vector xi usually 3dimensional space lowdimensional space xi values called latent space learning process estimate xi parameters input pose along parameters sgplvm model denoted wk learning process entails numerical optimization objective function lgp likelihood new poses described original poses model parameters order keep model efcient algorithm selects subset original poses keep called active set pose synthesis generate new poses optimize objective function likxyq derived sgplvm model function describes likelihood new poses given original poses learned model parameters new pose also optimize lowdimensional vector x several different applications supported described section 7 4 character model section dene parameterization use charac ters well features use learning describe 3d pose character vector q consists global position orientation root kinematic chain plus joint angles body root orientation represented quaternion joint angles represented exponential maps joint parameterizations rotated space natural motions include singularities parameterization pose additionally dene corresponding ddimensional feature vector feature vector selects features character poses wish learning algorithm sensitive vector includes following features joint angles joint angles q included omit global position orientation want learning sensitive vertical orientation include feature measures global orientation character respect di rection along zaxis dened follows let r rotation matrix maps vector characters local coordinate frame world coordinate frame take three canonical basis vectors local coordinate frame rotate matrix take zcomponents get estimate degree character leaning forward side reduces simply taking third row r velocity acceleration animations would like new pose sensitive pose previous time frame hence use velocity acceleration vectors features feature vector time velociy acceleration given yt yt1 yt 2yt1 yt2 respectively features pose may computed current frame previous frame write function yq omit previous frames notation always constant applications vectors paper column vectors 5 learning model poses section describe scaled gaussian process latent variable model sgplvm procedure learning model parameters training poses model based gaussian process gp model describes mapping x values values gps interpolation introduced ohagan 1978 neal 1996 williams rasmussen 1996 detailed tutorial gps see mackay 1998 additionally build upon gaussian process latent variable model recently poposed lawrence 2004 although mathematical background gps somewhat involved implementation straightforwardkernel function describing learning algorithm rst dene parameters gp model gp model describes mapping x values values given training data xiyi gp predicts likelihood new given new x key ingredient gp model denition kernel function measures similarity two points x x input space point 0 otherwise kxx 1 xx term vanishes whenever similarity measured two distinct variables kernel function tells us correlated two data values based corresponding x x values parameter tells us spread similarity function tells us correlated pairs points general tells us much noise predictions set n input vectors xiwe dene n n kernel matrix k different data dimensions different intrinsic scales equivalently different levels variance small change global rotation character affects pose much small change wrist angle similarly orientations vary much velocities hence need estimate separate scaling wk dimension scaling collected diagonal matrix used rescale features wy learning describe process learning sg plvm set n training data points yi rst compute mean training set collect kth component every feature vector vector yk subtract means yk y1k kynk kt sgplvm model parameters learned minimizing following objective respect unknowns xi wk objective function derived gaussian process model appendix formally lgp negative logposterior model pa rameters optimized parameters sgplvm provides likelihood function use realtime ik based training data model parameters intuitively minimizing objective function arranges xi values latent space similar poses nearby dissimilar poses far apart learns smoothness space poses generally trying adjust unknown parameters kernel matrix k matches correlations original ys appendix learning sgplvm model generalizes conventional pca lawrence 2004 corresponds xing using linear kernel described sgplvm also generalizes radial basis function rbf interpolation providing method learning rbf parameters constrained pose optimization simplest way minimize lgp numerical optimization methods lbfgs nocedal wright 1999 ever order realtime system efcient would like discard training data training points kept called active set optimized un knowns use heuristic lawrence et al 2003 determine active set moreover optimization may inefcient large datasets use heuristic optimization based lawrences 2004 order efciently learn model parameters select active set algorithm alternates figure 1 sgplvm latent spaces learned different motion capture sequences walk cycle jump shot baseball pitch points learning process estimates 2d position x associated every training pose plus signs indicate positions original training points 2d space red points indicate training poses included training set poses original poses shown along plots connected 2d positions orange lines additionally novel poses shown connected green lines positions 2d plot note new poses extrapolate original poses sensible way original poses arranged similar poses nearby 2d space likelihood plot grayscale plot visualizes ln2x 1 x2 position x component inverse kinematics likelihood lik measures good x observe points likely lie near similar training poses optimizing model parameters optimizing latent variables selecting active set algorithms tradeoffs described appendix b require user specify size active set although could also specied terms error tolerance choosing larger active set yields better model whereas smaller active set lead faster performance learning synthesis new poses parameters learned generalpurpose probability distribution new poses objective function new pose parameterized x k kernel matrix active set y1 ym matrix active set points meansubtracted kx vector ith entry contains kxxi ie similarity x ith point active set vector fx pose model would predict given x equivalent rbf interpolation training poses variance 2x indicates uncertainty prediction certainty greatest near training data derivation lik given appendix objective function lik interpreted follows optimization xy pair tries simultaneously keep close corresponding prediction fx due wy fx2 keeping x value close training data due tothe ln2x term since prediction reli able 1 x2 term little effect process isincluded mainly consistency learning 6 pose synthesis describe novel algorithms performing ik sg plvms given set motion capture poses qi compute corresponding feature vectors yi described section 4 learn sgplvm described previous sec tion learning gives us latent space coordinate xi pose yi well parameters sgplvm wk figure 1 show sgplvm likelihood functions learned different training sequences visualizations illustrate power sgplvm learn good arrangement training poses latent space also learning smooth likelihood function near spaces occupied data note pdf simply matter example gaussian distributions centered training data point since spaces inbetween data points likely spaces equidistant outside training data objective function smooth multimodal overtting signicant problem many popular pdf mod els particularly small datasets without redundancy ones shown sgplvm avoids overtting yields smooth objective functions large small data sets technical reason marginalizes space model representations mackay 1998 properly takes account uncertainty model figure 2 compare another common pdf model mixturesofgaussians mog model bishop 1995 redner walker 1984 exhibits problems overtting local minima learning1 addi 1the mog model similar used previously learning motion capture roughly speaking shmm brand hertzmann 2000 slds li et al 2002 reduce mogs synthesis gaussian components loglikelihood figure 2 mixturesofgaussians mog applied conventional pca reduce baseball pitch data 2d mog model em although assigns highest probability near data set loglikelihood exhibits number undesirable artifacts longandskinny gaussians assign high probabilities small regions create bumpy objective function contrast likelihood functions shown figure 1 much smoother appropriate data general nd 10d pca required yield reasonable model mog artifacts much worse higher dimensions tion using mog requires dimension reduction pca preprocess parameters need tuned principled ways estimate parameters difcult work practice able get reasonable results using mogs small datasets help heuristics manual tweaking model parameters 61 synthesis new poses q created optimizing likxyq respect unknowns x q examples learned models illustrated figure 1 number different scenarios synthesizing poses rst describe cases state optimization problems optimization techniques described section 62 general setting pose synthesis optimize q given constraints order get good estimate q also must estimate associated x general problem statement st cq0 8 constraints cq0 common case set handle constraints cq0 specied handle constraints may come user interactive session mocap system system also provides 2d visualization latent space allows user drag mouse window order view space poses model point window corresponds specic value x compute corresponding pose maximizing lik respect q third case occurs user species handle constraints drags mouse latent space case q optimized dragging provides alternative way user nd point space works well given constraints 611 model smoothing method produces objective function locally smooth thus wellsuited local optimization methods view single frame sequence isolation shmms entropic prior helps smooth model expense overlysmooth motions figure 3 annealing sgplvms top row leftmost plot shows unannealed original model trained baseball pitch plot right shows model retrained noisy data middle plot shows interpolation parameters outer models bottom row plots visualized 3d ever distributions likely poses must necessarily many local minima gradientbased numerical optimizer easily get trapped poor minima optimizing likwenow describe new procedure smoothing sgplvm model used annealinglike procedure search smoother versions model nal optimization given training data learned sgplvm goal create smoothed annealed versions sgplvm found simplest annealing strategy scaling individual model parameters example halving value work well since scales three parameters closely intertwined instead use following strategy produce smoother model rst learn normal unannealed sgplvm described section 5 create noisy version training set adding zeromean gaussian noise yi values active set learn new values using algorithm holding xi wk xed gives us new annealed parameters variance noise added data determines smooth model becomes given annealed model generate range models linear interpolation parameters normal sgplvm annealed sgplvm example range annealed models shown figure 3 62 realtime optimization algorithm system optimizes lik using gradientbased optimization meth ods experimented sequential quadratic programming sqp lbfgs nocedal wright 1999 sqp allows use hard constraints pose however hard constraints used underconstrained ik otherwise system quickly becomes infeasible solver fails general solution use convert constraints soft constraints adding term cq2 objective function large weight desirable approach would enforce hard constraints much possible convert constraints soft constraints necessary yamane nakamura 2003 lik objective rarely unimodal use annealinglike scheme prevent pose synthesis algorithm getting stuck local minima learning phase precompute annealed model described previous section tests set noise variance 05 smaller data sets larger data sets synthesis rst run steps optimization using smoothed model described previous section run additional steps intermediate model parameters interpolated 1 1 1 interpolation applied nish optimization respect original model interactive editing may enough time fully optimize dragging steps case optimization updated respect smoothest model case ner models used dragging stops 63 style interpolation describe simple new approach interpolating two styles represented sgplvms goal generate new stylespecic sgplvm interpolates two existing sgplvms lik0 lik1 given interpolation parameter new objective function generating new poses entails optimizing ls respect pose q well latent variables x0 x1 one original styles place interpolation scheme context following novel method interpolating stylespecic pdfs given two pose styles represented pdfs possible poses goal produce new pdf representing style input poses given two pdfs poses py0 py1 0 1 describe parameters styles interpolation parameter form interpolated style pdf new poses created maximizing psyq sgplvm case ln py0lik0 ln py0lik1we discuss motivation approach appendix c applications order explore effectiveness stylebased ik tested applications interactive character posing trajectory realtime motion capture missing markers determining human pose 2d image correspondences examples applications shown accompanying video 71 interactive character posing one basic powerful applications system interactive character posing animator interactively dene character pose moving handle constraints realtime experience posing way substantially faster intuitive posing without objective function 72 trajectory keyframing developed test animation system aimed rapidprototyping character animations system animator creates animation constraining small set points character constrained point controlled modifying trajectory curve animation played back realtime animator immediately view effects path modications resulting motion since animator constrains minimal set points rest pose time frame automatically synthesized using stylebased ik user use different styles different figure 4 trajectory keyframing using style learned baseball pitch data top row baseball pitch bottom row sidearm pitch case feet one arm keyframed constraints used sidearm contains poses different original data parts animation smoothly blending one style example creating motion keyframing shown figure 4 using three keyframed markers 73 realtime motion capture missing marker optical motion capture systems tracked markers often disappear due occlusion resulting inaccurate reconstructions noticeable glitches existing joint reconstruction methods quickly fail several markers go missing missing extended period time furthermore set missing markers reappears hard relabel one correspond correct points body designed realtime motion reconstruction system based stylebased ik lls missing markers learn style initial portion motion capture sequence use style estimate character pose experiments approach faithfully reconstruct poses even 50 markers missing expect method could used provide metric marker matching well course effectiveness stylebased degrades new motion diverges learned style could potentially addressed incrementally relearning style new pose samples processed 74 posing 2d images also use ik system reconstruct likely pose 2d image person given photograph person user interactively species 2d projections ie image coordinates character handles example user might specify location hands feet 2d positions establishes constraint selected handle project 2d position indicated user words 3d handle lie line containing camera center projected position 3d pose estimated minimizing lik subject 2d constraints three four established correspondences 2d image points character handles reconstruct likely pose little additional effort pose netuned several examples shown figure 5 baseball example bottom row gure system obtains plausible pose six projection constraints depth feasible solution many possible improvements sgplvm learning algorithm experimenting kernels selecting automatically based data set additionally current optimization algorithm employs heuristics convenience speed would desirable principled efcient method optimization nd annealing heuristic realtime synthesis requires tuning would desirable nd better procedure realtime optimization acknowledgements frontal view side view figure 5 3d posing 2d image yellow circles front view correspond userplaced 2d constraints 2d constraints appear line constraints side view right hand match image could xed one constraint eg another viewpoint temporal coherence 8 discussion future work presented inverse kinematics system based learned probability model human poses given set arbitrary algebraic constraints system produce likely pose satisfying constraints realtime demonstrated system context several applications expect stylebased ik used effectively problem necessary restrict space valid poses including problems computer vision well animation example sgplvm could used replacement pca rbfs examplebased animation methods additionally number potential applications games necessary motions character look realistic satisfy specic constraints eg catching ball reaching base realtime would require realtime posing potentially sort planning ahead encouraged fact leading game developer licensed early version system purpose rapid content development limitations system could addressed future work example system model dynam ics take account constraints produced original motion capture would also interesting incorporate stylebased ik closely animation pipeline ex ample approach may thought automating process rigging ie determining highlevel controls character production environment rigging designer might want design character controls specic way using automatic procedure controls would also useful principled method balancing hard soft constraints realtime perhaps similar yamane nakamura 2003 many hard constraints prevent problemmany thanks neil lawrence detailed discussions placing source code online indebted colin zheng creating 2d posing application jiachu wu lastminute image video production david hsu eugene hsu implemented rst prototypes system work supported part uw animation research labs nsf grants eia 0121326 ccr0092970 iis0113007 ccr0098005 nserc discovery grant connaught fund alfred p sloan fellowship electronic arts sony microsoft research background gaussian processes section briey describe likelihood function used paper gaussian processes gps learning originally developed context classication regression problems neal 1996 ohagan 1978 williams rasmussen 1996 detailed background gaussian processes see mackay 1998 scaled gaussian processes general setting regression follows given collection training pairs xiyi element xi yi vector wish learn mapping typically done leastsquared tting parametric function bspline basis neural network tting procedure sensitive number important choices eg number basis functions smooth nessregularization assumptions choices made care fully undertting results however bayesian point view never estimate specic function f gression instead marginalize possible choices f computing new points avoid tting undertting additionally learn smoothness parameters noise parameters remarkably turns wide variety types function f including polynomials splines singlehiddenlayer neural networks gaussian rbfs marginalization possible values f yields gaussian process model data gp model single output dimension k likelihood outputs given inputs using variables dened section 5 paper generalize gp models account different variances output dimensions introducing scaling parameters wk output dimension equivalent dening separate kernel function kxxw2 output dimension2 plugging gp likelihood dimension k yields 2alternatively derive model warped gp snelson et al 2004 warping function rescales features wkyk complete joint likelihood data dimensions pyixiwkk pyikxiwk sgplvms scaled gaussian process latent variable model sgplvm general technique learning pdfs based recent work lawrence 2004 given set data points yiwe model likelihood points scaled gp corresponding values xi initially unknown must learn xi well model parameters also place priors unknowns pxn 0i p 111 order learn sgplvm training data yiwe need maximize posterior pxiwkyi equivalent minimizing negative logposterior respect unknowns constant terms dropped expressions one way interpret objective function follows suppose ignore priors px p optimize lgp respect xi value optima occur lgp one condition occur similarly would make lgp optimal respect xi values parameters solve equation 15 obtain system equations form righthand side expression large two poses similar negative different means try arrange xs xi xj nearby yi yj similar generally kernel matrix match covariance matrix original data rescaled w prior terms px p help prevent tting small training sets parameters learned generalpurpose probability distribution new poses order dene probability augment data new pose xyin one xy unknown adding new pose lgp rearranging terms dropping constants yields logposterior equation 3 learning algorithm tested two different algorithms optimizing lgp rst directly optimizes objective function selects active set ie reduced set example poses training data second heuristic described based preliminary tests appears tradeoffs two al gorithms heuristic algorithm much faster tied initialization small data sets often producing x values close pca initialization full optimization algorithm produces better arrangements latent space x values especially larger data sets may require higher latent dimensionality 3d instead 2d tests however full optimization optimizes points get less active set points making efcient runtime nonetheless algorithms work well used heuristic algorithm examples shown paper videoactive set selection rst outline greedy algorithm selecting active set given learned model active set initially contains one training pose algorithm repeatedly determines points active set highest prediction variance 2x equation 5 point added active set algorithm repeats points active set limit predetermined user ef ciency variances computed incrementally described lawrence et al 2003 heuristic optimization algorithm examples paper used following procedure optimizing lgp based one proposed lawrence 2004 modied3 learn wk algorithm alternates updating active set following steps first algorithm optimizes model parameters numerical optimization lgp equa tion 2 however lgp modied active set included lgp next algorithm optimizes latent variables xi points included active set done numerical optimization lik equation 3 finally scaling updated closedform optimization lgp respect wk numerical optimization performed using scaled conjugate gradients algorithm although search algorithms could also used steps active set recomputed algorithm may summarized follows see lawrence 2004 details function learnsgplvmyi initialize x conventional pca applied yi select new active set minimize lgp active set respect select new active set point active set minimize likxiyi respect xi end select new active set data dimension end end return xiwk parameters active set size latent dimensionality tradeoff runtime speed versus quality typically used 50 active set points small data sets 100 large data sets using long walking sequence 500 frames training 100 active set points 3dimensional latent space gave 23 framespersecond synthesis 28 ghz pentium 4 increasing active set size slows performance without noticably improving quality found cases 3d latent space gave good better quality 2d latent space use higher dimensionality multiple distinct motions included training set c style interpolation although formal justication interpolation method section 63 eg maximizing known likelihood function motivate follows general reason believe interpolation two objective functions gives reasonable interpolation styles example suppose 3we adapted source code available httpwwwdcsshefacuk neilgplvm represent styles gaussian distributions py0n y02 py1n y12 0 1 means gaussians 2 variance simply interpolate pdfs ie psy1 sexpy 022sexpy 1222 interpolated function gaussian values two minima near 0 1 howver using logspace interpolation scheme get intuitive sult interpolated style psy also gaussian mean variance 2 words mean linearly interpolates means input gaussians variance unchanged similarlyintuitive interpolation results gaussians different covariances analyzing sg plvm case difcult nd practice scheme works quite well moreover straightforward interpolate two likelihood models eg interpolate sgplvm mog would difcult achieve otherwise gradients gradients lik lgp may computed help following derivatives along chain rule x x x x x x x containing mean subtracted training data r synthesizing constrained motions examples motion synthesis annotations neural networks pattern recognition process motion capture style machines shadow puppetry motion signal pro cessing handrix animating human hand computational modeling computer animation legged figures believable automatically synthesized motion knowledgeenhanced motion transformation inferring 3d structure statistical imagebased shape model bayesian reconstructions 3d human motion singlecamera video automated extraction parameterization motions large data sets motion graphs fast sparse gaussian process methods informative vector chine gaussian process latent variable models visualisation high dimensional data interactive control avatars animated human motion data motion texture twolevel statistical model character motion synthesis introduction gaussian processes bayesian learning neural networks numerical optimization motion capture assisted animation texturing synthesis automatic annotation everyday movements mixture densities learning body pose via specialized maps verbs adverbs multidimensional motion interpolation implicit probabilistic models human motion synthesis tracking warped gaussian processes reconstruction articulated objects point correspondences single image inverse kinematics geometric constraints articulated figure manipulation interpolation synthesis articulated figure motion gaussian processes regression parametric hidden markov models gesture recognition natural motion animation constraining deconstraining gesticulation behaviors virtual humans tr motion signal processing motion warping physically based motion transformation parametric hidden markov models gesture recognition computational modeling computer animation legged figures style machines bayesian learning neural networks neural networks pattern recognition motion texture motion graphs interactive control avatars animated human motion data motion capture assisted animation interpolation synthesis articulated figure motion verbs adverbs implicit probabilistic models human motion synthesis tracking gesticulation behaviors virtual humans handrix shadow puppetry motion synthesis annotations believable automatically synthesized motion knowledgeenhanced motion transformation inferring 3d structure statistical imagebased shape model automated extraction parameterization motions large data sets ctr neil lawrence joaquin quionerocandela local distance preservation gplvm back constraints proceedings 23rd international conference machine learning p513520 june 2529 2006 pittsburgh pennsylvania edmond l ho taku komura rynson w h lau computing inverse kinematics linear programming proceedings acm symposium virtual reality software technology november 0709 2005 monterey ca usa lionel reveret laurent favreau christine depraz mariepaule cani morphable model quadrupeds skeletons animating 3d animals proceedings 2005 acm siggrapheurographics symposium computer animation july 2931 2005 los angeles california guodong liu leonard mcmillan segmentbased human motion compression proceedings 2006 acm siggrapheurographics symposium computer animation september 0204 2006 vienna austria jack wang david j fleet aaron hertzmann multifactor gaussian process models stylecontent separation proceedings 24th international conference machine learning p975982 june 2024 2007 corvalis oregon edwin chang odest chadwicke jenkins sketching articulation pose facial animation proceedings 2006 acm siggrapheurographics symposium computer animation september 0204 2006 vienna austria kevin g der robert w sumner jovan popovi inverse kinematics reduced deformable models acm transactions graphics tog v25 n3 july 2006 j p lewis jonathan mooser zhigang deng ulrich neumann reducing blendshape interference selected motion attenuation proceedings 2005 symposium interactive 3d graphics games april 0306 2005 washington district columbia neil lawrence andrew j moore hierarchical gaussian process latent variable models proceedings 24th international conference machine learning p481488 june 2024 2007 corvalis oregon majkowska v b zordan p faloutsos automatic splicing hand body animations proceedings 2006 acm siggrapheurographics symposium computer animation september 0204 2006 vienna austria igarashi moscovich j f hughes spatial keyframing performancedriven animation acm siggraph 2006 courses july 30august 03 2006 boston massachusetts igarashi moscovich j f hughes spatial keyframing performancedriven animation proceedings 2005 acm siggrapheurographics symposium computer animation july 2931 2005 los angeles california yeuhi abe jovan popovi interactive animation dynamic manipulation proceedings 2006 acm siggrapheurographics symposium computer animation september 0204 2006 vienna austria michael neff eugene fiume methods exploring expressive stance proceedings 2004 acm siggrapheurographics symposium computer animation august 2729 2004 grenoble france neil lawrence probabilistic nonlinear principal component analysis gaussian process latent variable models journal machine learning research 6 p17831816 1212005 chen mao sheng feng qin david k wright sketchingout virtual humans 2d storyboarding immediate 3d character animation proceedings 2006 acm sigchi international conference advances computer entertainment technology june 1416 2006 hollywood california tomohiko mukai shigeru kuriyama geostatistical motion interpolation acm transactions graphics tog v24 n3 july 2005 philippe beaudoin pierre poulin michiel van de panne adapting wavelet compression human motion capture clips proceedings graphics interface 2007 may 2830 2007 montreal canada yuchi lai stephen chenney shaohua fan group motion graphs proceedings 2005 acm siggrapheurographics symposium computer animation july 2931 2005 los angeles california chen mao sheng feng qin david k wright sketchingout virtual humans 2d storyboarding immediate 3d character animation proceedings 2006 acm sigchi international conference advances computer entertainment technology june 1416 2006 hollywood california c karen liu aaron hertzmann zoran popovi composition complex optimal multicharacter motions proceedings 2006 acm siggrapheurographics symposium computer animation september 0204 2006 vienna austria kang hoon lee myung geol choi jehee lee motion patches building blocks virtual environments annotated motion data acm transactions graphics tog v25 n3 july 2006 robert w sumner matthias zwicker craig gotsman jovan popovi meshbased inverse kinematics acm transactions graphics tog v24 n3 july 2005 jackie assa yaron caspi daniel cohenor action synopsis pose selection illustration acm transactions graphics tog v24 n3 july 2005 franck multon richard kulpa benoit bideau mkm global framework animating humans virtual reality applications presence teleoperators virtual environments v17 n1 p1728 february 2008 lucas kovar michael gleicher automated extraction parameterization motions large data sets acm transactions graphics tog v23 n3 august 2004 michael neff eugene fiume methods exploring expressive stance graphical models v68 n2 p133157 march 2006 eugene hsu kari pulli jovan popovi style translation human motion acm transactions graphics tog v24 n3 july 2005 zonghua zhang nikolaus f troje 3d periodic human motion reconstruction 2d motion sequences neural computation v19 n5 p14001421 may 2007 jinxiang chai jessica k hodgins performance animation lowdimensional control signals acm transactions graphics tog v24 n3 july 2005 dragomir anguelov praveen srinivasan daphne koller sebastian thrun jim rodgers james davis scape shape completion animation people acm transactions graphics tog v24 n3 july 2005 c karen liu aaron hertzmann zoran popovi learning physicsbased motion style nonlinear inverse optimization acm transactions graphics tog v24 n3 july 2005 thomas b moeslund adrian hilton volker krger survey advances visionbased human motion capture analysis computer vision image understanding v104 n2 p90126 november 2006 david forsyth okan arikan leslie ikemoto james obrien deva ramanan computational studies human motion part 1 tracking motion synthesis foundations trends computer graphics vision v1 n2 p77254 july 2006