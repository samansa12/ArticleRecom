modulo scheduling reduced register pressure abstractsoftware pipelining scheduling technique used product compilers order expose instruction level parallelism innermost loops modulo scheduling refers class algorithms software pipelining previous research modulo scheduling focused reducing number cycles initiation consecutive iterations termed ii considered effect register pressure produced schedules register pressure increases instruction level parallelism increases register requirements schedule higher available number registers loop must rescheduled perhaps higher ii therefore register pressure important impact performance schedule paper presents novel heuristic modulo scheduling strategy tries generate schedules lowest ii possible schedules ii tries select lowest register requirements proposed method implemented experimental compiler tested perfect club benchmarks results show proposed method achieves optimal ii least 975 percent loops compilation time comparable conventional topdown approach whereas register requirements lower addition proposed method compared existing methods results indicate proposed method performs better heuristic methods almost well linear programming methods obtain optimal solutions impractical product compilers computing cost grows exponentially number operations loop body b introduction increasing instruction level parallelism observed trend design current microprocessors requires combined effort hardware software order effective since execution time common programs spent loops many efforts improve performance targeted loop nests software pipelining 5 instruction scheduling technique exploits instruction level parallelism loops overlapping execution successive iterations loop different approaches generate software pipelined schedule loop 1 modulo scheduling class software pipelining algorithms proposed begining last decade 23 incorporated product compilers eg 21 7 besides many research papers recently appeared topic 11 14 25 13 28 12 26 22 29 17 modulo scheduling framework relies generating schedule iteration loop schedule repeated regular intervals dependence violated resource usage conflict arises interval succesive iterations termed initiation interval ii constant initiation interval implies resource may used time modulo ii modulo scheduling approaches consists two steps first compute schedule trying minimize ii without caring register allocation variables allocated registers execution time software pipelined loop depends ii maximum number live values schedule termed maxlive length schedule one iteration ii determines issue rate loop iterations regarding second factor maxlive higher number available registers computed schedule feasible influence execution time otherwise actions must taken order reduce register pressure possible solutions outlined 24 evaluated 16 ffl reschedule loop increased ii general increasing ii reduces maxlive decreases issue rate negative effect execution time ffl add spill code negative effect since increases required memory bandwidth result additional memory penalties eg cache misses besides memory may become saturated resource therefore adding spill code may require increase ii finally length schedule one iteration determines cost epilogue executed main loop order finish last iterarions initiated main loop completed see section 21 cost may negligible iteration count loop high previous works focused reducing ii sometimes also length schelude one iteration considered register requirements proposed schedule may severe impact performance outlined current trend design new processors increase amount instruction level parallelism exploit exploiting instruction level parallelism results significant increase register pressure 19 18 exacerbates problem ignoring effect performance given schedule order obtain effective schedules recently proposed modulo scheduling approaches try minimize ii register requirements produced schedules approaches 10 9 based formulating problem terms optimization problem solve using integer linear programming approach may produce optimal schedules unfortunately approach computing cost grows exponentially number basic operations loop body therefore impractical big loops cases time consuming parts program thus may ones benefit software pipelining practical modulo scheduling approaches used product compilers use heuristics guide scheduling process two relevant heuristic approaches proposed literature try minimize ii register pressure slack scheduling 12 stage scheduling 8 slack scheduling iterative algorithm limited backtracking iteration scheduler chooses operation based previouly computed dynamic priority priority function slack operation ie measure scheduling freedom operation also depends much critical resources used operation selected operation placed partial schedule either early possible late possible choice two alternative made basically determining many operations inputs outputs stretchable choosing one minimizes involved values lifetimes scheduler cannot place selected operation due lack conflictfree issue slots forced particular slot conflicting operations ejected partial scheduler order limit type backtracking operations ejected many times ii incremented scheduling started stage scheduling whole modulo scheduler set heuristic techniques reduce register requirements given modulo schedule objective achieved shifting operations multiples ii cycles resulting schedule ii lower register requirements paper presents hypernode reduction modulo scheduling hrms 1 heuristic modulo scheduling approach tries generate schedules lowest ii possible schedules ii tries select lowest register requirements main part hrms ordering strategy ordering phase orders nodes scheduling predecessors successors node scheduled scheduled except recurrences scheduling step nodes scheduled earlylate possible predecessorssuccessors preliminary version work appeared 17 previously scheduled performance hrms evaluated compared conventional approach topdown scheduler care register pressure evaluation used thousand loops perfect club benchmark suite 4 account 78 execution time results show hrms achieves optimal ii least 975 loops compilation time comparable topdown approach whereas register requirements lower addition hrms tested set loops taken 10 compared two heuristic strategies two strategies previously mentioned slack scheduling frlc 27 heuristic strategy take account register requirements addition hrms compared spilp 10 linear programming formulation problem computing requirements latter approach small loops used comparison results indicate hrms obtains better schedules two heuristic approaches results close ones produced optimal scheduler compilation time hrms similar heuristic methods much lower linear programming approach rest paper organized follows section 2 example used illustrate motivation work reducing register pressure modulo scheduled loops achieving near optimal ii section 3 describes proposed modulo scheduling algorithm called hrms section 4 evaluates performance proposed approach finally section 5 states main conclusions work 2 overview modulo scheduling motivating ex ample section includes overview modulo scheduling motivation work presented paper detailed discussion modulo scheduling refer 1 21 overview modulo scheduling software pipelined loop schedule iteration divided stages execution consecutive iterations distinct stages overlapped number stages one iteration termed stage countsc number cycles per stage ii figure 1 shows dependence graph running example used along section graph nodes represent basic operations loop edges represent values generated consumed operations graph figure 2a shows execution six iterations software pipelined loop ii 2 sc 5 operations scheduled assuming fourwide issue machine generalpurpose functional units fully pipelined latency two cycles scheduling iteration obtained using topdown strategy gives priority operations g f figure 1 sample dependence graph critical path additional constraint resource used cycle modulo ii figure also shows corresponding lifetimes values generated iteration execution loop divided three phases ramp phase fills software pipeline steady state phase software pipeline achieves maximum overlap iterations ramp phase drains software pipeline code implements ramp phase termed prologue steady state phase execution pattern operations executed stage achieved iterating piece code termed kernel correspods one stage steady state phase third piece code called epilogue required drain software pipeline execution steady state phase initiation interval ii two successive iterations bounded either loopcarried dependences graph recmii resource constraints architecture resmii lower bound ii termed minimum initiation interval reader refered 7 22 extensive dissertation calculate resmii recmii since graph figure 1 recurrence circuits initiation interval constrained available resources number operations divided number resources notice scheduling figure 2a dependence violated every functional unit used even cycles cycle modulo odd cycles cycle modulo code corresponding kernel software pipelined loop obtained ovelapping different stages constitute schedule one iteration shown figure 2b subscripts code indicate relative iteration distance original loop operations instance example iteration kernel executes instance operation instance operation b previous iteration initial loop values used loop correspond either loopinvariant variables loopvariant variables loopinvariants repeatedly used never defined loop execution loopinvariants single value iterations loop therefore iteration 1 iteration 2 iteration 3 iteration 4 iteration 5 prologue steady epilogue ii kernel code iteration 6 g f g f g f g f g f g f figure 2 software pipelined loop execution b kernel c register requirements require one register regardless scheduling machine configuration loopvariants value generated iteration loop therefore different value corresponding iteration nature software pipelining lifetimes values defined iteration overlap lifetimes values defined subsequent iterations figure 2a shows lifetimes loopvariants corresponding every iteration loop overlapping lifetimes different iterations pattern length ii cycles indefinetely repeated obtained pattern shown figure 2c pattern indicates number values live given cycle shown 24 maximum number simultaneously live values maxlive accurate approximation number register required schedule 2 section register requirements given schedule approximated maxlive however experiments section measure actual register requirements register allocation values lifetime greater ii pose additional difficulty since new values generated previous ones used one approach fix problem provide form register renaming successive definitions value use distinct registers renaming performed compile time using modulo variable expansion 15 ie 2 extensive discussion problem allocating registers softwarepipelined loops refer 24 strategies presented paper almost always achieve maxlive lower bound particular wandsonly strategy using endfit adjacency ordering never required maxlive registers unrolling kernel renaming compile time multiple definitions variable exist unrolled kernel rotating register file used solve problem without replicating code renaming different instantiations loopvariant execution time 6 22 motivating example many modulo scheduling approaches lifetimes values unnecessarily large example figure 2a shows topdown scheduling figure 3a bottomup scheduling example graph figure 1 machine four generalpurpose functional units twocycle latency topdown strategy operations scheduled predecessors already scheduled node placed early possible order delay possible successors similary bottomup strategy operation ready scheduling successors already scheduled case node placed late possible order delay possible predecessors strategies several candidates scheduled algorithm chooses one critical scheduling topdown scheduling node e scheduled node f since e predecessors placed cycle order delay possible successor placed early possible figure 2a shows lifetimes loop variants topdown scheduling assuming value alive beginning producer operation beginning last consumer notice loop variant unnecessary large lifetime due early placement e scheduling bottomup approach e scheduled f therefore placed late possible reducing lifetime figure 3b unfortunately c scheduled b order delay possible predecessor scheduled late possible notice vb unnecessary large lifetime due late placement c hrms operation ready scheduling even predecessors successors scheduled condition guaranteed preordering step operation scheduled partial schedule contains predecessors successors none absence recurrences ordering done aim operations previously scheduled reference operation except first operation scheduled instance consider nodes graph figure 1 scheduled order fa b c f gg notice node f scheduled nodes fe gg predecessor successor respectively partial scheduling contain predecessor f scheduling order c e two conflicting operations topdown bottomup strategies reference operation already scheduled placed partial schedule figure 4a shows hrms scheduling one iteration operation scheduled cycle 0 operation b depends scheduled cycle 2 c later scheduled cycle 4 point operation f scheduled early possible g c9 cycle c figure 3 bottomup scheduling schedule one iteration b lifetimes variables c kernel register requirements ie cycle 6 depends available resources cycle delayed cycle 7 scheduler places operation e late possible scheduling successor e previously placed partial scheduling thus operation e placed cycle 5 finally since operation g predecessor previously scheduled placed early possible scheduling ie cycle 9 figure 4b shows lifetimes loop variants notice neither c e placed late early scheduler always takes previously scheduled operations reference point since f scheduled e scheduler reference operation decide late start e figure 4d shows number live values kernel figure 4c steady state phase execution loop 6 live values first row 5 second contrast topdown schedule simultaneosly live values bottomup schedule 9 following section describes algorithm orders nodes scheduling scheduling step 3 hypernode reduction modulo scheduling dependences innermost loop represented dependence graph set vertices graph g vertex operation loop e dependence edge set edge u v 2 e represents dependence two operations u v edges may correspond following types dependences register dependences memory dependences control dependences dependence distance ffi uv nonnegative integer associated edge dependence distance ffi uv two nodes u v execution operation v depends execution operation u ffi uv iterations latency u nonzero positive integer associated node u 2 v defined cycle c figure 4 hrms scheduling schedule one iteration b lifetimes variables c kernel register requirements number cycles taken corresponding operation produce result hrms tries minimize register requirements loop scheduling operation u close possible relatives ie predecessors u p redu successors u succu scheduling operations way shortens operands lifetime therefore reduces register requirements loop software pipeline loop scheduler must handle cyclic dependences caused recurrence circuits scheduling operations recurrence circuit must stretched beyondomega theta ii whereomega sum distances edges constitute recurrence circuit hrms solves problems splitting scheduling two steps preordering step orders nodes actual scheduling schedules nodes time order given preordering step preordering step orders nodes dependence graph goal scheduling loop ii close possible mii using minimum number reg isters gives priority recurrence circuits order stretch recurrence circuit also ensures node scheduled current partial scheduling contains predecessors successors node never unless node last node recurrence circuit scheduled ordering step assumes dependence graph connected component g connected component decomposed set connected components fg g g ordered separately finally lists nodes g concatenated giving higher priority g restrictive recurrence circuitin terms recmii next preordering step presented first assume dependence graph function pre orderingg l h freturns list nodes g orderedg fit takes input g fthe dependence graph g g fa list nodes partially ordered l g fan initial node ie hypernode h g list return list figure 5 function preorders nodes dependence graph without recurrence circuits recurrence circuits section 31 section 32 introduce modifications order deal recurrence circuits finally section 33 presents scheduling step 31 preordering graphs without recurrence circuits order nodes graph initial node call hypernode selected iterative process nodes dependence graph reduced hypernode reduction set nodes hypernode consists deleting set edges among nodes set hypernode replacing edges rest nodes reduced set nodes edges rest nodes hypernode finally deleting set nodes reduced preordering step figure 5 requires initial hypernode partial list ordered nodes current implementation selects first node graph ie node corresponding first operation program order node graph taken initial hypernode 3 node inserted partial list ordered 3 preliminary experiments showed selecting different initial nodes produced different schedules function hypernode reductionv 0 gh f creates subgraph g f reduces g 0 node h graph g g else return g 0 figure function hypernode reduction nodes preordering algorithm sorts rest nodes step predecessors successors hypernode determined nodes appear path among predecessors successors obtained function search paths 4 predecessors successors paths connecting obtained nodes reduced see function hypernode reduction figure hypernode subgraph contains topologically sorted topological sort determines partial order predecessors successors appended ordered list nodes predecessors topologically sorted using pala algorithm pala algorithm like alap late possible algorithm list ordered nodes inverted successors topologically sorted using asap soon possible algorithm example consider dependence graph figure 7a next illustrate ordering nodes graph step step 1 initially list ordered nodes empty list fg start designating node graph hypernode h figure 7 assume first node graph resulting graph shown figure 7b appended approximately register requirements minor differences caused resource constraints 4 execution time search paths okv k list ordered nodes list fag 2 next step predecessors h selected since predecessors successors selected ie node c node c reduced h resulting graph figure 7c c added list ordered nodes list fa cg 3 process repeated selecting nodes g h case selecting multiple nodes may paths connecting nodes algorithm looks possible paths topologically sorts nodes involved since paths connecting g h added list list fa c g hg reduced hypernode resulting graph figure 7d 4 h predecessor thus reduced producing graph figure 7e appended list list fa c g hdg 5 j successor h ordered list fa c g hdjg reduced producing graph figure 7f 6 point h two predecessors b path b contains node e therefore b e reduced h producing graph figure 7g subgraph contains b e topologically sorted partially ordered list fi e bg appended list ordered circuits order dependence graph shown subsection 31 presenting ordering algorithm recurrence circuits let us put forward considerations recurrences recurrence circuits classified ffl single recurrence circuits figure 8a g f g f g f f f f b c e f g h figure 7 example reordering without recurrences b c figure 8 types recurrences recurrence circuits share set backward edges figure 8b call recurrence subgraph set recurrence circuits share set backward edges way figures 8a 8b recurrence subgraphs ffl several recurrence circuits share nodes figures 8c 8d distinct sets backward edges case consider recurrence circuits different recurrence subgraphs recurrence circuits identified calculation recmii instance recurrence circuits graph figure 8b fa eg fa b c eg recurrence circuits grouped recurrence subgraphs worst case may recurrence subgraph backward edge instance recurrence circuits figure 8b grouped recurrence subgraph fa b c eg recurrence subgraphs ordered based highest recmii value recurrence circuits contained subgraph decreasing order nodes appear one subgraph removed excepting restrictive subgraph terms recmii instance procedure ordering recurrencesg l list h fthis procedure takes dependence graph gg fand simplified list recurrence subgraphs lg fit returns partial list ordered nodes listg fand resulting hypernode hg list pre orderingg 0 list h l 6 function generate subgraphv g fthis function takes dependence graph g subset nodes v g fand returns graph consists nodes v edgesg famong themg figure 9 procedure order nodes recurrence circuits list recurrence subgraphs associated figure 8c ffa c dg fb c egg simplified list ffa c dg fb egg algorithm orders nodes grah recurrence circuits see figure takes input list l recurrence subgraphs ordered decreasing values recmii entry list list nodes traversed associated recurrence subgraph trivial recurrence circuits ie dependences operation affect preordering step since impose scheduling constraints scheduler previously ensured ii recmii algorithm starts generating corresponding subgraph first recurrence circuit without one backward edges causes recurrence remove backward edge higher ffi uv therefore resulting subgraph recurrences ordered using algorithm without recurrences presented section 31 whole subgraph reduced hypernode nodes path hypernode next recurrence subgraph identified order properly use algorithm search paths required backward edges causing recurrences removed graph graph containing hypernode next recurrence circuit nodes paths connect ordered applying algorithm without recurrence circuits reduced hypernode path hypernode next recurrence circuit node recurrence circuit reduced hypernode recurrence circuit connected hypernode f g ih ka f g kh g b c e figure 10 example ordering recurrences procedure process repeated recurrence subgraphs list point nodes recurrence circuits paths connecting ordered reduced hypernode therefore graph contains hypernode remaining nodes graph without recurrence circuits ordered using algorithm presented previous subsection instance consider dependence graph figure 10a graph two recurrence subgraphs fa c fg fg j mg next illustrate reduction recurrence subgraphs 1 subgraph fa c fg one highest recmii therefore algorithm starts ordering isolating subgraph removing backward edge obtain graph figure 10b ordering graph list ordered nodes list fa c dfg graph figure 10b reduced hypernode h original graph figure 10a obtain dependence graph figure 10c 2 next step reduce following recurrence subgraph fg j mg purpose algorithm searches nodes possible paths h recurrence subgraphs graph contains nodes constructed see figure 10d since backward edges removed graph recurrence circuits ordered using algorithm presented previous section graph ordered list nodes appended previous one resulting partial list list fa c df g j mg subgraph reduced hypernode graph figure 10c producing graph figure 10e 3 point partial ordering nodes belonging recurrences initial graph reduced graph without recurrence circuits figure 10e graph without recurrence circuits ordered presented subsection 31 finally list ordered nodes list fa c df g j mheb l kg 33 scheduling step scheduling step places operations order given ordering step scheduling tries schedule operations close possible neighbors already scheduled operation scheduled scheduled different ways depending neighbors operations partial schedule ffl operation u predecessors partial schedule u scheduled early possible case scheduler computes early start u early start v cycle v scheduled v latency v ffi vu dependence distance v u psp u set predecessors u previously scheduled scheduler scans partial schedule free slot node u starting cycle early start u cycle early start u 1 notice due modulo constraint makes sense scan ii cycles ffl operation u successors partial schedule u scheduled late possible case scheduler computes late start u late start pssu set successors u previously scheduled scheduler scans partial schedule free slot node u starting cycle late start u cycle late start ffl operation u predecessors successors scheduler scans partial schedule starting cycle early start u cycle minlate start ffl finally operation u neither predecessors successors scheduler computes early start u early start scans partial schedule free slot node u cycle early start u cycle early start u found node ii increased 1 scheduling step repeated increased ii result opportunities finding slots advantage hrms nodes ordered even scheduling step several trials 4 evaluation hrms section present results experimental study first complexity performance hrms evaluated benchmark suite composed large number number registers6080100 loops l4 hrms l4 topdown l6 topdown figure cumulative distribution register requirements loop variants innermost loops perfect club 4 selected loops include single basic block loops conditionals body previously converted single basic block loops using ifconversion 2 included loops subroutine calls conditional exits dependence graphs obtained using experimental ictineo compiler 3 total 1258 loops account 78 total execution time 5 perfect club scheduled loops performance hrms compared performance topdown scheduler second compare hrms scheduling methods proposed literature using small set dependence graphs previously published results 41 performance evaluation hrms used two machine configurations evaluate performance hrms configurations 2 loadstore units 2 adders 2 multipliers 2 divsqrt units assume unit latency store instructions latency 2 loads latency 4 con figuration l4 6 configuration l6 additions multiplications latency 17 divisions latency roots units fully pipelined except divsqrt units pipelined order evaluate performance execution time cycles scheduled loop estimated ii loop times number iterations loop performs ie number times body loop executed purpose programs perfect club instrumented obtain number iterations selected loops hrms achieved loops means optimal terms ii least 975 loops average scheduler achieved 5 executed hp 9000735 workstation hrms topdown hrms topdown memory ideal regs regs figure 12 memory traffic infinite registers 64 registers registers hrms topdown hrms topdown l4 l61030cycles regs regs figure 13 cycles required execute loops infinite registers 64 registers registers considering dynamic execution time scheduled loops would execute 984 maximum performance register allocation performed using wandsonly strategy using endfit adjacency ordering extensive discussion problem allocating registers softwarepipelined loops refer 24 figure 11 compares register requirements loopvariants two scheduling techniques topdown care register requirements hrms two configurations mentioned figure plots percentage loops scheduled given number registers without spill code average hrms requires 87 registers required topdown scheduler since machines limited number registers also interest evaluate effect register requirements performance memory traffic loop requires available number registers spill code added loop rescheduled 16 different alternatives heuristics proposed speedup generation spill code among used heuristic spills variable maximizes quotient lifetime number additional loads stores required spill variable heuristic one produces best results figures 12 13 show memory traffic execution time respectively loops scheduled schedulers infinite 64 registers available notice general hrms requires less memory traffic topdown number registers limited difference memory traffic requirements schedulers increases number available registers decreases instance configuration l6 hrms requires 88 traffic required topdown scheduler 64 registers available 32 registers available requires 825 traffic required topdown scheduler addition assuming ideal memory system loops scheduled hrms execute faster ones scheduled topdown hrms gives priority recurrence circuits loops recurrences usually produces better results top additional factor increases performance hrms topdown reduces register requirements instance configuration l6 scheduling loops hrms produces speedup topdown 118 ideal assumption infinite register file available speedup 120 register file 64 registers 125 registers notice schedulers agressive configuration l6 requires registers l4 configuration degree pipelining functional units important effect register pressure 19 16 high register requirements aggressive configurations produces significant degradation performance memory traffic limited number registers available 16 instance loops scheduled hrms require 6 cycles execute configuration l6 l4 infinite number registers assumed 32 registers available l6 requires 16 cycles l4 42 complexity hrms scheduling testbench consumed 55 seconds sparc1040 workstation time compares 69 seconds consumed topdown scheduler breakdown scheduler execution time different steps shown figure 14 notice hrms computing recurrence circuits consumed 7 preordering step consumed 66 scheduling step consumed 27 even though time spent preordering step overall time extremely short extra time lost preordering nodes allows simple fast scheduling step topdown scheduler preordering step consumed small percentage time scheduling step required lot time scheduler fails find schedule given ii loop rescheduled increased initiation interval topdown reschedule loops much often hrms time seconds hrms topdown scheduling priority function find recurrences compute mii figure 14 time schedule 1258 loops hrms topdown schedulers 43 comparison scheduling methods section compare hrms three schedulers heuristic method take account register requirements frlc 27 lifetime sensitive heuristic method slack 12 linear programming approach spilp 10 scheduled 24 dependence graphs machine 1 fp adder 1 fp mul tiplier 1 fp divider 1 loadstore unit assumed unit latency add subtract store instructions latency 2 multiply load latency 17 divide table 1 compares initiation interval ii number buffers buf total execution time scheduler sparc1040 workstation four scheduling methods results three methods obtained 10 dependence graphs perform comparison supplied authors number buffers required schedule defined 10 sum buffers required value loop value requires many buffers number times producer instruction issued issue last consumer addition stores require one buffer 20 shown buffer requirements provide tight upper bound total register requirements table 2 summarizes main conclusions comparison entries table represent number loops schedules obtained hrms better ii equal ii worse ii schedules obtained methods terms application hrms spilp slack frlc program ii buf secs ii buf secs ii buf secs ii buf secs liver loop5 3 5 linpack whets cycle1 4 4 table 1 comparison hrms schedules scheduling methods initiation interval initiation interval also shows number loops hrms requires less buffers buf equal number buffers buf buffers buf notice hrms achieves performance spilp method terms ii buffer requirements compared methods hrms obtains lower ii 33 loops remaining 66 loops ii many cases hrms requires less buffers specially compared frlc finally table 3 compares total compilation time seconds four methods notice hrms slightly faster two heuristic methods addition methods perform noticeably worse finding good schedulings hand linear programming method spilp requires much higher time construct scheduling turns performance scheduling produced hrms fact time spent spilp due livermore loop 23 even without taking account loop hrms 40 times faster slack 7 1 table 2 comparison hrms performance versus 3 methods hrms spilp slack frlc compilation time 032 29072 093 071 table 3 comparison hrms compilation time 3 methods conclusions paper presented hypernode reduction modulo scheduling hrms novel effective heuristic technique resourceconstrained software pipelining hrms attempts optimize initiation interval reducing register requirements schedule hrms works three main steps computation mii preordering nodes dependence graph using priority function scheduling nodes following order ordering function ensures node scheduled partial scheduling contains least reference node predecessor successor except particular case recurrences tends reduce lifetime loop variants thus reduce register requirements addition ordering function gives priority recurrence circuits order penalize initiation interval provided exhaustive evaluation hrms using 1258 loops perfect club benchmark suite seen hrms generates schedules optimal terms ii least 974 loops although preordering step consumes high percentage total compilation time total scheduling time smaller time required convential topdown scheduler addition hrms provides significant performance advantage topdown scheduler limited number registers better performance comes reduction execution time memory traffic due spill code software pipelined execution also compared proposal three methods spilp integer programming formulation slack scheduling frlc scheduling schedules exhibit significant improvement performance terms initiation interval buffer requirements compared frlc significant improvement initiation interval compared slack lifetime sensitive heuristic obtained similar results spilp integer linear programming approach obtains optimal solutions prohibitive compilation time real loops r software pipelining conversion control dependence data dependence uniform representation highlevel instructionlevel transformations perfect club benchmarks effective performance evaluation supercomputers approach scientific array processing architectural design ap120bfps164 family overlapped loop support cydra 5 compiling cydra 5 stage scheduling technique reduce register requirements modulo schedule optimum modulo schedules minimum register requirements minimizing register requirements resourceconstrained software pipelining highly concurrent scalar processing circular scheduling new technique perform software pipelining software pipelining effective scheduling technique vliw machines systolic array optimizing compiler reducing impact register pressure software pipelined loops hypernode reduction modulo scheduling register requirements pipelined loops effect performance register requirements pipelined processors novel framework register allocation software pipelin ing software pipelining parisc compilers iterative modulo scheduling algorithm software pipelining loops scheduling techniques easily schedulable horizontal architecture high performance scientific computing register allocation software pipelined loops parallelisation loops exits pipelined architectures decomposed software pipelining new perspective new approach enhanced modulo scheduling loops conditional branches modulo scheduling multiple initiation intervals tr ctr spyridon triantafyllis manish vachharajani neil vachharajani david august compiler optimizationspace exploration proceedings international symposium code generation optimization feedbackdirected runtime optimization march 2326 2003 san francisco california david lpez josep llosa mateo valero eduard ayguad widening resources costeffective technique aggressive ilp architectures proceedings 31st annual acmieee international symposium microarchitecture p237246 november 1998 dallas texas united states david lpez josep llosa mateo valero eduard ayguad costconscious strategies increase performance numerical programs aggressive vliw architectures ieee transactions computers v50 n10 p10331051 october 2001 josep llosa eduard ayguad antonio gonzalez mateo valero jason eckhardt lifetimesensitive modulo scheduling production environment ieee transactions computers v50 n3 p234249 march 2001