common data management infrastructure adaptive algorithms pde solutions paper presents design development application computational infrastructure support implementation parallel adaptive algorithms solution sets partial differential equations infrastructure separated multiple layers abstraction paper primarily concerned two lowest layersof infrastructure layer defines implements dynamic distributed arrays dda layer several dynamic data programming abstractions implemented terms ddas currently implemented abstractions needed formulation hierarchical adaptive finite difference methods hpadaptive finite element methods fast multipole method solution linear systems implementation sample applications based methods described implementation issues performance measurements presented b introduction paper describes design implementation common computational infrastructure support parallel adaptive solutions partial differential equations motivations research 1 adaptive methods utilized solution almost largescale scientific engineering models adaptive methods executed largescale heterogeneous parallel execution environments 2 effective application complex methods scalable parallel architectures possible use programming abstractions lower complexity application structures tractable level 3 common infrastructure family algorithms result enormous savings coding effort effective infrastructure due pooling focusing effort goal research reduce intrinsic complexity coding parallel adaptive algorithms providing appropriate set data structures programming abstractions infrastructure developed result collaborative research among computer scientists computational scientists application domain specialists working three different projects darpa project hpadaptive computational fluid dynamics two nsf sponsored grand challenge projects one numerical relativity composite materials 11 conceptual framework figure 11 hierarchical problem solving environment parallel adaptive algorithms solution pdes figure 1 schematic perception structure problem solving environment pse parallel adaptive techniques solution partial differential equations paper primarily concerned lowest two layers hierarchy layers support implementation higher levels abstraction bottom layer hierarchical pse datamanagement layer layer implements distributed dynamic array dda provides array access semantics distributed dynamic data next layer programming abstractions layer adds application semantics dda objects layer implements data abstractions grids meshes trees underlie different solution methods design pse based separation concerns definition hierarchical abstractions based separation clean separation concerns 1 critical success infrastructure provide foundation several different solution methods particular pse presented paper supports finite difference methods based adaptive mesh refinement hpadaptive finite element methods adaptive fast multipole methods 12 overview paper defines common requirements parallel adaptive finite difference finite element methods solution pdes fast multipole solution linear systems demonstrates one data management system based distributed dynamic arrays dda efficiently meet common requirements paper describes design concepts underlying ddas sketches implementations parallel adaptive finite difference finite element multipole solutions using ddas based common conceptual basis common data management system performance evaluations method also presented primary distinctions ddabased data management infrastructures packages supporting adaptive methods 1 separation data management solution method semantics 2 separation addressing storage semantics dda design separation concerns enables preservation application locality multidimensional space mapped distributed onedimensional space computer memory efficient implementation dynamic behavior data structure traditionally used implementation problems multidimensional array informally array consists 1 index set lattice points ndimensional discrete space 2 mapping ndimensional index set one dimensional storage 3 mechanism accessing storage associated indices dda generalization traditional array targets requirements adaptive algorithms contrast regular arrays dda utilizes recursively defined hierarchical index space index index space may index space storage scheme associates contiguous storage spans index space relationship application array defined deriving index space directly ndimensional physical domain application problem description adaptive algorithms require definition operators complex dynamic data structures two problems arise 1 volume complexity bookkeeping code required construct maintain data structures overwhelms actual computations 2 maintaining access locality dynamic expansion contraction data requires complex copying operations standard storage layouts used implementation parallel distributed execution environments adds additional complexities partitioning distribution communication application domain scientists engineers forced create complex data management capabilities far removed application domain standard parallel programming languages provide explicit support dynamic distributed data structures data management requirements three different adaptive algorithms pdes described 21 adaptive finite difference data management requirements finite difference methods approximate solution pde discretized grid overlaid ndimensional physical application domain adaptation increases resolution discretization required regions refining segments grid finer grids computational operations grid may include local stencil based operations levels resolution transfer operations levels resolution global linear solves thus requirement datamanagement system adaptive finite difference methods seamless support operations across distribution refining coarsening grid storage dynamic grid array point grid mapped point hierarchical index space natural refinements coarsening grid become traversal hierarchy hierarchical index space 22 adaptive finite element data management requirement finite element method requires storage geometric information defining mesh elements spans application domain elements linear system arising finite element solutions generally computed fly need stored hpadaptive finite element methods adapt partitioning elements smaller elements h refinement increasing order polynomial approximating solution element p refinement partitioning adds new elements changes relationships among elements changing approximation function enlarges descriptions elements datamanagement requirements hpadaptive finite elements thus include storage dynamic numbers elements dynamic sizes requirements met mapping element mesh position hierarchical index space associated dynamic span onedimensional storage space partitioning element replaces position index space local index space representing expanded mesh 23 adaptive fast multipole data management requirements fast multipole methods partition physical space subdomains stored representation subdomain includes charge configuration subdomain various descriptive data adaptation consists selectively partitioning subdomains elements often generated fly storage values forces potential need stored requirement data management capability therefore similar adaptive finite element methods natural mapping associate subdomain point hierarchical index space 24 requirements summary clear extended definition array element array entity associated index position array object arbitrary variable size provides one natural representation data management requirements adaptive finite element adaptive finite difference adaptive fast multipole solvers challenge demonstrate efficient implementation array define implementations method terms storage abstraction difficult challenge efficient paralleldistributed implementation array 3 distributed dynamic datamanagement distributed dynamic datamanagement layer pse implements distributed dynamic arrays ddas layer provides pure array access semantics dynamically structured physically distributed data dda objects encapsulate distribution dynamic loadbalancing communications consistency management extended visualization analysis capabilities currently two different implementations data management layer built dda conceptual framework 1 hierarchical dynamic distributed array hdda 2 scalable dynamic distributed array sdda hdda hierarchical array element array recursively array dynamic array array level hierarchy expand contract runtime instead hierarchical arrays sdda implements distributed dynamic array objects arbitrary heterogeneous types differentiation results differences data management requirements structured unstructured meshes unstructured meshes convenient incorporate hierarchy information programming abstraction layer implements unstructured mesh two implementations derived common base implementation reintegrated single implementation near future arrays objects defined lowest layer specialized higher layers pse implement application objects grids meshes tree key feature dda based conceptual framework described following ability extract data locality requirements application domain maintain locality despite distribution dynamic structure achieved application principle separation concerns 1 dda design overview design shown figure 31 distributed dynamic arrays defined following subsection figure 31 dda design separation concerns hierarchical abstractions 31 distributed dynamic array abstraction sdda hdda implementations distributed dynamic array distributed dynamic array abstraction presented detail 2 summarized follows general array defined data set index space injective array dynamic elements may dynamically inserted removed data set array distributed elements data set distributed data set simply finite set data objects index space countable set indices welldefined linear ordering relation example set natural numbers two critical points array abstraction 1 cardinality data set necessarily equal less cardinality index space 2 element data set uniquely maps element index space 32 hierarchical index space spacefilling curves application partitions ndimensional problem domain finite number points andor regions region associated unique coordinate problem domain thus natural indexing scheme application discretization coordinates index space defined 1 2 n j corresponds discretization coordinate axis index space may hierarchical level discretization allowed vary perhaps correspondence hierarchical partitioning problem domain j components hierarchical index space could defined di 1 2 depth index recall index space requires welldefined linear ordering relation thus natural n dimensional hierarchical index space must effectively mapped linear onedimensional index space efficient family maps defined spacefilling curves sfc 3 one mapping defined hilbert spacefilling curve illustrated figure 32 mapping bounded domain hierarchically partitioned regions regions given particular ordering theory partitioning depth may infinite finite set points domain may fully ordered thus sfc mapping defines hierarchical index space theoretically infinite depth application domain mapped sfc bounding box figure 32 hilbert spacefilling curve natural index space sfc map efficiently defines linear ordering points andor subregions applications problem domain given linear ordering subregions distributed among processors simply partitioning index space partitioning easily efficiently obtained partitioning linearly ordered index space computational load partition roughly equal figure 33 spacefilling curve partitioning illustrates process irregularly partitioned two dimensional problem domain note figure 33 locality preserving property hilbert sfc map generates wellconnected subdomains figure 33 spacefilling curve partitioning 33 dda implementation array implementation provides storage objects data set access objects converse function f quality efficiency array implementation determined correlation storage locality index locality expense converse function f 1 example conventional onedimensional fortran array maximal quality efficiency dda implements distributed dynamic arrays storage objects data set distributed dynamic converse function f 1 provides global access objects ddas storage structure converse function consists two components 1 local object storage access 2 object distribution hdda sdda implementations dda use extendible hashing 4 5 redblack balanced binary trees 6 respectively local object storage access application instructs dda distribute data defining partitioning index space among processors index index space uniquely assigned particular processor p storage location particular data object determined associated index p thus storage location dynamically created data object welldefined dda provides global access distributed objects transparently caching objects processors example application applies ddas converse function data object present local processor data object transparently copied owning processor cache local processor 34 locality locality locality ddas object distribution preserves locality object storage objects global indices given natural index space spacefilling curve map corresponding domain partitioning ddas storage locality wellcorrelated geometric locality illustrated figure 34 figure 34 locality locality locality figure 34 application assigns sfc indices data objects associated subregion domain dda stores objects within span indices local memory specified processor thus geometrically local subregions associated data objects stored processor application programming interface api dda application programming interface consists small set simple methods hide complexity storage structure make transparent required interprocessor communication methods include get f 1 insert new remove old repartition forcing redistribution objects putlockunlock cache coherency controls implementation methods varies sdda hdda however abstractions methods common versions dda 36 dda performance dda provides object management services applications distributed dynamic data structures functionality introduces additional overhead cost accessing local objects example local object access could accomplished directly via c pointers instead dda indices however note pointer data object may invalidated data object redistribution overhead cost measured sdda version dda uses redblack balanced binary tree algorithm local object management overhead cost local get local insert local remove method measured ibm rs6000 local get method retrieves local data object associated input index value ie converse function local insert method inserts new data object associated specified index new fd new local storage structure local remove method removes given data object local storage structure computational time method measured given size existing data structure presented figure 35 sdda overhead local methods figure 35 sdda overhead local methods overhead cost local get method denoted middle line figure 35 increases logarithmically 2 microseconds empty sdda 8 microseconds sdda containing one million local data objects slow logarithmic growth expected search sddas balanced binary tree local insert method performs two operations 1 search proper point data structure insert object 2 modification data structure new object cost insert method uniform delta cost get operation thus overhead cost modifying data structure new data object independent size sdda note overhead cost local remove method denoted figure 35 lowest line also independent size sdda 4 method specific data programming abstractions next level pse specializes dda objects method specific semantics create highlevel programming abstractions directly used implement parallel adaptive algorithms design abstractions three different classes adaptive solution techniques pdes hierarchical dynamically adaptive grids hpadaptive finite elements dynamic tree dynamic trees data abstractions upon fast multipole methods implemented descried 41 hierarchical adaptive meshrefinement problem description figure 41 adaptive grid hierarchy 2d bergeroliger amr scheme dynamically adaptive numerical techniques solving differential equations provide means concentrating computational effort appropriate regions computational domain case hierarchical adaptive mesh refinement amr methods achieved tracking regions domain require additional resolution dynamically overlaying finer grids regions amrbased techniques start base coarse grid minimum acceptable resolution covers entire computational domain solution progresses regions domain requiring additional resolution tagged finer grids overlayed tagged regions coarse grid refinement proceeds recursively regions finer grid requiring resolution similarly tagged even finer grids overlayed regions resulting grid structure dynamic adaptive grid hierarchy adaptive grid hierarchy corresponding amr formulation berger oliger 7 shown figure 41 distributed datastructures hierarchical amr two basic distributed datastructures developed using fundamental abstractions provided hdda support adaptive finitedifference techniques based hierarchical amr 1 scalable distributed dynamic grid sddg distributed dynamic array used implement single component grid adaptive grid hierarchy 2 distributed adaptive grid hierarchy dagh defined dynamic collection sddgs implements entire adaptive grid hierarchy sddgdagh datastructure design based linear representation hierarchical multidimensional grid structure representation generated using spacefilling curves described section 3 exploits selfsimilar recursive nature mappings represent hierarchical dagh structure maintain locality across different levels hierarchy spacefilling mapping functions also used encode information original multidimensional space spacefilling index given index possible obtain position original multidimensional space shape region multidimensional space associated index spacefilling indices adjacent detailed description design datastructures found 8 figure 42 sddg representation figure 43 dagh composite representation sddg representation multidimensional sddg represented one dimensional ordered list sddg blocks list obtained first blocking sddg achieve required granularity ordering sddg blocks based selected spacefilling curve granularity sddg blocks system dependent attempts balance computationcommunication ratio block block list assigned cost corresponding computational load figure 42 illustrates representation 2dimensional sddg partitioning sddg across processing elements using representation consists appropriately partitioning sddg block list balance total cost processor since spacefilling curve mappings preserve spatial locality resulting distribution comparable traditional block distributions terms communication overheads dagh representation dagh representation starts simple sddg list corresponding base grid grid hierarchy appropriately incorporates newly created sddgs within list base grid gets refined resulting structure composite list entire adaptive grid hierarchy incorporation refined component grids base sddg list achieved exploiting recursive nature spacefilling mappings refined region sddg sublist corresponding refined region replaced child grids sddg list costs associated blocks new list updated reflect combined computational loads parent child dagh representation therefore composite ordered list dagh blocks dagh block represents block entire grid hierarchy may contain one grid level ie interlevel locality maintained within dagh block figure 43 illustrates composite representation two dimensional grid hierarchy amr grid hierarchy partitioned across processors appropriately partitioning linear dagh representation particular partitioning composite list balance cost associated processor results composite decomposition hierarchy key feature decomposition minimizes potentially expensive intergrid communications maintaining interlevel locality partition figure 44 sddgdagh storage datastructure storage maintained hdda described section 3 overall storage scheme shown figure 44 programming abstractions hierarchical amr figure 45 programming abstraction parallel adaptive meshrefinement developed three fundamental programming abstractions using datastructures described used express parallel adaptive computations based adaptive mesh refinement amr multigrid techniques see figure 45 objectives twofold first provide application developers set primitives intuitive expressing application second separate datamanagement issues implementations application specific operations grid geometry abstractions purpose grid geometry abstractions provide intuitive means identifying addressing regions computational domain abstractions used direct computations particular region domain mask regions included given operation specify region need resolution refinement grid geometry abstractions represent coordinates bounding boxes doubly linked lists bounding boxes coordinates coordinate abstraction represents point computational domain operations defined class include indexing arithmeticlogical manipulations operations independent dimensionality domain bounding boxes bounding boxes represent regions computation domain comprised triplet pair coords defining lower upper bounds box step array defines granularity discretization dimension addition regular indexing arithmetic operations scaling translations unions intersections also defined bounding boxes bounding boxes primary means specification operations storage internal information dependency communication information within dagh bounding boxes lists lists bounding boxes represent collection regions computational domain list typically used specify regions need refinement regriding phase adaptive application addition linkedlist addition deletion stepping operation reduction operations intersection union also defined bboxlist grid hierarchy abstraction grid hierarchy abstraction represents distributed dynamic adaptive grid hierarchy underlie parallel adaptive applications based adaptive meshrefinement abstraction enables user define maintain operate grid hierarchy firstclass object grid hierarchy attributes include geometry specifications domain structure base grid extents boundary information coordinate information refinement information information nature refinement refinement factor used used paralleldistributed environment grid hierarchy partitioned distributed across processors serves template application variables grid functions locality preserving composite distribution 9 based recursive spacefilling curves 3 used partition dynamic grid hierarchy operations defined grid hierarchy include indexing individual component grid hierarchy refinement coarsening recomposition hierarchy regriding querying structure hierarchy instant regriding repartitioning new grid structure dynamic loadbalancing required datamovement initialize newly created grids performed automatically transparently grid function abstraction grid functions represent application variables defined grid hierarchy grid function associated grid hierarchy uses hierarchy template define structure distribution attributes grid function include type information dependency information terms space time stencil radii addition user assign special fortran routines grid function handle operations intergrid transfers prolongation restriction initialization boundary updates inputoutput function called internally operating distributed grid function addition standard arithmetic logical manipulations number reduction operations minmax sumproduct norms also defined grid functions gridfunction objects locally operated regular fortran 9077 arrays 42 definition hpadaptive finite element mesh hpadaptive finite element mesh data structure consists two layers abstractions illustrated figure 46 first layer consists domain node abstractions second layer consists mesh specific abstractions vertex edge surface specializations node abstraction figure 46 layering mesh abstraction mesh domain finite element applications specialization sdda domain uses sdda store distribute dynamic set mesh nodes among processors domain provides mapping ndimensional finite element domain onedimensional index space required dda finite element mesh node associates set finite element basis functions particular location problem domain nodes also support internode relationships typically capture properties internode locality specializations finite element mesh node twodimensional problem summarized following table illustrated figure 47 mesh object reference location relationships vertex vertex point edge midside point vertex endpoints element owners irregular edge constraints element centroid point edge boundaries element refinement heirarchy figure 47 mesh object relationships extended relationships mesh nodes obtained minimul set relationships given example extended relationship relationship path element vertex element edge vertex element element element edge element normal element element element edge edge element constrained finite element hadaptation consists splitting elements smaller elements merging previously split elements single larger elements finite element padaptation involves increasing decreasing number basis functions associated elements application performs hpadaptations dynamically response error analysis finite element solution hpadaptation results creation new mesh nodes specification new relationships following hpadaptation mesh partitioning may lead load imbalance application may repartition problem dda significantly simplifies dynamic data structure update repartitioning operations insuring data structure consistency throughout operations 43 adaptive trees adaptive distributed tree requires two main pieces information first needs tree data structure methods gets puts pruning nodes tree infrastructure requires pointers nodes second adaptive tree needs estimation cost associated node tree order determine refinement take place node two abstractions algorithm utilize adaptive tree computation point developing distributed fast multipole method based balanced trees goal creating mildly adaptive tree near future adaptive trees could defined either ddas implementation described done using sdda references tree made generalization pointer concept pointers implemented indices sdda access controlled accessing sdda data object appropriate action index control provides uniform interface distributed data structure processor thus distributed adaptive trees supported sdda actual contents node includes list items 1 index node derived geometric location node 2 pointers parent children nodes 3 array coefficients used computation 4 list pointers nodes given node interacts information stored node called subdomain expected work subdomain derived amount computation performed specified data adaptivity determined basis expected work given node relative threshold addition since node registered sdda also compute total expected work per processor collecting total expected work per processor expected work per subdomain simple load balance implemented repartitioning index space 5 application codes follow sketches applications expressed terms parallel adaptive mesh refinement method parallel hpadaptive method parallel manybody problem built programming abstractions built upon dda 51 numerical relativity using hierarchical amr distributed adaptive version h3expresso 3d numerical relativity application implemented using datamanagement infrastructure presented paper h3expresso 3d numerical relativity application developed national center supercomputing applications ncsa university illinois urbana h3expresso developed national center supercomputing applications ncsa university illinois urbana concentrated version full h version 33 code solves general relativistic einsteins equations variety physical scenarios 10 original h3expresso code nonadaptive implemented fortran 90 representation overheads figure 51 dagh overhead evaluation overheads proposed daghsddg representation evaluated comparing performance handcoded unigrid fortran 90mpi implementation h3expresso application version built using datamanagement infrastructure handcoded implementation optimized overlap computations interior grid partition communications boundary storing boundary separate arrays figure 51 plots execution time two codes dagh implementation faster number processors composite partitioning evaluation results presented obtained 3d base grid dimension 8 x 8 x 8 6 levels refinement refinement factor 2 figure 52 dagh distribution snapshot figure 53 dagh distribution snapshot ii figure 54 dagh distribution snapshot iii figure 55 dagh distribution snapshot iv load balance evaluate load distribution generated composite partitioning scheme consider snapshots distributed grid hierarchy arbitrary times integration normalized computational load processor different snapshots plotted figures 52155 normalization performed dividing computational load actually assigned processor computational load would assigned processor achieve perfect loadbalance latter value computed total computational load entire dagh divided number processors residual load imbalance partitions generated tuned varying granularity sddgdagh blocks smaller blocks increase regriding time result smaller load imbalance since amr methods require redistribution regular intervals usually critical able perform redistribution quickly optimize distribution communications prolongation restriction intergrid operations performed locally processor without communication synchronization partitioning overheads table 51 dynamic partitioning overhead partitioning performed initially base grid entire grid hierarchy every regrid regriding level l comprises refining level l level finer generating distributing new grid hierarchy performing data transfers required initialize new hierarchy table 51 compares total time required regriding ie refinement dynamic repartitioning load balancing datamovement time required grid updates values listed cumulative times 8 base grid timesteps 7 regrid operations 52 hpadaptive finite element code parallel hpadaptive finite element code computational fluid dynamics development hpadaptive finite element computational fluid dynamics application two existing implementations 1 sequential fortran code 2 parallel fortran code fully duplicated data structure hpadaptive finite element data structure complexity great tractable distribute fortran data structure parallel fortran implementation scalable due memory consumed processor duplicating data structure make tractable development fully distributed hpadaptive finite element data structure necessary achieve separation concerns complexities hpadaptive finite element data structure complexities distributed dynamic data structures general separation concerns development fully distributed hpadaptive finite element data structure provided initial motivation developing sdda organization new finite element application illustrated figure 56 core application architecture index space index space provides compact succinct specification partition applications problem among processors specification used distribute mesh structure sdda define compatible distribution vectors matrices formed finite element method figure finite element application architecture finite element application uses second parallel infrastructure supports distributed vectors matrices denoted lower right corner figure 56 current release infrastructure documented 11 dda linear algebra infrastructures based upon common abstraction index space commonality provides finite element application uniform abstraction specifying data distribution 53 nbody problems general description figure 57 data flow fast multipole algorithm nbody particle problems arising various scientific disciplines appear require computational method however threshold number particles surpassed approximating interaction particles interactions sufficiently separated particle clusters allows computational effort substantially reduced best known fast summation approaches fast multipole method 12 certain assumptions gives method fast multipole method typical divide conquer algorithm cubic computational domain recursively subdivided octants finest level influence particles within cell onto sufficiently separated cells subsumed multipole series expansion multipole expansions combined upper levels root octtree contains multipole expansion local series expansions influence sufficiently separated multipole expansions cells formed finally reverse traversal octtree contributions cells distributed children cf figure 57 algorithm relies scaling property allows cells scale children sufficiently separated close scale current parents finest level influence sufficiently separated cells taken account together interaction particles remaining closeby cells mathematically oriented description shared memory implementation without periodic boundary conditions refer 1314 references therein figure 57 shows fast multipole algorithm readily decomposed three principal stages 1 populating tree bottomup multipole expansions 2 converting multipole expansions local expansions 3 distribute local expansions topdown three stages performed sequential order easily possible parallelize stages individually second stage interaction set cell defined set cells sufficiently separated cell parents sufficiently separated cells parent cell local expansion center cell found adding influences cells interaction set cell operations found completely independent notice majority communication requirements different processors incurred stage hence optimization communication patterns second stage account large performance gains detailed analysis nonadaptive algorithm reveals optimal performance attained leafcell contains optimal number particles thereby balancing work direct calculations conversion multipole expansions local expansions second stage distributed fast multipole results figure 58 total execution time sp2 vs problem size multiple levels approximation next describe preliminary performance results distributed fast multipole method implemented hdda verified fast multipole method computing exact answer fixed resolution computational domain test problems constructed storing one particles chosen randomly per leaf cell comparison repeated several small problems certain algorithm behaved expected performance measurements taken 16 node ibm sp2 parallel computer running aix version 41 total run times using 3 4 5 levels approximation variety problem sizes 1 2 4 8 processors presented figure 58 also plot expected run time aligned 8 processor results curve represents total execution time fixed level spatial resolution increasing number particles computation two major components run time approximation time direct calculation local particles approximation time function number levels resolution fixed curve direct calculation time grows 2 within given curve problem size two times equal represents optimal number particles stored per subdomain optimal problem size appears knee total execution time curves curves 8 processors align relatively well run time problem sizes thus algorithm appears scalable problems considered furthermore results show 1 processor longest time 8 processors shortest time indicates speedup attained ideally one would expect processors achieve speedup 8 presented results fast multipole method demonstrates computational complexity results exhibit scalability speedup small number processors preliminary results focused validity accuracy next step performance tune algorithm 6 conclusion future work significant conclusions demonstrated herein 1 common underlying computational infrastructure wide family parallel adaptive computation algorithms 2 substantial decrease effort implementation important algorithms attained without sacrifice performance use computational infrastructure much research needed complete development robust supportable computational infrastructure adaptive algorithms existing versions dda require extension engineering programming abstractions solution method require enriching hoped extend programming abstraction layer support adaptive methods wavelet methods need many applications define requirements programming abstraction layer interfaces acknowledgements research jointly sponsored argonne national laboratory enrico fermi scholarship awarded manish parashar binary blackhole nsf grand challenge nsf acsphy 9318152 arpa contract dabt 6392c0042 nsf national grand challenges program grant ecs9422707 authors would also like acknowledge contributions jrgen singer paul walker joan masso work r system engineering high performance computing software hddadagh infrastructure implementation parallel structured adaptive mesh refinement parallel infrastructure scalable adaptive finite element methods application least squares cinfinity collocation database system concepts linear hashing new tool file table addressing adaptive meshrefinement hyperbolic partial differential equations distributed dynamic datastructures parallel adaptive meshrefinement partitioning dynamic adaptive grid hierarchies hyperbolic system numerical relativity robert van de geijn rapid evaluation potential fields particle systems parallel fast multipole method molecular dynamics parallel implementation fast multipole method periodic boundary conditions tr algorithms parallel fast multipole method molecular dynamics database system concepts partitioning dynamic adaptive grid hierarchies ctr yun chris h q ding coupling multicomponent models mph distributed memory computer architectures international journal high performance computing applications v19 n3 p329340 august 2005 faith e sevilgen srinivas aluru unifying data structure hierarchical methods proceedings 1999 acmieee conference supercomputing cdrom p24es november 1419 1999 portland oregon united states sumir chandra manish parashar towards autonomic applicationsensitive partitioning samr applications journal parallel distributed computing v65 n4 p519531 april 2005 chandra x li parashar engineering autonomic partitioning framework gridbased samr applications high performance scientific engineering computing hardwaresoftware support kluwer academic publishers norwell 2004 karen devine bruce hendrickson erik boman matthew st john courtenay vaughan design dynamic loadbalancing tools parallel applications proceedings 14th international conference supercomputing p110118 may 0811 2000 santa fe new mexico united states andrew wissink richard hornung scott r kohn steve smith noah elliott large scale parallel structured amr calculations using samrai framework proceedings 2001 acmieee conference supercomputing cdrom p66 november 1016 2001 denver colorado j malard r stewart distributed dynamic hash tables using ibm lapi proceedings 2002 acmieee conference supercomputing p111 november 16 2002 baltimore maryland valerio pascucci randall j frank global static indexing realtime exploration large regular grids proceedings 2001 acmieee conference supercomputing cdrom p22 november 1016 2001 denver colorado james c browne madulika yalamanchi kevin kane karthikeyan sankaralingam general parallel computations desktop grid p2p systems proceedings 7th workshop workshop languages compilers runtime support scalable systems p18 october 2223 2004 houston texas johan steensland sumir chandra manish parashar applicationcentric characterization domainbased sfc partitioners parallel samr ieee transactions parallel distributed systems v13 n12 p12751289 december 2002