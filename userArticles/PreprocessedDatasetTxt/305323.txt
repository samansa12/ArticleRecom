preserving symmetry preconditioned krylov subspace methods consider problem solving linear system nearly symmetric system preconditioned symmetric positive definite matrix symmetric case recover symmetry using minner products conjugate gradient cg algorithm idea also used nonsymmetric case near symmetry preserved similarly like cg new algorithms mathematically equivalent split preconditioning require factored better robustness specific sense also observed combined truncated versions iterative methods tests show effective common practice forfeiting nearsymmetry altogether b introduction consider solution linear system preconditioned krylov subspace method assume first symmetric positive definite spd let spd matrix preconditioner matrix one possibility solve either leftpreconditioned system work supported nsf contract nsfccr9214116 onr contract onrn0001492j1890 aro contract daal039c0047 univ tenn subcontract ora 446604 amendment 1 2 department mathematics university california los angeles 405 hilgard avenue los angeles california 900951555 chanmathuclaedu myeungmathuclaedu z department computer science minnesota supercomputer institute university minnesota 4192 eecsci bldg 200 union st se minneapolis minnesota 554550154 chowcsumnedu saadcsumnedu rightpreconditioned system systems lost symmetry remedy exists preconditioner available factored form eg incomplete choleski factorization case simple way preserve symmetry split preconditioner left right ie could solve involves symmetric positive definite matrix also done factored unfortunately requirement available factored forms often stringent however remedy required wellknown preserve symmetry using different inner product specifically observe gamma1 selfadjoint minner product since rewrite cgalgorithm new inner product denoting r original residual z residual preconditioned system would obtain following algorithm see eg 4 algorithm 11 preconditioned conjugate gradient 1 start compute 2 iterate convergence b x c r z note even though minner products used need multiply may available explicitly need solve linear systems matrix coefficient also seen change variables iterates produced algorithm identical cg split preconditioning cg rightpreconditioning using gamma1 inner product three algorithms thus equivalent question raise following nearly symmetric often case exists preconditioner spd situation use either forms 2 3 unsatisfactory fully symmetric case indeed whatever degree symmetry available entirely lost although remedy based minner products always used symmetric case rather surprising problem seldom ever mentioned literature nearly symmetric case nonsymmetric case exists factored form form balancing also achieved splitting preconditioner left right however seem much work done exploiting minner products even available factored form dichotomy treatment symmetric nonsymmetric cases motivated study ashby et al fully considered case using alternate inner products matrix symmetric work aware consider use alternate inner products nearsymmetric young jea 9 meyer 5 latter product spd used orthomin orthodir paper organized follows section 2 shown alternative inner products may used preserve symmetry gmres section 3 considers use truncated iterative methods preconditioned system close symmetric hypothesized many authors example axelsson 2 meyer 5 section 4 consider symmetrically preconditioned bicg algorithm section 5 tests algorithms numerically using navierstokes problem parameterized reynolds number thus nearness symmetry conclude paper section 6 preconditioning gmres nearly symmetric split preconditioning may used preserve original degree symmetry alternatively leftpreconditioning minner product right preconditioning gamma1 inner product may used latter two alternatives developed arnoldi process used basis symmetric preconditioned versions gmres like cg shown symmetric versions mathematically equivalent split preconditioning require preconditioner symmetrically factored begin exploring options implementation issues associated left symmetric preconditioning 21 left symmetric preconditioning gmres algorithm based arnoldi process without preconditioning arnoldi algorithm based classical gramschmidt process follows algorithm 21 arnoldiclassical gramschmidt 1 choose vector v 1 norm 1 2 3 4 compute 5 7 h 8 9 enddo consider case leftpreconditioned ie matrix involved algorithm replaced preconditioner assume spd wish define procedure implement algorithm using minner products possible avoid additional matrixvector products eg accomplished define corresponding gmres procedure preconditioned case customary define intermediate vectors product preconditioned get reformulate operations algorithm minner product computation inner products changes classical gramschmidt formulation would first compute scalars h ij line 4 algorithm 21 would modify vector z j obtain next arnoldi vector normalization complete orthonormalization step must normalize final z j morthogonality z j versus previous v observe thus desired mnorm computed according 9 computing one potentially serious difficulty procedure inner product computed 9 may negative presence roundoff two remedies first compute mnorm explicitly expense additional matrixvector multiplication ie pointed earlier undesirable since operator often available explicitly indeed many cases preconditioning operation gamma1 available sequence operations case multigrid preconditioning another difficulty computing h ij 7 immediately amenable modified gramschmidt implementation indeed consider first step hypothetical modified gramschmidt step consists morthonormalizing z v 1 observed inner product z equal w v 1 computable need z compute modified gramschmidt process however vector available compute z v classical gramschmidt alternative save set vectors mv computed multiplying would allow us accumulate inexpensively vector z j vector via relation obtained 8 inner product z given form inner product guaranteed nonnegative desired leads following algorithm algorithm 22 arnoldiclassical gramschmidt minner products 1 choose vector w 1 v mnorm 1 2 3 4 compute 5 w noted algorithm requires save two sets vectors v j w v form needed arnoldi basis w required computing vector w j line 5 save two sets vectors also easily formulate algorithm modified gramschmidt version arnoldi procedure algorithm 23 arnoldimodified gramschmidt minner products 1 choose vector w 1 v mnorm 1 2 3 4 5 7 enddo 8 9 h 11 12 enddo 22 right symmetric preconditioning matrix gamma1 selfadjoint gamma1 inner product situation right preconditioning inner product much simpler mainly gamma1 z available z needs normalized gamma1 norm however gamma1 z normally computed next iteration standard arnoldi algorithm slight reorganization arnoldi modified gramschmidt algorithm yields following algorithm 24 arnoldimodified gramschmidt gamma1 inner products 1 choose vector v 1 gamma1 norm 1 compute w 2 3 4 5 7 enddo 8 note preconditioned vector computed line 8 standard algorithm computed line 3 vs ws need saved case additional storage ws however makes algorithm naturally flexible ie accommodates situation varies step gamma1 v result unspecified computation gamma1 constant operator basis rightpreconditioned krylov subspace cannot constructed vs alone however vectors w form basis subspace gamma1 denotes preconditioning operation jth step use extra set vectors exactly standard flexible variant gmres implemented 6 23 using minner products gmres vectors v form orthonormal basis krylov subspace following denote matrix whose column vectors vectors v produced arnoldimodified gramschmidt algorithm minner products algorithm 23 similar notation used matrix wm also denote hm m1 theta upper hessenberg matrix whose nonzero entries h ij defined algorithm hm denotes portion hm matrices satisfy number relations similar ones obtained using standard euclidean inner product proposition 21 following properties satisfied vectors v w algorithm 23 hm 2 hm 3 v 4 w 5 hermitian hm hermitian tridiagonal consider implementation gmres procedure based orthogonalization process algorithm 23 since using minner products able minimize mnorm residual vectors vectors affine subspace z first coordinate vector hm therefore equality hm result equality 12 minimize mnorm preconditioned residual vector gamma1 bgammaax simply minimizing 2norm fie 1 gamma hm standard gmres algorithm algorithm 25 leftpreconditioned gmres minner products 1 compute 2 3 4 5 7 enddo 8 9 h 11 12 enddo 13 compute minimizer kfie 1 gamma 14 compute approximate solution 15 satisfied stop else set x equality similar 12 shown rightpreconditioned case inner products summarize following theorem state without proof theorem 21 approximate solution xm obtained leftpreconditioned gmres algorithm minner products minimizes residual mnorm km gamma1 b gamma axkm vectors affine subspace x 0 km z also approximate solution xm obtained rightpreconditioned gmres algorithm gamma1 inner products minimizes residual gamma1 norm kb affine subspace 24 equivalence algorithms show left right symmetric preconditioning mathematically equivalent split preconditioning latter case must factored solve denoting b preconditioned matrix gmres procedure applied system u variable minimizes residual norm vectors u space u 0 k u note variables u x related result procedure minimizes x space x 0 k x make following observation k 0 z indeed easily proved induction hence space k x identical space nothing 13 noting proved following result theorem 22 let approximate solution obtained gmres applied split preconditioned system 14 identical obtained gmres algorithm left preconditioned system 2 using minner product statement made rightpreconditioning must noticed minimization 16 taking place minimization subspace left right split preconditioning options 7 sec 934 emphasize particular split preconditioned residual minimized three algorithms 3 truncated iterative methods truncated iterative methods alternative restarting number steps required convergence large computation storage krylov basis becomes excessive exactly symmetric threeterm recurrence governs vectors arnoldi process necessary orthogonalize current arnoldi vector previous two vectors nearly symmetric incomplete orthogonalization small number previous vectors may advantageous restarted methods advantage may offset cost maintaining extra set vectors maintain initial degree symmetry incomplete arnoldi procedure outlined stores previous k arnoldi vectors orthogonalizes new vectors differs full arnoldi procedure line 4 would normally loop 1 j considered full arnoldi procedure k set infinity algorithm 31 incomplete arnoldi procedure 1 choose vector v 1 norm 1 2 3 4 5 7 enddo 8 h 9 h 11 enddo truncated version gmres uses incomplete arnoldi procedure called quasigmres 3 practical implementation algorithm allows solution updated iteration thus called direct version dqgmres 8 suggest truncated iterative methods may effective cases near symmetry study asymptotic behavior iterates dqgmres coefficient matrix varies nonsymmetry skew symmetry first decompose symmetric skew symmetric set first establish asymptotic relations among variables incomplete full arnoldi procedures apply incomplete procedure full procedure using superscripts f distinguish variables appearing two procedures note since skew symmetric full procedure incomplete procedure k 2 moreover denote degree minimal polynomial v f 1 respect h f proof following lemma also use v j denote vectors w w f obtained end line 7 incomplete complete arnoldi procedures lemma 31 assume truncation parameter k 2 v h proof proof induction index j lines 5 6 arnoldi procedure h h hence lemma holds 1 assume lemma proved hypothesis prove arnoldi procedure h yields induction hypothesis therefore w w f line 3 arnoldi procedures using another induction index lines 5 6 induction hypothesis j mean time noting h last equation h induction step complete qed turn dqgmres algorithm consider linear system denote x g x q approximate solutions gmres dqgmres algo rithms respectively let degree minimal polynomial vector b gamma sx 0 respect result lemma stated follows theorem 31 given initial guess x 0 gmres dqgmres k 2 given step proof definitions dqgmres gmres h h h b b lemma h therefore desired equation holds qed let xa exact solution exact solution obvious since hand x g immediately following corollary corollary 31 initial guess x 0 k 2 corollary suggests may use dqgmres small k nearly symmetric nearly skew symmetric 4 symmetric preconditioning bicg bicg algorithm based lanczos biorthogonalization leftsymmetric rightsymmetric preconditioning relatively straightforward extra vectors quired reference algorithm 41 gives rightpreconditioned bicg algorithm preconditioner symmetric rightpreconditioned bicg algorithm rightpreconditioned using gamma1 inner products developed immediately afterward algorithm 41 rightpreconditioned bicg 1 compute 0 r 2 3 convergence 4 ff 5 x 6 r 7 r 8 9 11 enddo note line 7 algorithm preconditioned coefficient matrix dual system dual residual r residual leftpreconditioned version linear system develop symmetric rightpreconditioned bicg gamma1 inner products used algorithm 41 however preconditioned coefficient matrix dual system must adjoint gamma1 gamma1 inner product gamma1 shown dual system thus involves coefficient matrix gamma1 algorithm 42 gives symmetric rightpreconditioned bicg algorithm preconditioner algorithm 42 rightpreconditioned bicg gamma1 inner products 1 compute 0 gamma1 r 2 3 convergence 4 ff 5 x 6 r 7 r 8 9 11 enddo like gmres left right symmetric preconditioned versions bicg equivalent split preconditioned version shown change variables however left right symmetric preconditioned versions exact rather split preconditioned residual available unpreconditioned bicg algorithm cannot serious breakdown spd r 0 chosen r 0 r vectors ap never become orthogonal fact cosine bounded reciprocal condition number similarly symmetric rightpreconditioned version bicg spd r measure cosines rather quantities ap r vectors magnitudes going 0 algorithms progress recall case lucky breakdown case regular right leftpreconditioning r symmetrically preconditioned cases lower bounds exist algorithms liable break nearsymmetric hypothesis probability breakdown lower symmetrically preconditioned cases shown experiment next section 5 numerical experiments section 51 tests idea using symmetric preconditionings truncated iterative meth ods section 52 tests breakdown behavior symmetrically preconditioned bicg 51 truncated iterative methods test idea using symmetric preconditionings truncated iterative methods nearly symmetric systems selected standard fluid flow problem degree symmetry matrices parameterized reynolds number flow problem twodimensional squarelid driven cavity discretized galerkin finite element method rectangular elements used biquadratic basis functions velocities linear discontinuous basis functions pressure considered segregated solution method navierstokes equations velocity pressure variables solved separately matrices arising fullycoupled solution method otherwise indefinite particular considered expression conservation momentum u denotes vector velocity variables p denotes pressure variable reynolds number boundary conditions driven cavity problem unit square top edge square three sides corners reference pressure specified bottomleft corner 0 matrices initial jacobians newton iteration assuming zero pressure distribution convenience however chose righthand sides linear systems vector ones mesh 20 20 elements used leading momentum equation matrices order 3042 91204 nonzero entries nodes corresponding boundaries assembled matrix degrees freedom numbered element element reynolds number 0 matrix spd equal symmetric part matrices nonzero reynolds number generated matrices reynolds number less 10 gives rise nearly symmetric case reynolds number 1 degree symmetry measured value 75102 theta 10 gamma4 measure increases linearly reynolds number least re10 numerical experiments show number matrixvector products consumed gmresk dqgmresk reduce actual residual norm less gamma6 original residual norm zero initial guess several values k used dagger tables indicates convergence within 500 matrixvector products incomplete choleski factorization ic0 re0 problem used preconditioner problems comparison first show table 1 results using standard rightpreconditioning table 2 shows results using rightpreconditioning gamma1 inner products equiva lently split preconditioning latter case care taken assure gmres stop actual residual norm within twice tolerance dqgmres since accurate residual norm estimate available within algorithm exact residual norm computed used stopping criterion purpose com parison rightpreconditioned methods slight advantage comparison many 20 matvecs since directly minimize actual residual norm whereas symmetrically preconditioned methods minimize preconditioned residual norm 6 214 128 73 446 361 94 97 258 197 100 94 table 1 matvecs convergence rightpreconditioned methods table 2 matvecs convergence symmetric rightpreconditioned methods results table 1 show irregular performance dqgmresk small values k preconditioned system symmetric performance entirely regular table 2 preconditioned system near symmetric reynolds numbers 3 systems sufficiently symmetric dqgmres2 behaves dqgmres much larger k performance remains regular beyond reynolds number 7 number steps convergence begins become irregular like rightpreconditioned case gmres either right symmetric preconditioning show marked difference performance apparently symmetry preconditioned system essential problem however results show dqgmresk small values k may perform well terms number steps gmresk large values k particularly nearsymmetry preserved since former much efficient combination preserving symmetry truncated iterative methods may result much economical method well regular behavior shown also performed experiments orthogonal projection methods namely full orthogonalization method fom truncated variant direct incomplete orthogonalization method diom 7 results similar results shown indeed development algorithms theory identical methods interest also performed tests ilu0 preconditioner constructed matrix compared right split preconditioning nearsymmetric systems little difference results compared using ic0 constructed re0 case matrices thus deterioration performance reynolds number increases entirely due relatively less accurate preconditioner due increased nonsymmetry nonnormality matrices although eigenvalues preconditioned matrices identical eigenvectors hence degree nonnormality may change completely unfortunately difficult quantitatively relate nonnormality convergence 52 breakdown behavior bicg test breakdown behavior bicg matlab used generate random matrices order 300 approximately 50 percent normally distributed nonzero entries matrices adjusted ie symmetric part shifted lowest eigenvalue 10 gamma5 times skewsymmetric part added back parameter altered get varying degrees nonsymmetry tested 100 matrices generated smallest value cosines corresponding denominators algorithms recorded right preconditioned case recorded minimum j symmetric rightpreconditioned case recorded minimum j relative residual norm reduction 10 gamma9 iterations stopped initial guesses 0 r set r 0 ic0 symmetric part used preconditioner table 3 shows frequencies size minimum cosines rightpreconditioned first row pair rows symmetricallypreconditioned cases second row pair rows example 100 minimum cosines symmetricallypreconditioned case average number bicg steps average minimum cosine also shown last column labeled better shows number times minimum cosine higher improved algorithm table shows rightpreconditioned algorithm produce much smaller cosines indicating greater probability breakdown difference algorithms less degree nonsymmetry increased almost difference breakdown behavior algorithms table shows number bicg steps significantly reduced new algorithm average minimum cosine modified algorithm significantly increased probability small cosine encountered better important note behavior applies r 0 set r 0 r 0 chosen randomly gain symmetricallypreconditioned algorithm shown table 4 table 5 shows number steps minimum cosines two algorithms applied driven cavity problem described section 51 figure 1 shows plot minimum cosines two algorithms progress note minimum cosines higher much smoother symmetricallypreconditioned case 7 problem cosines still higher smoothness lost 6 conclusions solving linear systems matrices close symmetric paper shown possible improve upon standard practice using nonsymmetric preconditioner matrix along left rightpreconditioned iterative method original degree symmetry may maintained using symmetric preconditioner alternate inner product split preconditioning appropriate combining idea truncated iterative methods solution procedures converge quickly require less storage developed truncated methods also seem become robust truncation parameter k nearsymmetry maintained bicg algorithm also seems robust respect serious breakdown nearsymmetry maintained steps 3e6 1e5 3e5 1e4 3e4 1e3 3e3 1e2 3e2 average better 0 3177 100 187 74 table 3 frequencies minimum cosines rightpreconditioned first row pair rows symmetricallypreconditioned second row pair rows bicg steps 3e6 1e5 3e5 1e4 3e4 1e3 3e3 1e2 3e2 average better table 4 frequencies minimum cosines r chosen randomly bicg steps min cosines right symm right symm 3 73 72 202e4 907e3 table 5 steps minimum cosines driven cavity problem figure 1 minimum cosines rightpreconditioned bicg solid line symmetrically preconditioned bicg dashed line acknowledgments first third authors wish acknowledge support riacs nasa ames contract nas 213721 collaboration originated second third authors also wish acknowledge support minnesota supercomputer institute r taxonomy conjugate gradient methods conjugate gradient type methods unsymmetric inconsistent systems linear equations iterative solution method linear systems coefficient matrix symmetric mmatrix concept special inner products deriving new conjugate gradientlike solvers nonsymmetric sparse linear systems flexible innerouter preconditioned gmres algorithm iterative methods sparse linear systems dqgmres direct quasiminimal residual algorithm based incomplete orthogonalization generalized conjugategradient acceleration nonsym metrizable iterative methods tr