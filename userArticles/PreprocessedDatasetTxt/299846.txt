synthesizing efficient outofcore programs block recursive algorithms using blockcyclic data distributions abstractin paper present framework synthesizing io efficient outofcore programs block recursive algorithms fast fourier transform fft block matrix transposition algorithms framework uses algebraic representation based tensor products matrix operations programs optimized striped vitter shrivers twolevel memory model data distributed using various cyclicb distributions contrast normally used physical track distribution cyclicbd bd physical disk block size first introduce tensor bases capture semantics blockcyclic data distributions outofcore data also data access patterns outofcore data present program generation techniques tensor products matrix transposition accurately represent number parallel io operations required synthesized programs tensor products matrix transposition function tensor bases data distributions introduce algorithm determine data distribution optimizes performance synthesized programs formalize procedure synthesizing efficient outofcore programs tensor product formulas various blockcyclic distributions dynamic programming problem demonstrate effectiveness approach several examples show choice appropriate data distribution reduce number passes access outofcore data large eight times tensor product dynamic programming approach largely reduce number passes access outofcore data overall tensor product formulas b introduction due rapid increase performance processors communication networks last two decades cost memory access become main bottleneck achieving highperformance many applications modern computers including parallel computers use sophisticated memory hierarchy consisting example caches main memory disk arrays narrow gap processor memory system performance however efficient use deep memory hierarchy becoming challenging outofcore applications computational fluid dynamics seismic data processing involve large volume data task efficiently using io subsystem becomes supported nsf grant nsfiri9100681 rome labs contracts f3060294c0037 arpasisto contracts n00014 91j1985 n0001492c0182 subcontract ki92010182 z surface mail department computer science duke university box 90129 durham nc 277080129 extremely important spurred large interest various aspects outofcore applications including language support outofcore compilers parallel file systems outofcore algorithms outofcore program synthesis 2 18 7 4 program synthesis automatic program generation long history computer science 16 recent past tensor kronecker product algebra successfully used synthesize programs class block recursive algorithms various architectures vector shared memory distributed memory machines 11 9 5 memory hierarchies cache single disk systems 14 13 recently enhanced program synthesis framework multiple disk systems fixed physical track data distribution 10 captured twolevel disk model proposed vitter shriver 19 paper present framework using tensor products synthesize programs block recursive algorithms striped vitter shrivers twolevel memory model permits various blockcyclic distributions outofcore data disk array framework presented paper generalizes framework presented 10 use algebraic properties tensor products capture semantics blockcyclic data distributions cyclicb b logical block size disk array investigate implications various blockcyclic distributions cyclicb performance outof core block recursive algorithms fast fourier transform fft block matrix transposition algorithm tensor product representations block recursive algorithms may involve stride permutations since stride permutation interpreted matrix transposition synthesizing efficient outofcore programs stride permutations important 17 6 12 present procedure synthesizing efficient outofcore programs stride permutations using cyclicb distribution data algorithm determining block size b optimizes performance synthesized programs also presented discuss program generation techniques tensor products various blockcyclic data dis tributions discuss several strategies factor grouping data rearrangement improve performance tensor product formulas formalize procedure synthesizing efficient outof core programs tensor product formulas various data distributions dynamic programming problem however since data rearrangement expensive target model discussed section 7 incorporated dynamic programming approach sense stride permutations mainly understood method program synthesis matrix transpositions tensor product formulas illustrate effectiveness dynamic programming approach example outofcore fft program examine performance issues synthesized programs show 1 choice data distribution large influence performance synthesized programs 2 simple algorithm selecting appropriate data distribution size effective 3 dynamic programming approach always reduce number passes access outofcore data paper organized follows section 2 discusses formulation block recursive algorithms using tensor products matrix operations section 3 introduce twolevel computation model present semantics data distributions data access patterns also argue advantages using various blockcyclic distributions section 4 presents overview approach outofcore program synthesis generic program synthesizing outofcore programs tensor products given tensor bases also discussed section 5 presents framework synthesizing programs various blockcyclic data distributions stride permutations section 6 present approach synthesizing outof core programs tensor products section 7 presents multistep dynamic programming algorithm synthesizing programs tensor product formulas section 8 summarize performance results show effectiveness using various blockcyclic data distributions section 9 discusses related research conclusions provided section 10 appendix list selected set symbols used paper appendix b appendix c present additional details proving properties discussed section 4 section 5 respectively product algebra tensor kronecker product 8 theta n matrix mn p theta q matrix b pq block matrix obtained replacing element ij mn matrix ij b pq tensor product involving identity matrix implemented parallel operation identity matrix order n denoted n consider application vector n interpreted n copies b pq acting parallel disjoint segments x nq however interpret application mnomega p np parallel operations need understand stride permutations stride permutation l mn n mn theta mn permutation matrix application l mn n x mn results vector mn vector consisting elements set fx ij lambdan 1g one important property stride permutation l mp pqomega mn l nq q using stride permutations application mnomega p vector np also interpreted p parallel applications mn disjoint segments np using identity l mp pomega mn l np case however inputs application mn accessed stride p outputs also stored stride p general momega npomega q interpreted mq parallel applications np properties tensor products used transform tensor product representation algorithm another equivalent form take advantage parallel operations discussed example using following tensor product factorizations mnomega p mnomega q 4 aomega b implemented first applying q parallel applications parallel applications properties tensor products listed 11 1 aomega bomega aomega bomega c 2 aomega bcomega acomega bd assume ordinary multiplications ac bd defined 3 4 contrast tensor products used describe various computations tensor product vector bases called tensor basis used describe data access storage patterns multidimensional array vector basis e column vector length one position zeros elsewhere use e denote ith index onedimensional array size since inj use e represent index j twodimensional array assume rowmajor storage order multidimensional arrays memory general tensor basis e deltaomega e m1 corresponds index array indexing function needed access elements multidimensional array obtained linearizing tensor basis example linearizing tensor basis e deltaomega e m1 results vector basis e deltadeltadeltam 1 index linearized tensor basis exactly indexing function needed accessing tdimensional array rowmajor order equivalently vector basis e factorized tensor product vector bases deltaomega e m1 3 factorization vector basis corresponds viewing onedimensional array multidimensional array using tensor bases semantics stride permutation l mn n formally expressed l mn corresponds matrix transposition theta n array stored rowmajor order appropriately factorizing vector basis input vector use resulting tensor basis describe data access pattern tensor product example tensor product momega pnomega q input vector basis e mnq factorized obtain input tensor basis e j known operator basis output vector basis e mnq factorized obtain output tensor basis e also determined following identity momega pnomega q e iomega pn e n jomega q e q iomega pn e n replacing pn e n j e p using input output bases determine input output data elements application pn derive program twodimensional iteration space ignore dimensions matrices whenever clear context code kth application pn indices input data elements kth application obtained linearized input tensor basis e mnq inqqjk finq ng similarly output indices determined linearized output tensor basis fipq pg note loops corresponding indices j j 0 program 21 tensor product formulation block recursive algorithms tensor product formulation block recursive algorithm following generic form jomega v jomega c j v j v j theta v j linear transformation 6 definition stride permutation understood special case 1 theta 1 matrices v j permutation matrix corresponding stride permutation identity terms r j allow decomposition computation set smaller size computations may computed main memory similarly identity terms c j allow decomposition computation set subcomputations subcomputation accesses data storage stride fashion although parallel stride computational structures help decomposing computation smaller incore computations task combining decompositions goal minimizing io entire computation challenging problem next present example illustrate use tensor product formulas represent cooleytukey fft algorithm fast fourier transform fourier transform denoted following matrix vector multiplications fn n theta n discrete fourier matrix fn n nth primitive root unity rs discrete fourier matrix fn factorized follows 11 15 romega rs rs called twiddle factors defined rs using factorization recursively obtain following tensor product representation cooleytukey fft algorithm figure 1 data organization 4 column disk box physical block row consists physical track numbers box denote record indices 2 igamma1 diagonal matrix constants r 2 n permutes input sequence bitreversed order ignore initial bitreversal operation r 2 n notice 2 2 igamma1 diagonal matrix see computational structure f 2 n captured first factor 2 2omega 2 igamma1 easily verified major computational portion example formula 6 3 parallel io model blockcyclic data distributions use twolevel model similar vitter shrivers twolevel memory model 19 however model data disks called outofcore data distributed different logical block sizes model consists processor internal random access memory set disks storage capacity disk assumed infinite disk data organized physical block fixed size four parameters n size input size internal memory b size physical block number disks used model assume n bd model disk io occurs physical tracks defined size b physical blocks relative positions disk constitute physical track physical tracks numbered contiguously outermost track lowest address innermost track highest address ith physical track denoted fig 1 shows example data layout b parallel io operation simultaneously access physical blocks one block disk therefore parallelism data access two levels elements one physical block transferred concurrently physical blocks transferred one io operation paper use striped disk access model physical blocks one io operation come track opposed independent io model block come different tracks use parallel primitives parallel readi parallel writei denote read write physical track respectively define measure io performance number parallel ios required 31 blockcyclic data distributions blockcyclic distributions used distributing arrays among processors multiprocessor sys tem blockcyclic distribution partitions array equal sized block consecutive elements maps onto processors cyclic manner regard disks model processors 43 44 48 figure 2 data organization 8 column disk first left shadowed box denotes example logical block two logical tracks lt 0 lt 1 consists two physical tracks data organization described eg fig 1 exactly blockcyclic distribution denoted block size b moreover assume data distributed arbitrary block size 2 fig 2 shows data organization parameters fig 1 cyclic8 distribution notice size physical track size physical block changed however contain different records call b records block formed cyclicb distribution logical block similarly logical blocks relative positions disk consist logical track ith logical track denoted lt note parallel io operation still accesses physical track logical track hence several parallel io operations needed access logical track example load logical track lt 1 fig 2 two parallel read operations parallel read2 parallel read3 respectively load physical tracks 2 3 needed next use simple example show advantages using logical distributions developing ioefficient programs block recursive algorithms logical data distributions assume want implement f 8 target model parameters given fig 1 assume size main memory half size inputs mainly interested data access patterns ignore real computations conducted f 8 thing need remember f 8 needs eight elements stride eight existence identity matrix 8 first consider implementing f 8 physical block distribution discussion know first f 8 needs applied eight elements 0 8 16 24 28 32 40 48 56 fig 1 see elements required f 8 computation stored four physical tracks however main memory hold two physical tracks simply load four physical tracks main memory accomplish computation one pass io get around memory limitation use two different approaches first load first physical track keep first half records physical block loaded physical track throw half records every physical track computation half records main memory finishing computation half records write results repeat procedure however keep half 2 cormen called data organization disks banded data layout 3 studied performance class permutations several basic primitives nesl language1 records main memory loaded track computation way obviously need two passes load outofcore data another method use logical block distribution suppose size logical block eight shown fig 2 eight records required one f 8 stored two physical tracks physical track one three physical track two four therefore load physical tracks one three first computation load physical track two four computation finish computation one pass example clearly shows advantages using logical distributions comparing using physical track distributions however several problems addressed determine block size logical distribution determine data access patterns discuss issues rest paper note striped io model data distribution cyclicb transformed model 1 however transformation may true arbitrary cyclicb distribution therefore normally reduce problem simpler case 1 simplicity make following assumptions input output data stored separate set disks parameters power two 3 block size b distribution multiple b 32 semantics data distributions access patterns discussed 9 blockcyclic distribution algebraically represented tensor basis approach adopted disk model substituting disks processors however existence physical blocks physical tracks tensor basis used define blockcyclic distribution multiprocessors needs factorized call factorized tensor basis outofcore data distribution basis defined follows definition 31 vector n distributed according cyclicb distribution disks data distribution basis defined gomega e bomega e bd b use ds refer sth factor left eg example data distribution basis figure 2 e 2 b data distribution basis figure 1 written e 4 b selected portion distribution basis formula 10 used obtain indexing function needed denote particular data unit logical track physical track let 3 results easily generalized parameters power integer figure 3 example data organization access pattern logical track consists two physical tracks physical block decomposed two subblocks shadowed subblocks together used form data access pattern one example access shadowed subblocks rowmajor order indexing function accessing physical tracks obtained linearizing tensor basis obtained physicaltrackbasisd taking difference 4 data distribution basis tensor bases denote records inside logical track physical track respectively tensor bases called logical trackelement basis e bomega e bd b physical trackelement basis e domega e bd b respectively concept extended denote subsets data discuss later hand different orders instantiating indices data distribution basis defined formula 10 result different access patterns outofcore data example instantiate indices formula order left right ie g slowest b fastest changing index actually access data first first logical block first disk access first logical block second disk finishing access first logical track sequentially second logical track accessed obvious order indices data distribution basis instantiated differently outofcore data accessed different pattern example instantiate index b b e b b index e formula 10 results access pattern first data along physical track accessed successive physical tracks accessed change instantiation order indices regarded permutation 5 data distribution basis call permutation data distribution basis loop basis synthesized programs every index loop basis may used generate loop nest order loop nests determined order vector bases loop basis generally outofcore data viewed organized multidimensional structure example data layouts fig 2 viewed threedimensional array logical block viewed b b theta b matrix convert b twodimensional structure combine records disks together submatrix views denoted factorizing regrouping data distribution bases used form different data access patterns fig 3 shows example data organization access pattern assume logical 4 let g two tensor bases difference denoted sg tensor basis constructed deleting vector bases g 5 let tensor basis omega q let ff permutation permutation tensor basis defined follows ffs omega q track consists two physical tracks decompose physical block b two subblocks size b d1 order reflect data organization factorize data distribution basis follows gomega e b permute factorized data distribution basis loop basis accesses subsets data different patterns assume want access data fig 3 following order first accessing darker shadowed subblocks rowmajor order accessing lighter shadowed subblocks rowmajor order move d3 d4 d2 d1 respectively results following loop basis b bomega e verify correctness loop basis follows 1 index b d2 chooses shadowed subblocks 2 index g chooses logical tracks 3 index b b chooses logical blocks 4 index chooses disks 5 index b d1 chooses records inside subblock since increasing indices reversed order five steps first access records inside first darker shadowed subblock first disk access records shadowed subblock second disk accessing records darker shadowed subblocks rowmajor order repeat procedure lighter shadowed subblocks 4 overview program synthesis efficient implementations block recursive algorithms obtained using properties tensor products transform tensor product representations block recursive algorithms transformations use results performance individual tensor product performance tensor product obtained using method presented section 6 fig 4 shows procedure synthesizing efficient outofcore programs block recursive algorithms figure augmented tensor basis contains following information data distribution bases loop bases subcomputations memoryloads necessary code generation explained section input tensor product formula consists stride permutation quickly pass program transformation step use algorithm presented section 4 generate augmented tensor basis tensor product formulas successive steps applied tensor product transformed tensor product formulas obtain augmented tensor basis tensor product use method presented section 6 code generation step tensor products stride permutations use procedure presented section presentation deriving efficient implementations block recursive algorithms reversed order fig 4 first present procedure code generation using information contained target machine model tensor product formula tensor product formula data distribution augmented tensor basis code generation parallel io program program transformation computation partitioning access pattern analysis figure 4 procedure synthesizing efficient outofcore programs block recursive algorithms augmented tensor basis determine efficient implementations stride permutation simple tensor product given data distribution given model determining corresponding augmented tensor bases develop algorithm determine data distribution result efficient implementation simple tensor product using information obtained far use dynamic multistep dynamic programming algorithm determine efficient implementation block recursive algorithms rest section summarize synthesize efficient programs simple tensor products stride permutations target machine model details performance synthesized programs discussed next two sections minimize number io operations synthesized program need exploit locality reusing loaded data requires decomposing computation reorganizing data data access patterns maximize data reuse synthesized program subcomputation performed several times different data sets hence loop structure synthesized programs constructed follows outer loop nest enclosing three inner loop nests read loop nest reading data computation loop nest performing subcomputation loaded data write loop nest writing output data back disk inner read loop nest load outofcore data without overflowing main memory refer data sets memoryload inner computation loop nest perform subcomputation memoryload data sets accessed using parallel primitives parallel read parallel write load store physical track time one main results paper efficient tensor product decomposition computation generate loops indices n generate loops indices parallel read using input distribution basis construct memoryload end loops corresponding perform operations memoryload generate loops indices parallel write using output distribution basis end loops corresponding end loops corresponding n figure 5 procedure code generation tensor product efficient data access pattern obtained using algebraic properties data distribution bases loop bases words determined data distribution bases loop bases determine memoryloads subcomputations operations memoryload tensor product computation input output data may organized accessed differently therefore use input data distribution basis fi output data distribution basis ffi input loop basis output loop basis denote respectively data distribution bases obtained input output bases rewriting form formula 10 however nontrivial task determine loop bases goal minimizing number io operations next present generic synthesized program summarize general ideas determining loop bases discuss details determine loop bases therefore memoryloads operations memoryload next two sections consider task generating target code assuming data distribution bases loop bases memoryloads operations memoryload already determined discussed previous section striped io model io operation read store records physical track time hence part loop basis explicitly appear synthesized programs moreover input output loop basis separated two parts first part specifies memoryloads second part specifies records inside memoryload second part separated two parts one part denoted used construct memoryload another part denoted generate loop nests synthesized programs words write input output bases follows call n n memory basis since instantiation indices n corresponds memoryload using loop bases generic program obtained described fig 5 notice parallel read track track number obtained indexing function physicaltrackbasisfi part input data distribution basis defined formula 12 parallel write fig 6 shows example synthesized program 4 assume f 2 2 theta 2 matrix data distributed cyclic2 manner uses e 8 b input output distribution bases input output loop bases also e 2 b g1 factorization e 8 let us examine formulas 15 16 obviously consist physical track element bases input output data respectively outofcore data needs accessed terms memoryloads memoryload following properties input memory load occupies locations set physical tracks specified input data distribution basis computing records main memory organized occupy locations set physical tracks specified output data distribution basis call type memoryload perfect memoryload construct memoryloads manner synthesize program accesses outofcore data called onepass program however may possible construct perfect memoryloads computations case may need keep part records loaded physical track main memory discard records therefore multipass program needs synthesized physical track loaded several times terms tensor bases corresponds moving vector bases physical track basis memory basis consider example presented section 32 moved fi4 first factor since unit data access still physical track moving corresponds loading track bd times however loaded track half records kept main memory assume size main memory half input size records kept loaded track determined vector bases moved memoryload basis case e instantiation index b d2 determines subblocks physical track kept current memoryload detailed program loading outofcore data constructing memoryloads shown enddo enddo enddo enddo enddo temporary array holding physical track x holds memoryload summary order determine efficient loop bases construct initial loop bases consists physical trackelement bases input output data distribution bases respectively determine vector bases need moved n moved vector bases used determine portions physical block kept current memoryload size moved vector bases equal number times physical tracks loaded may need determine order rest vector bases gamma reflect order accessing physical tracks parallel read track enddo perform operations memory load write result back parallel write track parallel enddo enddo figure code tensor product 2omega 4 x array size 4 synthesizing programs stride permutations section present framework synthesizing efficient outofcore programs stride permutations using cyclicb distribution performance synthesized programs represented function size subtensor basis whose value obtained distribution size given also present algorithm determine distribution optimize performance 51 stride permutations cyclicb distribution mentioned goal decompose computations sequence subcomputations operated perfect memoryloads however may always possible limited memory size case minimize number times data loaded memoryload well ensure physical track output written parallel develop approach determine input output loop bases given distribution cyclicb based loop bases data distribution bases determine memoryloads operations memoryloads following program synthesized using procedure presented section 4 cost program also determined loop bases summarize results following theorem present constructive proof theorem 51 let input output vectors length n respectively let x distributed according cyclicb data distribution bases denoted fi ffi respectively denote fi2omega fi4 ffi2omega ffi4 program synthesized n operations stride permutation proof present algorithm shown fig 7 determining input output loop bases algorithm explained step 1 shown step 2 step 3 show construct 6 notation j j denotes size tensor basis equal multiplication dimensions vector basis initialization fi1omega fi3omega fi2omega fi4 onepass multipass implementation j else consists last factors factorized tensor basis bdd final input output loop bases figure 7 algorithm determining input output loop bases memoryloads operations memoryload step 4 show io costs obtained information 1 determine input output loop bases begin following construction input output loop bases use convention appearing right hand side refers original representation equal fi1omega fi3omega fi2omega fi4 appearing left hand side refers update assume ffi2omega ffi4 easy verify gamma permutation therefore denote records thus number records denoted j size main memory simply take however number records denoted j may exceed size main memory case want construct memoryloads obtained reading input data several times however writing output data terms tensor bases discussed section 4 reloading achieved looping part indices words need factorize 2 1 2 denotes subblocks kept loaded physical track 1 denotes records inside subblock j 2 j equal number times reload physical track reloading achieved taking moving 2 summary input output loop bases formulas 17 18 modified follows consists last factors factorized tensor basis size equal bdd ffl input loop basis let 2 therefore input output loop bases written verify following facts first contain vector bases however different order proof presented appendix b therefore denote records however different order second previous results j therefore records denoted fit memoryload third j dbd means loaded records fit main memory need discard records details determining records discarded discussed next step 4 n contain vector bases therefore set mn change order writing results onto physical tracks 2 determine memoryload j therefore records denoted used form perfect memoryload however condition satisfied need use formula 19 20 input output loop bases respectively j size memoryload set equal size main memory however mentioned need discard records loaded track form memoryload done linearize 2 instantiation 2 give set subblocks physical track kept 3 determine operations memoryload mentioned memory load tensor vectors input output loop bases denote records inside memoryload different order words one permutation another input output loop bases permutations input output data distribution bases actually permute memoryload data time therefore inmemory operation nothing permutation subset data distribution bases denoted momega 4 io cost synthesized programs readily see j program synthesized ie number parallel ios 2n bdd condition hold keep j records loaded physical track load physical track j 2 j times moreover since dbd easily determined j 2 write record number parallel io operations 1 bdd combining two cases together yield performance results presented theorem program performance synthesized using procedure listed fig 5 52 determining efficient data distributions previous subsection presented approach synthesizing efficient io programs given data distribution present algorithm determine data distribution optimizes performance synthesized program idea algorithm follows begin physical track distribution cyclicb ie initially onepass algorithm distribution b desired block size data distribution otherwise double value b performance synthesized program distribution increases continue procedure otherwise algorithm stops current block size desired size data distributions formalize idea fig 8 number ios using cyclicb cost 6 2n dbd b n number ios using cyclicb c new cost cost c new else break output distribution size b2 number figure 8 algorithm computing desired size data distributions 6 synthesizing programs tensor products tensor product romega vomega c main computation matrix v needs v records stride c call v records desired record first present possible general form input output loop bases given distribution cyclicb parameters form determined analyzing relative values parameters based loop bases also data distribution bases determine memoryloads operations memoryload therefore program generated using procedure discussed section 4 cost program also determined loop bases since tensor product romega vomega c change order inputs computed inplace use input output data distribution bases input output data also input output loop bases programs synthesized section therefore consider input input distribution input loop bases assume output bases input bases respectively summarize results theorem present constructive proof theorem 62 let input data distributed according cyclicb input data distribution basis denoted let fi2omega fi4 assume 1 denotes subset moved memory basis tensor product romega vomega c rv program synthesized 2n bdd parallel io operations otherwise program synthesized j 2 j 3n bdd parallel io operations proof 1 determine input loop basis desired records v computation stored physical tracks bdd simply load tracks parallel therefore onepass program generated however bdd keep records tracks main memory take simple approach keep many possible records follow desired records track main memory reload tracks finish computations records terms tensor basis need nothing factorizing permuting input data distribution basis reflect data access patterns specifically begin 2omega 4 defined initial value defined section 5 onepass program factorize permute nomega change order accessing physical tracks however multipass program need factorize permute since need keep part records loaded main memory discard records part records kept discarded denoted subset vector bases physical track basis one example factorizing permuting semantics discussed chapter 4 however general order factorize permute tensor basis desired form need examine relative values parameters targeted io model tensor product size b data distribution space limitation present major idea analysis appendix c following analysis enough say found subsets denoted 1 2 2 moved memory basis generate loop nests data access 2 determine memoryload onepass program simply factorize nomega bdd multipass program nomega j vector bases 2 appear n moreover multipass program discussed section 5 use 2 determine records kept current memoryload 3 determine operations memoryload original tensor product regarded r parallel applications v inputs stride c data distributed among disks loaded units physical tracks net effect possibly reduce stride v access main memory operations memoryload general form zomega vomega z however value z depend relative values parameters appendix c presents major ideas determine value z 4 io cost synthesized programs onepass program move vector bases number parallel ios simply equal 2n bdd words synthesized program optimal terms number ios multipass program need read inputs j 2 j times therefore number parallel io operations j 2 j 3n bdd constant 3 explained follows store physical track need read physical track main memory since part records physical track discarded reloading physical track reassemble physical track part updated records write parallel otherwise part records written physical track may correct reassembling physical track needs use tensor basis 2 notice 2 equal 2 put updated records correct locations physical track similar use 2 take subblocks loaded physical track current memoryload program performance discussed synthesized using procedure listed fig 5 however accurate synthesizing multipass program need incorporate idea reassembling physical track writeout part procedure listed fig 5 discussed nothing using linearization 2 put subblocks current memoryload correct locations reloaded physical track7 synthesizing programs tensor product formulas section discuss techniques program synthesis tensor product formulas several strategies developing diskefficient programs exploiting locality exploiting parallelism accessing data similar ideas discussed 13 use factor grouping exploit locality data rearrangement reduce cost io operations also presented greedy method uses factor grouping improve performance striped vitter shrivers twolevel memory model fixed block size data distribution 10 factor grouping combines contiguous tensor products tensor product formula together therefore reduces number passes access secondary storage consider core cooleytukey fft computation computation represented formula 9 ignoring initial bitreversal twiddle factor operations i2 3 following tensor products 2 2omega 2 2 respectively assuming tensor products implemented optimally number parallel io operations required implement two steps individually 4n db however successive tensor products formula 6 hence using properties tensor products combined one tensor product 2 2omega 2 may also implementable optimally using 2n dbd parallel io operations data rearrangement uses properties tensor products change data access pattern example tensor product romega c transformed equivalent form c best case number parallel ios required 6n dbd using transformation since least three passes needed transformed form extra passes introduced transformation profitable use targeted machine model first last terms transformed formula may implementable optimally therefore incorporated transformation current optimization procedures minimizing io cost using dynamic programming since factor grouping shown size data distribution shown next section large influence performance synthesized programs take following approach determining optimal manner tensor product formula implemented use algorithm determining optimal data distribution presented fig 8 main routine however cyclicb data distribution use dynamic programming algorithm determine optimal factor groupings hence also call method multistep dynamic programming method let ci j optimal cost minimum number io passes required access outofcore data computing j gamma tensor factors ith factor jth factor tensor product formula ci j computed follows ae formula c 0 denotes cost computing tensor product method determining cost tensor product discussed section 6 values c 0 different cases found table 3 table 4 presented section 82 special case needs explained assume cj use ci k represent cost grouping tensor product factors j together grouped tensor product simple tensor product value ci k case also determined using table 3 table 4 presented section 82 however case size grouped operations larger size main memory dont want group k gamma factors together assign large value 1 ck j avoid selected performance results 81 performance synthesized programs matrix transposition given flexibility choosing different data distributions synthesize programs better performance obtained using fixed size data distributions stride permutations present set experimental results number io operations required cyclicb distribution distribution size b distribution varies results summarized table 1 table 2 tables see number passes monotonically increasing decreasing function however normally decreases increases b increased therefore algorithm fig 8 good chance find efficient size data distributions also notice stride permutation always find distribution implement computation onepass table 1 number io passes required performing stride permutation l pq using various cyclicb distributions table 2 number io passes required performing stride permutation l pq using various cyclicb distributions 82 performance synthesized programs tensor products number io passes required synthesized programs tensor product summarized table 3 table 4 going various cases using approach presented appendix c verify results presented comprehensive results presented 10 cases using approach presented section 51 actually synthesize programs better performance example 10 program v bdd passes synthesized however conditions c b v c assume results table 3 synthesize program v c passes less v bdd table 3 number io passes required tensor product romega vomega c using cyclicb distribution bdd maximum number physical tracks memoryload table 4 number io passes required tensor product romega vomega c given size b data distributions n bdd maximum number physical tracks memoryload show using appropriate cyclicb data distribution better performance program synthesized cases several typical examples shown table 5 notice increase b reduce number passes data access cases decrease number passes large eight times values table also suggest use algorithm presented fig 8 find efficient size data distributions given tensor product also notice cases c b improve performance reason stride required v less physical block size reduce redistribution table 5 number io passes required various sizes data distributions tensor product romega vomega c let let size n rv c input vector large 83 performance synthesized programs tensor product formulas show effectiveness multistep dynamic programming method comparing programs synthesized programs synthesized greedy method dynamic programming method applied data distribution fixed size respectively example use core cooleytukey fft computation results several typical sizes inputs shown table 6 find using dynamic programming fixed size cyclicb distribution normally improve performance greedy method however using multistep dynamic programming reduce number passes synthesized programs least 1 n significantly large input size large performance gain reducing even one pass access outofcore data significant greedy dp mdp 4 table number io passes synthesized programs using greedy dynamic programming dp multistep dynamic programmingmdp methods 9 related research tensor product algebra successfully used synthesizing programs block recursive algorithms various architectures vector shared memory distributed memory machines 11 9 5 recently tensor product framework used synthesizing programs memory hierarchies example method program synthesis single disk system discussed 13 however addressed issues data distributions multiple disk systems 14 kumar huang sadayappan johnson discussed method program synthesis cache memory addressed issue data layouts setassociated cache 10 presented framework using tensor products synthesizing efficient programs deeper level memory hierarchy modeled vitter shrivers twolevel memory model however considered data distributions fixed physical track distributions programs synthesized also efficient programs synthesized approach presented paper also many recent research efforts areas io intensive applications include outofcore algorithms languages compilers parallel file systems performance models 2 example vitter shriver proved lower upper bounds matrix transposition fft graph computations twolevel memory model 19 cormen presented algorithms bmmc permutations 3 includes stride permutations subclass twolevel memory model conclusions presented novel framework synthesizing outofcore programs block recursive algorithms using algebraic properties tensor products use striped vitter shrivers two level memory model target machine model however instead using simpler physical track distribution normally used model use various blockcyclic distributions supported high performance fortran organize data disks moreover use tensor bases tool capture semantics data distributions data access patterns show using algebraic properties tensor products decompose computations arrange data access patterns generate outofcore programs automatically demonstrate importance choosing appropriate data distribution efficient outofcore implementations set experiments experimental results also shows simple algorithm choosing efficient data distribution effective observations importance data distributions factor grouping tensor products propose dynamic programming approach determine efficient data distribution factor grouping example fft computation dynamic programming approach reduce number io passes least one comparing using simpler greedy algorithm acknowledgements thank peter mills comments paper r vector models dataparallel computing virtual memory dataparallel computing integrating theory practice parallel file systems extent portable programming environment designing implementing high performance block recursive algorithms fast computer method matrix transposing parallel io systems interfaces parallel computers kronecker products matrix calculus applications synthesizing communicationefficient distributedmemory parallel programs block recursive algorithms generating efficient programs twolevel memories tensor products methodology designing efficient transposition algorithms large matrices methodology generating efficient diskbased algorithms tensor product formulas algebraic approach cache memory characterization block recursive algorithms computational frameworks fast fourier transform parallel algorithm derivation program transformation parallel processing perfect shuffle compilation outofcore data parallel programs distributed memory machines algorithms parallel memory twolevel memories tr