experimental comparison nearestneighbor nearesthyperrectangle algorithms algorithms based nested generalized exemplar nge theory salzberg 1991 classify new data points computing distance nearest generalized exemplar ie either point axisparallel rectangle combine distancebased character nearest neighbor nn classifiers axisparallel rectangle representation employed many rulelearning systems implementation nge compared knearest neighbor knn algorithm 11 domains found significantly inferior knn 9 several modifications nge studied understand cause poor performance show performance substantially improved preventing nge creating overlapping rectangles still allowing complete nesting rectangles performance improved modifying distance metric allow weights features salzberg 1991 best results obtained study weights computed using mutual information features output class best version nge developed batch algorithm bnge fwmi usertunable parameters bnge fwmis performance comparable firstnearest neighbor algorithm also incorporating feature weights however knearest neighbor algorithm still significantly superior bnge fwmi 7 11 domains inferior 2 conclude even improvements nge approach sensitive shape decision boundaries classification problems domains decision boundaries axisparallel nge approach produce excellent generalization interpretable hypotheses domains tested nge algorithms require much less memory store generalized exemplars required nn algorithms b introduction salzberg 1991 describes family learning algorithms based nested generalized exemplars nge nge exemplar single training example generalized exemplar axisparallel hyperrectangle may cover several training examples hyperrectangles may overlap nest nge algorithm grows hyperrectangles incrementally training examples processed generalized exemplars learned test example classified computing euclidean distance example generalized exemplars example contained inside generalized exemplar distance wettschereck tg dietterich generalized exemplar zero class nearest generalized exemplar output predicted class test example nge approach viewed hybrid nearest neighbor methods propositional horn clause rules like nearest neighbor methods euclidean distance applied match test examples training examples like horn clause rules training examples generalized axisparallel hyperrectangles salzberg reported promising classification results three domains however report nge tested 11 additional domains gives less accurate predictions many domains compared knearest neighbor knn algorithm goal paper demonstrate performance understand causes test algorithm modifications might improve nges performance first part paper devoted study compares nge knn 1 compare algorithms fairly necessary find optimal settings various parameters options nge knn hence first describe series experiments study performance nge lesser extent knn determined several key parameters options including number starting seeds treatment ungeneralized exemplars treatment nominal feature values present results showing nge best parameter settings substantially inferior knn 9 11 domains tested superior knn 2 domains second part paper attempts diagnose repair causes performance deficit present several hypotheses including inappropriateness nested hyperrectangle bias b inappropriateness overlapping hyperrectangle bias c poor performance search algorithm heuristics constructing hyperrectangles experiments presented test hypotheses version nge called nonge disallows overlapping rectangles retaining nested rectangles search procedure uniformly superior nge 11 domains significantly better 6 batch algorithm obnge incorporates improved search algorithm disallows nested rectangles still permits overlapping rectangles superior nge one domain worse two experiments lead us conclude major source problems nge creation overlapping rectangles also present batch version nonge called bnge efficient requires user tuning parameters recommend bnge employed domains batch learning appropriate third part paper takes issue learning feature weights weighted euclidean distance salzberg 1991 proposed online weight adjustment algorithm data presented showing algorithm often performs poorly erratically alternative feature weight algorithm based mutual information shown work well nn nge bnge final part paper compares best version nge bnge fwmi best version knn knn cv fwmi comparison shows despite improvements nge still significantly inferior knn 7 domains significantly superior 2 domains compared single nearest neighbor nn best version nge fares better significantly superior 3 domains significantly inferior 4 ideal behavior hybrid algorithm like nge domains axisparallel rectangles appropriate nge take advantage find concise interpretable representations learned knowledge however domains axisparallel rectangles appropriate nge behave like nearest neighbor algorithm versions nge developed take advantage hyperrectangles perform poorly domains hyperrectangles inappropriate research needed develop ngelike algorithm robust situations axisparallel hyperrectangles inappropriate 2 algorithms experimental methods 21 nge algorithm figure 1 summarizes nge algorithm following closely salzbergs definition nge nge constructs hyperrectangles processing training examples one time initialized randomly selecting userdefined number seed training examples constructing trivial point hyperrectangles seed new training example first classified according existing set hyperrectangles computing distance example hyperrectangle class nearest hyperrectangle training example coincide nearest hyperrectangle extended include training example otherwise second nearest hyperrectangle tried called second match heuristic first second nearest hyperrectangles different classes training example training example stored new trivial hyperrectangle query classified according class nearest hyperrectangle distances computed follows example lies outside existing hyperrectangles weighted euclidean distance computed example falls inside hyperrect angle distance hyperrectangle zero example equidistant several hyperrectangles smallest chosen implementation nge first make pass training examples normalize values feature interval 01 linear normalization aha 1990 features values test set normalized scaling factors note may fall outside 01 range aside scaling pass basic algorithm entirely incremental hyperrectangle h j labeled output class hyperrectangle represented lower left corner h j lower upper right corner h j upper distance h j example e features f 1 f nfeatures 4 wettschereck tg dietterich 1 build nge classifier input number seeds 2 initialization assume training examples given random order 3 first training examples e call createhyperrectanglee 4 training 5 remaining training example e 6 find two h j deh j minimal 7 case ties choose two h j minimal area 8 call hyperrectangles h closest h second closest 9 compareh closest e generalizeh closest e 10 else compareh second closest e generalizeh second closest e 11 else createhyperrectanglee 12 compare classes hyperrectangle example 13 comparehe 14 classe classh return true else return false 15 generalize hyperrectangle 17 features e 19 h lowerf 20 replmissfeaturesh e 21 create hyperrectangle 22 createhyperrectanglee 23 24 h lower 26 replmissfeaturesh e 27 replace missing features hyperrectangle 28 replmissfeatureshe 29 features e 30 feature e missing 33 classification test example 34 classifye 35 output classh j 36 case ties choose h j ties minimal area figure 1 pseudocode describing construction nge classifier classification test examples h generally denotes hyperrectangle e example defined follows w lowerf lowerf weight feature see section weight hyperrectangle j computed number times compareh j called number times compareh j returned true original nge algorithm designed continuous features discrete symbolic features require modification distance area computation nge adopted nge policy symbolic discrete feature set covered feature values stored hyperrectangle analogous storing range feature values continuous features hyperrectangle covers certain feature value value member covered set hyperrectangle generalized include missing discrete symbolic feature flag set corresponding feature hyperrectangle cover feature value future area nontrivial hyperrectangles computed follows 2 nfeatures computed follows 37 h f generalized include missing feature 38 else f continuous 39 h upperf h lowerf 40 else 41 else f discrete symbolic feature 42 number values f covered h number possible values f note maximum possible size hyperrectangle therefore 1 fur thermore probability line 39 executed low since unlikely two continuous feature values match exactly therefore deemed unnecessary adjust area hyperrectangles case original nge paper also specify policy handling examples containing missing features context nearest neighbor algorithms aha 1990 section 521 evaluated three methods distance computation missing fea tures adopted ignore method one simplest methods dealing missing features feature example missing distance feature 0 furthermore total distance features divided number known features distinguish perfect match missing feature distance 0 incorporated methodology generalization procedure nge follows whenever hyperrectangle nge extended include example missing features range hyperrectangle extended missing feature cover entire input space feature see lines 20 26 2732 figure 1 6 wettschereck tg dietterich 22 nearest neighbor algorithm one venerable algorithms machine learning nearest neighbor algorithm nn see dasarathy 1991 survey literature entire training set stored memory classify new example euclidean distance possibly weighted computed example stored training example new example assigned class nearest neighboring example generally k nearest neighbors computed new example assigned class frequent among k neighbors denote knn aha 1990 describes several spaceefficient variations nearestneighbor algorithms nge adopted ahas ignore method handling training examples missing features reader note performance nge nn may change substantially different missingvalues policy used 23 data sets study employed eleven data sets three synthetic remaining eight drawn ucirvine repository murphy aha 1994 aha 1990 machine learning databases synthetic data sets constructed test sensitivity nge shape decision boundaries number classes figure 2 tasks c axisparallel decision boundaries task b diagonal decision boundary tasks b 2class problems task c 10 classes figure 2 artificial data sets b gamma indicates location positive negative example c digits indicate locations examples class number examples decision region shown lower left corner region eight irvine data sets summarized table 1 important points note waveform40 domain identical waveform21 domain addition 19 irrelevant features random values b cleveland database detrano et al 1989 contains missing features c many input features hungarian database detrano et al 1989 voting record database missing 24 experimental methods measure performance nge nearest neighbor algorithms employed training settest set methodology data set randomly partitioned training set containing approximately 70 patterns test set containing remaining patterns see also table 1 training training set percentage correct classifications test set measured procedure repeated total 25 times reduce statistical variation experiment algorithms compared trained tested identical data sets ensure differences performance due entirely algo rithms generate learning curves follow procedure except subset training set used test set along learning curve constant larger training set contained smaller ones report average percentage correct classifications standard error twotailed paired ttests conducted determine level significance one algorithm outperforms conclude one algorithm significantly outperforms another algorithm pvalues obtained ttest smaller 005 table 1 domain characteristics modified aha 1990 domain training test number kind number set size set size features classes iris 105 display 200 500 hungarian 206 voting 305 letter recognition 16000 4000 3 experiments parameter sensitivity explored sensitivity nge knn userspecified parameters nge parameters interest number starting seeds b treatment ungeneralized exemplars c order presentation examples knn parameter interest number nearest neighbors k 31 number starting seeds figure 3 shows performance nge tasks b c several different numbers starting seeds performance shown relative performance simple nearest neighbor tasks b number classes small nges performance particularly poor small numbers seeds 8 wettschereck tg dietterich contradicts salzbergs findings 1991 page 257 first paragraph states performance nge found sensitive size seed set number seeds ngegammann task c ngegammann task ngegammann task btheta performance relative nns figure 3 performance nge relative nn nge initialized varying numbers seeds cv leaveoneout crossvalidation base performance nn 976 correct task 970 task b 824 task c datapoints represent means 25 replications 350 training examples 150 test examples see table a1 appendix detailed numbers figure 3 also shows surprisingly nge performs better tasks c decisionboundaries axisparallel task b boundary diagonal task b simple nn outperforms nge right end figure label cv show performance obtained leaveoneout crossvalidation weiss kulikowski 1991 employed determine optimal number seeds strategy worked well adopted subsequent experiments unless otherwise noted 3 following number seeds tested leaveoneout crossvalidation run 3 5 7 10 15 20 25 crossvalidation inherently nonincremental cost using crossvalidation destroys incremental nature nge note nge given sufficiently large number seeds algorithm becomes simple nearestneighbor algorithm limit one seed every data point limit reached three tasks however nge needed approximately 6 task 13 task b 28 task c storage required nn store entire training set detailed numbers provided table a1 appendix 32 treatment ungeneralized exemplars nge hyperrectangles initialized points input space therefore size 0 generalized nontrivial hyperrectangles see pseudocode figure 1 lines 7 25 found led7 domain however initialization size hyperrectangles 1 led significant performance improvement 430sigma14 correct 598sigma10 artifact led7 domain specifically results fact led7 large numbers training examples identical feature vectors belonging different classes initial size hyperrectangles effect nges performance domains experiments reported remainder paper chose initialize size hyperrectangles 0 except led7 domain initialized size 1 table 2 performance nge one specific trainingtest set partition numbers shown indicate performance nge run 25 random permutations training set domain mean median min max iris 918sigma08 933 844 978 hungarian 780sigma05 773 716 830 voting 933sigma04 931 892 962 33 order presentation training data nge sensitive order training examples presented table 2 shows results experiment training settest set partitions fixed order presentation training set randomly varied see performance varies widely across domains serious drawback nge algorithm 1 unfortunately difficult choose good order training set could find effective way apply crossvalidation methods example select good order results reported random order selected run nge algorithms mean 25 runs reported 34 value k knearest neighbor algorithm wellestablished noisy domains knearest neighbor algorithm performs better simple nearest neighbor hence chose k optimize leaveoneout crossvalidation performance algorithm training set tried possible values k done relatively efficiently ties broken favor smaller value k 4 comparison nge knn figure 4 compares performance knearest neighbor algorithm knn nge cv number seeds chosen via leaveoneout crossvalidation nge 3 seeds nge 3 seeds 4 nge number seeds increased wettschereck tg dietterich 50 training data nge limit rationale behind nge limit amount storage required hyperrectangle twice amount storage required store single data point hence number seeds equals 50 training data total space required nge equals space required knn assuming similar methods dealing ties missing features used beyond point nge data compression advantage knn 5 5 voting hungarian letter iris task c task recognition performance relative figure 4 performance ngecv nge 3seeds nge limit relative knn shown percentage point differences ngecv knn nge 3seeds knn nge limit knn significance difference knn different nge versions indicated see table a2 appendix detailed numbers knearest neighbor outperforms nge cv statistically significant amount eight nonconstructed domains displayed figure 4 domains nge cv achieved significant ie 60 85 compression data significantly increasing number seeds nge limit figure 4 possible significantly improve performance nge cv task b led7 cleveland hungarian letter recognition domains however nge limit still significantly inferior knn performance one nonconstructed domain drop performance voting domain task due fact domains leaveoneout crossvalidation small number different seed set sizes beneficial increasing size seed set 6 however improvement performance nge limit comes high cost cases nges performance improved also used memory knn figure 5 shows learning curves domains generally curves shape expect inductive learning algorithms performance increases number training examples increase levels nearesthyperrectangle comparison 1180100 task theta theta theta theta theta theta theta theta theta theta theta theta theta theta theta theta theta c c c c theta theta theta theta theta theta theta theta theta theta theta theta theta theta theta theta theta c c c task c theta theta theta theta theta theta theta theta theta theta theta theta theta theta theta theta theta c c c c iris c c c theta theta theta theta theta theta theta theta theta500 50 100 150 200 c c c theta theta theta theta theta theta theta theta theta theta theta6080 c c c c c theta theta theta theta theta theta theta theta theta theta theta theta theta6080 c c c c theta theta theta theta theta theta theta theta theta theta theta theta700 50 100 150 200 c c c c c theta theta theta theta theta theta theta theta theta theta theta theta700 50 100 150 200 hungarian c c c c c c theta theta theta theta theta theta theta theta theta theta theta theta800 50 100 150 200 250 300 voting c c c theta theta theta theta theta theta theta theta theta theta theta theta theta theta theta theta50100 letter recognition c c c c c c theta theta theta theta theta theta theta theta theta theta theta theta theta theta number training examples number training examples performance performance performance figure 5 performance nge knn different numbers training examples data point denotes mean 25 experiments note different scales axes graphs training set reached certain size waveform led7 letter recognition domains performance nge cv levels much earlier knns furthermore graphs cleveland hungarian voting domains show erratic behavior nge cv hungarian voting domains nge cv 12 wettschereck tg dietterich reaches near peak performance 25 training examples seen 25 training examples performance nge cv varies within two standard errors domains cleveland domain performance nge cv peaks also 25 examples 726sigma19 correct drops 669sigma18 inspection number sizes hyperrectangles constructed nge cv domains able determine cause unusual behavior number hyperrectangles stored nge cv grow linearly number training examples although desirable property machine learning algorithm may cause problems nge cv since existing hyperrectangles may generalized extended often means every time hyperrectangle enlarged may actually become less relevant conclude behavior constitutes serious deficiency nges search generalization procedure 5 possible explanations inferior performance nge given close relationship nge knn surprising nge performs much worse knn learning algorithm two fundamental sources problems first bias algorithm may inappropriate application domains second implementation bias may poor eg poor search algorithms employed salzberg never formally defines bias nge let us define find minimum number axisparallel hyperrectangles possibly nested overlapping correctly classifies training data evidence bias inappropriate know task nonaxisparallel decision boundary axisparallel bias inappropriate see figure 3 artificiallyconstructed domain however aha 1990 reports performance c45 quinlan 1992 six domains also used paper c45 also rectangular bias performs similar conditions significantly better nge six domains aha 1990 section 433 7 suggests axisparallel bias cause nges poor performance examination learned hyperrectangles several domains suggests permitting rectangles nest overlap problem common form nesting large generalized hyperrectangle created many singlepoint rectangles nested inside exceptions seen figure 6 plots number hyperrectangles created number actually generalized nonpoint rectangles see overwhelming majority hyperrectangles never generalized singlepoint hyperrectangles virtually never used classifying new test examples test example falls inside large hyperrectangle distance hyperrectangle zero singlepoint hyperrectangle used unless nearesthyperrectangle comparison 1310003500 letter recognition theta theta theta theta theta theta theta theta theta theta theta theta c c cccc c c c c c c c number training examples number exemplars figure 6 number exemplars stored nge trained 25 seeds differently sized training sets letter recognition task shown total number hyperrectangles stored training theta number hyperrectangles generalized least ffi data point denotes mean 25 experiments either test example exactly coincides singlepoint rectangle b singlepoint rectangle nested inside another rectangle nge also permits generalized rectangles overlap even dont nest may problem well one situation overlapping rectangles created distributions examples two classes b overlap optimal decision rule uniform loss function cf duda hart 1973 place decision boundary point probability density examples class equals probability density examples class b however nge instead arbitrarily assigns examples overlapping region one classesthe one smaller rectangle addition hypotheses bias nge considerable evidence bias implemented well nges incremental heuristic procedure sensitivity experiments know nge sensitive order training examples presented orders well tasks c determine optimal set hyperrectangles inspection 4 10 respectively nge find optimal solution instead constructs average 108sigma11 496sigma12 task b hand optimal solution involves large number rather small overlapping rectangles one every training example lies near decision boundary however nge find solution either constructs rectangles large nests smaller ones figure 7 displays rectangles constructed nge cv representative experiments tasks b c summary three hypotheses explain nge performing poorly relative knn h1 nested rectangles h2 overlapping rectangles h3 poor search algorithm test hypotheses conducted series experiments modified nge eliminate one suspected problems measured resulting change performance 14 wettschereck tg dietterich figure 7 rectanglesconstructedby ngecv tasks b c one representative experiment b dashed solid lines indicate location rectangles representing positive negative examples c digits indicate class rectangle represents trivial point rectangles displayed note task single rectangle class 0 covers entire input space first experiment tested h1 modifying nge produces relatively nested rectangles still permits overlapping rectangles otherwise change search procedure second experiment tested h2 modifying nge produces overlapping rectangles different classes exception rectangles entirely nested inside one another otherwise change search procedure third experiment tested h3 making simple modification incremental nge improve upon secondmatch heuristic goal finding fewer hyperrectangles finally fourth experiment tested hypotheses simultaneously implementing entirely different search procedure completely eliminates nested rectangles overlapping rectangles also reduces total number rectangles constructed describe experiments results 51 greedy nge avoid nesting test h1 want construct variant nge avoids nesting rectangles major cause nested rectangles second match heuristic line 9 figure 1 nearest rectangle wrong class second nearest rectangle right class secondnearest rectangle expanded cover new example many cases also cover nearest rectangle could single point thus create nesting salzberg 1991 section 35 introduced tested version nge called greedy nge second match heuristic greedy version stores example new hyperrectangle whenever closest previously stored hyperrectangle different class example according salzberg second match heuristic nge necessary construct nested overlapping hyperrectangles true nge may still con nearesthyperrectangle comparison 15 struct overlapping nested hyperrectangles even second match heuristic disabled grow hyperrectangle overlaps covers another hyperrectangle fact greedy nge construct overlapping hyperrectangles quite frequently nested hyperrectangles cases experiments conducted figure 8 shows predictive accuracy greedy nge significantly better nges three domains cleveland hungarian voting significantly worse 4 others task task c waveform21 waveform40 results task c waveform40 particularly poor based much evidence nested rectangles major problem nge voting letter hungarian iris task c task b520 5 task recognition greedy performance relative figure 8 performance greedy nge nge additional matching heuristic f2noc first two matches nearest exemplar examples class considered nonge relative nge indicates performance difference nge modification statistically significant p 005 see table a2 appendix detailed numbers 52 nge without overlapping hyperrectangles nonge test h2 want construct variant nge avoids overlapping rectan gles accomplished follows let us define p potential new hyperrectangle constructed calls generalize lines 9 10 figure 1 rectangle p rectangle formed extending either first match second match rectangle covers training example wettschereck tg dietterich nooverlap nge nonge construct p check whether would intersect hyperrectangle class p would intersect another rectangle reject p create new singlepoint rectangle instead however p would completely contained within another hyperrectangle accept p way nested rectangles permitted overlapping nonnesting rectangles forbidden figure 8 see nonge significantly better nge 6 11 domains never significantly worse nge strongly supports hypothesis h2that overlapping rectangles cause problems nge 53 better merge heuristic nge nge stores training example new hyperrectangle whenever two nearest hyperrectangles different output classes example cases however create unnecessary new rectangles consider figure 9 rectangle c away point p either rectangle rectangle b however rectangle c class point p could extended cover point p without overlapping either rectangles b extending rectangle c way avoid creating new generalized exemplar point p developed modified version nge called f2noc detects situ ation first two matches nearest secondnearest hyperrectangles fail f2noc finds nearest hyperrectangle class ex ample extends nearest hyperrectangle include new example expanded hyperrectangle would cover hyperrectangles classes otherwise stores example new hyperrectangle gives nge another chance generalize general reduce amount memory required nge class3 c class2 class1 class3 figure 9 example showing rectangle c extended cover point p f2noc considered weak test hypothesis h3 search algorithm nge needs improvement table a2 indicates additional matching heuristic indeed achieves reduction storage domains hence better implementation nge bias however shown figure 8 f2noc performs significantly better nge task c reduction storage directly related loss predictive accuracy five domains hence improvement nges search algorithm explain poor performance nge relative knn nearesthyperrectangle comparison 17 voting letter hungarian iris task c 5 recognition performance relative figure 10 performance obnge bnge relative nge shown percentage point differences obnge nge bnge nge indicates performance difference nge modification statistically significant p 005 table a2 appendix detailed numbers 54 batch nge obtain better test h3 constructed two batch algorithms obnge bnge nge bias algorithms begin training examples memory point hyperrectangles progressively merge form generalized hyperrectangles step two hyperrectangles nearest one another merged subject one following constraints obnge merge merge would cause misclassification training examples algorithm requires testing entire training set potential merge permits overlapping nesting rectangles call obnge overlapping batch nge bnge merge new hyperrectangle cover overlap hyperrectangles classes algorithm requires intersection potential merge hyperrectangles classes permit overlapping nesting call bnge batch nge merging process algorithms repeated merges found note algorithms somewhat dependent order potential merges considered greedy merge accepted soon mentioned conditions satisfied wettschereck tg dietterich algorithms conservative generalizing beyond training data original nge algorithm since generate hyperrectangles parts input space clearly belong certain class furthermore due fact bnge obnge repeatedly pass training data may also significantly reduce number hyperrectangles remain end bnge also faster easier use nge since crossvalidation free parameters required obnge however feasible large training sets numbers displayed figure 10 table a2 show bnge significantly outperforms nge 7 11 domains tested cases performance bnge better nge hand obnge significantly better nge task c significantly worse nge three domains task task b hungarian provides additional strong evidence overlapping rectangles inappropriate bias domains h2 test h3 examine first whether bnge implements better search algo rithm tasks c bnge attains optimal solution 4 hyperrectangles task c furthermore bnge also uses one hyperrectangle cover iris setosa class iris domain good evidence quality bnge search procedure incremental version nge similar bnge nonge nge without overlapping rectangles comparing figures 8 10 table a2 seen bnge outperforms nonge four domains nonge outperforms bnge two domains gives weak evidence improved search algorithm bnge responsible improved performance note bnge also trained incrementally cost storing training examples incremental bnge would split rebuild hyperrectangles whenever cover new example different class 55 discussion experiments see weak support h3 strong support h2 explanations poor performance nge relative nearest neighbor algorithm support h1 versions nge permit overlapping rectangles perform consistently better nge domains tested batch algorithm bnge permit nested overlapping rectangles different classes performs quite well avoids need choose number seeds nge crossvalidation 6 feature weights experiments conducted thus far treated features equally important computing euclidean distance nearest hyperrectangles nearest neighbors however many 11 domains involve noisy nearesthyperrectangle comparison 19 table 3 percentage nonseed examples covered least one hyperrectangle nge initialized 3 25 seeds domain training testing 3 seeds 25 seeds 3 seeds 25 seeds iris 43 38 72 68 hungarian voting 97 93 100 100 letter recognition 79 79 95 94 completely random features way improve performance nge knn introduce mechanism learning features important ignoring unimportant noisy features distance computations salzberg 1991 section 33 last paragraph describes method online learning feature weights nge assume new example e misclassified exemplar h input feature f weight f w f increased multiplying 1 match h f weight w f decreased multiplying global featureadjustment rate usually set 02 e classified correctly h feature weights adjusted opposite direction two problems heuristic first consider tasks one class much frequent another tasks new examples tend classified correctly chance feature weights change exponentially features always match weights zero features random receive infinite weight salzberg personal communication 1991 pseudo suggested adjusting feature weights matches ie nearest secondnearest hyperrectangles failed found empirically policy feature weights adjusted quite infrequently example iris task feature weights adjusted 1 training examples waveform40 feature weights adjusted 7 training examples second serious problem use feature weights nge high percentage test cases fall inside least one hyperrectangle means distance nearest hyperrectangle zero feature weights effect distance calculation table 3 shows percentage test training cases occurs suggests limits performance improvement obtained using feature weights nested hyperrectangles experiments salzbergs method computing feature weights found performance almost always decreased see therefore considered another procedure computing feature weights given promising results exemplarbased learning methods bakiri 1991 wettschereck tg dietterich 61 determining weights mutual information purpose feature weight mechanism give low weight features provide information classification eg noisy irrelevant features give high weight features provide reliable information hence natural quantity consider mutual information values feature class examples feature provides information class mutual information 0 feature completely determines class mutual information proportional log number classes let c probability class training example equals c probability value feature j example falls interval nintervals joint probability two events mutual information feature f j classification c log discrete features nintervals equal number possible distinct puts continuous features nintervals chosen 5 probabilities estimated training data missing values ignored mutual information measure also known machine learning literature information gain used splitting criterion id3 c45 quinlan 1992 62 experiments feature weights figure 11 table a2 shows effect including feature weights compares two different procedures computing weights salzbergs method gives statistically significant increase performance hungarian domain statistically significant decrease task c significant effect domains mutual information feature weights generally give slight though statistically insignificant improvements domains without irrelevant features improvements substantial domains irrelevant features give statistically significant improvement cleveland hungarian voting domains well task c small p 005 decrease performance observed mutual information feature weights letter recognition domain mutual information feature weights similar positive effect performance simple nearest neighbor lesser extent knn see table a2 mutual information weights small irrelevant inputs domains furthermore feature weights differ substantially one random partition data sets another contrast weights computed nearesthyperrectangle comparison 2155 recognition letter hungarian voting iris task c task performance relative figure 11 performance nge fwmi nge fw salzberg relative nge without feature weights shown percentage point differences nge fwmi nge nge fw salzberg nge indicates significance difference nge modi fications see table a2 appendix detailed numbers salzbergs method differed substantially one partition another varying much factor 1000 within given training settest set partition features less equally weighted experiments section 6 conclude salzbergs weight procedure significant impact nges behavior domains mutual information weight procedure performs well domains large number irrelevant features furthermore since mutual information weight procedure independent algorithm used procedure could used effectively many inductive learning algorithms filter irrelevant features 7 comparison best variants nge knn developed several modifications nge uniformly improve performance algorithm best combines batch nge mutual information feature weights bnge fwmi best corresponding version nearest neighbor algorithm k nearest neighbors crossvalidation determine mutual information feature weights knn fwmi section compare two algorithms determine whether modifications nge make competitive knn figure 12 shows results comparison main conclusion draw bnge fwmi significantly inferior knn fwmi 7 domains significantly superior 2 two domains domains know 22 wettschereck tg dietterich hungarian voting 5 letter iris task c task recognition knn fwmi performance relative fwmi figure 12 performance nge fwmi bnge fwmi relative knn fwmi shown percentage point differences nge fwmi bnge fwmi knn fwmi differences statistically significant see table a2 appendix detailed numbers axisparallel rectangle bias appropriate shows hyperrectangles appropriate bnge fwmi able exploit however domains rectangles evidently appropriate bnge fwmi performance suffers knn fwmi robust situations shows research still needed develop nge algorithm robust situations 8 related work simpson simpson 1992 introduced incremental algorithm extremely similar nge called fuzzy minmax neural networks main differences nge fuzzy minmax classifier hyperrectangles fmmc bounded size b fmmc always extends nearest hyperrectangle class given example include example long size new hyperrectangle larger userdefined value c ffmc shrinks hyperrectangles eliminate overlap hyperrectangles different classes carpenter et al 1992 introduced neural network architecture based fuzzy logic adaptive resonance theory art neural networks category boxes used fuzzy artmap complement coding comparable hyperrectan gles hyperrectangles fuzzy artmap complement coding grow monoton nearesthyperrectangle comparison 23 ically learning maximum size bounded vigilance parameter see carpenter et al 1992 short comparison nge ffmc fuzzy artmap neither fmmc fuzzy artmap use feature weights sense discussed paper 9 summary discussion extensive study nge algorithm conducted basic algorithm number modifications evaluated eleven domains nge found quite sensitive number starting seeds order presentation examples performance nge compared performance knearest neighbor algorithm found substantially worse several domains even crossvalidation applied optimize number starting seeds nge three hypotheses introduced explain difference performance nested rectangles provide poor bias b overlapping rectangles provide poor bias c incremental search algorithm nge needs improvement experimental modifications nge made order test hypotheses two versions nge avoid nested rectangles permit overlapping rectan gles perform substantially better nge however algorithm called nonge permits nested rectangles avoids overlapping rectangles performed uniformly better nge eleven domains improvement statistically significant 6 domains batch algorithm bnge implements better search algorithm allow nested overlapping rectangles also performs uniformly better nge performs better nonge 4 domains worse 2 experiments conclude overlapping rectangles primary source difficulty nge bnge best variant nge studied experiments reported wettschereck 1994 show bnge commits errors outside hyperrectangles using knn classify test examples fall outside hyperrectangle hybrid method bnge knn attains classification accuracy comparable knn alone large improvement classification speed versions nge effective compressing data compared knn also studied whether nge algorithms could improved incorporating feature weights distance metric computed algorithms feature weight mechanism introduced salzberg 1991 shown never provide significant improvement performance nge without feature weights deed significantly worse simple nge three domains hand feature weight mechanism based computing mutual information feature output class shown significantly better nge five domains significantly worse one mechanism wettschereck tg dietterich independent nge therefore used preprocessing step inductive learning algorithm 10 conclusions data presented strongly support conclusion nge algorithm described salzberg 1991 modified number ways first construction overlapping hyperrectangles avoided second entire training set available stored memory classifier trained batch mode eliminate computationally expensive crossvalidation number initial seeds third mutual information used compute feature weights prior running nge modifications nge gives superior performance domains axisparallel hyperrectangle bias appropriate however domains nge perform well knn hence generalization performance robustness critical knn algorithm choice hand understandability memory compression important nge modified recommended fast easytouse inductive learning algorithm 8 acknowledgements thank steven salzberg providing assistance implementation nge thank one anonymous reviewers pointing heuristic computing size symbolic features us also thank steven salzberg anonymous reviewers bill langford helpful comments earlier drafts note research supported part nsf grant iri8657316 nasa ames grant nag 2630 gifts sun microsystems hewlettpackard r study instancebased algorithms supervised learning tasks converting english text speech machine learning approach fuzzy artmap neural network architecture incremental supervised learning analog multidimensional maps nearest neighbornn norms nn pattern classification techniques pattern classification scene analysis simple classification rules perform well commonly used datasets nearest hyperrectangle learning method fuzzy minmax neural networks 1 computer systems learn hybrid nearestneighbor nearesthyperrectangle algorithm tr ctr j mitran bouillant e bourennane svm approximation realtime image segmentation using improved hyperrectanglesbased method realtime imaging v9 n3 p179188 june igor jurisica janice glasgow john mylopoulos incremental iterative retrieval browsingfor efficient conversational cbr systems applied intelligence v12 n3 p251268 mayjune 2000 jurisica p rogers j glasgow fortier j r luft j r wolfley bianca r weeks g detitta intelligent decision support protein crystal growth ibm systems journal v40 n2 p394409 february 2001 steven l salzberg comparing classifiers pitfalls toavoid recommended approach data mining knowledge discovery v1 n3 p317328 1997 j mitran j matas e bourennane paindavoine j dubois automatic hardware implementation tool discrete adaboostbased decision algorithm eurasip journal applied signal processing v2005 n1 p10351046 1 january 2005 charles x ling hangdong wang computing optimal attribute weight settings nearest neighboralgorithms artificial intelligence review v11 n15 p255272 feb 1997 tams horvth stefan wrobel uta bohnebeck relational instancebased learning lists terms machine learning v43 n12 p5380 aprilmay 2001 niloofar arshadi igor jurisica data mining casebased reasoning highdimensional biological domains ieee transactions knowledge data engineering v17 n8 p11271137 august 2005 melody kiang comparative assessment classification methods decision support systems v35 n4 p441454 july jos ranilla oscar luaces antonio bahamonde heuristic learning decision trees pruning classification rules ai communications v16 n2 p7187 katharina morik peter brockhausen multistrategy approach relational knowledge discovery indatabases machine learning v27 n3 p287312 june 1997 jos ranilla oscar luaces antonio bahamonde heuristic learning decision trees pruning classification rules ai communications v16 n2 p7187 april jinyan li guozhu dong kotagiri ramamohanarao limsoon wong deeps new instancebased lazy discovery classification system machine learning v54 n2 p99124 february 2004 randall wilson tony r martinez reduction techniques instancebasedlearning algorithms machine learning v38 n3 p257286 march 2000 francisco azuaje werner dubitzky norman black kenny adamson retrieval strategies casebased reasoning categorised bibliography knowledge engineering review v15 n4 p371379 december 2000 foster provost venkateswarlu kolluri data mining tasks methods scalability handbook data mining knowledge discovery oxford university press inc new york ny 2002 dietrich wettschereck david w aha takao mohri review empirical evaluation feature weighting methods aclass lazy learning algorithms artificial intelligence review v11 n15 p273314 feb 1997 vassilis g kaburlasos ioannis n athanasiadis pericles mitkas fuzzy lattice reasoning flr classifier application ambient ozone estimation international journal approximate reasoning v45 n1 p152188 may 2007 foster provost venkateswarlu kolluri survey methods scaling inductive algorithms data mining knowledge discovery v3 n2 p131169 june 1999