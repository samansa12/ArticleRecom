global optimization multilevel coordinate search inspired method jones et al 1993 present global optimization algorithm based multilevel coordinate search guaranteed converge function continuous neighborhood global minimizer starting local search certain good points improved convergence result obtained discuss implementation details give numerical results b introduction problems involving global optimization traditionally usually minimization multivariate function widespread mathematical modeling real world systems broad range applications see eg pint er 18 many problems described nonlinear relationships introduces possibility multiple local minima task global optimization find solution objective function obtains smallest value global minimum objective function huge number local minima local optimization techniques likely get stuck global minimum reached kind global search needed find global minimum reliability global optimization homepage world wide web www address httpsoloncmaunivieacat neumglopthtml contains large number commented links information software packages relevant global optimization algorithms solving global minimization problems classified heuristic methods find global minimum high probability methods guarantee find global optimum required accuracy important class belonging former type stochastic methods eg boender romeijn 3 involve function evaluations suitably chosen random sample points subsequent manipulation sample find good local hopefully global minima number techniques like simulated annealing eg ingber 9 10 genetic algorithms eg michalewicz 13 use analogies physics biology approach global optimum important class methods second type branch bound meth ods derive origin combinatorial optimization eg nemhauser wolsey 14 also global optima wanted variables discrete take values branch bound methods guarantee find global minimizer desired accuracy predictable though often exponential number steps basic idea configuration space split recursively branching smaller smaller parts done uniformly instead parts preferred others eliminated details depend bounding procedures lower bounds objective allow eliminate large portions configuration space early computation usually small part branching tree generated processed lower bounds may obtained using dcmethods eg horst tuy 8 techniques interval analysis eg hansen majorization resp minorization methods based knowledge lipschitz constants eg pint er 18 unlike heuristic methods however methods applicable something analytical properties objective known since one needs able compute powerful reliable underestimating functions algorithm going describe paper intermediate purely heuristic methods methods allow assessment quality minimum obtained spirit similar direct method global optimization jones et al 11 latter method method guaranteed converge long run objective continuous neighborhood global minimizer additional smoothness properties required contrast many stochastic methods operate global level therefore quite slow algorithm contains local enhancements lead quick convergence global part algorithm found point basin convergence global optimum moreover control variables algorithm meaningful default values chosen work simultaneously problems thus fine tuning usually required since deterministic method need multiple runs paper consider bound constrained optimization problem min fx 1 finite infinite bounds use interval notation rectangular boxes ng case bounds infinite obtain unconstrained optimization problem direct finite box normalized 0 1 n tree boxes constructed box characterized midpoint side lengths boxes always form 3 gammak k 2 n 0 disadvantage algorithm infinite box bounds cannot handled moreover since boundary never reached converges slowly necessary cases minimizer lies boundary example case functions monotonous variable optimizer vertex direct converges slowly especially bounds wide inspired direct devised global optimization algorithm based multilevel coordinate search mcs algorithm remedy shortcomings allow irregular splitting procedure section 2 first define class algorithms solving global optimization problem 1 satisfying six properties a1a6 prove convergence long run addition splitting procedure enhancement obtained starting local searches certain good points results improved convergence result section 3 outline implementation mcs algorithm given details explained sections 4 6 finally numerical results presented section 7 acknowledgments authors gratefully acknowledge partial support research austrian fond zur forderung der wissenschaftlichen forschung fwf grant p11516 mat 2 convergence results section consider class algorithms solving minimization problem 1 six properties a1a6 defined follows box tree characterized base point x function value fx moreover assign box level 2 g box split level set zero levels nonsplit boxes positive level initial box 1 base points boxes level max function values put base list levels mcs correspond side lengths direct ie boxes small level large boxes split often yet boxes level compete chosen branching cf a3 a2 box always split along single coordinate chosen appropriately splitting index two p 2 parts descendants box level latter base points descendants box chosen natural way differ base point parent box one coordinate thus procedure generating new function values variant standard coordinate search method contrast algorithm direct splits box along coordinates step a3 branching process proceeds series sweeps levels defining record sweep defined following three steps step 1 scan list nonsplit boxes define record list containing level pointing box lowest function value among boxes level box level set b initialize lowest level b 6 0 step 2 box label b candidate splitting split mark split list insert children update record list children yields strict improvement f level considered worthwhile split box level increased one possibly b s1 updated step 3 increase 1 new sweep else b step 3 else go step 2 clearly sweeps ends visits step 3 like direct strategy yields combination global local search key balancing global local search multilevel approach fact start boxes lowest levels ie large boxes sweep constitutes global part selection box lowest function value level forms local part algorithm a4 0 exists rm 2 0 1 independent choice max box x generated splitting box x along ith coordinate assumption ensures descendants finite box eventually become arbitrarily narrow sufficiently many splits along coordinate worst case rm shrinking factor may depend box bounds since want overemphasize large numbers case wide box bounds a5 rule splitting boxes infinite bounds follows ith box bounds gamma1 1 split along ith coordinate interval always split one values contained fixed interval z 1 independent choice means turns two semibounded intervals contained gamma1 z 2 possibly additional finite ones contained z 1 box ith bounds x along ith coordinate obtain box ith bounds one boxes finite ith bounds generally let x unbounded part generated splitting ith bounds x ith coordinate exist functions monotonously increasing 0 1 f 2 implies f x 0 2 r analogous assumption holds successive splits gamma1 x 0 lower bound f 1 guarantees unbounded descendants interval x shrink sufficiently fast however shrink fast since bounded descendants could become arbitrarily large therefore impose upper bound assume box level 1 max candidate splitting since b points box let n j number times coordinate j split history box hn minn j box case eligible splitting splitting index coordinate assumption makes sure along path tree coordinate split inifinitely often proposition 1 sweep one box leaves lowest nonempty level box added level level eventually become empty particular splitting procedure come end nonsplit boxes level max proof directly a1a3 proposition 2 let 2 0 max levels empty p defined a2 particular algorithm finishes p smax proof course algorithm root box u v level one descendants level 2 max gamma1 since worst case splitting box p parts advancing levels descendants steps one proposition 1 levels 1 cleared expected upper bound exponential however practice bound pessimistic since split box lowest nonempty level advance level one sweep nonempty level moreover levels descendants boxes sometimes advance two box advanced next level without split also good approximations global minimizer usually reached long levels cleared proposition 3 box level 0lm coordinate split least times proof proved induction statement trivial box level number times coordinate split history b 0 induction hypothesis assume minn parent box b level since levels advance steps one two 2m 1n possible go back n times history box b 0 obtained splitting b according a6 let n 0 number times coordinate split history box b n since b 0 generated b n splitting n times according a6 minn 0 contradiction induction hypothesis consequently assumption wrong must proposition 4 exists ffi ith side length box containing point x 2 u v finite coordinates less ffi split least ffi times along ith coordinate proof consider ith side length box containing x case 1 u finite implies exists l 2 n f l l splits ith coordinate box containing x finite ith bounds contained u problem reduced case 2 case treated similarly case reduced semibounded interval contained z 1 finite interval contained z 1 one split case 2 u v finite maxju j jv j exists k 2 n 0 defined a4 ie a4 ith side length containing x less ffi box split least k times along ith coordinate propositions 24 yield following convergence theorem theorem 5 suppose global minimization problem 1 finite solution r continuous neighborhood x let algorithm satisfy assumptions a1a6 let 0 exists 0 p base point x found ie algorithm converges number levels tends 1 order enhance performance algorithm subject base point box level max local search putting base list another local feature algorithm theoretical purposes make obviously idealized assumption local search algorithm reaches local minimizer finitely many steps started basin attraction theorem 6 let assumptions theorem 5 satisfied assume local search started candidate base list least function value smaller function value local minimum already base list assume 0 fy nonglobal local minimizer 2 u v sufficiently large norm exist numbers l max l global minimizer found sweeps proof assumption exists propositions 3 4 exists l side lengths box containing x smaller ffi level least l proposition 2 box containing x reaches level l p sweeps let max l sweeps box b containing x level l base point x fx fx two cases possible found better point already made local search point finds global minimizer assumption case x current best point end sweep point fy fx subjected local search means find global minimizer 3 mcs algorithm many ways design algorithms satisfying a1a6 hence guaranteeing convergence global minimizer however trivial implementations slow number heuristic enhancements needed obtain high quality method following describe useful version could find first give overview techniques used discuss details technique sections 4 6 unlike direct base point box algorithm usually midpoint point boundary often always vertex base point belong one box moreover also assign box opposite point construction base point x opposite point determine box call box bx algorithm starts socalled initialization procedure producing initial tree boxes whenever box split along coordinate first time either initialization procedure later done three given values x l function values computed adaptively chosen intermediate points least four subboxes obtained initialization procedure start sweeps described a3 box level 1 max selected splitting procedure described a3 use following splitting strategy box ffl either split according a6 splitting rank ffl split along coordinate maximal gain function value expected according local separable quadratic model obtained fitting 2n1 function values splitting expected gain ffl expected gain large enough split level increased one box already split along coordinate chosen splitting index single new function evaluation needed obtain two three subboxes new function value taken point predetermined box bounds case splitting rank case splitting expected gain point maximal gain function value expected putting base point box level max base list first check whether basin attraction local minimum already base list case subject local search local search algorithm used program essentially consists building local quadratic model triple searches defining promising search direction aid quadratic model modified cholesky factorization making line search along direction procedure repeated stopping criterion fulfilled moreover devices handle case one coordinates current point boundary included 4 initialization building tree boxes one first evaluates f initial point x 0 sets evaluated two points u v agree x coordinates k 6 thus one x l f l x point smallest function value renamed x repeating procedure next coordinate numbers x l indices l stored initialization list choice initialization list left user may incorporate knowledge likely distribution good points good known starting point defined x 0 possible choices discussed section 7 initialization list corresponding list function values initial tree boxes constructed follows root box bx point one corners u v farthest away x note x need vertex coordinates infinite current box split along ith coordinate 2l exactly one x l endpoints depending whether two one none x l boundary means addition x l split z l additional splitting points chosen z l golden section ratio chosen part smaller function value gets larger fraction interval resulting subboxes get base point point x 0 obtained current changing x x l boundary point corresponding ith coordinate interval fx 0 opposite point point obtained changing end interval information available far allows us define priorities coordinates compute union ranges quadratic interpolant three consecutive take difference upper lower bound obtained crude measure variability f ith component components higher variability get higher priority ranking saved vector component index th highest estimated variability moreover x obtained splitting ith coordinate belongs two boxes one containing minimizer quadratic models taken current box coordinate 1 root box gets level 1 box level split boxes smaller fraction golden section split get level thus current box splitting next coordinate case one level finishing initialization procedure first level empty nonsplit boxes levels implies meaningful take two examples set boxes base points levels initialization procedure twodimensional case shown figure 1 6 cases x easy see connection golden section split assignment levels box b level split along coordinate according golden section split larger part b 0 subjected golden section split along ith coordinate larger descendant b 0 ith coordinate length smaller descendant b boxes level 2 moreover box better function value gets larger fraction interval smaller r r figure r r figure level likely split quickly also strategy adopted direct choice x l including endpoints u v list guarantees simple case f monotonous variable final x initialization phase already global minimizer 5 splitting store box bounds box information tree label parent box splitting index splitting value label identifying many children etc keeps amount storage proportional number function evaluations allows us recover information build separable quadratic model going back history box suppose want split ith coordinate interval u fx ith component base point box considered order fulfil assuption a4 may split close x large also want new component x 0 large therefore force smaller interval u f g choose interval according function subint given following pseudomatlab function function else selected box level max splitting procedure described a3 recover base point x opposite point number n times coordinate split history box splitting rank let hn minn h function defined a6 select splitting index among indices smallest n one lowest hence highest variability rank splitting done according initialization list x l golden section split points discussed section 4 new base points opposite points defined boxes smaller fraction golden section split thus larger function values get level mins ones get level 1 ith component ranges x splitting value chosen z box split z golden section split point obtain three parts one additional function evaluation point x 0 obtained changing ith coordinate x z smaller fraction golden section split gets level mins two parts get level 1 moreover base point first child taken x base point second third child point x 0 defined opposite points obtained changing end ith coordinate interval corresponding box splitting expected gain let hn minn order build local separable quadratic model need two additional points corresponding function values coordinate whenever split ith coordinate history box obtain values used quadratic interpolation coordinate coordinate take first two points function values found pursuing history box back u v since points expected closest base point x coordinates yet split obtain information initialization list let local separable model f generated interpolation x 2n additional points collected coordinate define expected gain e function value evaluate new point obtained changing coordinate base point two cases distinguished case 1 history current box coordinate never split ie split according initialization list points already know obtainable function differences therefore compute expected gain case 2 n 0 ith component ranges x quadratic partial correction function disposal calculate maximal gain expected changing value reasons discussed choose splitting value u f compute minimum achieved expected best function value satisfies 1in best f best current best function value including function values obtained local optimization expect box contain better point split using splitting index component minimal e condition 2 prevents wasting function evaluations splitting boxes bad base point function values boxes eventually split rank anyway case 1 split according initialization list definition new base points opposite points assignment levels case 2 use z splitting value box split z z 6 golden section split point obtain two three parts larger fraction golden section split gets level 1 smaller fraction level mins third part larger smaller fraction golden section split gets level moreover base point first child taken x base point second third z 6 child obtained changing ith coordinate x z opposite points obtained changing end ith coordinate interval box 2 violated expect improvement therefore split increase level 1 6 local search theory local optimization provides powerful tools task optimizing smooth function knowledge gradient even hessian assumed derivative information available traditional methods based employment conjugate directions successive line searches cf direction set method powell 19 modification due brent 4 algorithms however allow specification bound constraints elster neumaier 6 developed algorithm optimization lowdimensional bound constrained functions based use quadratic models restriction evaluation points successively refined grids hoewever work algorithm grows dimension n 6 hence unsuitable larger dimensions local optimization algorithm going describe sequel also makes use quadratic models successive line searches devices handle bound constraints incorporated first explain procedure building local quadratic model triple searches since want start local searches points belonging domain attraction local minimum put several copies essentially point base list devised criterion avoid see step 2 subsection base list conclude discussion validity assumptions a4 a5 algorithm triple search want use function values construct quadratic model best best best assume three vectors x l componentwise inequalities function values taken points x x follows x best denotes current best point triple search denote x i1 x i2 points obtained x best changing ith coordinate two values fx l x ik k points obtained changing ith kth coordinate ones smaller qx resp qx k current quadratic model q thus obtain following procedure going describe detail sequel best compute fx i1 fx i2 compute g g ii store x newbest update x best compute qx k1 qx k2 current model compute fx ik update x newbest update x best compute g ik x newbest 6 x best update x best f g 1i computed approximations g l g lk interpolating points differ first best current best point triple search obtain approximations g g ii determining numbers quadratic polynomial best best best interpolates x ij 2 stage yet update x best minfx i1 fx i2 best store point x bestnew assume addition already calculated approximations g il 1 l 1 compute best best best current quadratic model q qx ij 2 let x ik defined choose g ki quadratic model interpolates x ik ie equation best best best best best best best satisfied update x best update x bestnew x ik yields strict improvement function value finishing loop k reexpand model around best point g kl x bestnew best l easy see method gives unique quadratic interpolant f distinct points particular recover exact objective function diagonal triple search carry diagonal part algorithm take offdiagonal elements hessian previous iteration thus 2n additional function values needed coordinate search find x l use coordinate search based line search routine matlab version actual line search used obtained electronically httpsoloncmaunivieacat neummsls univariate line search program ls0 contains parameter smaxls limiting number points used line search possible feed points addition starting point function values program points included smaxls line search along coordinate first line search started candidate base list line search along first coordinate take best point two nearest neighbors sides points exist two nearest neighbors one side fx l g subsequent line searches started current best point obtained previous line search line search coordinate 1 take best point starting point line search different best point possible nearest neighbor best point side fx l g ith coordinate old x best among fx l since otherwise would lose points surface fitted previously local search ingredients disposal describe steps local search algorithm used implementation mcs step 1 starting candidate base list make full triple search fx l found coordinate search described procedure yields new point x function value f approximation g gradient approximation hessian g step 2 set modified cholesky factorization without scaling 15 section 2 starting value used set current x u k v k p k points outward set make line search ls0 along xffp let ff min ff max bounds ff obtained box bounds u v p gp 0 gammag pp gp 2 gammag pp gp otherwise take one expect smaller function value according quadratic model values ff used input ls0 g positive definite chosen modified cholesky factorization cholesky factorization search direction g positive definite ff close 1 set old function value current point beginning step 2 f pred function value predicted quadratic model ff ie fac measure quality quadratic model 1 line search improve function value step 3 stop limit number visits step 3 per local search limit function calls exceeded also stop none components current x boundary last triple search full one stopping criterion fulfilled step 4 components current x boundary stopping criterion fulfilled make line searches smaxls points along coordinates function value improved coordinate searches stop step 5 fac 01 stopping criterion fulfilled step 3 make full triple search otherwise make diagonal triple search make triple searches coordinates component x current x boundary set fx l taken consist x two neighbors distance ffi 2ffi x lies boundary denotes machine accuracy obtain new point x approximation reduced gradient g reduced hessian g step 6 set components current x boundary p make line search along x ff input quantity fac defined step 2 f old function value current point beginning step 6 go step 3 stopping criterion fulfilled function value improved steps 5 6 resp steps 1 2 jgj maxjxj jx old input parameter program f 0 typical function value eg smallest function value found initialization procedure base list local searches carried end sweep candidates base list collected sweep ordered ascending function value gone follows let x candidate base list step 1 check whether already made local search point often point belongs two boxes case take next candidate base list go step 1 step 2 let w points already base list renumbering assume sorted distance x starting nearest point following step 2a compute function value x increase one go step 2a step 2b compute function value x increase one go step 2a else points seem lie valley however discard x local search set x value x 0 x 00 smaller function value increase one go step 2a else subject x local search four function values monotonous take next candidate base list go step 1 step 3 local search step 4 new point x obtained local search subjected procedure similar step 2 order find whether really found new point case put base list disregard splits according initialization list a4 fulfilled algorithm 0001 factor 09 applies regular case x 00 safeguarded case subint large worst case largest three numbers 1 1 moreover a5 satisfied choice 7 numerical results test functions jones et al 11 gave extensive comparison direct method various methods seven standard test functions dixon szeg 5 two test functions yao 24 since mcs algorithm based important insights 11 first consider test set evaluate efficiency mcs test function dimensions box bounds used jones et al inadvertently omitted 11 given table 1 thank jones providing us code test functions label test function dimension default box bounds gp goldsteinprice 2 gamma2 2 2 c6 sixhump camel 2 gamma3 3 theta gamma2 2 table 1 dixon szeg dimensions box bounds last four lines table 2 taken one tables results 11 fourth last line contains results differential evolution algorithm de storn price 22 httphttpicsiberkeleyedu storncodehtml using matlab program devec2m default values control parame ters since algorithm operates global level takes rather long time find minimum high accuracy therefore used obtaining relative error stopping criterion number function evaluations needed convergence averaged 25 runs test function case hartman6 one run converge 12 000 function evaluations averaged remaining 24 runs last three lines give results mcs first bound constraints 11 averages strongly perturbed box bounds finally unconstrained version details given assessing results bear mind first 11 algorithms already appeared 1978 anthology edited dixon szeg therefore somewhat old number function calls needed convergence method assessing quality algorithm important one case real life applications function evaluations expensive termination presentation test results methods usually compared basis performance problems known solutions algorithm terminated function value within tolerance global minimum found also adopt strategy however practical problems one know solution advance needs criterion tells program stop searching better local minimizer criterion stringent enough waste many function values global minimum found also loose enough ensure typical cases algorithm terminate global minimizer found stochastic approaches design suitable stopping criteria surveyed section 6 boender romeijn 3 one methods proposed consists stopping number n local searches done larger function nw number w different local minima found far function nw depends assumptions several specific implicit definitions nw given 3 method s5 s7 s10 h3 h6 gp br c6 shu bremmerman 5 250 mod bremmerman 5 515 300 160 zilinskas 5 8641 5129 gomulkabranin 5 5500 5020 4860 torn 5 3679 3606 3874 2584 3447 2499 1558 gomulkatorn 5 6654 6084 6144 gomulkavm 5 7085 6684 7352 6766 11125 1495 1318 price 5 3800 4900 4400 2400 7600 2500 1800 mockus 5 1174 1279 1209 513 1232 362 189 belisle et al 1 b 339 302 4728 1846 boender et al 2 567 624 755 235 462 398 235 kostrowickipiela 12 g g g 200 200 120 120 perttunen 16 c 516 371 250 264 82 97 54 197 perttunen stuckman 17 c 109 109 109 140 175 113 109 96 jones et al 11 c 155 145 145 199 571 191 195 285 2967 stornprice 22 dh6400 6194 6251 476 7220 1018 1190 416 1371 last four lines taken 11 missing entries available literature method converged local minimum b average evaluations converges h6 converged 70 time c convergence defined obtaining relative error 001 convergence defined obtaining relative error perturbed box bounds f unconstrained optimization problem g global minimum found less 12 000 function calls average 25 cases h6 average 24 cases one case converge within 12 000 function values asterisk indicates first local optimization gave local optimum table 2 number function evaluations various methods compared mcs result theoretically justified random multiple start method may serve guideline also methods use local searches however mcs try local optimizations reasoning appears inadequate far yet found useful general purpose stopping criterion mcs purposes numerical tests reported stopping criterion mcs taken obtaining relative error 001 optimal objective function value happens nonzero always ie gamma4 also criterion used jones et al 11 algorithms quoted 11 results based definition convergence used authors reported mcs control parameter settings applied matlab version mcs dimension problem hn test functions used simple initialization list consisting midpoint boundary points ie 2 limit visits step 3 per local search set 50 parameter fl stopping criterion local optimization taken cf subsection local search section 6 note examples run identical parameter settings tuning individual test problems involved modified bounds also investigated stability results mcs respect random perturbations box bounds instead default box bounds u v given table 1 employed box bounds u j random variable uniformly distributed interval gamma05 05 value j accepted given problem least one global minimizers u results given second last line table 2 taken average 25 runs different perturbed box bounds test function hartman6 obtained one outlier algorithm found global minimum 12 000 function calls report result averaged 24 remaining runs moreover applied mcs unconstrained optimization problem dixon test set added results table 2 case cannot use initialization list consisting midpoint boundary points infinite wide bounds suggest use following safeguarded version based function subint take l 2 else discussion results show mcs seems strongly competitive existing algorithms case problems reasonable finite bound constrained mcs unperturbed box bounds wins 7 9 test cases every competing algorithm beaten perttunen resp perttunenstuckman remaining two test functions mcs perturbed box bounds still wins competing algorithms 5 test functions results shekels functions seem depend heavily choice box bounds comparable results algorithms know whether direct stable respect perturbation box bounds unconstrained problems dimension n 4 performance mcs less satisfactory reason exploration unbounded domain easy miss region global minimum lies one already found lowlying nonglobal minimizer example shekels functions algorithm gets caught second best local minimizer 1 1 1 1 point initialization list test problems dixon szeg test set criticized containing mainly easier test problems challenging test set used first contest evolutionary optimization iceo icec96 conference cf storn price 23 test bed contains five problems 5dimensional 10dimensional version test functions mcs showed limitations names default box bounds iceo test functions given table 3 results shown table 4 first two lines table 4 results taken 23 two different versions de problem name box bounds 1 sphere model gamma5 5 n 3 shekels foxholes 0 10 n 4 michalewiczs function 0 n 5 langermans function 0 10 n table 3 iceo test functions box bounds applied mcs three different choices initialization list iceo test functions mcs1 standard version midpoints boundary points mcs2 took x 1 ie points uniformly spaced include boundary points mcs3 generated initialization list aid line searches starting absolutely smallest point u v made line searches ls0 along coordinate best point taken starting point next line search parameter nloc ls0 determines local global line search since algorithm tries find nloc minima within smaxls function values line searches local search method described section 6 taken entirely local line search coordinate local minimizers found line searches put initialization list number less three supplemented values obtained ls0 closest u v finally applied mcs test functions used storn price 22 names box bounds shown table 5 definition found 22 problem 7 shifted version iceo2 problems 8 9 general constraints therefore used first four lines table 6 taken 22 anm denotes annealed nelder mead strategy 20 asa adaptive simulated annealing method ingber 9 10 de1 de3 two different versions de mcs used initialization list consisting midpoint boundary points global minimizer among points initialization list latter case different initialization list problem 4 contains random variable result presented mcs averaged problem 1 problem 2 problem 3 problem 4 problem 5 5d 10d 5d 10d 5d 10d 5d 10d 5d 10d mcs3 26 51 26956 32077 1903 values dash indicates global minimizer found 100 000 function calls table 4 number function values iceo functions problem name dimension n box bounds 4 quartic random noise 8 zimmermans problem 2 constraints 9 polynomial fit 9 17 constraints table 5 test functions storn price 22 dimensions box bounds anm 95 106 90258 asa 397 11275 354 4812 1379 3581 discontinuous test function 2 noisy test function dash indicates global minimizer found 100 000 function calls table 6 number function values storn price functions runs convergence defined reaching point function value 15 problem 1 quadratic objective function hence easy mcs problem 3 easy mcs since objective function monotonous 8 conclusions multilevel coordinate search algorithm mcs presented paper excellent theoretical convergence properties function continuous neighborhood global minimizer current implementation test results show mcs strongly competitive existing algorithms case problems reasonable finite bound constrained comparison mcs outperforms competing algorithms almost always classical test problems set dixon szeg unconstrained problems dimension n 4 performance mcs less since exploration unbounded domain easy miss region global minimum lies one already found lowlying nonglobal minimizer problem applies hard test problems huge number local minima r rinnoy kan algorithms minimization without derivatives global optimization problem introduction grid algorithm bound constrained optimization noisy func tions deterministic approaches fast simulated reannealing lipschitzian optimization without lipschitz constant diffusion equation method global minimization performance standard test functions genetic algorithms integer combinatorial optimization satisfying secondorder optimality conditions using modified cholesky factoriza tions rank transformation applied multiunivariate method global optimization efficient method finding minimum function several variables without calculating derivatives numerical recipes c multistart global minimization algorithm dynamic search trajectories differential evolution simple efficient adaptive scheme global optimization continuous spaces minimizing real functions icec96 contest differential evolution dynamic tunneling algorithm global optimization tr ctr rong yan alexander g hauptmann combination limit multimedia retrieval proceedings eleventh acm international conference multimedia november 0208 2003 berkeley ca usa hm gutmann radial basis function method global optimization journal global optimization v19 n3 p201227 march 2001 evket lker birbil shucherng fang rueylin sheu convergence populationbased global optimization algorithm journal global optimization v30 n23 p301318 november 2004 j gablonsky c kelley locallybiased form direct algorithm journal global optimization v21 n1 p2737 september 2001 wu l ozdamar kumar triopt triangulationbased partitioning algorithm global optimization journal computational applied mathematics v177 n1 p3553 1 may 2005 u garciapalomares f j gonzalezcastao j c burguillorial combined global local search cgls approach global optimization journal global optimization v34 n3 p409426 march 2006 jaewook lee novel threephase trajectory informed search methodology global optimization journal global optimization v38 n1 p6177 may 2007 ismael vaz lus n vicente particle swarm pattern search method bound constrained global optimization journal global optimization v39 n2 p197219 october 2007 panayiotis g georgiou chris kyriakakis maximum likelihood parameter estimation impulsive conditions subgaussian signal approach signal processing v86 n10 p30613075 october 2006 michael bartholomewbiggs bruce christianson ming zuo optimizing preventive maintenance models computational optimization applications v35 n2 p261279 october 2006