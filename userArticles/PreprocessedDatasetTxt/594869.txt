bayesian methods efficient genetic programming bayesian framework genetic programming gp presented motivated observation genetic programming iteratively searches populations fitter programs thus information gained previous generation used next generation bayesian gp makes use bayes theorem estimate posterior distribution programs prior distribution likelihood fitness data observed offspring programs generated sampling posterior distribution genetic variation operators present two gp algorithms derived bayesian gp framework one genetic programming adaptive occams razor aor designed evolve parsimonious programs genetic programming incremental data inheritance idi designed accelerate evolution active selection fitness cases multiagent learning task used demonstrate effectiveness presented methods series experiments aor reduced solution complexity 20 idi doubled evolution speed without loss solution accuracy b table 1 summary previous efforts upscaling genetic programming subjects selected references angeline w3x banzhaf w7x bloat landgon w22x nordin w27x wu w39x zhang w44x blickle w10x iba w18x parsimony kinnear w19x rosca w31x soule w36x zhang w44 45x andre w1x angeline w2x modularity koza w21x oreilly w28x spector w37x rosca w32x angeline w4x banzhaf w7x operators chellapilla w11x koza w20x luke w24x poli w29x gathercole w13 14x hillis w17x subset schoenauer w33x siegel w34x teller w38x zhang w40 42x list contains references directly related present work thus far complete produced solution soule et al w35xobserve removing nonfunctional codes every generation halt programs growth instead programs generate code functional never actually executed similar observations made biological evolution introns noncoding segments emerge dna well wu lindsay w39xgive recent review biological introns banzhaf et al w7xcharacterize introns two salient features intron segment genotype emerges process evolution variable length structures intron affect survivability individual directly introns gp turned mixed blessing one hand may benefit evolution since enable genetic program protect destructive effect crossover allow population preserve highlyfit building blocks w27x hand practical point view introns usually result run stagnation poor results heavy drain memory cpu time w8 35x explanation cause bloat gp provided zhang muhlenbein w44x based statistical learning theory show total error genetic programs decomposed two terms attributed bias variance since complex models expressive thus better reducing bias error simple models gp programs tend grow fit fitness data perfectly unless complex models penalized reduce variance error empirical evidence supporting explanation given analysis distributions fitness vs complexity genetic programs generated large number gp runs solving 7parity problems recently langdon poli w22xprovide similar general explanation bloat argue stochastic search techniques including gp tend find common programs search space current best fitness since general long short gp starts shorter onesthe population tends filled longer longer programs based argument langdon w23xpresents new crossover operators carefully control variation size produce much less bloat purposes upscaling genetic programming several reasons generally preferring parsimonious programs complex ones parsimonious programs typically require less time less space run particularly important gp process may need store evaluate populations hundreds thousands programs hardware implementation gp solutions mind gp evolves hardware circuits online evolvable hardware simple circuits require less hardware resource execution time complex circuits addition statistical theory says simpler models likely generalize better unseen fitness cases shown w44x biases equal simple program less variance average complex program resulting smaller total error statistical background behind principle occams razor w43xand necessity parsimony pressure genetic programming however effective control program growth difficult task since much pressure parsimony reduces variance errormay lead loss diversity thus result low accuracy greater bias error adaptive occam method w44xwas presented method striking balance accuracy parsimony gp solutions adapts complexity penalty fitness function run programs minimal complexity evolved without loss accuracy several researchers support parsimony pressure one easiest effective methods avoiding bloat w10 18 19 31 35 36x controlling program growth one method reducing time complexity well space complexity genetic programming another approach upscaling gp increasing modularity reusability programs koza w21xintroduces automatically defined functions adfs adf subroutine evolved run genetic programming may called main program simultaneously evolved run reports gp adfs produced parsimonious solutions required fewer fitness evaluation angeline pollack w2xdevelop alternative method called module acquisition rosca ballard w32xpresent mechanisms adaptive representation learning arlthat creates new subroutines discovery generalization blocks code spector w37xpresents method evolving collection automatically defined macros adms shows adms sometimes provide greater benefit adfs search process gp made efficient designing novel genetic operators traditionally crossover considered primary operator mutation secondary view consistent notion gp best made combining building blocks hypothesized bitstring genetic algorithms koza w20xhas argued mutations little utility gp positionindependence gp subtrees however roles building blocks crossover become increasingly controversial recent years luke bayesian genetic programming 221 spector w24xexperimentally demonstrate mutation fact utility crossover consistently considerable advantage mutation banzhaf et al w6xalso found increasing mutation rate significantly improve generalization performance genetic programming angeline w4x shows mutation operations perform par subtree crossover suggests building block hypothesis may accurately describe operational chracteristics subtree crossover chellapilla w11xproposes method evolving computer programs without crossover poli langdon w29xpropose various crossover operators compare search properties haynes w16x small changes representation decoding evaluation genetic programs increase probability destructive crossover mutation changing search space recently several authors shown fitness evaluation genetic programming significantly accelerated selecting subset fitness cases basic idea things equal evolution time minimized reducing effective data size generation siegel w34x example describes gp method evolving decision trees using subsets given training cases fitness case fitness measure training cases tend incorrectly classified decision trees become fit therefore selected frequently method motivated competitive selection fitness cases hostparasite model hillis w17x similar ideas competitive selection fitness cases refined teller andre w38xand gathercole ross w14x fitness training case evaluated cost evaluating another fitness case outweighed expected utility new information provide another class methods fitness case selection based incremental learning schoenauer et al w33xpropose successive optimization scheme gradually refines fitness evolution typically starts considering unique fitness case additional fitness cases gradually taken account current population meets performance criterion though slightly different context zhang w40xpresents selective incremental learning sel method incrementally chosen subset given training data used fitness evaluation model data sets divided two disjoint sets candidate training sets candidate data point fitness value called criticality defined error made model fitter candidates incrementally selected training set learning proceeds sel evolves single data set single model method presented section 5 generalizes populations multiple models multiple data sets 22 eoling multiagent cooperation strategies using gp multiagent learning task used testbed bayesian genetic programming methods single table four robotic agents placed random positions shown figure 1 specific location designated destination goal robots transport table 222 zhang figure 1 environment multiagent learning grid world four robots obstacles table task robots transport table group motion designated target position g destination group motion robots need move herd since table heavy large transported single robots robots share common control program best objective gp run find multirobot algorithm best executed robots parallel causes efficient table transport behavior group evolve programs robots activate candidate program parallel run team trial beginning trial robot locations chosen random arena different positions orientations trial robot granted total elementary movements robot allowed stop less steps reaches goal end trial robot gets fitness value measured summing contributions various factors terminal function symbols used building gp trees solve problem listed tables 2 3 terminal set consists six primitive actions forward avoid random move turn table turn goal stop function set consists six primitives obstacle robot table goal prog2 prog3 fitness case represents world 32 grid four robots 64 obstacles table transported see figure 1 example fitness cases set 200 training cases generated random used evolving programs independent set 200 random cases used test performance evolved programs robots use control program evaluate fitness robots made complete run program one robot fitness another measured raw fitness value eg individual generation g case c computed considering various factors include distance target robot number steps moved robot number collisions made robot distance starting final position robot penalty moving away robots bayesian genetic programming 223 table 2 gp terminal symbols multiagent task symbol description forward move one step forward current direction avoid check clockwise make one step first direction avoids collision random move move one step random direction turn make clockwise turn nearest direction table turn goal make clockwise turn nearest direction goal stop stay position table 3 gp function symbols multiagent task symbol description obstacle check collision obstacles robot check collision robots check table nearby goal check table nearby prog2 prog3 evalute two threesubtrees sequence 3 bayesian genetic programming section present theory genetic programing based bayesian inference general theory applied addressing two important issues genetic programming ie control program growth acceleration fitness evaluation section aims provide outline bayesian gp approach figure 2 example genetic program multiagent learning task nonterminal nodes denote checking sensor inputs robots terminals indicate actions taken meaning symbols described tables 2 3 distinguishing features different implementations algorithms detailed following two sections 31 bayesian formulation genetic programming genetic programming works initializing population programs iteratively producing next generation fitter programs ie ags ag4 ag denotes ith program generation g population size genetic operators mutation crossover used produce offspring programs parent programs new generations produced repeatedly maximum number generations g reached termination condition satisfied goodness fitness program measured terms set fitness cases training data programs considered model unknown process f generating data bayesian gp best program defined probable model given data plus prior knowledge problem domain bayes theorem provides direct method calculating probabilities w15x states posterior ie observing data dprobability program pdn apa pan ds pd pdn apa 1 space possible programs case taking discrete values integral replaced summation pais prior ie observing dataprobability distribution programs pdn ais likelihood program data relationship states models probabilities established borrowing concept energy statistical physics w26x regard gp system thermodynamic system every possible state system definite energy es system energy fluctuate assumed probability thermodynamicsystem state given temperature given aspss expyesrt 2 z esis energy system z normalization constant needed make distribution integrate sumto one distribution known canonical boltzmanndistribution bayesian genetic programming 225 canonical distribution prior distribution programs expressed aspas expyafa 3 zaa z ais normalizing constant 1rt f energy model aa equilibrium state example energy function f chosen total number nodes genetic program choice prior distribution says expect program size small rather large thus implementing parsimony pressure similarly likelihood models fitness data expressed aspdn expybfd 4 zd b f error function fitness set b controls variance noise z b normalization factor likelihood factor gives preference programs fit better less error forthe fitness cases initially shape prior probability distribution programs pais flat reflect fact little known advance evolution considered iterative process revising posterior distribution models pan dby combining prior pawith likelihood pdn generation bayes theorem 1is used estimate posterior fitness individuals prior fitness values posterior distribution pan dis used generate offspring objective bayesian genetic programming figure 3is find program ag maximizes posterior probability best est min arg max pgaig n 5 aiggag figure 3 outline bayesian genetic programming procedure 226 zhang g maximum number generations agis populations size ags aig 4mis16 posterior probability p ag n program ag computed respect gth population pdn aig paig pgaig n ds mg7 p dn ag likelihood pag prior probability degree ii belief ag note posterior probability approximated fixedsize population agwhich typically small subset entire program space genetic operators applied generate l offspring ax k 1l mally proceeds two steps first candidates generated sampling proposal distribution qgaxkn aig 8 specific form q n determined variation operators example subtree crossover two parents given qcaxkn ais psajn aipraxkn ai aj9 ajgag pan probability individual agbeing selected mate p ax n probability producing ax crossover candidate generated genetic operators accepted probability pgaxkn agaxkn aig min 1 pgaig n d5 10 p ag n dis computed 7and pax n posterior probability estimated respect current population pdn axkpaxk pgaxkn ds mg1 ax rejected 10 ag retained ie ax ag note kiki acceptance function exclude case generated crossover agand another parent ag ag j bayesian genetic programming 227 l offspring ax k 1l generated selected build new population ag q 1s aigq14is112 defines posterior distribution p ag n next generation mentioned formulation offspring selection intentionally general accommodate various forms existing selection schemes lselection w5 25x effect evolutionary inference step generation g g q 1 considered induce new fitness distribution p ag n dfrom priors pag posterior distribution p ag n following bayes formula using genetic operators based theoretical framework present following subsections two examples bayesian genetic programming employ specific techniques effective control evolutionary dynamics detailed procedures experimental results described sections 4 5 32 gp adaptie occams razor first bayesian gp focuses fact genetic programming iteratively searches populations probable likely terms data priors programs thus information gained previous generation used next generation revise prior seeing databelief true programs thus posterior distribution written form pdn aig pgy1aig pgaig n ds 13 mjs1 pdn ajgpgy1ajg priors p ag expressed explicitly function generation rather fixed prior p ag 7 computing posterior probabilities ag priors paare revised paby belief update function u pgas upgy1a pgaig n d14 implementation u described next section basically bayesian gp starts generating programs according initial prior distribution paon program sizes typically prior distribution given uniform reflecting fact little known advance optimal size genetic programs fitness fgof programs measured training cases results estimation likelihood programs details section 4 combining prior likelihood bayes formula get posterior probabilities p ag n current prior updated reflect new information gained posterior distribution note assign prior distributions complexity models programs addition information theory w12xsays probability code length complexityof models related lasylog pa making use complexity programs controlled evolve parsimonious accurate programs fact show section 4 adaptive occam method presented w44xis derived bayesian genetic programming framework 33 gp incremental data inheritance second example bayesian approach gp make use fact bayes formula suggests incremental evolutionary learning rule infer programs higher posterior probability existing programs observing new fitness cases leads writing posterior distribution pdig n aig pgy1aig n digy1 pgaign digs mg 15 data set dg variable generation number g specific single program ag collection dgconstitutes data population dg ii observation new data lead update prior distribution pgan dig upgy1an digy1 pgaig n dig 16 revision prior distribution process equation 14 except data dgy1 estimation likelihood thus posterior probabilities programs function generation rather fixed genetic programming incremental data inheritance idimethod w42xis example approach dg specifically satisfies following conditions entire data set given training method fitness programs estimated incrementally chosen data subsets dg rather whole data set thus evolution accelerated reducing effective number fitness evaluations details described section 5 4 bayesian gp parsimonious solutions statistical theories suggest models simple lack sufficient learning capability models complex may generalize poorly unseen bayesian genetic programming 229 data reviewed section 21 several researchers observed program size tends grow without bound bloat see example w22x section describe bayesian method evolving parsimonious programs 41 algorithm description gp algorithm adaptive occam method figure 4is bayesian gp procedure described figure 3 except three differences first raw fitness calculated step 2 derivation fitness function given section 42 second difference step 3 posterior probability 13is substituted 7 13the prior revised generation 7it constant third additional step 6 revision priors 42 fitness ealuation convenient implementation bayesian evolutionary algorithm take negative logarithm posterior probability p ag n use fitness function figsylog pgaig n d18 figure 4 outline bayesian genetic programming adaptive occams razor aor ag g ag evolutionary process reformulated minimization process est min arg min fig19 aiggag fitness function expressed figsylog pdn aig log paig 20 described previous section write likelihood function bayes theorem 1in form w9xpdn expybfd 21 zd b f error function b controls variance noise z b normalization factor assume data additive zeromean gaussian noise probability observing data value given input vector x would z b normalizing constant provided data points drawn independently distribution pdn aigs pycn xc aig 23 cs1s expybfd 24 zd b x g training cases f given cc fds edn aigs f xc aigy yc225cs1 also assume gaussian prior architecture program agwe zaa bayesian genetic programming 231 z ais normalizing constant example f chosen form aa fas caig uk2 27ks1 u parameters defining program ag choice prior distribution says expect complexity parameters small rather large thus implementing parsimony pressure substituting 24and 26into 20 fitness function expressed figs bfdq afa 28 bedn aig q acaig 29 first term reflects error second model complexity exact calculation constants b equation 29requires true probability distribution underlying data structure real situations unknown instead define adaptive fitness function general form figs eigq gcig30 egand cgare measures error complexity program ii parameter b absorbed adaptive parameter agwhich balances error complexity factors follows ebestgy 1 e gs n 2 cbestg 31n 2 ebestgy 1 cbestg otherwise adaptive occam method w44x userdefined constant e specifies maximum training error allowed run egy 1is error best best program generation g 1 c gis size best program generation best g estimated generation g 1 used balance error complexity terms obtain programs parsimonious possible sacrificing accuracy procedure estimating c gis given w44x best 43 discussion necessity difficulty noncoding segments introns genetic programming studied many authors w22 27 39x adaptive occam method deals noncoding segment problem using twophase strategy means adaptive fitness function first stage egy 1 e best growth noncoding segments encouraged increase diversity partial solutions second stage egy 1f e strong parsimony best pressure enforced prefer compact solutions transfer first stage second controlled bayesian inference constraint userdefined parameter e note posterior probability p ag n computed fitness values fgby taking exponential function pdn aig pgy1aig mjs1 pdn ajgpgy1ajg expyeigy gcig 32 mjs1 expyejgy gcjg shows revision priors p ag implemented update agin adaptive occam method since priors reflected c gwhich best leads revision ag also note minimization fgis equivalent minimum description length mdlprinciple w18 30x best model model whose total code length model description l agn dgand data description ldgn ag ii ii minimal information theory w12xthe optimal description length model given negative logarithm probability model laig sylog paig 3 similarly code length data given program ag given ldn aig sylog pdn aig 434 correspond two terms 20 ii 44 experimental results adaptive occam method complexity control applied multiagent learning task experiments performed using parameter values listed table 4 terminal set function set consist six primitives respectively given tables 2 3 set 200 training cases used evolving programs independent set 200 cases used evaluating generalization performance evolved programs egvalues program generation g measured average raw fitness values egless betterfor cases c eigs ei cg35 bayesian genetic programming 233 table 4 parameters used experiments gp adaptive occams razor parameter value population size 100 max generation crossover rate 09 mutation rate 01 training cases 200 test cases 200 n number fitness cases training set egvalue computed considering factors distance target robot number collisions made robot distance starting final position robot note multiagent task target output given training case goodness program measured scores penaltiesit collects running robots complexity gp tree defined size depth tree cigs k constant used k 2 figure 5 compares fitness values gp without aor averaged runs shown egvalues best individuals generation method tendency observed gp adaptive occams razor converges slightly faster baseline gp though significant difference final fitness figure 6 compares typical changes program complexity methods tree complexity measured terms tree size plus figure 5 comparison fitness values eigcomponent onlyof genetic programs multiagent task tendency observed gp adaptive occams razor converges slightly faster baseline gp though significant difference final fitness figure 6 evolution complexity gp trees multiagent task gp adaptive occams razor aorpromotes trees grow significant fitness errorreduction required prefers smaller trees larger ones fitness erroris comparable contrast gp without occam factor tends grow generation goes detailed results summarized table 5 gp aor achieved average 20 reduction program complexity without loss solution accuracy fact slight improvement training test performances concluded adaptive occam method evolves smaller programs without loss generalization capability programs 5 bayesian gp accelerated evolution 51 algorithm description algorithm incremental data inheritance figure 7is gp aor except training set dg increases generation additional step modification step 5 training set grows inheritance next section details data inheritance procedure table 5 effects adaptive occams razor gp programs multiagent learning task method complexity baseline 314 65 aoron complexity average fitness values average fitness training test sizes training test sets 200 respectively values averaged ten runs bayesian genetic programming 235 figure 7 outline bayesian genetic programming incremental data inheritance idi 52 data inheritance procedure basic idea genetic programming incremental data inheritance programs data evolved time program associated separate data set describe variant uniform crossover call uniform data crossover simplified example illustrating process given figure 8 first two parent data sets dg dg crossed inherit subsets two offspring data sets dgq1 dgq1 second data parents mixed union set redistributed two offspring dgq1 dgq1 size offspring data sets equal n n q l l g 1 data increment size thus size data sets monotonically increases generation goes maintain diversity training data inheritance import portion data base set import rate r given ris r 1 di0f r f 1 38 figure 8 illustrative example data inheritance idi two parent data sets dig dig merged form union set digqj inherited two offspring data sets digq1 djgq1 offspring data points shaded circles examples imported base data set maintain diversity data sets r constant import strength diversity measured ratio distinctive examples union set sy1 0f f 1 39 figure 8 illustrates process data inheritance two parent data sets size 6 unioned form genetic pool size 10 two offspring data sets size 8 inherited marked data imported basis data set maintain diversity 53 fitness ealuation implementation bayesian gp adaptive occams razor take negative logarithm p ag n use fitness function figsylog pgaig n dig 40 ag g ag dg p agn dgare defined 15 ii gi gg pdig n aig pgy1aig n digy1 pgain dis mg gy1 41 evolutionary process reformulated minimization process est min arg min fig42 gfgmam aig dig fitness function expressed figsylog pdig n aig log pgy1aig n digy1 43 bayesian genetic programming 237 using similar arguments section 42 write likelihood function prior distribution programs formpdign aigs expybfd 44 zaa z b z aare normalizing constants define adaptive fitness function general form figs eigq gcig46 egand cgrepresents error factor f complexity factor f ii b absorbed ag parameter agbalances two factors follows ebestgy 1 e gs ng2 cbestg 47ng2 ebestgy 1 cbestg otherwise generalization adaptive occam method n variable rather fixed value n scheduled increase monotonically function generation g 54 discussion incremental data inheritance method interesting properties characterize stability algorithm generation goes maintains growing subset given fitness cases program monotonic growth fitness subset ensures idi method eventually achieves level performance baseline algorithm contrasted many existing methods fitness case selection including lef w14xand rat w38x maintaining separate subset program allows learned component program retained evolution time import new fitness cases base set allows programs learn new situations thus leading gradual improvement performance 55 experimental results compare performance gp incremental data inheritance idito baseline gp ie one uses complete training cases outset experiments performed using parameter values listed table 6 gp table 6 parameters used experiments gp incremental data inheritance parameter value population size 100 max generation crossover rate 09 mutation rate 01 training cases 200 test cases 200 initial data size n 20data increment size l 6 runs idi used 20 q 6g examples generation g selected given data set ie n 20 l 6 fitness evaluation methods totalof 200 training cases used training independent set 200 test cases used evaluating generalization performance evolved programs fitness factors egand cgare experiments gp ii adaptive occams razor except training set size egis n ig instead n results shown two different forms figure 9 shows evolution fitness function generation number significant difference performance since gp idi uses variable data size computing time generation measured product population size data size result shown figure 10 idi achieved speedup factor approximately four compared baseline gp detailed results summarized table 7 incremental data inheritance idimethod used 50 time required baseline gp interesting see despite figure 9 comparison fitness values function generation number curves mean values ten runs gp idi shows loss fitness values compared baseline gp algorithm bayesian genetic programming 239 figure 10 comparison fitness values function number function evaluations curves mean values ten runs gp idi converges much faster baseline gp algorithm table 7 effects incremental data inheritance idion time average fitness values lower betterof gp programs multiagent learning task sizes training test sets 200 respectively values averaged ten runs time measured total number fitness evaluations also shown standard deviations average fitness method time training test baseline 1220000 2158 83 2213 76 idi reduced data size generalization performance gp idi slightly better baseline gp 6 concluding remarks presented bayesian framework genetic programming two specific gp algorithms derived framework reducing time space complexity genetic programming applied multiagent learning task first method ie gp adaptive occams razor aor achieved approximately 20 reduction program complexity without loss fitness values second bayesian approach genetic programming ie incremental data inheritance idimethod used 50 time required baseline gp achieve little better fitness level though improvement significant mentioned still much room reduction spacertime complexity genetic program ming future work address following issues among others one incorporating better genetic operators focused present work dynamics phenotypic level improvement achieved finding intelligent variation operators adapt dynamics genotypic level terms bayesian genetic programming involves adapting proposal functions second issue improve modularity genetic programs automatically designing adapting reusable submodules adfs libraries combined occams razor improve comprehensibility reusability genetic programs well speed gp process theoretical point view bayesian framework genetic programming provides number important features one formulating gp process bayesian inference principled techniques driving evolutionary dynamics conventional gps developed addition methods presented paper one think methods within framework example decisionmaking made robust combining multiple programs instead single gp tree bayesian gp framework provides principled way combine multiple programs build committee machine another important feature bayesian inference allows background knowledge problem domain incorporated formal way instance guess distribution specific function symbols good gp trees knowledge reflected application probabilities genetic operators background knowledge important especially solving reallife problems practical interest finally bayesian analysis genetic programming appears useful tool incorporating various genetic programming procedures uniform frame work theoretical framework crucial design comparative analysis various genetic programming algorithms acknowledgments research supported korea science engineering foundation kosefgranta 98109203502 korea research foundation krf granta 1998001e01025 korea ministry science technology kistep granta br21g06 thanks wolfgang banzhaf three anonymous reviewers helpful comments improved readability paper r automatically defined features simultaneous evolution 2dimensional feature detectors algorithm using genetic programming emergent intelligence introduction evolving compact solutions genetic programming case study evolutionary programming tree mutations evolving computer programs without crossover elements information theory dynamic training subset selection supervised learning genetic programming small populations many generations beat large populations generations genetic programming bayesian data analysis perturbing representation decoding evaluation chromosomes coevolving parasites improves simulated evolution optimization procedure genetic programming using minimum description length principle generality difficulty genetic programming evolving sort programming computers means natural selection scalable learning genetic programming using automatic function definition fitness causes bloat mutation size fair homologous tree crossovers comparison crossover mutation genetic programming science breeding application breeder genetic algorithm probabilistic inference using markov chain monte carlo methods explicitly defined introns destructive crossover genetic programming search properties different crossover operators genetic programming stochastic complexity modeling analysis complexity drift genetic programming discovery subroutines genetic programming evolutionary identification macromechanical models competitively evolving decision trees fixed training cases natural language processing code growth genetic programming effects code growth parsimony pressure populations genetic programming simultaneous evolution programs control structures automatically choosing number fitness cases rational allocation trials empirical studies genetic algorithm noncoding segments accelerated learning active example selection bayesian framework evolutionary computation genetic programming incremental data inheritance genetic programming minimal neural nets using occams razor balancing accuracy parsimony genetic programming evolutionary induction sparse neural trees tr ctr byoungtak zhang unified bayesian framework evolutionary learning optimization advances evolutionary computing theory applications springerverlag new york inc new york ny sean luke modification point depth genome growth genetic programming evolutionary computation v11 n1 p67106 spring