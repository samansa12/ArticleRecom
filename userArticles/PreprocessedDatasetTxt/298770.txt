coevolution successful learning backgammon strategy following tesauros work tdgammon used 4000 parameter feedforward neural network develop competitive backgammon evaluation function play proceeds roll dice application network legal moves selection position highest evaluation however backpropagation reinforcement temporal difference learning methods employed instead apply simple hillclimbing relative fitness environment start initial champion zero weights proceed simply playing current champion network slightly mutated challenger changing weights challenger wins surprisingly worked rather well investigate peculiar dynamics domain enabled previously discarded weak method succeed preventing suboptimal equilibria metagame selflearning b introduction took great chutzpah gerald tesauro start wasting computer cycles temporal difference learning game backgammon tesauro 1992 letting machine learning program play hopes becoming expert indeed dream computers mastering domain selfplay introspection around since early days ai forming part samuels checker player samuel 1959 used donald michies menace tictactoe learner michie 1961 selfconditioning systems later generally abandoned field due problems scale weak nonexistent internal representations moreover self playing learners usually develop eccentric brittle strategies appear clever fare poorly expert human computer players yet tesauros 1992 result showed selfplay approach could powerful refinement millions iterations selfplay tdgammon program become one best backgammon players world tesauro 1995 derived weights viewed corporation significant enough intellectual property keep trade secret except leverage sales minority operating system international business machines 1995 others replicated td result backgammon research purposes boyan 1992 commercial purposes reinforcement learning limited success areas zhang dietterich 1996 crites barto 1996 walker et al 1994 respect goal selforganizing learning machine starts minimal specification rises great sophistication tdgammon stands alone success understood explained replicated domains hypothesis success tdgammon principally due backpropagation reinforcement temporaldifference technologies inherent bias dynamics game backgammon coevolutionary setup training task dynamically changes learning progresses test hypothesis using much simpler coevolutionary learning method backgammon namely hillclimbing 2 implementation details use standard feedforward neural network two layers sigmoid set fashion tesauro 1992 4 units represent number players pieces 24 points plus 2 units indicate many bar board addition added one unit reports whether game reached endgame race situation making total 197 input units fully connected 20 hidden units connected one output unit judges position including bias hidden units makes total 3980 weights game played generating legal moves converting proper network input picking position judged best network started weights set zero initial algorithm hillclimbing 1 add gaussian noise weights 2 play network mutant number games 3 mutant wins half games select next generation noise set step would 005 rms distance euclidean distance divided surprisingly worked reasonably well networks evolved improved rapidly first sank mediocrity problem perceived comparing two close backgammon players like tossing biased coin repeatedly may take dozens even hundreds games find sure better replacing welltested champion dangerous without enough information prove challenger really better player lucky novice rather burden system much computation instead introduced following modifications algorithm avoid buster douglas effect 2 firstly games played pairs order play reversed random seed used generate dice rolls games washes unfairness due dice rolls two networks close particular identical result would always one win though admittedly make different moves early game good dice roll particular move one game may turn bad roll corresponding move parallel game secondly challenger wins contest rather replacing champion challenger instead make small adjustment direction champion idea similar inertia term backpropagation rumelhart et al 1986 introduced assumption small changes weights would lead small changes decisionmaking evaluation function biting ear challenger adding champion current decisions preserved would less likely catastrophic replacement champion lucky novice challenger initial stages evolution two pairs parallel games played challenger required win 3 4 games although would liked rank players players used neurogammon gammontool available us figure 1 shows first 35000 players rated pubeval moderately good publicdomain player trained tesauro using human expert preferences three things note 1 percentage wins pubeval increases 0 33 20000 generations 2 frequency successful challengers increases time player improves 3 epochs eg starting 20000 performance pubeval begins falter first fact shows simple self 2 buster douglas world heavyweight boxing champion 9 months 1990 playing hillclimber capable learning second fact quite counterintuitive expected player improved would harder challenge true respect uniform sampling 4000 dimensional weight space true sampling neighborhood given player player good part weight space small changes weights lead mostly similar strategies ones make mostly moves situations however games using determine relative fitness increased rate change allows system drift may account subsequent degrading performanceto counteract drift decided change rules engagement evolution proceeds according following annealing schedule 10000 generations number games challenger required win increased 3 4 5 6 70000 generations increased 7 8 course bout abandoned soon champion one game making average number games per generation considerably less 8 numbers 10000 70000 chosen ad hoc basis observing frequency successful challenges buster douglas effect particular run later experiments showed determine annealing schedule principled manner see section 32 100000 games using simple hillclimb developed surprising player capable winning 40 games pubeval networks sampled every 100 generations order test performance networks generation 1000 10000 100000 extracted used benchmarks figure 2 shows percentage wins sampled players three benchmark networks note three curves cross 50 line 1 10 100 respectively show general improvement time endgame backgammon called bearoff used another yardstick progress learning bearoff occurs players pieces home board first 6 points dice rolls used remove pieces figure 1 percentage wins first 35000 generation players pubeval match consisted 200 games generation win board test networks ability endgame set racing board two pieces players 1 7 point one piece 8 point graph figure 3 shows average number rolls bearoff network playing using fixed set 200 random dicestreams note pubeval stronger 166 rolls discuss strengths tesauros 1992 results section 5 figure 2 percentage wins benchmark networks 1000 upper 10000 middle 100000 lower shows noisy nearly monotonic increase player skill evolution proceeds win generation generation figure 3 average number rolls bearoff generation sampled 200 dicestreams pubeval averaged 166 rolls task 3 analysis 31 learnability unlearnability learnability formally defined time constraint search space hard randomly pick 4000 floatingpoint weights make good backgammon evaluator simply impossible hard find weights better current initially weights random quite easy playing improves would expect get harder harder perhaps similar probability tornado constructing 747 junkyard however search neighborhood current weights find many similar players make mostly moves capitalize others slightly different choices exposed weaknesses tournament note different point tesauro originally made feedforward neural network could exploit similarity positions although setting parameters initial runs involved guesswork large set players examine try understand phe nomenon taking champion networks generation 1000 10000 100000 run sampled random players neighborhoods different rms distances find likely find winning challenger thousand random neighbors 11 different rms distances played 8 games corresponding champion figure 4 plots fraction games challengers function rms distance graph shows players improve time probability finding good challengers neighborhood increases accounts frequency successful challenges goes 3 successive challenger required 3 number good challengers neighborhood go algorithm falter nonetheless several factors require study may due general growth weights less variability strategy among mature players less ability simply tell expert players apart games figure 4 distance versus probability random challenger winning champions generation 1000 10000 100000 distance champion 100k wins challenger take small step changing moves champion order beat hope coevolution apparently unlearnable becomes learnable convert single question continuous stream questions one dependent previous answer 32 replication experiments first successful run tried evolve ten players using parameters annealing schedule 10000 70000 found one ten players even competitive closer examination suggested nine runs failing annealed early frequency successful challenges reached appropriate level premature annealing made task challengers even harder challenger success rate fell even lower therefore abandoned fixed annealing schedule instead annealed whenever challenger success rate exceeded 15 averaged 1000 generations ten players evolved regime competitive though quite good original player apparently benefitted extra inductive bias due tailormade annealing schedule refining heuristics schedules could lead superior players goal 33 relative versus absolute expertise backgammon allow relative expertise absolutely optimal strategy theoretically exists perfect policy backgammon would deliver minimax optimal move position perfect policy could exactly rate every player linear scale practice especially without running 10000 games verify seems many relative cycles help prevent early convergence cellular studies iterated prisoners dilemma following axelrod 1984 stable population tit tat invaded cooperate allows exploitation defect kind relativeexpertise dynamics seen clearly simple game rockpaperscissors littman 1994 might initially seem bad selfplay learning looks like advance might actually lead cycle mediocrity small group champions dominance circle arise hold temporal oligopoly preventing advance hand may basic form instability prevents formation suboptimal oligopolies allows learning progress problems specific nonzerosum games zero sum games appropriate use selfplay shown converge optimal play parties 4 discussion believe evidence success learning backgammon using simple hillclimbing relative fitness environment indicates reinforcement temporal difference methodology used tesauro 1992 paper led tdgammon providing advantage essential success rather major contribution came coevolutionary learning environment dynamics back gammon result thus similar bias found mitchell et al packards evolution cellular automata edge chaos packard 1988 mitchell et al 1993 obviously suggesting 11 hillclimbing advanced machine learning technique others bring many tasks without internal cognition opponents behavior coevolution usually requires population therefore must something domain helpful permitted td learning hillclimbing succeed selfplay would clearly fail problemsolving tasks scale section discuss issues coevolutionary learning dynamics backgammon may critical learning success 41 evolution versus coevolution tdgammon major milestone kind evolutionary machine learning initial specification model far simpler expected learning environment specified implicitly emerges result coevolution learning system training environment learner embedded environment responds improvements hopefully neverending spiral though elusive goal achieve practice coevolutionary effect seen population models completely unexpected 11 hillclimbing evolution coevolution explored sorting network problem hillis 1992 tictactoe strategy games angeline pollack 1994 rosin belew 1995 schraudolph et al 1994 predatorprey games cliff miller 1995 reynolds 1994 classification problems intertwined spirals problem juille pollack 1995 however besides tesauros tdgammon date viewed instance coevolutionary learning sims artificial robot game sims 1994 domain complex backgammon substantial success since weak player sometimes defeat strong one theory possible network learn backgammon static evolutionary environment playing fixed opponent rather coevolutionary one playing course interesting acheivement learning without expert hand tdgammon simply learned neurogammon wouldnt startling result order isolate contribution coevolutionary learning modify training setup original algorithm appropriate selfplay new setup current champion mutant play number games opponent called foil dicestreams weights adjusted champion loses games mutant wins number pairs games initially set 1 incremented whenever challenger success rate exceeded 15 averaged 1000 generations lower three plots figure 5 track performance algorithm three benchmark networks original experiments acting foil seem show relationship learning rate probability winning weak foil 1k learning fast initially probability winning around 50 tapers probability increases strong foil 100k learning slow initially probability winning small speeds increases towards 50 evolutionary runs outperformed coevolutionary version foil algorithm coev champion network plays role foil coevolution seems maintain high learning rate throughout run automatically providing new generation player opponent appropriate skill level keep probability winning near 50 moreover weaknesses foil less likely bias learning process automatically corrected coevolution proceeds see also section 43 42 dynamics backgammon general problem learning selfplay discovered repeatedly early ai ml learner could keep playing kinds games exploring narrow region strategy space missing critical areas game would vulnerable programs human experts problem particularly prevalent deterministic games chess tictactoe tesauro 1992 pointed features backgammon make suitable approaches involving selfplay random initial conditions unlike chess draw impossible game played untrained network making random moves eventually terminate though may take much longer game competent players moreover randomness dice rolls leads selfplay much larger part search space would likely explore deterministic game worked using population get around limitations selfplay angeline pollack 1994 schraudolph et al 1994 added nondeterminism game go choosing moves according boltzmann distribution statistical mechanics others fogel 1993 expanded exploration forcing initial moves epstein 1994 studied mix training using selfplay random testing playing expert order better understand aspects game learning generation figure 5 performance pubeval players evolved playing benchmark networks original run generation 1k 10k 100k compared coevolutionary variant algorithm plots average four runs performance original algorithm included comparison original coev 100k believe enough add randomness game force exploration alternative training paradigms something critical dynamics backgammon sets apart games random elements like monopoly namely outcome game continues uncertain contact broken one side clear advantage monopoly early advantage purchasing properties leads accumulating returns many observers find exciting backgammon helps novice sometimes overcome expert number situations one dice roll improbable sequence dramatically reverse player expected win order quantify reversibility effect collected statistics games played 100000th generation network n 0 120 collected 100 different games still contact move n n6 100 games reached racing stage move n still move number standard deviation 12005move number contact racing game probability figure standard deviation probability winning contact positions racing positions contact racing figure b probability game still contact racing stage move n figure 7 smoothed distributions probability winning function move number contact positions left racing positions right density density probability winning move number move probability progress estimated probability winning 100 positions playing 200 different dicestreams figure 6 shows standard deviation probability assuming mean 05 function n well probability game still contact racing stage move n figure 7 shows distribution probability winning function move number symmetrized smoothed convolution gaussian function data indicate probability winning tends hover near 50 early stages game gradually moving play proceeds typically remaining within range 15 85 long still contact thus allowing reasonable chance reversal numbers could different players less reversability stronger players perhaps weaker ones believe effect remains integral part game dynamics regardless expertise conjecture dynamics facilitate learning process providing almost every situation nontrivial chance winning nontrivial chance losing therefore potential learn consequences current move deep contrast many domains early blunders could lead hopeless situation learning virtually impossible reward already become effectively unat tainable seems feature backgammon may also shared tasks tdlearning successful zhang dietterich 1996 crites barto 1996 walker et al 1994 43 avoiding suboptimal equilibria metagame learning learning system viewed interaction teacher student teachers goal expose students weaknesses correct students goal placate teacher avoid correction build model teacherstudent interaction formal game call metagame learning mgl avoid confusion game learned metagame teacher presents student sequence questions prompting responses r student backgammon domain questions responses would legal positions rolls moves receive payoffs process attempt maximize choices questions answers limited abilities selfmodification generally assume goal learning prepare student interaction complex environment e provide objective measure perfor mance 4 e thus play similar roles assumed identical question find payoff matrix enable performance continually improve measured e rewards closely correlated may tempted ask questions easy anticorrelated example te questions might difficult either case hard learn see section 41 4 general theory evolution selforganization e necessary attractive solution problem two students play role teacher indeed single student act teacher thus providing questions always appropriate level difficulty dynamics mgl selfteaching coevolutionary situation would hopefully lead continuing spiral improvement may instead get bogged antagonistic collusive dynamics depending payoff structure hillclimbing setup may think mutant teacher trying gain advantage adjustment weights exploiting weaknesses champion champion student trying avoid adjustment allowing weaknesses exploited since student teacher approximately equal ability advantage student narrow scope search thus limiting domain within teacher able look weakness games chess tictactoe student could achieve aiming draw instead win always playing particular style game draws allowed teacher student may figure way collude example throwing alternate games angeline 1994 making suboptimal sequence early moves effects selflearning systems may appear early convergence evolutionary algorithms narrowing scope drawing collusion teacher student fact nash equilibria mgl call mediocre stable states 5 hypothesis certain features backgammon operate formation mediocre stable states mgl backgammon ergodic sense position reached position 6 sequence moves dice rolls apparently create enough randomness prevent either player following strategy narrows scope game appreciably moreover early suboptimal moves unlikely provide opponent easy win see section 42 collusion throwing alternate games prevented mediocre stable states also arise human education systems example student gets answers right rewards teacher positive teaching evaluations asking harder questions work hope apply kind mgl equilibrium analysis issues human education 5 conclusions tdgammon remains tremendous success machine learning causes success well understood fundamental research tesauros 1992 paper basis tdgammon reportedly beat suns gammontool 60 65 time depending number hidden units achieved parity neurogammon 10 following seminal 1992 paper tesauro incorporated number handcrafted expertknowledge features eventually engineering network achieved world 5 mss follows maynard smiths ess maynard smith 1982 6 exception racing situations positions pieces play master level play tesauro 1995 features included concepts like existence prime probability blots hit probability escape behind oppo nents barrier evaluation function also improved using multiple ply search best players weve able evolve win 45 time pubeval believed level tesauros 1992 networks tesauro never compared 1992 networks pubeval used gammontools heuristic endgame ratings level play achieved players somewhat murky testing procedure play game network becomes race use gammontools algorithm move sides end also penalize td net learned rather poorly racing phase gamep 272 compare networks performance pubeval must noted use networks weak endgame rather substituting much stronger expert system like gammontool gerald tesauro commentary issue graciously cleared matter comparing pubeval 1992 results differs somewhat conclusions two phenomena fom 1992 paper relevant work performance 248position racing test set reached 65 substantially worse racing specialists described previous section p training times order 50000 training games networks games 20hidden unit net 200000 games 40hidden unit net p 273 achieve similar levels skills observe phenomena training endgame weakness convergence believe achieved results substantially similar tesauros 1992 result without advanced learning algorithms could make stronger players tuning learning parameters adding input features point claim 100000th generation player anywhere near good current enhanced versions tdgammon ready challenge best humans surprisingly good considering humble origins hillclimbing relative fitness measure tuning parameters adding input features would make powerful players point study also claim anything wrong td learning hillclimbing good reinforcement learning general course isnt point environment representation refined work well machine learning method benchmarked weakest possible algorithm credit learning power properly distributed noticed several weaknesses player stem training yet reward punish double triple costs associated severe losses gammoning backgammoning take account gambling process doubling continuing develop player sensitive issues game interested players challenge 100000th network using web browser home page conclusion replicating tesauros 1992 tdgammon success much simpler learning paradigm find reinforcement temporal difference methods primary cause success rather dynamics backgammon combined power coevolutionary learning isolate features backgammon domain enable coevolutionary reinforcement learning work well may lead better understanding conditions neces sary general complex selforganization acknowledgments work supported onr grant n000149610418 krasnow foundation postdoctoral fellowship thanks gerry tesauro providing pubeval subsequent means calibrate jack laurence pablo funes development www front end evolved player comments brandeis demo group anonymous referees justin boyan tom dietterich leslie kaelbling brendan kitts michael littman andrew moore rich sutton wei zhang r competitive environments evolve better solutions complex tasks alternate interpretation iterated prisoners dilemma evolution nonmutual cooperation evolution cooperation modular neural networks learning contextdependent game strategies tracking red queen measurements adaptive progress coevolutionary simulations improving elevator performance using reinforcement learning massively parallel genetic programming markov games framework multiagent reinforcement learning algorithms sequential decision making revisiting edge chaos evolving cellular automata perform computations adaptation towards edge chaos studies machine learning using game checkers temporal difference learning position evaluation game go evolving 3d morphology behavior competition learning predict methods temporal differences connectionist learning expert preferences comparison training practical issues temporal difference learning temporal difference learning tdgammon temporal difference tr ctr gerald tesauro comments coevolution successful learning backgammon strategy machine learning v32 n3 p241243 sept 1998 david b fogel beyond samuel evolving nearly expert checkers player advances evolutionary computing theory applications springerverlag new york inc new york ny ji grim petr somol pavel pudil probabilistic neural network playing learning tictactoe pattern recognition letters v26 n12 p18661873 september 2005 multiagent system integrating reinforcement learning bidding genetic algorithms web intelligence agent system v1 n34 p187202 december multiagent system integrating reinforcement learning bidding genetic algorithms web intelligence agent system v1 n34 p187202 march gerald tesauro programming backammon using selfteaching neural nets artificial intelligence v134 n12 p181199 january 2002 elizabeth sklar mathew davies multiagent simulation learning environments proceedings fourth international joint conference autonomous agents multiagent systems july 2529 2005 netherlands yeo keun kim jae yun kim yeongho kim tournamentbased competitive coevolutionary algorithm applied intelligence v20 n3 p267281 mayjune 2004 elizabeth sklar mathew davies min san tan co simed simulating education multi agent system proceedings third international joint conference autonomous agents multiagent systems p9981005 july 1923 2004 new york new york edwin de jong maxsolve algorithm coevolution proceedings 2005 conference genetic evolutionary computation june 2529 2005 washington dc usa jordan b pollack hod lipson gregory hornby pablo funes three generations automatically designed robots artificial life v7 n3 p215223 summer 2001 pablo funes jordan pollack evolutionary body building adaptive physical designs robots artificial life v4 n4 p337357 october 1998 frans oliehoek edwin de jong nikos vlassis parallel nash memory asymmetric games proceedings 8th annual conference genetic evolutionary computation july 0812 2006 seattle washington usa jordan b pollack hod lipson sevan ficici pablo funes greg hornby richard watson evolutionary techniques physical robotics creative evolutionary systems morgan kaufmann publishers inc san francisco ca 2001 edwin de jong monotonic archive paretocoevolution evolutionary computation v15 n1 p6193 spring 2007 john cartlidge seth bullock combating coevolutionary disengagement reducing parasite virulence evolutionary computation v12 n2 p193222 june 2004 stephan k chalup alan blair incremental training first order recurrent neural networks predict contextsensitive language neural networks v16 n7 p955972 september edwin de jong jordan b pollack ideal evaluation coevolution evolutionary computation v12 n2 p159192 june 2004 cooperative multiagent learning state art autonomous agents multiagent systems v11 n3 p387434 november 2005 darse billings lourdes pea jonathan schaeffer duane szafron learning play strong poker machines learn play games nova science publishers inc commack ny 2001