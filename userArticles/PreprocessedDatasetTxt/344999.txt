distributed schur complement techniques general sparse linear systems paper presents preconditioning techniques solving general sparse linear systems distributed memory environments techniques utilize schur complement system deriving preconditioning matrix number ways two preconditioners consist approximate solution process global system exploits approximate lu factorizations diagonal blocks schur complement another preconditioner uses sparse approximateinverse technique obtain certain local approximations schur complement comparisons reported systems varying difficulty b introduction successful solution many grandchallenge problems scientific computing depends largely availability adequate large sparse linear system solvers context iterative solution techniques becoming mandatory replacement direct solvers due moderate computational storage demands typical grandchallenge application requires use powerful parallel computing platforms well parallel solution algorithms run platforms distributedmemory environments iterative methods relatively easy implement compared direct solvers often preferred spite unpredictable performance certain types problems however users iterative methods face number issues arise direct solution methods particular easy predict fast linear system solved certain accuracy whether solved certain types iterative solvers depends algebraic properties matrix condition number clustering spectrum good preconditioner total number steps required convergence reduced dramatically cost slight increase number operations per step resulting work supported part arpa grant number nist 60nanb2d1272 part nsf grant ccr9618827 part minnesota supercomputer institute department computer science engineering university minnesota 200 union street se min neapolis mn 55455 emailsaadcsumnedu z department computer science 320 heller hall 10 university drive duluth minnesota 558122496 mashadumnedu much efficient algorithms general distributed environments additional benefit preconditioning reduces parallel overhead therefore decreases total parallel execution time parallel overhead time spent parallel algorithm performing communication tasks idling due synchronization requirements algorithm efficient construction application preconditioning operation small parallel overhead parallel preconditioner may developed two distinct ways extracting parallelism efficient sequential techniques designing preconditioner start specifically parallel platforms two approaches advantages disadvantages first approach preconditioners yield good convergence properties sequential method often low degree parallelism leading inefficient parallel implementations contrast second approach usually yields preconditioners enjoy higher degree parallelism may inferior convergence properties paper addresses mainly issue developing preconditioners distributed sparse linear systems regarding systems distributed objects viewpoint common framework parallel iterative solution techniques 15 14 18 20 10 1 2 8 borrows ideas domain decomposition methods prevalent pde literature key issue develop preconditioners global linear system exploiting distributed data structure recently number methods developed exploit schur complement system related interface variables see example 12 2 8 particular several distributed preconditioners included parpre package 8 employ variants schur complement techniques one difference work 2 approach construct matrix approximate global schur complement instead preconditioners constructed entirely local however also global nature attempt solve global schur complement system approximately iterative technique paper organized follows section 2 gives background regarding distributed representations sparse linear systems section 3 starts general description class domain decomposition methods known schur complement techniques section also presents several distributed preconditioners defined via various approximations schur complement numerical experiment section section 4 contains comparison preconditioners solving various distributed linear systems finally concluding remarks made section 5 distributed sparse linear systems consider linear system form large sparse nonsymmetric real matrix size n often solve system distributed memory computer graph partitioner first invoked partition adjacency graph based resulting partitioning data distributed processors pairs equationsunknowns assigned processor thus processor holds set equations rows linear system vector components associated rows good distributed data structure crucial development effective sparse iterative solvers important example convenient representation local equations well dependencies local external vector components preprocessing phase thus required determine dependencies information needed iteration phase approach described follows used psparslib package see 20 22 14 additional details figure 1 shows physical domain viewpoint sparse linear system representation borrows domain decomposition literature term subdomain often used instead proper term subgraph point node belonging subdomain actually pair representing equation associated unknown common distinguish three types unknowns 1 interior unknowns coupled local equations 2 local interface unknowns coupled nonlocal external local equations 3 external interface unknowns belong subdomains coupled local equations matrix figure 2 viewed reordered version equations associated local numbering equationunknown pairs note local equations necessarily correspond contiguous equations original system points external interface points internal points figure 1 local view distributed sparse matrix figure 2 rows matrix assigned certain processor split two parts local matrix acts local vector components rectangular interface matrix x acts external vector components accordingly local equations written follows represents vector local unknowns iext external interface variables b local part righthand side vector similarly global matrixvector product ax performed three steps first multiply local vector components x receive external interface vector components iext processors finally multiply received data x add result already obtained figure 2 partitioned sparse matrix important feature data structure used separation interface points interior points processor local points ordered interface points listed last interior points ordering local data presents several advantages including efficient interprocessor communication reduced local indirect addressing matrixvector multiplication local ordering local vector unknowns x split two parts subvector internal vector components followed subvector local interface vector components righthand side b conformally split subvectors f g ie block partitioned according splitting local matrix residing processor form local equations 2 written follows n set indices subdomains neighbors subdomain term part product x iext reflects contribution local equation neighboring subdomain j sum contributions result multiplying x external interface unknowns clear result multiplication affects local interface unknowns indicated zero top part second term lefthand side 4 preprocessing phase construct datastructure representing matrices x also form additional data structures required prepare intensive communication takes place iteration phase particular processor needs know 1 processors must communicate 2 list interface points 3 breakup list sublists must communicated among neighboring processors details see 20 22 14 3 derivation schur complement techniques schur complement techniques refer methods iterate interface unknowns implicitly using internal unknowns intermediate variables strategies deriving schur complement techniques described first schur complement system derived 31 schur complement system consider equation 2 block form 4 schur complement systems derived eliminating variable u system 4 extracting first equation u yields upon substitution second equation local schur complement equations 5 subdomains constitute system equations involving interface unknown vectors reduced system natural block structure related interface points subdomainb diagonal blocks system matrices dense general offdiagonal blocks identical involved global system 4 sparse system 7 written vector interface variables g righthand side vector throughout paper abuse notation slightly transpose operation defining rather actual transpose matrix column vectors p matrix global schur complement matrix exploited section 33 32 schur complement iterations one simplest ideas comes mind solving schur complement system 7 use block relaxation method associated blocking system schur complement system solved interface variables available variables obtained solving local systems known consistent choice initial guess blockjacobi sor iteration reduced system equivalent blockjacobi iteration resp sor global system see eg 11 19 kth step blockjacobi iteration global system takes following local form gammas asterisk denotes nonzero block whose actual expression unimportant worthwhile observation iterates interface unknowns satisfy independent relation equivalently nothing jacobi iteration schur complement system 7 global viewpoint primary iteration global unknowns explained vectors interface unknowns associated primary iteration satisfy iteration called schur complement iteration matrix g known explicitly easy advance iteration one step arbitrary starting vector v meaning easy compute gvh v viewpoint taken 13 12 sequence k accelerated krylov subspace algorithm gmres 21 one way look acceleration procedure consider solution system righthand side h obtained one step iteration 12 computed initial vector 0 ie given initial guess 0 initial residual obtained matrixvector products gamma g obtained one step primary iteration compute gy proceed follows 1 perform one step primary iteration 2 set w 3 compute presented global viewpoint shows schur complement technique derived primary fixedpoint iteration global unknowns among possible choices primary iteration jacobi sor iterations well iterations derived somewhat artificially ilu preconditioning techniques main disadvantage solving schur complement system solve system needed operate matrix accurate compute dense matrix explicitly solve system 5 using computation matrixvector product carried three sparse matrixvector multiplies one accurate linear system solve known see 23 large computational expense accurate solves resulting decrease iteration counts sufficient make schur complement iteration competitive numerical experiments confirm 33 induced preconditioners key idea domain decomposition methods develop preconditioners global system 1 exploiting methods approximately solve reduced system 7 tech niques termed induced preconditioners see eg 19 best explained considering reordered version global system 1 internal vector components labeled first followed interface vector components reordering leads block systemb also rewritten b f u note b block acts interior unknowns eliminating unknowns system leads schur complement system 7 induced preconditioners global system obtained exploiting block lu factorization consider factorization global schur complement schur complement matrix identical coefficient matrix system 7 see eg 19 global system 15 preconditioned approximate lu factorization constructed approximation two techniques type discussed rest section first one exploits relation lu factorization schur complement matrix second uses approximateinverse techniques obtain approximations local schur complements preconditioners presented next based global system equations schur unknowns system 5 equivalent form 7 global block lu factorization 16 rather approximated version 17 34 approximate schur lu preconditioner idea outlined previous subsection approximation schur complement available approximate solve whole matrix global unknowns obtained require approximate exact solves b also possible think locally order act globally consider 4 5 readily seen 4 approximations components interface unknowns available corresponding approximations internal components u immediately obtained solving matrix b processor practice often simpler solve slightly larger system obtained 2 availability specific local data structure return problem finding approximate solutions schur unknowns convenience 5 rewritten preconditioned system diagonal blocks note simply blockjacobi preconditioned schur complement system system 19 may solved gmreslike accelerator requiring solve step least three options carrying solve 1 compute exactly form lu factorization seen shortly representation obtained directly lu factorization 2 use approximate lu factorization obtained approximate lu factorization 3 obtain approximation using approximateinverse techniques see next sub section factor using ilu technique methods options 1 2 based following observation see 19 let form 3 factored u rather useful result l equal schur complement associated partitioning 3 result easily established transferring matrices ub u umatrix lmatrix factorization result follows comparison 16 approximate factorization available approximate lu factorization obtained canonically extracting related parts l u matrices words ilu factorization schur complement trace global ilu factorization unknowns associated schur complement local schur complement ilu factorization obtained manner leads approximation local schur complement instead exact schur complement system 5 equivalently 19 following approximate local schur complement system derived 19 considered processor global system related equations 20 solved krylov subspace method eg gmres matrixvector operation associated solve involves certain matrix cf equation 17 global preconditioner 17 defined given local ilu factorization factorization associated following algorithm applies processor global approximate schur lu preconditioner block vector f obtain solution algorithm uses iterations gmres without restarting solve local part schur complement system 20 interior vector components calculated using equation 18 lines 2125 description algorithm 31 p represents projector maps whole block vector subvector vector g associated interface variables algorithm 31 approximate schurlu solution step gmres 1 given local righthand side 2 define hm set 3 arnoldi process 4 0 5 r l u 7 8 exchange interface vector components 9 l 11 12 h lj w v l 13 w 14 enddo 15 h j1j kwk 2 v j1 wh j1j 16 enddo 17 18 form approximate solution interface variables 19 compute 21 find local unknowns 22 exchange interface vector components 24 rhs rhs gamma 25 explanations order lines 46 compute initial residual gmres iteration initial guess zero normalize residual obtain initial vector arnoldi basis according expression inverse 8 identical expression line 5 replaced approximation l u comparing bottom part righthand side expression righthand side 20 seen vector p r obtained line 6 algorithm indeed approximation local righthand side schur complement system lines 8 correspond matrixvector product preconditioned schur complement matrix ie computation lefthand side 20 35 schur complements via approximate inverses equation 17 describes general terms approximate block lu factorization global system 15 particular factorization stems approximating schur complement matrix using one several approximateinverse techniques described next given arbitrary matrix approximateinverse preconditioners consist finding approximation q inverse solving approximately optimization problem 3 min q2s certain set n theta n sparse matrices minimization problem decoupled n minimization problems form jth columns identity matrix matrix q 2 respectively note n columns computed independently different strategies selecting nonzero structure approximateinverse proposed 4 9 9 initial sparsity pattern taken diagonal fillin allowed depending improvement minimization work 4 suggests controlling sparsity approximate inverse dropping certain nonzero entries solution search directions suitable iterative method eg gmres iterative method solves system paper approximateinverse technique proposed 4 5 used consider local matrix blocked block lu factorization similar one given 16 sparse approximateinverse technique applied approximate b certain matrix done 5 resulting matrix sparse therefore sparse approximation approximation constructed using ilu factorization matrix previous subsection approximation global schur complement obtained approximately solving reduced system 7 ie solving approximated version righthand side also computed approximately system 22 requires approximation local schur complement matrix defined approximateinverse technique outlined used approximation schur complement matrix available induced global preconditioner matrix defined considering global system 14 also written 15 schur variables correspond bottom part linear system global preconditioner given block factorization 17 approximation obtained iteratively solving system 22 thus block forwardbackward solves factors 17 amount following threestep procedure 1 solve 2 solve iteratively system 22 obtain 3 compute fy threestep procedure translates following algorithm executed processor algorithm 32 approximateinverse schur complement solution step gmres 1 given local righthand side 2 solve b u approximately 3 calculate local righthand side 4 use gmres solve distributed system 5 compute approximation 6 compute note steps lines 2 5 6 require communication among processors since matrixvector operations steps performed local vector components contrast solution global schur system invoked line 4 involves global matrixvector multiplications interface exchange matrix consisting interface matrices x approximate solution carried several steps gmres forwardbackward solves incomplete l u factors b assuming factorization available approximation g line 3 local righthand side system 22 calculated line 5 several choices approximating possible solve linear system using gmres line 2 alternative exploit matrix approximates b construction equation 21 4 numerical experiments experiments compared performance described preconditioners distributed additive schwarz preconditioning see eg 19 2d elliptic pde problems several problems matrices harwellboeing davis collections 7 flexible variant restarted gmres fgmres 16 used solve original system since accelerator permits change preconditioning operation step useful example iterative process used precondition input system thus possible use ilutpreconditioned gmres lfil fillin elements recall ilut 17 form incomplete lu factorization dual threshold strategy dropping fillin elements convenience following abbreviations denote preconditioners solution techniques used numerical experiments distributed approximate block lu factorization approximated using matrix constructed using approximateinverse technique described 4 sapinvs distributed approximate block lu factorization approximated using approximateinverse technique 4 b gamma1 applied using one matrixvector multiplication followed solve slu distributed global system preconditioning defined via approximate solve l bj approximate additive schwarz ilutpreconditioned gmresk used precondition submatrix assigned processor si pure schur complement iteration described section 32 41 elliptic problems consider elliptic partial differential equation x x x rectangular regions dirichlet boundary conditions n x points x direction n points direction mesh mapped virtual p x theta p grid processors subrectangle n x p x points x direction n p points direction belongs processor fact subproblems associated subrectangles generated parallel figure 3 shows domain decomposition mesh mapping onto virtual processor grid comparison timing results iteration numbers global 360 theta 360 mesh mapped virtual square processor grids increasing size given figure 4 figure 4 omit solution time bj preconditioning 9543 seconds residual norm reduction 10 gamma6 achieved flexible gmres10 preconditioning ilut lf dropping tolerance 10 gamma4 used choice incomplete lu factorization five iterations figure 3 domain decomposition assignment 12 theta 9 mesh 3 theta 3 virtual processor grid gmres relative tolerance 10 gamma2 used application bj slu sapinv forwardbackward solves performed line 2 algorithm 32 since problem mesh size fixed increase number processors subproblems become smaller overall time decreases preconditioners based schur complement techniques less expensive additive schwarz preconditioning especially noticeable small numbers processors keeping subproblem sizes fixed increasing number processors increases overall size problem making harder solve thus increasing solution time ideal situations perfectly scalable algorithms execution time remain constant timing results fixed local subproblem sizes 15 theta 15 30 theta 30 50 theta 50 70 theta 70 presented figure 5 premature termination curves si indicates nonconvergence 300 iterations growth solution time number processors increases rather pronounced pure schur complement iteration additive schwarz whereas rather moderate schur complementbased preconditioners 42 general problems table describes three test problems harwellboeing davis collections column pattern specifies whether given problem structurally symmetric matrix three test problems matrix rows followed columns scaled 2norm also partitioning problem one level overlapping data exchanging used following 13 tables show iteration numbers required fgmres20 sapinv sapinvs slu processors seconds solid line bj dashdot line sapinv dashstar line slu 100100200300400processors iterations iterations solid line bj dashdot line sapinv dashstar line slu figure 4 times iteration counts solving 360 theta 360 discretized laplacean problem 3 different preconditioners using flexible gmres10 name n n z pattern discipline calculation raefsky1 3242 22316 unsymm incompressible flow pressure driven pipe sherman3 5005 20033 symm oil reservoir modeling table 1 description test problems bj till convergence different numbers processors asterisk indicates nonconvergence preconditioning phase approximate solves processor carried gmres reduce residual norm 10 gamma3 five steps allowed choice ilu factorization ilut lfil fillin elements shown column lfil used experiments lfil corresponds also number elements matrix column created approximateinverse technique general hard compare methods since number fillin elements resulting preconditioners different words sapinv sapinvs lfil specifies number nonzeros blocks preconditioning slu lfil total number nonzeros preconditioning therefore number nonzeros given approximation known exactly given problem iteration counts sapinv sapinvs suggest clear trend achieving convergence fewer iterations increasing number processors means high degree parallelism preconditioners impede convergence may even enhance significantly cf rows 14 table 2 main explanation fact approximations local global schur complement matrices 04081216processors seconds pe solid line bj dashdot line sapinv dashstar line slu dashcircle line si processors seconds pe dashdot line sapinv dashstar line slu dashcircle line si solid line bj 10051525processors seconds 50 x 50 mesh pe solid line bj dashdot line sapinv dashstar line slu dashcircle line si 10010305070processors seconds pe solid line bj dashdot line sapinv dashstar line slu dashcircle line si figure 5 solution times laplacean problem various local subproblem sizes using different preconditioners bj sapinv slu schur complement iteration si name precon lfil 4 8 table 2 number fgmres20 iterations raefsky1 problem name precon lfil 28 43 table 3 number fgmres20 iterations af23560 problem name precon lfil table 4 number fgmres20 iterations sherman3 problem global preconditioner derived actually improve processor numbers become larger since matrices become smaller furthermore sapinv sapinvs slu suffer information loss happens bj since bj disregards local matrix entries corresponding external interface vector components note effectiveness bj degrades increasing number processors cf subsection 41 comparison sapinv sapinvs raefsky1 sherman3 confirms conclusions 5 using approximate b algorithm 32 efficient applying b directly also computationally expensive general distributed matrices especially true since iterative solves b may inaccurate experiments sparse approximations appear quite accurate usually reducing frobenius norm 10 gamma2 could attributed small dimensions matrices used approximations reduction frobenius norm attained 10 iterations mr method smaller numbers iterations also tested effect overall solution process amounted average one extra iteration fgmres20 problems considered conclusions paper several preconditioning techniques distributed linear systems derived approximate solutions related schur complement system preconditioners built upon already available distributed data structure original matrix approximation global schur complement never formed explicitly thus communication overheard incurred construct preconditioner making preprocessing phase simple highly parallel preconditioning operations utilize communication structure precomputed original matrix preconditioning global matrix defined terms block lu factorization involves solve global schur complement system preconditioning step system turn solved approximately steps gmres exploiting approximations local schur complement preconditioning two different techniques incomplete lu factorization approximateinverse used approximate local schur complements distributed preconditioners constructed applied manner allow much flexibility specifying approximations local schur complements local system solves defining global induced blocklu preconditioner original matrix increasing number processors krylov subspace method fgmres 16 preconditioned proposed techniques exhibits moderate growth execution time scaled problem sizes experiments show proposed distributed preconditioners based schur complement techniques superior commonly used additive schwarz preconditioning addition advantage comes additional cost codecomplexity memory usage since data structures additive schwarz preconditioners used acknowledgments computing resources work provided minnesota supercomputer institute virginia tech computing center r petsc 20 users manual parallel algebraic nonoverlapping domain decomposition method flow problems iterative solution large sparse linear systems arising certain multidimensional approximation problems approximate inverse preconditioners general sparse matrices approximate inverse techniques blockpartitioned matrices approximate inverse preconditioning sparse linear systems sparse matrix test problems parpre parallel preconditioners package new approach parallel preconditioning sparse approximate inverses aztec users guide comparison domain decomposition techniques elliptic partial differ ential equation parallel implementation parallel solution general sparse linear systems iterative solution general sparse linear systems clusters workstations distributed ilu0 sor preconditioners unstructured sparse linear systems krylov subspace methods distributed computing environments flexible innerouter preconditioned gmres algorithm ilut dual threshold incomplete ilu factorization krylov subspace methods distributed computing environments iterative methods sparse linear systems psparslib portable library distributed memory sparse iterative solvers gmres generalized minimal residual algorithm solving nonsymmetric linear systems design iterative solution module parallel sparse matrix library p sparslib domain decomposition parallel multilevel methods elliptic partial differential equations tr ctr basermann u jaekel nordhausen k hachiya parallel iterative solvers sparse linear systems circuit simulation future generation computer systems v21 n8 p12751284 october 2005 chi shen jun zhang fully parallel block independent set algorithm distributed sparse matrices parallel computing v29 n1112 p16851699 novemberdecember sosonkina saad x cai using parallel algebraic recursive multilevel solver modern physical applications future generation computer systems v20 n3 p489500 april 2004 chi shen jun zhang parallel two level block ilu preconditioning techniques solving large sparse linear systems parallel computing v28 n10 p14511475 october 2002 chi shen jun zhang kai wang distributed block independent set algorithms parallel multilevel ilu preconditioners journal parallel distributed computing v65 n3 p331346 march 2005