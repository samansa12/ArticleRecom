scalable efficient distributed failure detectors process groups distributed applications services rely failure detectors detect process failures completely quickly accurately scalably possible even face unreliable message deliveries paper look quantifying optimal scalability terms network load messages per second messages size limit distributed complete failure detectors function applicationspecified requirements requirements 1 quick failure detection nonfaulty process 2 accuracy failure detection assume crashrecovery nonbyzantine failure model network model probabilistically unreliable wrt message deliveries process failures first characterize certain independence assumptions optimum worstcase network load imposed failure detector achieves applications requirements discuss traditional heart beating schemes inherently unscalable according optimal load also present randomized distributed failure detector algorithm imposes equal expected load per group member protocol satisfies application defined constraints completeness accuracy speed detection average imposes network load differs frown optimal suboptimality factor much lower traditional distributed heartbeating schemes moreover suboptimality factor vary group size large groups b introduction failure detectors central component faulttolerant distributed systems based process groups running unreliable asynchronous networks eg group membership protocols 3 supercomputers computer clusters 13 etc ability failure detector detect process failures completely eciently presence unreliable messaging well arbitrary process crashes recoveries major impact performance sys tems completeness guarantee failure group member eventually detected every nonfaulty group member eciency means failures detected quickly well accurately ie without many mis takes first work address properties failure detectors chandra toueg 5 authors showed impossible failure detector algorithm deterministically achieve completeness accuracy asynchronous unreliable network result lead flurry theoretical research ways classifying failure detectors importantly served guide designers failure detector algorithms real systems example distributed applications opted circumvent impossibility result relying failure detector algorithms guarantee completeness deterministically achieving eciency probabilistically 1 2 4 6 7 8 14 recent emergence applications large scale distributed systems created need failure detector algorithms minimize network load bytes per second equivalently messages per second limit maximum message size used well load imposed participating processes 7 14 failure detectors settings thus seek achieve good scalability addition eciency still deterministically guaranteeing completeness recently chen et al 6 proposed comprehensive set metrics measure quality service qos complete ecient failure detectors paper presented three primary metrics quantify performance failure detector one process detecting crashrecovery failures single process unreliable network authors proposed failure detection time recurrence time duration times mistaken detection primary metrics complete ecient failure detectors however paper neither deal optimal relation among metrics focussed distributed scalable failure detectors paper first address question quantifying optimum worstcase network load messages per sec ond limit messages sizes needed complete distributed failure detector protocol satisfy eciency requirements specified application concerned distributed failure detectors working group uniquely identifiable processes subject failures recoveries communicate unreliable net work deal complete failure detectors satisfy applicationdefined eciency constraints 1 quickness detection group member failure nonfaulty member within time bound 2 accuracy probability within time bound nonfaulty member detecting given nonfaulty member failed first quickness requirement merits discussion many systems multidomain server farm clusters 7 13 virtual synchrony implementations 3 rely single central computers aggregate failure detection information across system computers responsible disseminating information across entire system systems ecient detection failure depends time failure first detected nonfaulty member even absence central server notification failure typically communicated first member detect entire group via possibly unreliable broadcast 3 thus although achieving completeness important ecient detection failure often related time first detection another nonfaulty member failure derive optimal worstcase network load messages per second limit maximum message size imposed network complete failure detector satisfying applicationdefined constraints discuss traditional popular distributed heartbeating failure detection schemes eg 7 14 achieve optimal scalability limits finally present randomized distributed failure detector configured meet applicationdefined constraints completeness ac curacy expected speed detection reasonable assumptions network unreliability member message failure rates 15 worstcase network load imposed protocol suboptimality factor much lower traditional distributed heartbeat schemes suboptimality factor depend group size large groups application specified eciency constraints network unreliability probabilities furthermore average load imposed per member independent group size arriving results assume message loss member failures characterized probabilistic distributions independent across messages failures practicality assumptions real networks probably subject criticism assumptions necessary order take first step towards quantifying achieving scalable ecient failure detectors sides believe independence assumptions partially justified 1 randomized nature new failure detector algorithm 2 large temporal separation protocol periods typically oseconds practice mitigating much correlation among message loss probability distributions rest paper organized follows section 2 briefly summarizes previous work area section 3 formally describe process group model assumed paper section 4 presents discussion application specify eciency requirements failure de tector quantifies optimal worstcase network load failure detector must impose order meet quirements section 5 presents new randomized failure detector protocol conclude section 6 2 previous work chandra toueg 5 first formally address completeness accuracy properties failure detec tors subsequent work focused dierent properties classifications failure detectors area literature treated failure detectors oracles used solve distributed consensusagreement problem 9 unsolvable general asynchronous network model classifications failure detectors primarily based weakness model required implement order solve distributed consensusagreement problem 11 proposals implementable failure detectors sometimes assumed network models weak unreliability semantics eg timedasynchronous model 8 quasisynchronous model 2 partial synchrony model 12 etc proposals treated failure detectors tool eciently reach agreement ignoring eciency application designers viewpoint example failure detectors 12 provide eventual guarantees applications typically concerned real timing constraints reallife distributed systems failure detection service implemented via variants heartbeat mech anism 1 2 4 6 7 8 14 popular guarantee completeness property however existing heartbeat approaches shortcomings centralized heartbeat schemes create hotspots prevent scaling distributed heartbeat schemes oer dierent levels accuracy scalability depending exact heartbeat dissemination mechanism used show inherently ecient scalable claimed probabilistic network models used analyze heartbeat failure detectors 4 6 single process detecting failures single process 6 first paper propose metrics nondistributed heartbeat failure detectors crashrecovery model metrics inclusive scalability concerns work diers prior work first approach design failure detectors distributed application developers viewpoint quantify performance failure detector protocol network load requires impose network order satisfy applicationdefined constraints completeness quick accurate detection 1 also present ecient scalable distributed failure detector new failure detector incurs constant expected load per process thus state applicationdefined requirements formally section 4 avoiding hotspot problem centralized heartbeating schemes 3 model consider large group n 1 members 2 set potential group members fixed priori group members unique identifiers group member maintains list called view containing identities group members faulty otherwise protocol specification analysis assumes maximal group membership always members results extended model dynamically changing membership members incomplete views using methodologies similar 10 members may suer crash nonbyzantine failures recover subsequently unlike papers failure detectors eg 14 consider member faulty perturbed sleep time greater prespecified duration notion failure considers member faulty really crashed perturbations members might lead message losses accounted message loss rate pml define shortly whenever member recovers failure new incarnation distinguishable earlier incarnations member integer nonvolatile storage incremented every time member recov ers suces serve members incarnation number members group model thus crashrecovery semantics incarnation numbers distinguishing dierent failures recoveries member crashes fails current incarnation say lth incarnation say failure detected exactly first instant time nonfaulty member detects either 1 failure incarnation greater equal l 2 recovery incarnation strictly greater l characterize member failure probability parameter f probability random group member faulty random time member crashes assumed independent across members assume synchronization clocks across group mem bers require individual members clock drift rate fixed clock rate remains constant members communicate using unicast pointtopoint messaging asynchronous faultprone network since interested characterizing network bandwidth uti lized assume maximal message sizes con stant containing bytes data assuming bound size message identifiers headers typical ip packets message sent network fails delivered recipient due network congestion buer overflow sender receiver due member perturbations etc probability pml 0 1 worstcase message prop 2 either processes servers network adaptors etc agation delay sender receiver network delivered message assumed small compared applicationspecified detection time typically several seconds henceforth practical pur poses assume message either delivered immediately recipient probability 1 pml never reaches recipient 3 message loss distribution also assumed independent across messages message delivery losses could fact correlated network however application specified failure detection times much larger message propagation congestion repair times network messages exchanged failure detector considerable temporal separation reduces correlation among loss distributions dierent messages randomized selection message destinations new failure detector also weakens message loss correlation rest paper use shorthands q f qml instead 1 pf 1 pml respectively 4 scalable efficient failure first formal characterization properties failure detectors oered 5 laid following properties distributed failure detectors process groups fstrongweakg completeness crashfailure group member detected allsome nonfaulty members 4 strong accuracy nonfaulty group member 5 declared failed nonfaulty group member 5 also showed perfect failure detector ie one satisfies strong completeness strong accuracy sucient solve distributed consensus impossible implement faultprone network subsequent work designing ecient failure detectors attempted trade completeness accuracy properties several ways however completeness properties required distributed applications lead popular use failure detectors guarantee strong completeness always even eventually 1 2 4 5 6 7 8 14 course means failure detectors cannot guarantee strong accuracy always probability less 1 example alltoall distributed 3 assumption made simplicity fact optimality results section 4 hold pml assumed probability message delivery within time units send randomized protocol section 5 analysis extended hold pml probability message delivery within sixth protocol period 4 recollect model since members recover unique incarnations detection members failure recovery also implies detection failure previous incarnations 5 current incarnation heartbeating schemes popular guarantee strong completeness since faulty member stop sending heartbeats providing varying degrees accuracy explained section 1 many distributed applications although failure group member must eventually known nonfaulty members important failure detected quickly nonfaulty member necessarily nonfaulty members words quickness failure detectors depends time member failure weak completeness respect failure although strong completeness necessary property requirements imposed application designer failure detector protocol thus formally specified parameterized follows 1 completeness satisfy eventual strong completeness member failures 2 efficiency speed every member failure detected nonfaulty group member within time units occurrence worstcase message round time b accuracy time instant every nonfaulty member yet detected failed probability nonfaulty group member mistakenly detect faulty within next time units least 1 pmt pmt thus parameters specified application designer example application designer might specify measure scalability failure detector algorithm use worstcase network load imposes denoted l since several messages may transmitted simultaneously even one group member define definition 1 worstcase network load l failure detector protocol maximum number messages transmitted run protocol within time interval length divided also require failure detector impose uniform expected send receive load member due trac goal nearoptimal failure detector algorithm thus satisfy requirements completeness effi ciency guaranteeing scale worstcase network load l imposed algorithm close optimal possible equal expected load per member brings us question optimal worstcase network load call l needed satisfy applicationdefined requirements completeness speed accuracy able answer question network model discussed earlier group size n large 1 pmt small pml theorem 1 distributed failure detector algorithm group size n 1 deterministically satisfies completeness speed accuracy requirements given values pmt pml imposes minimal worstcase network load messages per time unit defined logpml furthermore failure detector achieves minimal worstcase bound satisfying complete ness speed accuracy requirements l thus optimal worstcase network load required satisfy completeness speed accuracy requirements proof prove first part theorem showing nonfaulty group member could transmit logpmt logp ml messages time interval length consider group member random point time let detected failed yet group member stay nonfaulty least time maximum number messages sent time interval possible run failure detector protocol starting time time event messages sent time interval tt lost happens probability least p ml occurrence event entails indistinguishable set rest nonfaulty group members ie members whether faulty speed requirement event would imply detected failed nonfaulty group member thus probability time given nonfaulty member yet detected faulty detected failed nonfaulty group member within next time units least p ml accuracy requirement p ml pmt implies logpmt logp ml failure detector satisfies completeness speed accuracy requirements meets l bound works follows uses highly available nonfaulty server group leader 6 every group member sends logpmt logp ml alive messages server every time units 6 set central computers collect failure information disseminate system designated server server declares member failed receive alive message time units 7 corollary optimal bound theorem 1 applies crashstop model well proof exactly arguments proof theorem 1 2 definition 2 suboptimality factor failure detector algorithm imposes worstcase network load l satisfying completeness efficiency quirements defined l traditional distributed heartbeating failure detection algorithms every group member periodically transmits heartbeat message incremented counter every group member member declared failed nonfaulty member j j receive heartbeats consecutive heartbeat periods duration detection time distributed heartbeating schemes popular implementation failure detectors guarantee completeness failed member send heartbeat messages however accuracy scalability guarantees heartbeating algorithms dier depending entirely actual mechanism used disseminate heartbeats simplest implementation member transmits alive messages group member knows every time units worstcase number messages transmitted member per unit time n worstcase total network load l n 2 suboptimality factor ie l n values pml pf gossipstyle failure detection service proposed van renesse et al 14 uses mechanism every tgossip time units member gossips n list latest heartbeat counters group members randomly selected group members authors show scheme new heartbeat count typically takes average time logn tgossip reach arbitrary group member speed requirement thus leads us choose logn worstcase network load imposed gossipstyle heartbeat scheme thus suboptimality factor varies n logn values pml pf pmt fact distributed heartbeating schemes meet optimality bound theorem 1 inherently attempt communicate failure notification group members seen overkill systems rely centralized coordinated set 7 implementation essentially centralized heartbeat mechanism undesirable requires highly available server bad load balancing satisfy scale property servers disseminate failure information systems require nonfaulty member detect given failure heartbeating schemes centralized heartbeating discussed proof theorem 1 heartbeating along logical ring group members 7 configured meet optimal load l problems creating hotspots centralized heartbeating unpredictable failure detection times presence multiple simultaneous faults larger group sizes heartbeating ring 5 randomized distributed preceding sections characterized optimal worstcase load imposed distributed failure detector satisfies completeness speed accuracy requirements application specified values theorem 1 studied traditional heartbeating schemes inherently scalable section relax speed condition detect failure within expected rather exact time bound time units failure present randomized distributed failure detector algorithm guarantees completeness probability 1 detection member failure within expected time failure accuracy probability 1 pmt protocol imposes equal expected load per group member worstcase average case network load l diers optimal l theorem 1 suboptimality factor ie l independent group size n 1 large groups reasonable values member message delivery failure rates pf pml suboptimality factor much lower suboptimality factors traditional distributed heartbeating schemes discussed previous section 51 new failure detector algorithm failure detector algorithm uses two parameters protocol period time units integer k size failure detection subgroups show values parameters configured required values pmt network parameters p f pml parameters k assumed known priori group members note need clocks synchronized across members requires member steady clock rate able measure algorithm formally described figure 1 nonfaulty member steps 13 executed every units call protocol period steps executed whenever necessary data contained message shown parentheses message sequence numbers allowed wrap around maximal message size bounded figure 2 illustrates protocol steps initiated member one protocol period length time units start protocol period random member integer pr local period number every time units 1 select random member j view send pingm j pr message j wait worstcase message roundtrip time 2 received ackm j pr message yet select k members randomly view send pingreqm j pr message wait ackm j pr message end period pr 3 received ackm j pr message yet declare j failed anytime 4 receipt pingreqmm send pingm j mm pr message j receipt ackm j mm pr message j send ackmm j pr message received mm anytime 5 receipt pingmm l pr message member mm reply ackmm l pr message mm anytime 6 receipt pingmm pr message member mm reply ackmm pr message mm figure 1 protocol steps group member data message shown parentheses message message also contains current incarnation number sender selected case j ping message sent receive replying ack j within timeout determined message roundtrip time selects k members random sends pingreq message nonfaulty members among k receives pingreq message subsequently pings j forwards ack received j back example figure 2 one k members manages complete cycle events j suspect j faulty end protocol period protocol member uses randomly selected subgroup k members outsource pingreq messages rather sending k repeat ping messages target eect using randomly selected subgroup distribute decision failure detection across subgroup members although analyze paper shown new protocols properties preserved even presence degree variation message delivery loss probabilities across group members sending k repeat ping messages may satisfy property analysis section 52 shows cost terms suboptimality factor network load using 1sized subgroup significant 52 analysis section calculate protocol expected detection time member failure well probability inaccurate detection nonfaulty choose random choose k random members ack ping ack ping ack ping figure 2 example protocol period shows possible messages protocol period may initiate message contents excluded simplicity member least one nonfaulty member lead calculation values k protocol function parameters specifying applicationspecified requirements network unreliabil ity ie pmt pf pml group member j faulty otherwise pr least one nonfaulty member chooses ping j directly time interval thus expected time failure member j detection nonfaulty member 1 gives us configurable value function given time instant nonfaulty member j detected faulty another nonfaulty member l within next time units l chooses ping j within next time units receive acks directly indirectly transitive pingreqs j pmt probability inaccurate failure detection member j within next time units simply probability least one member l group random group member l nonfaulty probability qf probability member choosing ping given probability l receives back acks direct indirect according protocol section 51 equals therefore gives us log pmt ml e q f log1 qf q 4 2 thus new randomized failure detector protocol configured using equations 1 2 satisfy speed accuracy requirements parameters et pmt moreover given member j failed stays failed every nonfaulty member eventually choose ping j protocol period discover j failed hence theorem 2 randomized failure detector protocol satisfies eventual strong completeness ie completeness requirement b configured via equations 1 2 meet requirements expected speed accuracy c uniform expected sendreceive load group members proof discussion equations 1 2 finally upperbound worstcase expected network load l el respectively imposed failure detector protocol worstcase network load occurs every time units member initiates steps 16 algorithm figure 1 steps 16 involve 2 messages steps 25 involve 4 messages per pingreq target member therefore worstcase network load imposed protocol messagestime unit theorem 1 equations 12 log pmt ml e q f log1 qf q 4 logpmt thus diers optimal l factor independent group size n furthermore 3 written linear function 1 4a gpf pml log1 q f q 4 4b fpf pml log1 qf q 4 4c theorem 3 suboptimality factor l l protocol figure 1 independent group size n 1 furthermore 1 fpf pml 0 l l monotonically increasing logpmt b pmt 2 fp f pml 0 l l monotonically decreasing logpmt b pmt proof equations 4a 4c next calculate average network load imposed new failure detector algorithm every time units nonfaulty member numbering n q f average executes steps 13 algorithm figure 1 steps 16 involve 2 messages steps 25 executed ack received target ping step 1 happens probability 1 q f q 2 ml involve 4 messages per nonfaulty pingreq target member therefore average network load imposed protocol messagestime unit theorem 1 equations 12 log pmt ml e q f log1 qf q 4 logpmt even el upperbounded optimal l factor independent group size n values l high compared ideal value 10 answer values pf pml low yet reasonable figure 3a shows variation l equation 3 low reasonable values pf pml pmt plot shows suboptimality factor network load imposed new failure detector rises pml pf increase decreases bounded function gpf pml values pmt happens low values pf pml seen figure 3b theorem 31 thus applies figure 3a function gpf pml bottommost surface attain high values staying 26 values shown thus performance new failure detector algorithm good reasonable assumptions network unreliability figure 3c shows upper bound el low 8 values pf pml 15 pmt decreased bound el actually decreases curve reveals advantage using randomization failure detector unlike traditional distributed heartbeating algorithms average case network load behavior new protocol much lower worstcase network load behavior figure 3 reveals values p f pml 15 l l new randomized failure detector stays 26 el 8 evident equations 3 5 variation suboptimality factors depend group size large group sizes compare suboptimality factors distributed heartbeating schemes discussed section 4 typically least n reality message loss rates process failure rates could vary time time parameters p f pml needed configure protocol parameters k may di cult estimate however figure 3 shows assuming reasonable bounds message loss ratesfailure rates using bounds configure failure detector suf fices words configuring protocol parameters ensure failure detector preserves application specified constraints pmt imposing network load diers optimal worstcase load l factor 26 worstcase 8 average case long message lossprocess failure rates exceed 15 load lower loss failure rates lower variation l according equation versus pml pf dierent values pmt low values pml pf gpf pml upper bound l b values pf pml fpf pml positive negative pf 0006012pml26ell c variation el according equation 5 figure 3 performance new failure detector algorith 53 future work optimizations cornell university currently testing performance scalable distributed membership service uses new randomized failure detection algorithm extending protocol crashstop model inherent dynamic groups involves several protocol extensions every group member join leave failure detection entails broadcast nonfaulty group members order update view broadcast may reliable implementing protocol group spanning several subnets requires load connecting routers gateways low protocol currently imposes load bytes per second routers every protocol period reducing load inevitably leads compromising efficiency properties protocol pings sent less frequently across subnets protocol also optimized trade worse scale properties better accuracy properties one optimization follow failure detection individual nonfaulty member described protocol multicast suspicion failure waiting time turning suspicion declaration member failure suspicion multicast place protocol periods dierent nonfaulty group members targeting suspected member correlated improve accuracy properties would also reduce eect correlated message failures frequency mistaken failure declarations disadvantage protocol since messages restricted contain bytes data large message headers mean higher overheads per message protocol also precludes optimizations involving piggybacked mes sages primarily due random selection ping targets discussion paper also points us several new interesting questions possible design failure detector algorithm asynchronous network setting satisfies complete ness efficiency scale requirements speed requirement section deterministic bound time detection failure rather average case done paper 8 notice dicult achieve synchronous network setting modifying new failure detector algorithm choose ping targets deterministic globally known manner every protocol period also leave open problem specification realization optimality load conditions failure detector speed timing parameter set time achieve strong completeness group member failure rather weak completeness 8 heartbeating along logical ring among group members eg 7 seems provide solution question ever pointed ring heartbeating unpredictable failure detection times presence multiple simultaneous failures course would ideal extend results models assume degree correlation among message losses perhaps even member failures 6 concluding comments paper looked designing complete scal able distributed failure detectors timing accuracy parameters specified distributed application restricted simple probabilistically lossy network model certain independence assumptions first quantified optimal worstcase network load messages per second limit maximal message size required complete failure detector algorithm process group network derived application specified constraints 1 detection time group member failure nonfaulty group member 2 probability within detection time period nonfaulty member detecting given nonfaulty member failed shown popular distributed heartbeating failure detection schemes inherently satisfy optimal scalability limit finally proposed randomized failure detector algorithm imposes equal expected load group members failure detector configured satisfy applicationspecified requirements completeness accuracy speed failure detection average analysis protocol shows imposes worstcase network load diers optimal suboptimality factor greater 1 stringent accuracy requirements pmt low e 30 reasonable message loss probabilities process failure rates network 15 suboptimality factor large traditional distributed heartbeating protocols fur ther suboptimality factor vary group size groups large currently involved implementing testing behavior protocol dynamic group membership sce narios involves several extensions optimizations described protocol acknowledgments thank members oceano group feedback also immensely grateful anonymous reviewers michael kalantar suggestions towards improving quality paper 7 r heartbeat timeoutfree failure detector quiescent reliable communication timing failure detection realtime group communication realtime systems process group approach reliable distributed computing probabilistic analysis group failure detection protocol unreliable failure detectors reliable distributed systems quality service failure detectors impossibility distributed consensus one faulty process probabilistically correct leader election protocol large groups solving agreement problems failure detectors optimal implementation weakest failure detector solving consensus search clusters gossipstyle failure detection service tr process group approach reliable distributed computing impossibility distributed consensus one faulty process unreliable failure detectors reliable distributed systems failawareness timed asynchronous systems search clusters 2nd ed optimal implementation weakest failure detector solving consensus brief announcement heartbeat probabilistically correct leader election protocol large groups quality service failure detectors probabilistic analysis group failure detection protocol ctr horita k taura chikayama scalable efficient selforganizing failure detector grid applications proceedings 6th ieeeacm international workshop grid computing p202210 november 1314 2005 jin yang jiannong cao weigang wu corentin travers notification based approach implementing failure detectors distributed systems proceedings 1st international conference scalable information systems p14es may 30june 01 2006 hong kong tiejun jane hillston stuart anderson evaluation qos crashrecovery failure detection proceedings 2007 acm symposium applied computing march 1115 2007 seoul korea greg bronevetsky daniel marques keshav pingali paul stodghill automated applicationlevel checkpointing mpi programs acm sigplan notices v38 n10 october wei xu jiannong cao beihong jin jing li liang zhang gcsma group communication system mobile agents journal network computer applications v30 n3 p11531172 august 2007 implementation unreliable failure detectors partially synchronous systems ieee transactions computers v53 n7 p815828 july 2004 andrei korostelev johan lukkien jan nesvadba yuechen qian qos management distributed service oriented systems proceedings 25th conference proceedings 25th iasted international multiconference parallel distributed computing networks p345352 february 1315 2007 innsbruck austria kelvin c w emin gn sirer latency bandwidthminimizing failure detectors acm sigops operating systems review v41 n3 june 2007 michel reynal short introduction failure detectors asynchronous distributed systems acm sigact news v36 n1 march 2005