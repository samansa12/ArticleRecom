algorithms constrained weighted nonlinear least squares hybrid algorithm consisting gaussnewton method secondorder method solving constrained weighted nonlinear least squares problems developed analyzed tested one advantages algorithm arbitrarily large weights handled weights merit function get unnecessarily large iterates diverge saddle point local convergence properties gaussnewton method thoroughly analyzed simple ways estimating calculating local convergence rate gaussnewton method given assumption constrained weighted linear least squares subproblems attained gaussnewton method ill conditioned global convergence towards firstorder kkt point proved b introduction assume f r n continuously differentiable function diagonal matrix weights discuss gaussnewton method second order method solving problem min x2r denotes 2norm simplicity without loss generality assume weights normalized sorted normalization easily done first sorting zero weights reducing problem dividing remaining nonzero weights smallest positive weight knowledge existing algorithms solving 11 based unweighted problem min assume ordinary gaussnewton method used solve 13 search direction p got solving min p2 1 rg note 14 solved unweighted problem thus condition problem determined kkk kk k k pseudo inverse k hand linearize 11 without explicitly multiplying weights solve weighted linear least squares problem obtain search direction p condition problem 15 mainly determined kbk kjk detailed discussion condition numbers 15 see 11 problem 14 may ill conditioned regarded unweighted linear least squares problem despite fact 15 well conditioned regarded weighted linear least squares problem obviously important look 11 class weighted nonlinear least squares problem another important advantage using 11 instead 13 former defines general problem class latter evident allow weights infinitely large precise define vector equations weights correspond zero elements note lagrange multiplier corresponding ith constraint consequently defined 16 return proper way calculating lagrange multipliers problem 11 rewritten using 16 hence allowing infinite weights original problem formulation 11 defines class weighted nonlinear least squares problems nonlinear equality constraints even specific assume p infinite weights problem 17 stated theta f equivalent formulation problem 18 using min 2 course could started defining problem one 19 instead 15 without need 17 18 notations would get unnecessarily complicated next section describe gaussnewton method solving 11 local convergence properties gaussnewton method analyzed section 3 section 4 show certain assumptions nondegeneracy global convergence achieved gaussnewton method slow converge second derivatives available reasonable cost newton method may used solve 11 however large possibly infinite weights pure newton method based forming hessian gx may work infinite weights even defined natural approach use perturbation method 9 call generalized newtonraphson method gnr method section 5 construct analyze algorithm solving 11 based gnr method computational experiments presented section 6 finally discuss results give hints possible future work 2 gaussnewton method using system equations gaussnewton method nonlinear least squares problem 11 linearized around current iteration point x k search direction p k computed solution next iterate x steplength presence large weights possibly infinite adequate reformulate 21 gammaf simplicity dropped iteration index k several names linear system equations 22 equilibrium equations system equations augmented system equations call 22 system equations matrix 22 called system matrix less obvious reason using 22 elements corresponding infinite weights approximations lagrange multipliers used second order method described section 5 following lemma gives relevant conditions system matrix nonsingular lemma 21 system matrix 22 nonsingular rows j correspond infinite weights linearly independent j full column rank exist several stable algorithms solve 22 see eg 5 references chosen use modified qr decomposition see 5 reasons following modified qr decomposition simple easy compute identical ordinary qr decomposition weights equal modified qr decomposition also easily reused second order gnr method see section 5 modified qr decomposition j 2 r mthetan defined r nthetan upper triangular matrix pi permutation matrix decomposition 23 q r nonsingular exists system matrix 22 nonsingular see lemma 21 system equations solved modified qr decomposition following way using decomposition 23 22 get6 6 4 r make partition mmgamman solution 24 mgamman 3 local rate convergence gaussnewton method section describe local convergence properties gaussnewton method described previous section analysis depends much upon perturbation analysis constrained weighted linear least squares problem done 11 12 defined inverse system matrix using notation 11 state prove two important theorems local convergence rate projected residual fact local convergence properties two quantities shall see similar finally show j k p k local convergence rate x x j k p k independent parametrization r n assuming b x solution 11 define b corresponding notation quantities evaluated b x necessary condition algorithm converge without regularization system matrix 22 full rank convenient make following definition definition 31 system matrix 22 nonsingular x say x nondegenerate point nondegenerate point inverse system matrix 22 given generalized inverse j see 11 25 immediately get following theorem describes local behaviour x theorem 31 assume fp k g generated solving 22 points x nondegenerate b x solution 11 b vector 22 b x r 1f 00 proof x f using taylor expansion first term 34 expressed express second term gammab k f 34 use perturbation identity 22 p 11 says using identity z 1f 00 equation 36 becomes equations 35 37 inserted 34 gives theorem gaussnewton method written b theorem 31 conclude 8 get following theorem theorem 32 define eigenvalues h x lim sup xk xk easy get estimation local convergence rate use matrix defined 32 b r r gammat pi useful quantity estimating close x k solution b x projected residual oblique projection f k onto rj k following theorem shows j k p k locally convergence behaviour theorem 33 assume fp k g generated solving 22 points x nondegenerate x k vector 22 x k r 1f 00 proof denote projection j k b k p k using taylor expansion multiplying p k1 obtain equality b k holds identify last term equation 314 31 get hence perturbation identity 21 p 16 11 get using 318 fact k1 ffij k b k equation 317 becomes last equality follows 316 identities gammap k together taylor expansion ffij k give theorem follows inserting 315 319 314 matrix corresponding h x projected residual k b easy show h x h nonzero eigenvalues hence theorem 33 following corollary corollary 31 define b k inverse system matrix 31 lim sup ks eigenvalues matrix h x defined 311 relation 312 also used determine ks k1 kks k k reflects linear convergence rate second order method used convergence gaussnewton method slow use higher order method if2 see also algorithm 61 several quantities invariant change parametrization example following theorem theorem 34 matrix b independent parametrization r n proof assume fx want show b generalized inverse ry b consider taylor expansion delta x 00 delta x 00 comparing taylor expansion 321 taylor expansion using j 323 finally get proves theorem consequence theorem 34 local convergence x main argument choosing jp measure closeness solution following theorem direct consequence 323 theorem 35 projection f rj independent parametrization r n 4 global convergence section assume x k k iteration index nondegenerate p k solution 22 x k nothing else stated assume limits denoted k 1 sums explicitly stated upper lower limit one infinity 41 merit function merit function chosen goal find matrix k merit weights step length ff k iteration global convergence towards first order kuhntucker point proved compute k use approximation fixed matrix define obviously sufficient condition p k descent direction phix x k oe 0 realize determine good matrix upsilonx k merit weights solving min kupsilonk st ffi small positive constant lower limit weights determined previously computed weights see always solution 42 lim argf min ff note keeping weights large important practice global convergence constraints 42 must satsified describe algorithm computing merit weights k using upsilonx k become unnecessarily large first describe method solving 42 algorithm computing actual merit weights k solving 42 chosen use maxnorm since gives simple algorithm problem 42 rewritten min kuk1 u diagonal upsilonx k diagonal w jp given problem 43 consists vectors matrices first step algorithm reduce 43 u get new problem corresponding parts u left reduction ae ready solution otherwise choose e vector ones thus attain equality constraints respectively reduce problem copy 44 vectors shorter ae smaller procedure repeated whole u found easily realized infinite weights change algorithm algorithm terminate solution 44 determine actual merit weights k solution upsilonx k 42 weights may get large close saddle point iterates diverge saddle point always case gaussnewton method would like weights decrease accomplished saving say older versions merit weight matrices initially iteration kth iteration update algorithm 41 algorithm 41 solve 42 vector ux k k new sequence 1 algorithm 42 gaussnewton algorithm described line search quadratic merit function algorithm 42 initiate start vector x k convergence compute compute using modified qr decomposition j k determine k algorithm 41 determine step length ff k 42 proving global convergence need following two technical lemmas prove algorithm globally convergent lemmas use k arbitrary diagonal element k lemma 41 assume k 0 fd k g bounded let subsequence fd k g k j1 k j positive series converges converges proof take converges assume b n converges b n hence k g bounded sequence increases limit b gamma converges b lemma 42 assume arbitrary component k diagonal k stays bounded k 1 let v k corresponding diagonal element v lim series converges proof let us first exclude trivial case v k becomes equal upper bound finite k sequence fv k g increasing infinite sequence hence lim v k exists denoted v take ffl arbitrary small fixed positive number k kvalues hence v since ffl 0 arbitrary implies v k v k follows thus v v consequently k v let fd k g subsequence fd k g k1 k lemma 41 know series converges converges let us prove latter series converges v k k follows hence since subserie increases v series converges since positive series sufficient prove bounded hence remains prove series converges since saved older weights updated step 1 reach 1t updates v 1t equals one earlier way eliminate v 1t corresponding j way seen v 1t1 equals one j pair also eliminated series go eliminate elements way get thus positive series bounded converges completes proof main global convergence theorem covers bounded unbounded sequences merit weights theorem 43 let fx k g fd k g generated algorithm 42 assume bounded system matrix 22 nonsingular closure g sequence fx k g either finite termination kkt point accumulation point kkt point 11 proof trivial finite termination kkt points let us assume infinite sequence algorithm 42 implies sufficient consider following two cases cases treated separately exist subsequence fx k g fx k g kd bounded possible choose subsequence fx j k g fx k g x x e x algorithm 42 follows kd k k 1 continuous points closure fx k g except kkt points e x accumulation point fx k g kkt point ii inequality one prove point e x cannot accumulation point fx k g exist proof 45 trivial extension similar proof 7 pp 2122 lemma 42 know converges goldstein armijo condition algorithm 42 given k follows every point e x closure fx k g kkt point exists constants ffl 0 45 satisfied hence kkt points remain possible accumulation points proves theorem case ii 43 line search chosen keep things simple therefore use standard cubic interpolation 3 approximate minimum merit function oeff another efficient line search algorithm found 6 44 regularization use simple form subspace minimization described unweighted constrained case 7 able prove general global convergence result one theorem 43 shall see computational experiments regularization seems work appropriately 5 generalized newtonraphson method constrained newton method solving 19 based quadratic subproblem gp first order approximations lagrange multipliers solution p 51 given linear system equations g gammaf main disadvantage using 52 large weights w 2 quadratic subproblem 51 matrix 52 may ill conditioned avoid ill conditioning due large weights w 2 solve gammaf 22 method generalized newtonraphson method 9 gnr method gnr method interesting theoretical motivation assume reached point x k first order approximation 15 known x k solves perturbed problem min projection onto rj k hence know solution x k 54 want compute solution perturbed problem min use quadratic approximation zx x k compute solution problem 55 whose error okp k f k k 2 change back original notations fx perturbed solution found solving problem 53 53 seen exists matrix n k x solution 11 take x quadratic approximation 56 get 56 also seen j k n k depends surface parameterization x consequently j k p k independent parameterization r n generalized newtonraphson method fact quadratically convergent method j k p k independent parametrization see assume exists another method computes e e series expansion 56 unique j k e implies e define z 1 matrix whose columns span null space j 1 call p descent direction p z drawback constrained newton method based 52 gnr method nonsingular matrix 52 53 sufficient p descent direction however use gnr method close solution see 320 algorithm 61 therefore use gnr method undamped 22 get needed g gaussnewton search direction matrix 53 singular use already available gaussnewton direction use modified qr decomposition solve 22 possible reduce size system 53 ignoring permutation matrix possible rewrite r implies q reduce 57 r gammag gamma first n elements q q respectively matrix 58 may indefinite must either use stable method indefinite systems see eg 4 add condition submatrices 58 one possibility latter kind assume r well conditioned use r reduce 58 r gammag gamma solution matrix rmnr gammat g nonsingular take gaussnewton step 6 computational experiments algorithm use tests shown algorithm 61 initialize determine jacobian j vector f compute gn direction p solving 22 regularization needed second false close second rate 05 compute gnr direction p gnr solving 53 matrix 53 nonsingular gn compute merit weights algorithm 41 determine step length ff using line search described section 43 merit function oeff x use pure gn method variable second fixed value false tested algorithm three different problems described ap pendix schittkowski 308 10 boggs 2 boggs 8 2 intention tests show algorithms faster existing algorithms show algorithms handles large weights inadequate models ill conditioning linear problems another important aim tests verify theoretical results local convergence rate therefore natural use small simple test problems define two different measures convergence rate gaussnewton method emphasize k excellent way estimating convergence rate regularization needed b x known first problem schittkowski 308 first solved gaussnewton method result tab 1 largest weight 10 20 weights multiplied explicitly f forming algorithm breaks numerical instability note slow growth merit weights first problem schittkowski 308 gaussnewton method 5 73e3 66e5 27e3 46e2 44e2 30 99 10 6 34e4 30e6 12e4 46e2 46e2 29 39 10 8 72e7 65e9 27e7 46e2 47e2 30 10e2 10 9 33e8 30e10 12e8 46e2 46e2 30 10e2 10 table schittkowski 308 gnr method 5 59e5 56e12 22e5 10 6 29e11 23e16 11e11 10 solved gnr method showed tab 2 asterisk indicates gnr method used step second problem boggs 2 constrained problem solved gaussnewton method tab 3 gnr method tab 4 merit weights gaussnewton method equal one shown tab 3 remaining two test problems illustrate regularization rank problem shown headline rank tab 5 second test problem boggs 2 solved gaussnewton method jacobian rank deficient starting point third problem boggs 8 jacobian solution rank deficient result shown tab 6 7 discussion claim developed efficient fairly robust algorithm solving 11 possibly infinite weights discussed intro duction however difficult us measure effectiveness algorithm boggs 2 gaussnewton method 3 12e2 28e2 23 013 050 26 10 5 24e4 25e4 73e3 010 19e2 16e2 10 6 64e5 66e5 30e4 027 42e2 16e2 10 table boggs 2 gnr method 3 12e2 28e2 23 10 5 24e4 25e4 73e3 10 6 64e5 66e5 30e4 10 9 82e16 10e15 19e15 10 knowledge algorithm solve general problem 11 local convergence properties well understood gaussnewton algo rithm especially interesting local convergence results valid whole problem class defined 11 independent parametrization r n merit function especially suited weighted constrained problem technique choosing merit weights effective lead unnecessary large weights boggs 2 gaussnewton rank deficient starting point 9 047 071 014 10 012 3 robustness shown algorithm globally convergent iteration points nondegenerate remains find way regularize rows j corresponding large weights become almost linearly dependent believe difficult challenging problem solve appendix test problems appendix define three test problems weight sequences also give starting points x start solutions b residuals fbx examples 10 2 includes unconstrained well constrained problems schittkowski 308 10 unconstrained problem modified incorporation weights constrained problem jacobian rank deficient second boggs 8 gaussnewton rank deficient solution 28 11 10 065 10 34e10 5 29 11 099 16 35 95e7 4 53 42e9 025 050 35 10 4 54 10e9 025 050 35 10 3 starting point x start2 boggs 8 2 constrained problem jacobian rank deficient solution r strategy global convergence sequential quadratic programming algorithm numerical methods unconstrained optimization nonlinear equations matrix computations modifying qr decomposition weighted constrained linear least squares iterative solution nonlinear equations several variables comparision algorithms nonlinear least squares problem test examples nonlinear programming codes perturbation theory condition numbers generalized constrained linear least squares problems tr ctr hiroshi hosobe hierarchical nonlinear constraint satisfaction proceedings 2004 acm symposium applied computing march 1417 2004 nicosia cyprus