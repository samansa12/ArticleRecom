convergence properties augmented lagrangian algorithm optimization combination general equality linear constraints consider global local convergence properties class augmented lagrangian methods solving nonlinear programming problems methods linear general constraints handled different ways general constraints combined objective function augmented lagrangian iteration consists solving sequence subproblems subproblem augmented lagrangian approximately minimized region defined linear constraints subproblem terminated soon stopping condition satisfied stopping rules consider encompass practical tests used several existing packages linearly constrained optimization algorithm also allows different penalty parameters associated disjoint subsets general constraints paper analyze convergence sequence iterates generated algorithm prove global fast linear convergence well show potentially troublesome penalty parameters remain bounded away zero b introduction introduction paper consider problem calculating local minimizer smooth function x required satisfy general equality constraints linear inequality constraints f c map n pbyn matrix b 2 p classical technique solving problem 1113 minimize suitable sequence augmented lagrangian functions consider problem 11 12 functions defined components vector known lagrange multiplier estimates known penalty parameter see instance hestenes 18 powell 23 bertsekas 3 question arises deal additional linear inequality constraints 13 case identity matrix specifies bounds variables considered conn et al 5 research supported part advanced research projects agency department defense monitored air force office scientific research contract f4962091c0079 united states government authorized reproduce distribute reprints governmental purposes notwithstanding copyright notation hereon work also supported belgian national fund scientific research 7 propose keeping constraints explicitly outside augmented lagrangian formulation handling directly level augmented lagrangian minimization sequence optimization problems 14 approximately minimized within region defined simple bounds attempted approach linear inequalities bound constraints converted equations introducing slack variables incorporated augmented lagrangian function strategy implemented successfully applied within lancelot package largescale nonlinear optimization see conn et al 6 ever method may inefficient linear constraints present number effective techniques specifically designed handle constraints directly see arioli et al 1 forsgren murray 14 toint tuyttens 24 carpenter 25 instance especially important largescale problems purpose present paper therefore define analyze algorithm constraints 13 kept outside augmented lagrangian handled level subproblem minimization thus allowing use specialized packages solve subproblem proposal extends method conn et al 5 bounds general linear inequalities treated separately fletcher 13 page 295 remarks potential advantages strategy furthermore often worthwhile practical point view associate different penalty parameters subsets general constraints 12 reflect different degrees nonlinearity possibility considered many authors including fletcher 13 page 292 powell 23 bertsekas 3 page 124 case formulation augmented lagrangian 14 refined partition set constraints 12 q disjoint subsets fq j g q redefine augmented lagrangian qdimensional vector whose jth component j 0 penalty parameter associated subset q j potential usefulness analysis difficult directly infer single penalty parameter case refined formulation adopted present paper theory presented handles linear inequality constraints purely geometric way hence theory applies without modifications linear equality constraints also imposed iterates assumed stay feasible respect new constraints indeed enough apply theory affine subspace corresponding feasible set consequence linear constraints need included augmented lagrangian thus desirable property impact structure hessian matrix paper organized follows section 2 introduce basic assumptions problem necessary terminology section 3 presents proposed algorithm definition suitable stopping criterion subproblem global convergence analysis developed section 4 rate convergence analyzed section 5 second order conditions investigated section 6 section 7 considers possible extensions theory finally conclusions perspectives outlined section 8 2 problem related terminology consider problem stated 1113 make following assumptions as1 region nonempty as2 functions fx c x twice continuously differentiable x 2 b assumption as1 clearly necessary problem make sense note prevent b unbounded introduce notation used throughout paper let gx denote gradient r x fx fx hx denote hessian matrix r xx fx also define jx mbyn jacobian cx hence denote hessian matrix r xx c x c x finally let g x denote gradient r x x hessian matrix r xx x lagrangian function note x lagrangian solely respect c constraints define firstorder lagrange multiplier estimates componentwise w denotes jsjdimensional subvector w whose entries indexed set shall use identity r x phix suppose fx k 2 bg f k g f k g infinite sequences nvectors mvectors positive qvectors respectively function f shall use notation f k denotes f evaluated arguments x instance using identity 22 written 21 compact form denote vector w iteration k w k ith component w ki also use w ks denote jsjdimensional subvector w k whose entries indexed let fx k subset k natural numbers n convergent subsequence limit point x denote matrix whose rows corresponding active constraints x constraints satisfied equalities x furthermore choose z matrix whose columns form orthonormal basis null space z define leastsquares lagrange multiplier estimates corresponding points right generalized inverse jxz well defined note whenever jxz full rank x differentiable derivative given following lemma lemma 21 suppose as2 holds jxz z jx nonsingular x differentiable derivative given ith row rx z proof result follows observing 25 may rewritten gx jxz vector rx differentiating 27 eliminating derivative rx resulting equations gives required result 2 stress stated lagrange multiplier estimate 25 directly calculable requires priori knowledge x merely introduced analytical device finally symbol k delta k denote 2 norm induced matrix norm position describe precisely algorithm propose use 3 statement algorithm consider algorithmic model wish use order solve problem 1113 model proceeds iteration k computing iterate x k satisfies 13 approximately solves subproblem min x2b values lagrange multipliers k penalty parameters k fixed subproblem subsequently update lagrange multipliers andor decrease penalty parameters depending much constraint violation 12 reduced within subset constraints motivation simply ensure global convergence driving worst case penalty parameters zero case algorithms essentially reduce quadratic penalty function method see example gould 15 tests size general constraint violation designed allow multiplier updates take neighbourhood stationary point approximate minimization problem 31 performed inner iteration stopped soon current iterate sufficiently critical propose base decision identification linear constraints dominant x even though might active measure criticality part problem constraints irrelevant given 0 criticality tolerance subproblem define vector x 2 b set dominant constraints x constraints whose indices set 0 0 ith row matrix b corresponding component righthand side vector b denoting dx submatrix consisting rows whose index dx also define cone spanned outward normals dominant constraints associated polar cone clv denotes closure set v cone x tangent cone respect dominant constraints x tolerance note dx might empty case dx assumed zero nx reduces origin x full space formulate sufficient criticality criterion subproblem fol lows require projection onto convex set v k suitable tolerance iteration k x k satisfying 33 determined inner iteration denote future reference define z k matrix whose columns form orthonormal basis v k null space ad k k matrix whose columns form orthonormal basis w k k full space n k reduces origin k empty note case z identity operator also note v k k hence since z k z k orthogonal projection onto v k important note stopping rule 33 covers number specific choices including rule used much existing software linearly constrained optimization minos 21 lsnno 24 ve14 ve19 harwell subroutine library 17 reader referred section 72 details position describe algorithmic model precisely model define ff k maximum penalty parameter iteration k see 310 iteration parameters k j k represent criticality feasibility levels respectively partition set disjoint subsets given well initial vectors lagrange multiplier estimates 0 positive penalty parameters 0 strictly positive constants specified set ff approximately solves 31 ie 33 holds test convergence kp k step 3 disaggregated updates step 3b otherwise step 3a update lagrange multiplier estimates set step 3b reduce penalty parameter set step 4 aggregated updates define set otherwise set increment k one go step 1 algorithm 31 specifically designed firstorder estimate 21 formula potential advantages largescale computations refer reader section 71 discussion flexible choice multipliers covering among others choice leastsquares estimates x defined 25 immediately verify algorithm coherent lim indeed obtain 36 ff k 1 k 314 follows 312 313 ff k tends zero 313 alone ff k bounded away zero restriction 36 imposed order simplify exposition practical setting may ignored provided definition ff 0 310 replaced ff respectively constant fl 2 0 1 311 replaced algorithm 31 may extended ways instance one may replace definition 0 first equation 312 first equation 313 definition j 0 second equation 312 may replaced j 0 none extensions alter results convergence theory developed values used lancelot package similar context relation 315 also used ensuring 001 values also seem suitable parameters j specify final accuracy requested user finally purpose update 39 put emphasis feasibility constraints whose violation proportionally higher order achieve balance amongst constraint violations balance allows true asymptotic regime algorithm reached advantage 39 balancing effect obtained gradually enforced every major iteration case powell 23 furthermore powells approach increases penalties corresponding constraints becoming slowly feasible based 1 norm thus changed sufficiently within constraint violation tolerance lagrange multiplier update performed contrast update multipliers wellbehaved constraints assuming correspond particular partition likely since partly least partitions exist independently badly behaved ones addition virtue using norm give quite emphasis violated constraint 4 global convergence analysis proceed show algorithm 31 globally convergent following assumptions as3 iterates fx k g considered lie within closed bounded domainomegagamma as4 matrix jx z column rank smaller limit point x sequence fx k g considered paper notice as3 implies exists least convergent subsequence iterates course guarantee subsequence converges stationary point ie algorithm works also note always satisfied practice linear constraints 13 includes lower upper bounds variables either actual implied finite precision computer arithmetic assumption as4 guarantees dimension null space large enough provide number degrees freedom necessary satisfy nonlinear constraints require gradients constraints projected onto null space linearly independent every limit point sequence iterates assumption direct generalization as3 used conn et al 5 shall analyse convergence algorithm case convergence tolerances j zero first need following lemma proving 33 prevents reduced gradient augmented lagrangian orthogonal complement arbitrarily large k small lemma 41 let fx k g ae b k 2 k sequence converges point x suppose k positive scalar parameters converge zero k 2 k increases 1 0 k 2 k sufficiently large proof observe k 2 k sufficiently large k sufficiently small sufficiently close x ensure constraints k active x implies subspace orthogonal normals dominant constraints x k v k contains subspace orthogonal normals constraints active x hence deduce used 35 obtain second inequality 33 deduce third proves first part 41 turn second k empty k zero matrix second part 41 immediately follows assume therefore k 6 first select submatrix ad k maximal full rowrank note orthogonal projection onto subspace spanned fa g i2d k nothing hence obtain orthogonality k bound jd k j p 32 34 fact constraints k active x k sufficiently large finite number nonempty sets k possible choices x k may thus deduce second part 41 42 defining minimum taken possible choices k 2 examine behaviour sequence fr x phi k g first recall result extracted classical perturbation theory convex optimization problems result well known found instance 12 pp 1417 lemma 42 assume u continuous pointtoset mapping power set n set u convex nonempty assume realvalued function f defined continuous space convex fixed realvalued function f defined continuous show converges sequence fr x phi k g tends vector linear combination rows nonnegative coefficients lemma 43 let fx k g ae bk 2 k sequence converges point x suppose gradients r x phi k k 2 k converge limit r x phi assume furthermore 33 holds k 2 k k tends zero k 2 k increases r x phi vector 0 matrix whose rows corresponding active constraints x proof first define aim show quantity tends zero k 2 k increases obtain 43 moreau decomposition 20 r x phi k cauchyschwarz inequality 1g sufficiently close x k sufficiently small constraints k must active x n k included normal cone nx 0 therefore vector pn k belongs normal cone moreover since maximization problem last righthand side 44 concave program since x feasible 13 since kx large enough thus deduce global solution problem observing obtain used cauchyschwarz inequality deduce last inequality may apply lemma 41 deduce second part 41 45 contractive character projection onto convex set containing origin thus 44 assumptions assumption k sequence implies oe k converges zero k increases k consider minimization problem subject ax since sequences fr x phi k g fx k g converge r x phi x respectively deduce lemma 42 applied optimization problem 43 choices convergence sequence oe k zero optimal value problem 46 zero vector thus solution problem 46 satisfies r x phi vector 0 ends proof 2 important part convergence analysis next lemma lemma 44 suppose as1 as2 hold let fx k g ae b k 2 k sequence satisfying as3 converges point x as4 holds let satisfies 25 assume f k g k 2 k sequence vectors nonincreasing sequence qdimensional vectors suppose 33 holds k positive scalar parameters converge zero increases positive constants 2 3 sufficiently large suppose addition cx ii x kuhntucker point firstorder stationary point problem 11 corresponding vector lagrange multipliers sequences converge k 2 k iii gradients r x phi k converge g proof consequence as2as4 k 2 k sufficiently large exists bounded converges jx z thus may write constant 2 0 equations 23 24 inner iteration termination criterion 33 lemma 41 give k 2 k large enough assumptions as2 as3 as4 25 x bounded x neighbourhood x thus may deduce 25 410 411 moreover integral mean value theorem lemma 21 z 1r x xsds delta r x x given equation 26 terms within integral sign bounded x sufficiently close x hence k 2 k sufficiently large constant 3 0 implies inequality 48 x k converges combining 412 414 obtain gives required inequality 47 since assumption k tends zero k increases 415 implies k converges therefore identity 23 r x phi k converges g x furthermore multiplying 21 kj obtain taking norms 416 using 415 derive 49 suppose cx lemma 43 convergence r x phi k g x give vector 0 last equation 417 show x kuhntucker point corresponding set lagrange multipliers moreover 47 48 ensure convergence sequences f x k hence lemma proved 2 finally require following lemma proof global convergence shows lagrange multiplier estimates cannot behave badly lemma 45 suppose j 1 j q kj converges zero k increases algorithm 31 executed product kj k converges zero proof kj converges zero step 3b must executed infinitely often jth subset let k set indices iterations step 3b executed consider jth subset lagrange multiplier estimates changes two successive iterations indexed set k j firstly note kv1q j iteration summation null substituting 419 418 multiplying sides kvtj taking norms using 39 yields hence using fact 37 holds deduce defining obtain k thus 422 inequality 1 ae v converges zero ffi v hence 421 kvtj k converge zero complete proof therefore suffices show ae v converges zero v tends infinity suppose first ff k bounded away zero must 313 used k sufficiently large ff 1 definition ae v 420 imply min sufficiently large v 313 also guarantees j k tends zero deduce ae v converges zero completes proof first case suppose ff k converges zero implies q independent penalty parameters reduced infinite number times consider progress ff k course q successive decreases 311 311 happens currently largest penalty parameter kj say reduced 39 requires penalty parameter reduced possibly parameters interval kj kj follows ff k must reduced least q successive decreases 311 thus considering possible outcomes 312 313 j kvl must bounded quantity form furthermore q terms involve particular therefore since ff kv 1 obtain thus see ff kv converges zero ae v completing proof second case 2 derive desired global convergence property algorithm 31 analogous theorem 44 conn et al 5 theorem 46 assume as1 as2 hold let x limit point sequence fx k g generated algorithm 31 section 3 as3 as4 hold let k set indices infinite subsequence x k whose limit x finally let conclusions ii iii lemma 44 hold proof assumptions sufficient reach conclusions part lemma 44 show cx therefore cx see consider analyze two separate cases first case kj bounded away zero hence step 3a must executed every iteration k sufficiently large implying 37 always satisfied k large enough deduce 314 cx k converge zero second case kj converges zero lemma 45 shows tends zero using limit 314 49 obtain tends zero desired consequence conclusions ii iii lemma 44 hold 2 finally note global convergence algorithm 31 proved much weaker assumptions kj k reader referred conn et al 9 details 5 asymptotic convergence analysis distinction dominant nondominant floating linear inequality constraints implications terms identification constraints active limit point sequence iterates generated algorithm given point x know theorem 46 critical ie gammag corresponding lagrange multipliers consider linear constraint index active x may define normal cone n cone spanned outwards normals linear inequality constraints active x except ith one say ith linear inequality constraint strongly active x gammag words ith constraint strongly active critical point point ceases critical constraint ignored let us denote sx set strongly active constraints x nonstrongly active constraints x called weakly active x prove reasonable result strongly active constraints limit point x dominant k large enough theorem 51 assume as1as3 hold let fx k g k 2 k convergent subsequence iterates produced algorithm 31 whose limit point x corresponding lagrange multipliers assume furthermore as4 holds x k sufficiently large proof consider linear inequality constraint 2 sx definition latter set gammag since theorem 46 guarantees r x phi k converges g x n closed gammar x phi k 62 n large enough therefore one obtains moreau decomposition 20 gammar x phi k ffl 0 sufficiently large k 2 k also 33 kp k gammar x phi k k arbitrarily small k tends zero see 314 assume arbitrarily large k 2 k 62 k implies hence 51 impossible therefore deduce must belong k proves theorem 2 result important generalization theorem 54 conn et al 5 also interpreted means active constraint identification clear following easy corollary corollary 52 suppose conditions theorem 51 hold assume furthermore linear inequality constraints active x linearly independent normals nondegenerate sense riv denotes relative interior convex set v k identical set active linear inequality constraints x k 2 k sufficiently large proof nondegeneracy assumption linear independence active constraints normals imply unique strictly negative components therefore active linear inequality constraints x strongly active x desired conclusion follows theorem 51 2 note nondegeneracy assumption corresponds strict complementarity slackness context see instance dunn 11 burke et al 4 make additional assumptions pursuing local convergence analysis intend show penalty parameters bounded away zero as5 second derivatives functions fx c x 1 lipschitz continuous limit point x sequence iterates fx k g suppose x kuhntucker point problem 1113 let subset linear inequality constraints active x contains strongly active constraints sx plus arbitrary subset weakly active constraints x columns matrix z form orthonormal basis subspace orthogonal normals constraints assume matrix nonsingular possible choices weakly active constraints set note as6 implies as4 seems reasonable definition strongly weakly active constraints may vary small perturbations prob lem instance lies one extreme faces cone n assumption might seen safeguard possible effect perturbations make distinction subsets penalty parameter converges zero stays bounded away zero define also denote ae k prove analog lemma 51 conn et al 5 suitable general framework lemma 53 assume as1as3 hold let fx k g k 2 k convergent subsequence iterates produced algorithm 31 whose limit point x corresponding lagrange multipliers assume as5 as6 hold x assume furthermore z 6 positive constants integer k 1 ff k 1 ff ii hand p 6 positive constants integer k 1 k 1 z ff proof denote gradient hessian lagrangian function taken respect x limit point h respectively similarly j denote jx also define observe assumptions lemma guarantee theorem 46 used first note finite number possible k may thus consider subsequences k k constant subsequence also note k 2 k belongs unique subsequence order prove result thus sufficient consider arbitrary infinite subsequence k independent k constant index set denoted consequence cones n k k subspaces v k w k orthogonal matrices z k k also independent k denoted n v w z respectively using 23 taylors expansion around x obtain ds boundedness lipschitz continuity hessian matrices f c neighbourhood x together convergence k imply positive constants 8 9 moreover using taylors expansion along fact theorem 46 ensures equality cx obtain z 1s ds see gruver sachs 16 page 11 boundedness hessian matrices c neighbourhood x gives positive constant 10 combining 59 512 obtain suppressed arguments residuals r 1 r 2 r 3 brevity using orthogonal decomposition n v phi w defining may rewrite 514 r 4 r 4 expanding last equation gives thatb 515b r observe 33 inclusion v fact k tends zero imply 0 substituting 516 515 removing middle horizontal block rearranging terms latter equation yields roughly speaking proceed showing righthand side relation order k ensure vector lefthand side size essentially result aim prove first observe 41 obtain 47 519 furthermore 510 511 513 519 520 bound cx k distinguishing components z p first note since penalty parameters subset p bounded away zero test 37 satisfied k sufficiently large moreover remaining components cx k satisfy bound j 2 z k sufficiently large using 49 hence using 53 37 522 deduce note first term last righthand side appears p empty since algorithm ensures may obtain 41 523 519 assumption as6 coefficient matrix lefthand side 517 nonsingular let norm multiplying sides equation inverse taking norms obtain 518 521 524 525 suppose k sufficiently large ensure let recall ff 0 hence ff relations 526528 give converge zero k large enough hence inequalities 529 530 yield p empty use 519 531 518 fact inequality deduce 54 4 deduce 55 47 54 using 21 56 follows 532 55 hand p empty 57 results 47 519 531 finally 58 results 21 57 2 remaining section restrict attention case sequence iterates converges single limit point obviously makes as3 unnecessary briefly comment end section additional assumption cannot relaxed show maximum penalty parameter ff k converges zero lagrange multiplier estimates k converge true values lemma 54 assume as1 as2 hold assume fx k g sequence iterates generated algorithm 31 converges single limit point x as6 holds corresponding lagrange multipliers ff k tends zero sequence k converges proof recall as6 implies as4 therefore assumptions sufficient apply theorem 46 observe desired convergence holds kq j converges thus sufficient show latter result arbitrary j 1 q result obvious step 3a executed infinitely often jth subset indeed time step executed inequality 47 guarantees converges q j suppose therefore step 3a executed infinitely often subset k remain fixed executed remaining iteration implies kcx k ff k tends zero ff k sufficiently large ff k strictly decreases inequality 37 must satisfied k k 3 impossible would imply step 3a executed jth subset hence step 3a must executed infinitely oftenwe consider behaviour maximum penalty parameter ff k show important result stated assumptions bounded away zero proof result inspired technique developed conn et al 5 single penalty parameter definition augmented lagrangian 14 used equivalently one avoids steadily increasing illconditioning hessian augmented lagrangian note illconditioning also avoided q 1 show theorem 56 theorem 55 assume as1 as2 hold suppose sequence iterates fx k g algorithm 31 converges single limit point x corresponding lagrange multipliers as5 as6 hold constant ff min 2 0 1 ff proof suppose otherwise ff k tends zero kj tends zero j 1 q step 3b must executed infinitely often subset aim obtain contradiction statement showing step 3a always executed subset sufficiently large k note assumptions sufficient apply theorem 46 furthermore may apply lemma 53 complete sequence iterates first observe k k 1 ff k 1 lemma 53 note k k 1 follows definition 312 executed otherwise consequence fact ff k unchanged k reduced 313 occurs let k 4 smallest integer k 533 535 imply 5 k k 5 possible lemma 54 define k let gamma set fk j 312 executed iteration k gamma 1 k k 6 g let k 0 smallest element gamma assumption ff k tends zero gamma infinite number elements definition gamma iteration k 0 inequality 56 gives j 536 533 534 consequence inequality step 3a executed j k 0 inequality 55 together 537 guarantee shall make use inductive proof assume j step 3a executed iterations inequalities 538 539 show true aim show true assumption step 3a executed gives iteration inequality 56 yields j 540 536 hence step 3a executed j inequality 55 implies 540 535 establishes 540 executed iterations k k 0 implies gamma finite contradicts assumption step 3b executed infinitely often subset hence theorem provedthis theorem needed conn et al 5 however situation complex q may larger one illconditioning hessian avoided must prove stronger result penalty parameters stay bounded away zero theorem 56 assume as1 as2 hold suppose sequence iterates fx k g algorithm 31 converges single limit point x corresponding lagrange multipliers as5 as6 hold constant 0 kj k proof assume otherwise z empty hence kz converges zero step 3b must executed infinitely often j 2 z aim obtain contradiction statement showing j 2 z step 3a always executed sufficiently large k may deduce theorem 55 ff k attains minimum value ff min 2 0 1 iteration k max say hence p 6 furthermore may apply lemma 53 complete sequence iterates let k 7 k max smallest integer ff k 1 lemma 53 note ff fi j ffl consider jth subset j 2 z iteration k k 7 algorithm ensures step 3b executed jth subset 57 ensures step 3a executed subset summing j 2 z defining executed jth subset iteration kg executed jth subset iteration kg obtain ae purpose obtaining contradiction assume ae k 2 k k 7 542 gives k k 7 ae k1 ae k 541 hence obtain 544 ae therefore since ae k 7 ff kgammak 7 1ffl min tends zero obtain ae k1 2 ff ff jk 7 gammak min ff kgammak 7 1fij sufficiently large k last equality results definition k max 313 contradicts 543 implies 543 hold sufficiently large consequence exists subsequence k ae k 2 k 2 k consider k using 542 545 deduce ae used 541 obtain second inequality consequence k1 2 k 545 holds k sufficiently large returning subset j 2 z obtain 58 545 k sufficiently large 541 hence step 3a executed subset j sufficiently large k implies j belong z therefore z empty proof theorem completed 2 conn et al 5 examine rate convergence algorithms theorem 57 assumptions theorem 56 iterates x k lagrange multipliers k algorithm 31 least rlinearly convergent rfactor ff fi j min ff min smallest value maximum penalty parameter generated algorithm proof proof parallels lemma 53 first theorem 55 shows maximum penalty parameter ff k stays bounded away zero thus remains fixed value ff min 0 k k max subsequent iterations 5 hold moreover theorem 56 implies hold sufficiently large hence 41 bound righthand side 525 may replaced therefore k sufficiently large inequalities 547549 rearranged yield 519 gives 550 show x k converges x least rlinearly rfactor ff fi j min inequalities 47 550 guarantee property conclude section note conclusions theorems 55 56 57 require complete sequence iterates converges unique limit point indicated assumption cannot relaxed counterexample presented conn et al 5 linear inequality constraints simple bound constraints problems variables shows sequence penalty parameters may indeed converge zero single limit point 6 second order conditions strengthen stopping test inner iteration beyond 33 include secondorder conditions guarantee algorithms converge isolated local solution specifically require following additional assumption as7 suppose x k satisfies 33 converges x k 2 k z rank strictly greater z defined as6 assume z r xx phi k z uniformly positive definite smallest eigenvalue uniformly bounded away zero k 2 k sufficiently large prove following result theorem 61 assumptions as1as3 as5as7 iterates x k k 2 k generated algorithm 31 converge isolated local solution 1113 proof definition phi r xx phi x jacobian cx q j note rank z least z as7 implies exists nonzero vector hence j vector as7 implies 21 0 turn gives 61 62 continuity h x k approach limits ensures nonzero satisfying implies x isolated local solution 1113 see instance avriel 2 thm 311 2 assume inner iteration stopping test tightened r xx phi k required uniformly positive definite null space dominant constraints assume nondegeneracy condition 52 holds corollary 52 ensures z sufficiently large k theorem 61 holds weaker version result also holds positive semidefiniteness augmented lagrangians hessian required yielding x possibly isolated minimizer problem 7 extensions 71 flexible lagrange multiplier updates formula 21 definite advantages largescale computations may otherwise appear unduly restrictive purpose first extension consider introduce freedom algorithmic framework replacing formula general condition allowing much larger class lagrange multiplier updates used specifically consider modifying algorithm 31 follows algorithm 71 algorithm identical algorithm 31 except step 3 replaced following fl constant 0 1 step 3 disaggregated updates compute new vector lagrange multiplier estimates k1 step 3b otherwise step 3a update lagrange multiplier estimates set step 3b reduce penalty parameter set kj defined 39 algorithm 31 algorithm 71 allows flexible choice multipliers algorithm 31 requires control enforced prevent growth unacceptably fast rate covers among others choice leastsquares estimates x defined 25 global convergence theory presented section 4 algorithm 31 extended cover algorithm 71 extension detailed conn et al 9 conn et al 10 extend local convergence analysis section 5 algorithm 71 additional condition holds positive constants 22 23 k 2 k sufficiently large k index set subsequence iterates generated algorithm 71 converging critical point x corresponding lagrange multipliers 21 25 satisfy condition theorem 46 also note corollary 52 ensures leastsquares multiplier estimates implementable nondegeneracy condition 52 holds mean estimates identical defined 25 k sufficiently large unlike 25 well defined x unknown 72 alternative criticality measures algorithms 31 71 used criticality measure kp k order define stopping criterion inner iteration see 33 general however quantity might easily computed course numerical method used calculate x k especially dimension problem high therefore interest examine criticality measures might easier calculate purpose section analyze alternative proposals given k n k ad k first claim 33 replaced requirement exists set nonpositive dominant multipliers f ik g i2m k jd k jdimensional vector whose ith component ik zero otherwise prove claim lemma 71 assume exists nonpositive k 71 holds x k 33 also holds x k proof since vector belongs construction cone n k defined 34 immediately deduce definition othogonal projection 71 desired inequality 2 condition 71 appealing two reasons firstly set possibly approx imate multipliers available many numerical procedures might used perform inner iteration compute suitable x k one select multipliers correspond dominant constraints restrict choice nonpositive ones finally check 71 scheme implicitly used harwell 17 barrierfunction quadratic programming codes ve14 ve19 imsl 19 general linearly constrained minimization package lcong alternatively suitable multipliers computed instance approxi mately solving leastsquares problem min k selecting nonpositive components resulting vector approxi mately solving constrained leastsquares problem min k condition 71 also appealing provides single condition stopping condition inner iteration measure tolerated inexactness solving associated leastsquares problem procedure chosen obtain dominant multipliers may therefore deduce lemma 71 convergence theory holds algorithms 31 71 whenever 71 used instead 33 condition 71 specialized instance one might choose impose familiar reduced gradient criterion orthogonal matrix whose columns span null space constraints active x k provided multipliers associated linear constraints nonpositive case tangent cone set determined linear inequality constraints active x k contains k consequence convergence theory still holds criterion implemented several subroutines minimizing general objective function subject linear constraints example nag 22 quadratic programming code e04nff general package e04ucf used inneriteration stopping rule within algorithms 31 71 also true reduced gradient methods eg minos 21 lsnno 24 compute full column rank matrix whose columns generally non orthonormal depend upon subset finite number coefficients linear constraints indeed norm bounded away zero relationship weighted form 73 thus also holds cases order preserve coherence framework presented conn et al 8 finally note oe k defined 43 may also viewed criticality measure hence might decide stop inner iteration reader referred conn et al 9 proof global convergence still obtained modification algorithms 31 71 however authors able prove desired local convergence properties 74 instead local convergence theory covered algorithms 31 71 stronger condition see conn et al 10 details condition theoretically interesting might practically strong note show implies variant 33 theorem 72 assume fx k g k 2 k convergent subsequence vectors b 75 holds k 2 k k converge zero k increases k inequality also holds k 2 k sufficiently large 24 1 proof first consider simple case linear inequality present case easy check 43 oe must gammar x phi k k therefore obtain holds large enough ensure k 1 assume p 0 moreau decomposition gammar x phi k 20 given obviously holds choice 24 assume therefore p k nonzero show x k b define kak1 assume first 2 k gammaa 2 n k polarity n k k since x k 2 b obtain hand 62 k gathering 78 79 obtain x k b desired furthermore since kd k k 1 definition verified k feasible minimization problem 43 associated definition oe k hence used successively moreau decomposition gammar x phi k definition k orthogonality terms moreau decomposition ffl 75 710 imply sufficiently large otherwise deduce 710 75 77 consequence 711 712 therefore obtain 76 holds combining cases conclude 76 holds last value 24 2 finally note lemma 71 theorem 72 depend actual form augmented lagrangian 15 valid independently function minimized inner iteration observation could useful alternative techniques augmenting lagrangian considered merit function 8 conclusion considered class augmented lagrangian algorithms constrained nonlinear optimization linear constraints present problem handled directly multiple penalty parameters allowed algorithms class advantage efficient techniques handling linear constraints may used inner iteration level also sparsity pattern hessian augmented lagrangian independent linear constraints global local convergence results available specific case linear constraints reduce simple bounds extended general useful context form linear constraint permitted finally note theory presented directly relevant practical compu tation inner iteration stopping rule 33 covers type optimality tests used available packages linearly constrained problems means packages applied obtain approximate solution subproblem constitutes realistic attractive algorithmic development authors intention perform extensive numerical experiments largescale problems development requires considerable care sophistication efficient solver subproblem integrated class algorithms described acknowledgements authors wish acknowledge funding provided nato travel grant also grateful j nocedal anonymous referees constructive comments r computing search direction largescale linearly constrained nonlinear optimization calculations nonlinear programming analysis methods constrained optimization lagrange multiplier methods convergence properties trust region methods linear convex constraints globally convergent augmented lagrangian algorithm optimization general constraints simple bounds lancelot fortran package largescale nonlinear optimization release number inner iterations per outer iteration globally convergent algorithm optimization general nonlinear equality constraints simple bounds global convergence class trust region algorithms optimization using inexact projections convex constraints global convergence two augmented lagrangian algorithms optimization combination general equality linear constraints local convergence properties two augmented lagrangian algorithms optimization combination general equality linear constraints convergence projected gradient processes singular critical points introduction sensitivity stability analysis nonlinear programming practical methods optimization newton methods largescale linear equalityconstrained minimization convergence sequential penalty function method constrained minimization algorithmic methods optimal control catalogue subroutines release 11 multiplier gradient methods decomposition orthogonale dun espace hilbertien selon deux cones mutuellement polaires mark 16 method nonlinear constraints minimization problems lsnno fortran subroutine solving large scale nonlinear network optimization problems indefinite systems interior point methods tr