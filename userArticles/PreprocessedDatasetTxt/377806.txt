data locality enhancement memory reduction paper propose memory reduction new approach data locality enhancement approach use compiler reduce size data repeatedly referenced collection nested loops reuses data likely remain higherspeed memory devices cache specifically present optimal algorithm combine loop shifting loop fusion array contraction reduce temporary array storage required execute collection loops applied 20 benchmark programs technique reduces memory requirement counting data code 51 average transformed programs gain speedup 140 average due reduced footprint consequently improved data locality b introduction compiler techniques tiling 29 30 improves temporal data locality interchanging nesting order time iterative loops arraysweeping loops unfortunately data dependences many programs often prevent loop interchange therefore important seek locality enhancement techniques beyond tiling paper propose memory reduction new approach data locality enhancement approach use compiler reduce size data repeatedly referenced collection nested loops end end end ei end b ei ei end end ei end c figure 1 example 1 reuses data likely remain higherspeed memory devices cache even without loop inter change specically present optimal algorithm combine loop shifting loop fusion array contraction contract number dimensions arrays exam ples twodimensional array may contracted single dimension whole array may contracted scalar opportunities memory reduction exist often natural way specify computation task may memoryecient programs written array languages f90 hpf often memory inecient consider extremely simple example example 1 figure 1a array assumed dead loop l2 rightshifting loop l2 one iteration figure 1b l1 l2 fused figure 1c array contracted two scalars a1 a2 figure 1d shows positive sideeect temporal locality array e also improved aggressive fusion proposed also improves temporal data locality dierent loop nests 8 fraboulet et al present network ow algorithm memory reduction based retiming theory 16 given perfect nest retiming technique shifts number iterations either left right statement loop body dierent statements may dierent shifting factors three issues remain unresolved work algorithm assumes perfectlynested loops applies one loop level loops reality ever mostly multiple levels arbitrarily nested perfectlynested loops although one may apply algorithm one level time approach provably minimize memory requirement program transformations loop coalescing coalesce multiple loop levels single level unfortunately however algorithm applicable resulting loop required loop model longer satised since work target data locality relationship memory minimization local ityperformance enhancement studied general minimization memory requirement guarantee better locality better performance care must taken control scope loop fusion lest increased loop body may increase register spills cache replacements even though footprint whole loop nest may reduced experimental data show memory requirement actually reduced using algorithm moreover since algorithm addresses memory minimization experimental data verify conjecture reduced memory requirement result better locality better performance especially scope loop fusion careful control paper make following main contributions present network ow algorithm provably minimizes memory requirement multidimensional cases handle imperfectly nested loops contains collection perfect nests combination loop shifting loop fusion array contraction completely reformulate network ow exactly models problem multidimensional general case work 8 could also applied program model loop fusion applied rst possibly enabled loop shifting however new algorithm preferable 1 multidimensional cases algorithm optimal polynomialtime solvable complexity heuristic 8 2 algorithm models imperfectlynested loops directly general case propose heuristic control fusion estimated numbers register spills cache misses greater ones original loop nests even though benchmarking cases tested far small utilize heuristic important bigger cases many realistic programs may immediately program model even though already general 8 use number compiler algorithms transform programs order model implemented memory reduction technique research compiler apply technique benchmark programs experiments results show memory requirement reduced substantially also reduction indeed lead better cache locality faster execution test cases average memory requirement benchmarks reduced 51 counting code data using arithmetic mean transformed programs average speedup 140 using geometric mean original programs rest paper present preliminaries section 2 formulate network ow problem prove complexity section 3 present controlled fusion discuss enabling techniques section 4 section 5 provides experimental results related work discussed section 6 conclusion given section 7 2 preliminaries section introduce basic concepts present basic idea algorithm present program model section 21 introduce concept loop dependence graph section 22 make three assumptions algorithm section 23 section 24 illustrate basic idea algorithm section 25 introduce concept reference window based algorithm developed lastly section 26 show original loop dependence graph simplied preserving correctness algorithm 21 program model consider collection loop nests l1 l2 1 lexical order shown figure 2a label l denotes perfect nest loops indices l i1 starting outmost loop example 1 ie figure 1a loop l ij lower bound l ij upper bound respectively l ij u ij loop invariants simplicity presentation loop nests l 1 assumed nesting level n apply technique dierent loop levels incrementally 27 cases strictly satisfy requirement transformed model using techniques code sinking 30 array regions referenced given collection loops divided three classes input array region upwardly exposed beginning l1 output array region live lm local array region intersect input output array regions utilizing existing dependence analysis region analysis live analysis techniques 4 11 12 19 compute input output local array regions eciently note input output regions overlap example 1 figure 1a e0 n input array c b e figure 2 original transformed loop nests region output array region a1 n local array region figure 3a shows complex example example 2 resembles one wellknown livermore loops example 2 declared array dimension jn zz za12kn zb2jnkn1 input array regions zp zr zq zz output array regions za2jn2kn zb2jn2kn local array regions figure 2b shows code form loop shifting loop fusion p j l represents shifting factor loop l ij rest paper assume loops coalesced singlelevel loops 30 27 loop shifting loop fusion figure 2c shows code form loop coalescing loop fusion l lower upper loop bounds coalesced loop nest l figure 2d shows code form loop fusion loops coalesced ease code generation general cases however common cases loop coalescing unnecessary 27 figure 2e shows code form loop fusion without loop coalescing applied array contraction applied code shown either figure 2d figure 2e 22 loop dependence graph extend denitions traditional dependence distance vector dependence graph 14 collection loops follows denition 1 given collection loop nests l1 lm figure 2a data dependence exists iteration n loop l1 iteration j1 loop l2 say distance vector j1 denition 2 given collection loop nests l1 l2 lm loop dependence graph ldg directed graph e node v represents loop nest directed edge e represents data dependence ow anti output dependence l end end l2 end end l3 end end l4 end end b c figure 3 example 2 original simplied loop dependence graphs edge e annotated distance vector 1 dve dependence edge e distance vector constant replace set edges follows let set dependence distances e represents let lexicographically minimum distance let f d1 6 d1g vector 29 30 u lexicographically greater n d1 s1 subset lexically neither smaller equal vector replace original edge e js1 annotated d0 figure 3b shows loop dependence graph example figure 3a without showing array regions example ow dependence l1 l3 due array region figure 3b multiple dependences type ow anti output exist one node another use one arc represent gure associated distance vectors marked single arc 23 assumptions make following three assumptions order simplify formulation section 3 assumption 1 loop trip counts perfect nests l l j equal corresponding loop level h n also stated u ih l ih assumption 1 satised applying techniques loop peeling loop partitioning dierence iteration counts small loop peeling quite eective otherwise one partition iteration spaces certain loops equal pieces throughout paper use h denote loop trip count loop l level h constant symbol icly constant wrt program segment consid eration denote equals number accumulated leveln loop iterations executed one levelh loop iteration let paper also denote number static write references due local array regions 2 loop l arbitrarily assign static write reference l number 1 k order distinguish take loops figure 3b example assumption 2 sum absolute values dependence distances loop level h loop dependence graph e less onefourth trip count loop level h assumption also stated jej dvek j 1 ek 2 e annotated dependence distance vector assumption 2 reasonable programs constant dependence distances generally small nonconstant dependence distances exist techniques discussed section 42 loop interchange circular loop skewing may utilized reduce dependence distances 2 rest paper term static write ref erence means static write reference due local array regions d2 d2 b figure 4 illustration memory minimization assumption 3 static write reference r instance r writes distinct memory location ifstatement within loops guard statement contains reference r dierent static references write dierent memory locations static write reference write distinct memory location loop iteration apply scalar array expansion reference 30 later technique minimize total size local array regions case statements assume branches taken 27 discussed case regions written two dierent static write references overlap 24 basic idea loop shifting applied loop fusion order honor dependences associate integer vector pl loop nest l loop dependence graph denote shifting factor l j loop level k figure 2b dependence edge distance vector dv new distance vector pl dv pl memory minimization problem therefore reduces problem determining shifting factor p j l loop l ij total temporary array storage required minimized loops coalesced legally fused dv pl distance vector dv loop shifting paper v2 denotes inner product v1 v2 loop coalescing distance vector v becomes v call coalesced dependence distance order make loop fusion legal must hold legality transformation stands key memory minimization count number simultaneously live local array elements transformation loop shifting loop coalescing loop fusion example figure 4a shows part iteration space three loop nests loop coalescing loop fusion rectangle bounds iteration space loop nest point gure represents one iteration two arrows represent two ow dependences d1 d2 due static write reference say r1 r1 source d1 d2 loop fusion iteration spaces dierent loop nests map common iteration space figure 4b shows three separate iteration spaces map common one based assumption 3 v also represents number simultaneously live variables due v common iteration space figure 4b number simultaneously live variables 1 d1 3 d2 however could overlaps simultaneously live local array elements due dierent dependences figure 4b simultaneously live array elements dependences d1 d2 overlap case number simultaneously live local array elements due static write reference r1 greater two due dependences d1 d2 ie 3 case given collection loops fusion total number simultaneously live local array elements equals sum number simultaneously live local array elements due static write reference 25 reference windows 9 gannon et al use reference window quantify minimum cache footprint required dependence loopinvariant distance shall use concept quantify minimum temporary storage satisfy ow dependence denition 3 9 reference window w x dependence variable x time dened set elements x referenced s1 also referenced according dependence s2 figure 1a reference window due ow dependence l1 l2 due array beginning loop l2 iteration f ai ai g reference window size ranges 1 n figure 1c reference window due ow dependence caused array beginning loop iteration f ai 1 g reference window size 1 next extend denition 3 set ow dependences follows denition 4 given ow dependence edges e1 e2 es suppose reference windows time w1 w2 respectively dene reference window f es g time since reference window characterizes minimum memory required carry computation problem minimizing memory required given collection loop nests equivalent problem choosing loop shifting factors loops legally coalesced fused fusion reference window size ow dependences due local array regions minimized given collection loop nests legally fused need predict reference window loop coalescing fusion denition 5 loop node l ldg writes local array regions r suppose iteration j1 becomes iteration j loop coalescing fusion dene predicted reference window l iteration j1 reference window ow dependences due r beginning iteration j coalesced fused loop suppose predicted reference window iteration jn largest size due r dene predicted reference window size entire loop l due r dene predicted reference window due static write reference r l predicted reference window l due array regions written r convenience l writes nonlocal regions dene predicted reference window empty based denition 5 following lemma lemma 1 predicted reference window size kth static write reference r l must smaller predicted reference window size ow dependence due r proof predicted reference window size ow dependence smaller minimum required memory size carry computation dependence predicted reference window size kth static write reference r l smaller memory size carry computation ow dependences due r theorem 1 minimizing memory requirement equivalent minimizing predicted reference window size ow dependences due local array regions proof denition 5 lemma 1 given dependence distance vector dv dependence distance loop coalescing loop fusion also call coalesced dependence distance due assumption 3 dv also represents predicted reference window size coalesced iteration space original iteration space lemma 2 loop fusion legal coalesced dependence distances nonnegative proof preserve original dependences take loop node l2 figure 3c example predicted reference window size l2 due static write reference zbj k predicted reference window size l2 since zbj k write reference l2 26 ldg simplification loop dependence graph simplied keeping dependence edges necessary memory reduction simplication process based following three claims claim 1 dependence l automatically preserved loop shifting loop coalescing loop fusion reordering computation within loop l 2 among dependence edges l l j suppose edge e lexicographically minimum dependence distance vector loop shifting coalescing dependence distance associated e nonnegative legal fuse loops l l j loop shifting coalescing dependence distances dependence edges remain equal greater edge e thus remain nonnega tive words fusionpreventing dependences ex ist prove claim section 3 lemma 3 3 amount memory needed carry computation determined lexicographically maximum owdependence distance vectors due local array regions according lemma 1 simplication also classify edges two classes ledges medges ledges used determine legality loop fusion medges determine minimum memory requirement medges ow dependence edges ledge could ow anti output dependence edge possible edge classied ledge medge simplication process follows combination node l static reference r l 0 among dependence edges l due r keep one whose ow dependence distance vector lexicographically maximum edge medge node l remove dependence edges l node l 0 among dependence edges l l j j 6 keep one dependence edge legality dependence distance vector lexicographically minimum edge ledge static write reference r l among dependence edges l l j j 6 due r keep one ow dependence edge whose distance vector lexicographically maximum edge medge node l among dependence edges l l j j 6 keep dependence edge whose dependence distance vector lexicographically minimum edge ledge process simplies program formulation makes graph traversal faster figure 3c shows loop dependence graph simplication figure 3b figure 3c mark classes dependence edges example dependence edge l1 l3 marked 0 0 ledge one marked 0 1 medge latter edge associated static write reference zaj k 3 objective function section rst formulate graphbased system minimize number simultaneously live local array el ements reduce problem network ow problem solvable polynomial time 31 formulating problem 1 given loop dependence graph g objective function minimize number simultaneously live local array elements loop nests formulated follows edge g subject call system dened problem 1 2 ik represents number simultaneously live array elements due kth static write reference l constraint 3 says coalesced dependence distance must nonnegative ledges loop coalescing loop fusion constraint 4 says number simultaneously live local array elements due kth static write reference l must smaller number simultaneously live local array elements every medge originated l due kth static write reference l combining constraint 3 assumption 2 following lemma says coalesced dependence distance also nonnegative medges lemma 3 constraint 3 holds pl holds medges e g proof dve holds otherwise assume rst nonzero component dve hth component based assumption 2 medge e2 must exist ledge e1 constraint 3 guarantees holds pl dve1 denition ledges medges similar proof case prove holds proof lemma 3 also see dependence eliminated simplication process section 26 coalesced dependence distance also nonnegative given constraint 3 holds hence coalesced dependence distances original dependences simplication section 26 nonnega tive loop shifting coalescing loop fusion loop fusion legal according lemma 2 section 26 know ow dependence edge e3 l l j due static write reference r eliminated simplication process must exist medge e4 l l j due r proof holds hence constraint 4 computes predicted reference window size ow dependences originated l due kth static write reference unsimplied loop dependence graph see section 22 according lemma 1 constraint 4 correctly computes predicted reference window size ik 32 transforming problem 1 dene new problem problem 2 adding following two constraints problem 1 e edge g lemma 4 given optimal solution problem 1 construct optimal solution problem 2 value objective function 2 proof search space problem 2 subset problem 1 given ldg g optimal objective function value 2 problem 2 must equal greater problem 1 given optimal solution problem 1 nd shifting factor p ik values problem 2 follows 1 initially let p ik values problem 1 solution problem 2 following steps adjust values constraints problem 2 satised value objective function 2 changed 2 p values satisfy constraint 5 go step 4 otherwise go step 3 3 step nds p values satisfy constraint 5 following topological order nodes g nd rst node l exists ledge constraint 5 satised ignore self cycles since must represent medges g suppose sth rst nonzero component let two nonzero components sth pl new p values including pl j satisfy constraints 3 4 value objective function 2 also changed pl lexicographically nega tive repeat process process terminate within n times since otherwise constraint 3 would hold optimal solution problem 1 note node l selected based topological order shifting factor pl j increased compared original value ledge destination node l j constraint 5 holds updating pl j still holds update property guarantee process terminate go step 2 4 step nds ik values satisfy constraint 6 given nd value satises constraint 6 constraint 6 becomes equal least one edge achieved satises constraint 4 done otherwise increase nth component ik value constraint 4 holds becomes equal least one edge find values value objective function 2 changed p ik values value objective function 2 problem 2 problem 1 hence get optimal solution problem 2 value objective function 2 theorem 2 optimal solution problem 2 also optimal solution problem 1 proof given optimal solution problem 2 take p ik values solution problem 1 p ik values satisfy constraints 34 value objective function 2 problem 1 problem 2 solution must optimal problem 1 otherwise construct problem 1 another solution problem 2 lower value objective function 2 according lemma 4 contradicts optimality original solution problem 2 expanding vectors problem 2 integer programming problem results general solutions ip problems however take ldg graphical characteristics account instead solving ip problem 1 1 figure 5 transformed graph g1 figure 3c transform network ow problem discussed next subsection 33 transforming problem 2 given loop dependence graph g generate another graph node l create corresponding node g1 node l outgoing medge let weight l w static reference rk 1 k l create another node g1 called sink due rk let weight w node l 2 g outgoing medge let weight medge g due static write reference rk suppose distance vector dv add edge g1 distance vector dv ledge suppose distance vector dv add edge g1 distance vector dv original graph figure 3c figure 5 shows transformed graph assign vector q node g1 follows node node new system call problem 3 dened follows edge g1 annotated dk subject dk 0 8e 8 theorem 3 problem 3 equivalent problem 2 proof 0 hence objective function 2 equivalent 7 edge e g1 inequality 8 equivalent e1 ledge g l l j inequality 10 equivalent 5 hence inequality 8 equivalent 5 edge e g1 inequality 8 equivalent e1 medge g l l j due kth static write reference l inequality 11 equivalent 6 hence inequality 8 equivalent 6 similarly easy show constraints 3 4 equivalent constraint 9 note one edge g could ledge edge corresponds two edges g1 assumption 2 following inequality holds transformed graph dvek j 1 ek 2 e1 annotated dependence distance vector problem 2 problem 3 solved linearizing vector representation original problem becomes integer programming problem general form npcomplete next subsections however show achieve optimal solution polynomial time problem 3 utilizing network ow property 34 optimality conditions develop optimality conditions solve problem 3 utilize network ow property network ow consists set vectors vector fe corresponds edge e 2 e1 node v 2 v1 sum ow values inedges equal wv plus sum ow values outedges ek v represents inedge v represents outedge v lemma 5 given exists least one legal network ow proof find spanning tree g1 assign ow value 0 edges hence nd legal network ow ow assignment also legal g1 assign ow value edges reverse topological order since total weight nodes equal 0 legal network ow exists based equation 13 given legal network ow node v 2 v1 1 network ow algorithm abstract factor wv wv represented c abstraction give ow value ck integer constant suppose fek 0 edge ek 2 e1 equivalent ck 0 constraint 9 hence therefore equation 14 fek 0 collectively optimality conditions stated following theorem hold inequality becomes equality optimality achieved problem 3 theorem 4 following three conditions hold 1 constraints 8 9 satised 2 legal network ow fek exists ck 3 jv 1 j dk holds ie inequality becomes equality problem 3 achieves optimal solution dk proof obvious discussion solving problem 3 let us consider vector wv dk single computation unit based duality theory 24 2 problem 3 excluding constraint 9 equivalent subject constraint 9 mandatory equivalence problem 3 dual problem following development optimality conditions section 34 1 constraint 19 dual system precisely denes ow property edge e associated ow vector fe dene problem 4 system 78 18 20 similar wv vector fek represented ck although apparently search space problem 4 encloses problem 3 problem 4 correct solutions within search space dened problem 3 based property duality problem 4 achieves optimal solution constraints 8 19 20 holds objective function values 7 18 equal dk holds prove constraint 9 holds optimal solution problem 4 solution must also optimal problem 3 according theorem 4 exist plenty algorithms solve problem 4 1 2 although algorithms targeted scalar system vector length equals 1 directly adapted system vector summation subtraction comparison operations network simplex algorithm 2 directly utilized solve sys tem algorithmic complexity however exponential worst case terms number nodes edges g1 several graphbased algorithms 1 hand polynomialtime complexity examples include successive shortest path algorithm complexity scaling algorithm complexity ojv1 jje1 jlogjv1 j 1 current fastest polynomialtime algorithm solving network ow problem enhanced capacity scaling algorithm complexity logjv1 j algorithms following lemma lemma 6 optimal solution q problem 4 exists spanning tree g1 edge proof true due foundation simplex method 2 let spanning tree lemma 6 x q 0 determined uniquely uniquelydetermined ds e dk inequality 21 ds j 22 inequality 22 based inequality 12 annotated lemma 7 annotated dk subject constraints 8 23 proof holds otherwise assume rst nonzero component hth dk q q h constraint 23 hence inequality 12 guarantees constraint holds optimality problem 4 achieved optimal solution problem 4 also optimal solution problem 3 36 successive shortest path algorithm brie present one network ow algorithms successive shortest path algorithm 1 used solve problem 4 algorithm depicted figure 6 let fek scalars rst iteration algorithm always output procedure initialize sets select node vk 2 e v l 2 determine shortest path distances j node vk nodes g1 respect residue costs c edge annotated ij g1 let p denote shortest path vk v l update ow value residue network ow graph augment units ow along path p residue graph figure successive shortest path algorithm maintains feasible shifting factors nonnegativity ow values satisfying constraints 8 20 adjusts ow values constraint 19 holds edges g1 algorithm ends complete description algorithm including concept reduced cost residue network ow graph semantics sets e etc please refer 1 example 2 figure 3 applying successive shortest path algorithm pl1 pl2 figure 7 shows transformed code example 2 memory reduction 4 refinements 41 controlled fusion although array contraction loop fusion decrease overall memory requirement loop fusion many loop levels potentially increase working set size loop body hence potentially increase register spilling cache misses particularly true large number loops consideration control number fused loops computing shifting factors minimize memory requirement use simple greedy heuristic pick reject see figure 8 incrementally select loop nests actually fused new addition cause estimated cache misses register spills worse fusion loop nest consideration fused heuristic continues select fusion candidates remaining loop nests loop nests examined order loops whose fusion saves memory considered rst estimate register spilling using approach 22 estimate cache misses using approach 7 may also important avoid performing loop fusion many loop levels corresponding loops shifted loop shifting loop fusion many loop levels potentially increase number operations either due ifstatements added loop body due eect loop peeling coalescing applied may also introduce subscript computation overhead although costs tend less signicant costs cache misses register spills still carefully end end end end end end figure 7 transformed code figure 3a memory reduction control fusion innermost loops rate increased operations fusion exceeds certain threshold fuse outer loops 42 enabling loop transformations use several wellknown loop transformations enable eective fusion long backward datadependence distances make loop fusion ineective memory reduction long distances sometimes due incompatible loops 26 corrected loop interchange long backward distances may also due circular data dependences corrected circular loop skewing 26 fur thermore technique applies loop distribution node dependence distance vectors originated l dierent case distributing loop may allow dierent shifting factors distributed loops potentially yielding favorable result 5 experimental results implemented memory reduction technique research compiler panorama 12 implemented net work ow algorithm successive shortest path algorithm 1 loop dependence graphs experiments relatively simple successive shortest path algorithm takes less 006 seconds benchmarks measure eectiveness tested memory reduction technique 20 benchmarks sun ultra ii uniprocessor workstation mips r10k processor within sgi procedure pick reject input 1 collection loop nests 2 estimated number register spills np estimated number cache misses nm original loop nests output set loop nests fused fs procedure 1 initialize fs empty let os initially contain loop nests 2 os empty return fs otherwise select loop nest l os local array regions r written l reduced ie dierence size r number simultaneously live array elements due static write references l lexically neither smaller equal loop nest os let tr set loop nests os contain references r estimate number register spills b number cache misses fusing loops fs tr performing array contraction fused loop npb nm fs fstr os os tr otherwise os os fl g go step 2 figure 8 procedure pick reject origin 2000 multiprocessor present experimental results r10k results ultra ii similar 27 r10k processor 32kb 2way setassociative data cache 32byte cache line 4mb 2way setassociative unied l2 cache 128byte cache line cache miss penalty 9 machine cycles l1 data cache 68 machine cycles l2 cache 51 benchmarks memory reduction table 1 lists benchmarks used experiments descriptions input parameters benchmarks chosen either readily program model transformed enabling algorithms additional enabling algorithms developed future hope collect test programs table mn represents number loops loop sequence maximum loop nesting level n note array size iteration counts chosen arbitrarily ll14 ll18 jacobi dierentiate benchmark swim spec95 spec2000 denote spec95 version swim95 spec2000 version swim00 program swim00 almost identical swim95 except larger data size combustion change array size n1 n2 1 10 execution time last several seconds programs climate laplacejb laplacegs purdue set problems hpf benchmark suite rice university 20 21 except lucas benchmarks written f77 manually apply technique lucas written f90 among 20 benchmark programs algorithm nds purdueset programs lucas ll14 combustion need perform loop shifting benchmarks table 1 loops fused together swim95 swim00 hydro2d outer loops fused benchmarks n loop levels fused benchmarks examine three versions code ie original one one loop fusion array contraction one array contrac table 1 test programs benchmark name description input parameters mn ll14 livermore loop 14 jacobi jacobi kernel wo convergence test tomcatv mesh generation program spec95fp reference input 51 swim95 weather prediction program spec95fp reference input 22 swim00 weather prediction program spec2000fp reference input 22 hydro2d astrophysical program spec95fp reference input 102 lucas promality test spec2000fp reference input 31 mg multigrid solver npb23serial benchmark class w 21 combustion thermochemical program umd chaos group purdue02 purdue set problem02 reference input 21 purdue03 purdue set problem03 reference input 32 purdue04 purdue set problem04 reference input 32 purdue07 purdue set problem07 reference input 12 purdue08 purdue set problem08 reference input 12 purdue12 purdue set problem12 reference input 42 purdue13 purdue set problem13 reference input 21 climate twolayer shallow water climate model rice reference input 24 laplacejb jacobi method laplace rice laplacegs gaussseidel method laplace rice combustion purdue02 purdue03 purdue04 purdue07 purdue08 purdue12 purdue13 climate laplacejb laplacegs benchmarks benchmark left right original transformed codes normalized occupied memory size code data data size original programs unit kb swim00 hydro2d lucas mg combustion 191000 11405 142000 8300 89 purdue12 purdue13 climate laplacejb laplacegs 4194 4194 169 6292 1864 figure 9 memory sizes transfor mation tion among programs combustion purdue07 purdue08 program model 8 cases algorithm 8 derive result need list results versions benchmarks use native fortran compilers produce machine codes simply use optimization ag o3 following adjust ments switch prefetching laplacejb software pipelining laplacegs loop unrolling purdue03 swim95 swim00 native compiler fails insert prefetch instructions innermost loop body memory reduction manually insert prefetch instructions three key innermost loop bodies following exactly prefetching patterns used native compiler original codes figure 9 compares code sizes data sizes original transformed codes compute data size based global data common blocks local data dened main program data size shown original program normalized 100 actual data size varies greatly dierent benchmarks listed table associated gure mg climate memory requirement diers little program transformation due small size contractable local array benchmarks technique reduces memory requirement considerably arithmetic mean reduction rate counting data code 51 benchmarks several small purdue benchmarks reduction rate almost 100 52 performance figure compares normalized execution time mid represents execution time codes loop fusion array contraction final represents execution time codes array contraction geometric mean speedup memory reduction 140 benchmarks best speedup 567 achieved program purdue03 combustion purdue02 purdue03 purdue04 purdue07 purdue08 purdue12 purdue13 climate laplacejb laplacegs normalized execution time original mid final figure 10 performance transfor combustion normalized cache refmiss count dl1hit dl1miss l2miss original mid final left right figure 11 cache statistics trans formation program contains two local arrays a1024 1024 p 1024 carry values three adjacent loop nests technique able reduce arrays scalars fuse three loops one 53 memory reference statistics understand eect memory reduction performance examined cache behavior dier ent versions tested benchmarks measured reference count dynamic loadstore instructions miss count l1 data cache miss count l2 unied cache use perfex package get cache statistics figures 11 12 compare statistics total reference counts original codes normalized 100 arrays contracted scalars register reuse often increased figures 11 12 show number total references get decreased cases total number reference counts benchmarks reduced 211 however cases total reference counts get increased instead examined assembly codes found number reasons50150250350purdue02 purdue03 purdue04 purdue07 purdue08 purdue12 purdue13 climate laplacejb laplacegs normalized cache refmiss count dl1hit dl1miss l2miss original mid final left right figure 12 cache statistics transformation cont 1 fused loop body contains scalar references single iteration fusion increases register pressure sometimes causes register spilling 2 native compilers perform scalar replacement 3 references noncontracted arrays fused loop body may prevent scalar replacement two reasons register pressure high certain loop native compiler may choose perform scalar replacement loop fusion array data ow may become complex may defeat native compiler attempt perform scalar replacement 3 loop peeling may decease eectiveness scalar replacement since fewer loop iterations benet despite possibility increased memory reference counts cases due reasons figures 11 12 show cache misses generally reduced memory duction total number cache misses bench marks reduced 638 memory reduction total number l1 data cache misses reduced 573 memory reduction improved cache performance seems often bigger impact execution time total reference counts 54 experiments 27 reported memory reduction technique aects prefetching software pipelining register allocation unrollandjam conclude technique seem create diculties optimizations 6 related work work fraboulet et al closest memory reduction technique 8 given perfectlynested loop use retiming 16 adjust iteration space individual statements total buer size minimized compared algorithm introduction section 51 callahan et al present unrollandjam scalar replacement techniques replace array references scalar variables improve register allocation 3 however consider innermost loop perfect loop nest consider loop fusion neither consider array partial contraction gao sarkar present collective loop fusion 10 perform loop fusion replace arrays scalars consider partial array contraction perform loop shifting therefore cannot fuse loops fusionpreventing dependences sarkar gao perform loop permutation loop reversal enable collective loop fusion 23 enabling techniques also used framework lam et al reduce memory usage highlyspecialized multidimensional integral problems array subscripts loop index variables 15 program model allow fusionpreventing dependences lewis et al proposes apply loop fusion array contraction directly array statements array languages f90 17 result achieved array statements transformed various loops loop fusion array contraction applied consider loop shifting formulation strout et al consider minimum working set permits tiling loops regular stencil dependences 28 method applies perfectlynested loops 6 ding indicates potential combining loop fusion array contraction example ever apply loop shifting provide formal algorithms evaluations loop fusion studied extensively name publications kennedy mckinley prove maximizing data locality loop fusion nphard 13 provide two polynomialtime heuristics singhai mckinley present parameterized loop fusion improve parallelism cache locality 25 perform memory reduction loop shifting recently darte analyzes complexity loop fusions 5 claims problem maximum fusion parallel loops constant dependence distances npcomplete combined loop shifting goal nd minimum number partitions loops within partition fused possibly enabled loop shifting fused loop remains parallel mainly dierent objective functions problem yield completely dierent complexity manjikian abdelrahman present shiftandpeel 18 shift loops order enable fusion none works listed address issue minimizing memory requirement collection loops techniques dierent 7 conclusion paper propose enhance data locality via memory reduction technique combination loop shifting loop fusion array contraction reduce memory reduction problem network ow problem solved optimally ojv j 3 time experimental results far show technique reduce memory requirement signicantly time speeds program execution factor 140 average furthermore memory reduction seem create diculties number backend compiler optimizations also believe memory reduction vitally important computers severely memoryconstrained applications extremely memorydemanding 8 acknowledgements work sponsored part national science foundation grants ccr9975309 aciitr0082834 mip9610379 indiana 21st century fund purdue research foundation donation sun microsys tems inc 9 r network flows theory linear programming network flows improving register allocation subscripted variables interprocedural array region analyses complixity loop fusion improving estimating enhancing cache e loop alignment memory accesses optimization strategies cache local memory management global program transformation collective loop fusion array contraction structured data ow analysis arrays use optimizing compiler experience ecient array data ow analysis array privatization maximizing loop parallelism improving data locality via loop fusion distribution structure computers computations optimization memory usage communication requirements class loops implementing multidimensional integrals retiming synchronous circuitry implementation evaluation fusion contraction array languages fusion loops parallelism locality array data ow analysis use array privatization applications benchmark set fortrand high performance fortran problems test parallel vector languages optimized unrolling nested loops optimization array accesses collective loop transformations theory linear integer programming parameterized loop fusion algorithm improving parallelism cache locality new tiling techniques improve cache temporal locality performance enhancement memory reduction improving locality parallelism nested loops high performance compilers parallel computing tr theory linear integer programming strategies cache local memory management global program transformation linear programming network flows 2nd ed structured dataflow analysis arrays use optimizing complier improving register allocation subscripted variables optimization array accesses collective loop transformations network flows arraydata flow analysis use array privatization improving locality parallelism nested loops interprocedural array region analyses fusion loops parallelism locality experience efficient array data flow analysis array privatization implementation evaluation fusion contraction array languages scheduleindependent storage mapping loops new tiling techniques improve cache temporal locality optimized unrolling nested loops high performance compilers parallel computing structure computers computations optimization memory usage requirement class loops implementing multidimensional integrals estimating enhancing cache effectiveness collective loop fusion array contraction maximizing loop parallelism improving data locality via loop fusion distribution complexity loop fusion loop alignment memory accesses optimization improving effective bandwidth compiler enhancement global dynamic cache reuse ctr g chen kandemir j irwin g memik compilerdirected selective data protection soft errors proceedings 2005 conference asia south pacific design automation january 1821 2005 shanghai china yonghong song cheng wang zhiyuan li polynomialtime algorithm memory space reduction international journal parallel programming v33 n1 p133 february 2005 david wonnacott achieving scalable locality time skewing international journal parallel programming v30 n3 p181221 june 2002 g chen kandemir karakoy constraint network based approach memory layout optimization proceedings conference design automation test europe p11561161 march 0711 2005 apan qasem ken kennedy profitable loop fusion tiling using modeldriven empirical search proceedings 20th annual international conference supercomputing june 28july 01 2006 cairns queensland australia alain darte guillaume huard new complexity results array contraction related problems journal vlsi signal processing systems v40 n1 p3555 may 2005 benny thrnberg qubo hu martin palkovic mattias onils per gunnar kjeldsberg polyhedral space generation memory estimation interface memory models realtime video systems journal systems software v79 n2 p231245 february 2006 daniel cociorva gerald baumgartner chichung lam p sadayappan j ramanujam marcel nooijen david e bernholdt robert harrison spacetime tradeoff optimization class electronic structure calculations acm sigplan notices v37 n5 may 2002 geoff pike paul n hilfinger better tiling array contraction compiling scientific programs proceedings 2002 acmieee conference supercomputing p112 november 16 2002 baltimore maryland yonghong song rong xu cheng wang zhiyuan li improving data locality array contraction ieee transactions computers v53 n9 p10731084 september 2004 zhiyuan li yonghong song automatic tiling iterative stencil loops acm transactions programming languages systems toplas v26 n6 p9751028 november 2004 chen ding ken kennedy improving effective bandwidth compiler enhancement global cache reuse journal parallel distributed computing v64 n1 p108134 january 2004 mahmut taylan kandemir improving wholeprogram locality using intraprocedural interprocedural transformations journal parallel distributed computing v65 n5 p564582 may 2005