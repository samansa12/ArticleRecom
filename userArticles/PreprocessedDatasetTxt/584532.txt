interiorpoint algorithms semidefinite programming based nonlinear formulation recently burer et al mathematical programming submitted authors paper introduced nonlinear transformation convert positive definiteness constraint n n matrixvalued function certain form positivity constraint n scalar variables keeping number variables unchanged based transformation proposed firstorder interiorpoint algorithm solving special class linear semidefinite programs paper extend approach apply transformation general linear semidefinite programs producing nonlinear programs n positivity constraints also n additional nonlinear inequality constraints despite complication transformed problems still retain desirable properties propose firstorder secondorder interiorpoint algorithms type nonlinear program establish global convergence computational results demonstrating effectiveness firstorder method also presented b introduction semidefinite programming sdp generalization linear programming linear function matrix variable x maximized minimized affine subspace computational results reported paper obtained sgi origin2000 computer rice university acquired part support nsf grant dms9872009 school mathematics georgia tech atlanta georgia 30332 usa author supported part nsf grants int9910084 ccr9902010 email burermathgatechedu z school isye georgia tech atlanta georgia 30332 usa author supported part nsf grants int9600343 int9910084 ccr9902010 email monteiroisyegatechedu x department computational applied mathematics rice university houston texas 77005 usa author supported part doe grant defg0397er25331 doelanl contract 038919923 nsf grant dms9973339 email zhangcaamriceedu symmetric matrices subject constraint x positive semidefinite due nice theoretical properties numerous applications sdp received considerable attention recent years among theoretical properties semidefinite programs solved prescribed accuracy polynomial time fact polynomialtime interiorpoint algorithms sdp extensively studied algorithms especially primaldual pathfollowing algorithms proven efficient robust practice small mediumsized problems even though primaldual pathfollowing algorithms theory solve semidefinite programs efficiently unsuitable solving largescale problems practice high demand storage computation 1 benson et al proposed another type interiorpoint algorithm polynomialtime potentialreduction dualscaling method better take advantage special structure sdp relaxations certain combinatorial optimization problems moreover efficiency algorithm demonstrated several specific largescale applications see 2 6 drawback method however operation count per iteration still quite prohibitive due reliance newtons method computational engine drawback especially critical number constraints sdp significantly greater size matrix variable addition contrast benson et al algorithm several methods recently proposed solve specially structured largescale sdp problems common algorithms use gradientbased nonlinear programming techniques 8 helmberg rendl introduced firstorder bundle method solve special class sdp problems trace primal matrix x fixed subclass includes sdp relaxations many combinatorial optimization problems eg goemans williamsons maxcut sdp relaxation lovaszs theta number sdp primary tool spectral bundle method replacement positivesemidefiniteness constraint dual slack matrix equivalent requirement minimum eigenvalue nonnegative maxcut sdp relaxation received attention homer peinado 9 using original form goemanswilliamson relax ation ie change variables nthetan original variable show maxcut sdp reformulated unconstrained maximization problem standard steepest ascent method used burer monteiro 3 improved upon idea homer peinado simply noting without loss generality v required lower triangular recently vavasis 14 shown gradient classical logbarrier function dual maxcut sdp computed time space proportional time space needed computing cholesky factor dual slack matrix since sparse whenever underlying graph sparse vavasiss observation may potentially lead efficient gradientbased implementations classical logbarrier method exploit sparsity recent paper 5 showed class linear nonlinear sdps reformulated nonlinear optimization problems simple feasible sets size matrix variable x problemdependent nonnegative integer n positive orthant n reformulation based idea eliminating positive definiteness constraint x first applying substitution done 3 l lower triangular matrix using novel elimination scheme reduce number variables constraints also showed compute gradient resulting nonlinear objective function efficiently hence enabling application existing nonlinear programming techniques many sdp problems 5 also specialized approach subclass linear sdps diagonal primal matrix x fixed reformulating dual sdp working directly space transformed problem devised globally convergent gradientbased nonlinear interiorpoint algorithm simultaneously solve original primal dual sdps remark class fixeddiagonal sdps includes known sdp relaxations combinatorial optimization problems recently vanderbei benson 13 shown positive semidefinite constraint x replaced n nonlinear concave constraints dx ii 0 unique diagonal matrix appearing standard positive semidefinite matrix x moreover show concave constraints utilized solution linear sdp using interiorpoint algorithm general convex nonlinear programs since discussion vanderbei bensons method mainly theoretical nature 13 question whether method offers practical advantages largescale sdps yet determined paper extend ideas 5 solve general linear sdps specifically 5 showed diagonal primal matrix variable x constrained equal vector dual sdp could transformed nonlinear programming problem simple feasible set n size matrix variable number additional primal constraints general case described case diagonal x necessarily constrained based similar ideas requires feasible points new nonlinear problem satisfy nonlinear inequality constraints addition lying set n new inequality constraints however handled effectively algorithmic standpoint using ideas interiorpoint methods propose two interiorpoint algorithms solving general linear sdps based ideas first generalization firstorder gradientbased logbarrier algorithm presented 5 whereas second potential reduction algorithm employs use secondderivative information via newtons method believe first algorithm strong candidate solving large sparse sdps general form second algorithm also relevance solving small mediumsized sdps even though current perspective mainly theoretical paper organized follows section 2 introduce sdp problem studied paper along corresponding assumptions briefly summarize main results previous paper reformulate sdp nonlinear programming problem mentioned previous subsection introduce analyze certain lagrangian function play important role algorithms developed paper sections 3 4 respectively develop prove convergence two aforementioned algorithms one firstorder logbarrier algorithm another secondorder potential reduction algorithm solving sdp section 5 present computational results show performance firstorder logbarrier algorithm set sdps compute socalled lovasz theta numbers graphs liter ature also section 5 discuss advantages disadvantages two algorithms presented paper last section conclude paper final comments 11 notation terminology use nthetan denote space real numbers real ndimensional column vectors real n theta n matrices respectively n denote subsets n consisting entrywise nonnegative positive vectors respectively n denote space real n theta n symmetric matrices define n subsets consisting positive semidefinite positive definite matrices respectively indicate 2 n respectively also make use fact 2 n unique matrix square root 12 satisfies denotes space real n theta n lower triangular matrices l n l n subsets l n consisting matrices nonnegative positive diagonal entries respectively addition define l nae l n set n theta n strictly lower triangular matrices let tra denote trace matrix 2 nthetan namely sum diagonal elements moreover nthetan define addition denotes hadamard product u v ie entrywise multiplication u v define u gamma1 unique vector satisfying e e vector ones also define diag nthetan u diagonal matrix u diag defined adjoint diag ie use notation e denote ith coordinate vector 1 position zeros elsewhere use k delta k denote euclidean norm vectors induced operator norm unless otherwise specified frobenius norm matrix defined 2 sdp problem preliminary results section introduce generalform sdp problem state two standard assumptions problem discuss optimality conditions central path sdp describe transformation converts dual sdp constrained nonlinear optimization problem consideration optimality conditions new problem leads us introduce certain lagrangian function develop derivative formulas several important results end section detailed description properties certain primal estimate associated lagrangian function properties prove crucial development algorithms sections 3 4 21 sdp problem corresponding assumptions paper study following slight variation standard form sdp problem matrix variable data problem given matrix vectors linear function defined ax given set matrices fa k g ae n remark differs usual standard form sdp additional inequality diagx also note every standard form sdp written form p simply adding redundant constraint diagx nonpositive vector 2 n fact form p general usual standard form sdp need considering general form p rather usual standard form sdp arises requirement order apply transformation alluded introduction dual sdp must possess special structure dual sdp p st diagz z 0 0 matrix variable z vector variables adjoint operator defined term diagz found equality constraint precisely special structure transformation exploit describe transformation detail following subsection denote f 0 p f 0 sets strictly feasible solutions problems p respectively ie make following assumptions throughout presentation assumption 1 set f 0 p theta f 0 nonempty assumption 2 matrices fa k g linearly independent note standard form sdp converted form p addition redundant constraint diagx nonpositive 2 n strict inequality diagx definition f 0 p redundant ie feasible x 0 inequality diagx automatically satisfied hence f 0 p equals usual set strictly feasible solutions defined fx 2 0g particular assume usual set interior solutions nonempty f 0 p also nonempty addition difficult see set fy dual strictly feasible solutions usual standard form sdp nonempty set f 0 nonempty total conclude standard form sdp put form p assumption 1 equivalent usual assumption original primal dual sdps strictly feasible solutions assumption 1 wellknown problems p optimal solutions respectively c ffl x last condition called strong duality alternatively expressed requirement equivalently x addition assumptions 1 2 wellknown problems min loggammaz unique solutions x z respectively 2 nthetan identity matrix e 2 n vector ones set solutions known primaldual central path problems upcoming sections central path play important role development algorithms solving problems p 22 transformation subsection present primary result 5 allows us transform problem equivalent nonlinear program n nonlinear inequality constraints introduce certain lagrangian function associated new nonlinear program prove key results regarding function recall l nae l n denotes set n theta n strictly lower triangular matrices following result stated proved 5 see theorem 4 section 6 therein theorem 21 following statements hold w exists unique b functions lw zw defined according 2 infinitely differentiable analytic domain n c spaces n bijective correspondence according assignment w 7 z important note set 3 differs strictly feasible set f 0 inequality z 0 enforced immediate consequence theorem 21 dual sdp recast nonlinear program st vector variables remarks order concerning nld relationship firstly functions lw zw introduced theorem cannot uniquely extended boundary n necessary w strictly positive nld secondly vector constraint zw 0 arises directly corresponding constraint could equivalently replaced zw 0 chosen strict inequality zw 0 bijective correspondence f 0 feasible set nld finally elements w allowed take value zero nld general optimal solution fact nld optimal solution set even though feasible set nld include boundary points may still consider hypothetical situation inequalities w 0 zw relaxed w 0 zw 0 particular investigate firstorder optimality conditions resulting hypothetical constrained nonlinear program using lagrangian function defined w additional issues regularity ignored hypothetical discussion firstorder necessary conditions optimality could stated follows w local minimum function zw yb subject constraint zw 0 exists rw w r w one may suspect optimality conditions little use since based hypothetical assumption zw lw defined boundary n theta following sections however show precisely conditions guarantee optimality satisfied limit sequence points fw k 23 first second derivatives lagrangian subsection establish key derivative results lagrangian function introduced last subsection addition definition 4 lagrangian define functions l respectively mapping set n formulas note sw lw positive definite slack matrix cholesky factor respectively associated w via bijective correspondence theorem 21 4 evident derivatives lagrangian closely related derivatives function h defined w arbitrary fixed vector theorems 23 24 establish derivatives h v based auxiliary matrix x defined following proposition since proposition 22 immediate consequence lemma 3 5 omit proof see also proposition 7 5 proposition 22 let l 2 l n v 2 n given system linear equations unique solution x n remark proof following theorem basically identical one theorem 2 5 included sake completeness paving way towards derivation second derivatives h v theorem 24 theorem 23 let w denote unique solution rw h v w b r h v w proof prove suffices show h v w w differentiating 8 respect w obtain w w w last equality follows fact differentiating 2 respect w obtain diag w w w taking inner product sides equation x using fact x symmetric obtain w second equality follows fact strictly lower triangular xl upper triangular view 9 combining 10 12 conclude holds differentiating 8 respect k fixed k 2 w w second equality due fact differentiating 2 respect k obtain diag w w w taking inner product sides equation x using arguments similar ones conclude w w statement b follows 13 last identity definition theorem 24 let w denote unique solution 9 n l j lw every ng k l 2 k l w proof prove 15 since proofs equations 16 17 follow similar arguments note also proof 15 similar proofs theorems 23a 23b proof somewhat abridged indeed differentiating 8 respect w respect w j obtain w differentiating 11 respect w j obtain diag w w w immediately implies w combining 19 20 conclude 15 holds assume x 0 using 15 16 17 straightforward see f proves final statement theorem giving first second derivatives lagrangian corollaries theorems 23 24 establish another technical result used later section 4 lemma 25 let w denote unique solution 9 l j lw suppose also q q r 2 h v w diagonal matrix q vector last components q proof recall proof theorem 24 positive semidefiniteness x implies q 2 nm r given 21 using hypotheses lemma straightforward see 22 using definition r 11 14 18 defined diagzw w k defined similarly equation thus evident diagonal matrix remark final statement theorem 24 strengthened using lemma 25 linear independence matrices fe e k1 assumed particular shown r 2 h v w 0 whenever x 0 collection data matrices linearly independent assumption however stronger assumption 2 since intend results paper directly applicable sdps satisfy usual assumptions assume linear independence k1 theorems 23 24 lemma 25 immediate consequences derivatives lagrangian detailed following definition corollary note result define wy n theta n leading principal block hessian r 2 w lagrangian function definition 26 w denote unique solution 9 n v j l j lw refer xw primal estimate p associated w corollary 27 let w n given define l j lw x j rw w b r w c r w diagonal matrix q vector last components q mention assumed linear independence matrices fe e k1 would also able claim however weaker assumption 2 claim necessarily hold 24 properties primal estimate subsection establishes several important properties primal estimate xw given definition 26 following proposition analogue lemma 5 5 lemma 28 let w xl upper triangular equivalently l xl diagonal rw 0 addition x 0 rw 0 c w rw proof upper triangularity xl follows directly 9 since l xl upper triangular product l xl also symmetric hence l xl must diagonal hand l xl diagonal say equals upper triangular follows prove first part b note nonsingularity l implies x 0 l xl 0 since l xl diagonal l xl 0 diagl xl 0 given l xl upper triangular matrices easy see diagl xl hadamard product diagl diagxl since diagxl 0 first statement b follows sequence implications derived fact rw corollary 27a second part b proved argument similar one given previous paragraph need replace inequalities strict inequalities statement c follows 7 proposition 27a simple observation diagonal l xl hadamard product diagl since l xl upper triangular following proposition establishes matrix xw plays role possibly primal estimate w justification name definition 26 particular gives necessary sufficient conditions xw feasible strictly feasible solution p interesting note conditions based entirely gradient lagrangian function proposition 29 let w x feasible p rw 0 r b x strictly feasible p rw 0 r proof definition x x 2 n theorem easy consequence corollary 27b lemma 28b following proposition provides measure duality gap closeness optimal ity points w xw feasible nlp p respectively proposition 210 let w feasible proof feasibility x follows proposition 29 z definitions z assumption z 0 equality follows substitutions well lemma 28c 3 logbarrier algorithm well known homeomorphic transformation path domain mapped path range vice versa furthermore given continuous function f range extremers f range mapped corresponding extremers composite function fdelta domain particular f unique minimizer range minimizer mapped unique minimizer fdelta domain view observations easy see transformation introduced section 2 central path sdp problem becomes new central path transformed space furthermore since points original central path unique minimizers defining logbarrier function corresponding different parameter values points transformed central path therefore unique minimizers transformed logbarrier function corresponding different parameter values general however possible extraneous nonextreme stationary points could introduced transformed logbarrier function nonlinear transformations applied section show transformed logbarrier functions fact nonextreme stationary points use fact establish globally convergent logbarrier algorithm solving primal dual sdp first subsection describe central path transformed space technical results ensure convergence sequence primaldual points given second subsection finally precise statement logbarrier algorithm well convergence presented last subsection 31 central path transformed space given strict inequality constraints nld natural problem consider following logbarrier problem associated nld depends choice log loggammaz ith coordinate function zw reason factor 2 become apparent shortly sur prisingly nld nothing standard dual logbarrier problem introduced section 21 transformation given theorem 21 ie equivalent nonlinear program min loggammaz exactly nld simplification logdet log nexttolast equality follows fact determinant triangular matrix product diagonal entries recall discussion section 21 primal dual logbarrier problems p unique solutions z respectively 1 holds one ask whether nld also unique solution unique solution relates z following theorem establishes nld fact unique stationary point w simply inverse image point z bijective correspondence given theorem 21 theorem 31 0 problem nld unique minimum point also unique stationary point minimum w equal inverse image point z bijective correspondence theorem 21 particular proof let w stationary point nld define rw z j r z zw r z j r zw since w stationary point satisfies firstorder optimality conditions nld recall rw z n theta n matrix r z theta n matrix using definitions f easily see rz using relation definition easily see optimality conditions equivalent vector ones clear 24 proposition 29 x strictly feasible solution p corollary 27a implies first equation 24 equivalent diagxl equality turn implies diagl since xl upper triangular definition x since l xl diagonal follows l hence note also definitions xw satisfy conditions 1 clearly implies conclude nld unique stationary point satisfying conditions stated theorem stationary point also global minimum follows fact global minimum 32 sufficient conditions convergence accordance discussion last paragraph section 22 consider lagrangian function w open set given sequence points fw k following result gives sufficient conditions sequences fz bounded lemma 32 let fw aeomega sequence points ffl sequences fw k rw k g f k z k g bounded sequences fz bounded proof assumption 1 exists point definition f 0 p clearly n 0 bounded open set containing hence linearity 0 open set containing 27b assumption conclude lim k1 ax k hence ax k k sufficiently large say k hence exists k k 0 define k k 0 since moreover f bounded sequence let z point f 0 feasible solution k k 0 combine information previous paragraph fact diag adjoints diag respectively inequalities obtain following inequality z 0 using inequality fact k k 0 last inequality follows fact x k 0 follows proposition 29b assumption rw 0 proposition 28 assumption fw k rw k g bounded implies fx k ffl k g bounded together fact f k z k g f bounded implies lefthand side inequality bounded k k 0 thus follows positive definiteness 0 ji fx k g fs k g bounded boundedness fs k g clearly implies boundedness fl k g hence boundedness fw k g addition since boundedness fx k g implies f k g bounded together boundedness f k z k g implies fz k g bounded using boundedness fs k g fz k g along assumption 2 easily see fy k g bounded section 22 used hypothetical discussion optimality conditions nld motivate use lagrangian function following theorem see hypothetical optimality conditions 5 fact relevance solution nld particular theorem shows fw k sequence points satisfying 5a k 0 5b 5c 5d satisfied limit accumulation points corresponding sequences fx k g fz optimal solutions p respectively theorem 33 let fw aeomega sequence points z k 0 lim z k sequences fx k g fz bounded b accumulation points fx k g fz optimal solutions p respectively proof proof statement follows immediately lemma 32 prove b let accumulation points sequences fx k g fz f k g respectively boundedness f k g also follows lemma 32 assumptions proposition 29 imply lim clearly implies x 1 feasible solution p since z k k feasible solution follows z 1 feasible solution moreover proposition 28 w k 0 follows x 1 ffl also diagx k 0 follows thus shown x 1 z 1 optimal solutions p 33 globally convergent logbarrier algorithm short subsection introduce straightforward logbarrier algorithm solving nld convergence algorithm simple consequence theorem 33 let constants given 0 define set points w satisfying e ffl kr k gamma vector ones note unique minimizer w nld n see proof theorem 31 equation 24 particular propose following algorithm log barrier algorithm 1 use unconstrained minimization method solve nld k approximately obtaining point w 2 set increment k 1 return step 1 end stress since nld k unique stationary point k 0 also unique minimum step 1 algorithm succeed using reasonable unconstrained minimization method specifically convergent gradientbased method eventually produce point set n k define based definition n proposition 28b algorithm clearly produces sequence points fw k satisfies hypotheses theorem 33 hence logbarrier algorithm converges sense theorem 4 potential reduction algorithm section describe prove convergence potential reduction interiorpoint algorithm solving nld basic idea produce sequence points satisfying hypotheses theorem 33 via minimization special merit function defined minimization performed using armijo line search along newton direction related equality system throughout section assume point w 2omega given satisfies 41 definitions technical results algorithm define f phi w note w xi potential reduction algorithm state explicitly end subsection initialized point w subsequently produce sequence points fw k requirement rw w nonnegative k reasonable light goal producing sequence satisfying hypotheses theorem 33 third requirement fw k k less f k 0 technical used prove special properties sequence produced algorithm also define f xi omega f w r w gamma zw y7 5 j6 4 e gamma vector ones let f denote ith coordinate function f note ff n scalar functions corresponding elements w rw w hold ff r w well ff gamma zw addition define n ng fn mg definition f goal producing sequence points satisfying hypotheses theorem 33 stated simply goal producing sequence ae xi lim primary tool allows us accomplish 27 merit function log arbitrary constant satisfying n usefulness merit function comes fact 27 accomplished via iterative minimization taking step along newton direction system f w current point follows investigate minimization scheme since apply newtons method nonlinear system f w need study nonsingularity jacobian f 0 w simplify notation w 2omega define recall leading principal block r 2 defined 23 straightforward differentiation f w e gamma e gamma multiplying f 0 w diagonal matrix diagw gammat w newton equation f 0 w z deltaw deltay equivalent deltaw deltay delta e note p w 2nm theta 2nm matrix general asymmetric use matrix p w help establish nonsingularity f 0 w following lemma lemma 41 w xi matrix p w positive definite consequently jacobian f 0 w proof since f 0 w product p w positive diagonal matrix suffices prove first part lemma combining fact w lemma 28b corollary 27d see however necessarily positive definite even though x 0 see discussion corollary 27 moreover hence conclude 29 sum two positive semidefinite matrices one skewsymmetric matrix follows p positive semidefinite remains show p invertible equivalently deltaw deltay unique solution system deltaw deltay delta sizes zerovectors righthand side clear context delta solution 32 premultiplying sides 32 row vector using 29 obtain sum three terms corresponding three matrices 29 add zero skewsymmetry third matrix 29 corresponding term zero positive semidefiniteness first two matrices first two terms nonnegative thus must zero term corresponding first matrix 29 leads deltaw together 31 implies deltaw delta 0 0 rewriting 32 reflect information obtain equations deltay 0 deltay sizes zerovectors clear context since w see lemma 28b xw fact together first equation 33 implies hypotheses corollary 27e hold follows deltay diagonal matrix deltay let x 2 n denote unique solution respect v l j lw using second equation 33 fact rh v theorem 23b obtain deltay r fifth equality due identity sixth due fact diagx v hence conclude assumption 2 implies deltay 0 thus completing proof p positive definite remark assumed linear independence entire collection fe proof p w positive definite proof would trivial due fact xw see discussion corollary 27 case even though proof difficult weaker assumption 2 still suffices establish nonsingularity f 0 w direct consequence lemma 41 w xi newton direction deltaw deltay delta system f w exists w stated differently lemma 41 shows system unique solution w xi following lemmas show newton direction descent direction f deltaw deltay used direction also lemma 42 let w delta newton direction w given 34 deltaw deltay descent direction f w proof let p n theta n leading minor p w r r consists first components r note p positive definite since p w positive definite lemma 41 equation 34 implies 30 holds using 26 29 35 easy see 30 rewritten following two equations deltaw deltay deltaw deltay solving delta second equation substituting result first equation obtain deltaw deltay gammaz deltaw deltay multiplying equation left row vector deltaw deltay noting using positive definiteness p deltaw deltay deltaw deltay deltaw deltay 0 fact gammaz gamma1 0 clearly implies matrix rz diaggammaz gamma1 rz positive semidefinite combined inequality proves deltaw deltay descent direction f w lemma 43 let w delta newton direction w given 34 deltaw deltay delta descent direction w proof first state simple results combine prove lemma let equation 34 implies deltaw deltay delta deltaw deltay delta lemma 42 deltaw deltay 0 37 addition using 28 rf note r fw using 36 37 38 inequalities n fw deltaw deltay delta proves deltaw deltay delta descent direction w given w deltaw deltay delta newton direction given 34 important step potential reduction algorithm armijo line line search selects stepsize ff 0 wff yff ff 2 xi deltaw deltay delta due fact xi open set also due lemma 43 ff found finite number steps ready state potential reduction algorithm potential reduction pr algorithm 1 solve system 34 w obtain newton direction 2 let j k smallest nonnegative integer w 40 holds 3 set w increment k 1 return step 1 end remark due lemma 43 algorithm pr monotonically decreases merit function 42 convergence algorithm pr subsection prove convergence potential reduction algorithm given previous subsection key component analysis boundedness sequence produced algorithm established lemmas 45 47 k0 sequence produced potential reduction algorithm define f lemma 44 sequence ff k g bounded result sequences fx k ffl k g also bounded proof consider function p defined log log constant appearing 28 difficult verify see monteiro pang 12 example p coercive ie every sequence fr 1 property implies level set compact definition implies w assumption primal sdp p dual sdp feasible duality implies exists constant f gamma dual objective value implies w combining 42 fact potential reduction algorithm decreases merit function iteration see k 0 turn shows conclude ff k g contained compact set hence bounded boundedness fx k ffl k g fax k follows immediately 26 lemma 28c corollary 27b lemma 45 sequences fz bounded proof suffices show sequences fs k g fz k g bounded since proof lemma 32 boundedness fs k g fz k g immediately implies boundedness fy k g fl k g fw k g let k 0 follows inequalities bounded addition since 0 relation boundedness fz k g imply fs k g bounded lemma 46 sequence fc ffl x k g bounded proof lemma 44 exists implies following relation holds k 0 bounded since fax k fy k g bounded lemmas 44 45 conclude relation fc ffl x k g bounded lemma 47 sequences fx k g f k g bounded proof let z note consider following relation holds k 0 relation lemmas 44 46 fact conclude fx k g bounded addition since diagx k conclude f k g bounded following theorem proves convergence potential reduction algorithm remark key result convergence ff k g zero stated part theorem part b already implied lemmas 45 47 established part c follows immediately theorem 33 theorem 48 let fw sequence produced algorithm pr lim b sequences fx k g fz bounded c accumulation points fx k g fz optimal solutions p respectively proof prove assume contradiction hold lemma 44 implies exists convergent subsequence ff k g k2k f 1 j lim k2k f k 6 0 lemmas 45 47 may also assume sequence fw k point w bounded due weak duality p exist constants log log f k three inequalities together imply lim since otherwise mw towards infinity impossibility since algorithm produced sequence monotonically decreases merit function hence conclude f 1 2 omegagamma clearly w 1 follows newton direction deltaw exists w 1 addition sequence fdeltaw k deltay k delta k g k2k newton directions converges moreover inequality 39 found proof lemma 43 deltay 1 delta converges xi since continuous xi follows converges using relation deltaw k deltay k delta first second inequalities follow 40 39 spectively clearly see lim k2k ae since lefthand side tends zero k 2 k tends infinity implies lim k2k tends infinity k 2 k tends infinity conclude armijo line search requires trial stepsizes k 2 k increases recall line search two simultaneous objectives given w xi line search finds stepsize ff 0 wff yff ff 2 xi relation 40 satisfied converge w 1 respectively xi open set straightforward see exist j 0 k w j j k 2 k k k hence due fact lim k2k exists k k k implies 46 holds 40 satisfied stepsize ae deltaw k deltay k delta letting k 2 k tend infinity expression obtain deltay 1 delta deltay 1 delta contradicts 45 fact oe 2 0 1 hence conclude statement fact hold statements b c hold discussed prior statement theorem computational results discussion section discuss advantages disadvantages two algorithms presented sections 3 4 also present computational results firstorder logbarrier algorithm 51 firstorder versus secondorder wellknown phenomenon nonlinear programming firstorder fo methods ie methods use gradient information calculate search directions typically require large number iterations convergence high accuracy secondorder methods ie also use hessian information converge accuracy far fewer iterations benefit fo methods methods hand gradient information typically much less expensive obtain hessian information fo iterations typically much faster iterations many problems approaches favored fo approaches since small number expensive iterations produces overall solution time better fo methods large number inexpensive iterations problems reverse true clearly relative advantages disadvantages must decided casebycase analysis semidefinite programming current interiorpoint methods either primaldual dualscaling proven robust solving small mediumsized problems high accuracy performance largesized problems large n andor mostly discouraging cost per iteration increases dramatically problem size fact methods often inappropriate obtaining solutions even low accuracy void filled fo methods proven capable obtaining moderate accuracy reasonable amount time see discussion introduction useful consider two algorithms presented paper light comments feel fo logbarrier algorithm greatest use solution large sdps fact next section give computational results indicating case n moderate size large potential reduction method however likely immediate impact except possibly small mediumsized problems addition may advantages potentialreduction algorithm conventional interiorpoint methods example search direction computation may less expensive w yspace either one solves newton system directly approximates solution using conjugate gradient method current topic investigation overall value potential reduction method twofold demonstrates convexity lagrangian neighborhood xi allows one develop methods transformed problem ii methods may practical advantages solving small mediumsized sdps 52 logbarrier computational results given graph g vertex set ng edge set e lovasz theta number g see 11 computed optimal value following primaldual sdp min variables n theta n identity matrix e 2 n vector ones e k 2 n kth coordinate vector note primal dual problems strictly feasible solutions ran logbarrier algorithm nineteen graphs n small moderate size large particular size makes solution lovasz theta problems difficult secondorder interiorpoint methods first nine graphs randomly generated graphs 100 vertices varying edge density 10 90 last ten graphs complements test graphs used second dimacs challenge maximum clique problem 10 note graphs lovasz theta number gives upper bound size maximum clique initialized logbarrier algorithm specific w 0 corresponding z gammae w found direct cholesky factorization way able begin algorithm feasible point initial value set 1 logbarrier subproblem solved decreased factor 10 moreover criterion considering subproblem solved slightly altered theoretical condition described section 33 computational results found efficient consider subproblem solved norm gradient barrier function became less 10 gamma3 overall algorithm terminated reached value 10 gamma6 computer code programmed ansi c run sgi origin2000 gigabytes ram rice university although stress code parallel table 52 give results logbarrier algorithm nineteen test problems first four columns information regarding problems listed including problem name sizes n lower bound optimal value sdp remark lower bounds computed primal sdp code described 4 next four columns give objective value obtained code relative accuracy final solution respect given lower bound time seconds taken method number iterations performed method table see method obtain nearly optimal solution evidenced good relative accuracies small amount time even though quite large also see number iterations quite large surprising since method firstorder algorithm 6 concluding remarks conventional interiorpoint algorithms based newtons method generally costly solving largescale semidefinite programs search alternatives recent papers focused formulations facilitate one way another application gradientbased algorithms present paper one efforts direction paper apply nonlinear transformation derived 5 general linear sdp problem obtain nonlinear program positivity constraints variables additional inequality constraints well standard assumptions primaldual strict feasibility linear independence constraint matrices establish global convergence logbarrier algorithmic framework potentialreduction algorithm initial computational experiments indicate logbarrier approach based transformation promising solving least classes largescale sdp problems including particular problems number constraints far greater size matrix variables potential reduction algorithm also interesting theoretical standpoint advantages may provide solving small mediumscale problems believe methods worth investigation improvement table 1 performance logbarrier algorithm lovasz theta graphs low bd obj val acc time iter rand2 100 992 221225 221234 42egamma05 528 18877 rand3 100 1487 170210 170221 64egamma05 629 21264 rand4 100 1982 131337 131355 14egamma04 682 22560 rand5 100 2477 104669 104678 86egamma05 696 22537 rand6 100 2972 83801 83814 15egamma04 829 24539 rand7 100 3467 70000 70001 21egamma05 137 4106 rand8 100 3962 50000 50000 95egamma06 176 5218 rand9 100 4457 40000 40000 45egamma06 118 3612 brock2001co 200 5068 274540 274585 16egamma04 3605 16083 brock2004co 200 6813 212902 212946 21egamma04 4544 20092 cfat2001co 200 18368 120000 120029 25egamma04 2560 9337 johnson0844co 70 562 140000 140004 31egamma05 28 2519 san200071co 200 5972 300000 300002 55egamma06 273 973 r solving largescale sparse semidefinite programs combinatorial optimization approximating maximum stable set minimum graph coloring problems positive semidefinite relaxation projected gradient algorithm solving maxcut sdp relaxation nonlinear programming algorithm solving semidefinite programs via lowrank factorization solving class semidefinite programs via nonlinear programming application semidefinite programming circuit partitioning improved approximation algorithms maximum cut satisfiability problems using semidefinite programming spectral bundle method semidefinite programming design performance parallel distributed approximation algorithms maxcut shannon capacity graph potential reduction newton method constrained equations formulating semidefinite programming problems smooth convex nonlinear optimization problems note efficient computation gradient semidefinite program ming tr improved approximation algorithms maximum cut satisfiability problems using semidefinite programming design performance parallel distributed approximation algorithms maxcut cliques coloring satisfiability potential reduction newton method constrained equations solving largescale sparse semidefinite programs combinatorial optimization spectral bundle method semidefinite programming