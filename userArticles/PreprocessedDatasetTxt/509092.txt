making sparse gaussian elimination scalable static pivoting propose several techniques alternatives partial pivoting stabilize sparse gaussian elimination numerical experiments demonstrate wide range problems new method stable partial pivoting main advantage new method partial pivoting permits priori determination data structures communication pattern gaussian elimination makes scalable distributed memory machines based priori knowledge design highly parallel algorithms sparse gaussian elimination triangular solve show suitable largescale distributed memory machines b introduction earlier work 8 9 22 developed new algorithms solve unsymmetric sparse linear systems using gaussian elimination partial pivoting gepp new algorithms highly efficient workstations deep memory hierarchies shared memory parallel machines modest number processors portable implementations algorithms appear software packages superlu serial superlu mt multithreaded publically available netlib 10 among fastest available codes problem shared memory gepp algorithm relies finegrained memory access synchronization shared memory provides manage data structures needed fillin created dynamically discover columns depend columns symbolically use centralized task queue scheduling load balancing reason perform dynamically computational graph unfold runtime contrast cholesky pivot order numerically stable however techniques expensive research used resources national energy research scientific computing center supported office energy research us department energy contract deac0376sf00098 research supported part nsf grant asc9313958 doe grant defg0394er25219 ut subcontract ora4466 arpa contract daal0391c0047 doe grant defg0394er25206 nsf infrastructure grants cda8722788 cda9401156 doe grant defc0398er25351 1 rowcolumn equilibration row permutation p r delta r delta delta c r c diagonal matrices p r row permutation chosen make diagonal large compared offdiagonal 2 find column permutation p c preserve sparsity c control diagonal magnitude set ii p endif using l u factors following iterative refinement iterate multiply solve delta goto iterate endif figure 1 outline new gesp algorithm distributed memory machines instead distributed memory machines propose pivot dynamically enable static data structure optimization graph manipulation load balancing cholesky 20 25 yet remain numerically stable retain numerical stability variety techniques prepivoting large elements diagonal iterative refine ment using extra precision needed allowing low rank modifications corrections end section 2 show promise proposed method numeric experiments call algorithm gesp gaussian elimination static pivoting section 3 present mpi implementation distributed algorithms lu factorization triangular solve algorithms use elaborate 2d nonuniform blockcyclic data distribution initial results demonstrated good scalability factorization rate exceeding 8 gflops 512 node cray t3e stability traditionally partial pivoting used control element growth gaussian elimination making algorithm numerically stable practice 1 however partial pivoting way control element growth variety alternative techniques section present alternatives show experiments appropriate combinations effectively stabilize gaussian elimination furthermore techniques usually inexpensive compared overall solution cost especially large problems 21 gesp algorithm figure 1 sketch gesp algorithm incorporates techniques considered motivate step 1 recall diagonally dominant matrix one diagonal entry ii larger magnitude sum magnitudes offdiagonal entries row p exist even gepp unstable rare 7 19 column p j known choosing diagonal pivots ensures stability matrices 7 19 expect diagonal entry somehow made larger relative offdiagonals row column diagonal pivoting stable purpose step 1 choose diagonal matrices r c permutation p r make ii larger sense experimented number alternative heuristic algorithms step 1 13 depend following graph representation n theta n sparse matrix represented undirected weighted bipartite graph one vertex row one vertex column edge appropriate weight connecting row vertex column vertex j nonzero entry ij finding permutation p r puts large entries diagonal thus transformed weighted bipartite matching problem graph diagonal scale matrices r r chosen independently make row column r ad c largest entries equal 1 magnitude using algorithm lapack subroutine dgeequ 3 algorithms 13 choose p r maximize different properties diagonal p r r ad c smallest magnitude diagonal entry sum product magnitudes best algorithm practice seems one 13 picks p r r c simultaneously diagonal entry p r r ad c sigma1 offdiagonal entry bounded 1 magnitude product diagonal entries maximized report results algorithm worst case serial complexity algorithm delta nnza delta log n nnza number nonzeros practice much faster actual timings appear later step 2 new needed superlu superlu mt 10 column permutation p c obtained fillreducing heuristic use minimum degree ordering algorithm 23 structure future use approximate minimum degree column ordering algorithm davis et al 6 faster requires less memory since explicitly form also use nested dissection 17 note also apply p c rows ensure large diagonal entries obtained step 1 remain diagonal step 3 simply set tiny pivots encountered elimination p machine precision equivalent small half precision perturbation original problem trades numerical stability ability keep pivots getting small step 4 perform steps iterative refinement solution accurate enough also corrects perturbations step 3 termination criterion based componentwise backward error berr 7 condition berr means computed solution exact solution slightly different sparse linear system nonzero entry ij changed one unit last place zero entries left unchanged thus one say answer accurate data deserves terminate iteration backward error berr smaller machine epsilon decrease least factor two compared previous iteration second test avoid possible stagnation figure 5 shows berr always small 22 numerical results subsection illustrate numerical stability runtime gesp algorithm 53 unsymmetric matrices wide variety applications application domains matrices given table 1 except two ecl32 wu obtained harwellboeing collection 14 collection davis 5 matrix ecl32 provided jagesh sanghavi eecs department uc berkeley matrix wu provided yushu discipline matrices fluid flow cfd af23560 bbmat bramley1 bramley2 ex11 fidapm11 garon2 graham1 lnsp3937 lns 3937 raefsky3 rma10 venkat01 wu fluid mechanics goodwin rim circuit simulation add32 gre 1107 jpwh 991 memplus onetone1 onetone2 twotone device simulation wang3 wang4 ecl32 chemical engineering extr1 hydr1 lhr01 radfr1 rdist1 rdist2 rdist3a west2021 petroleum engineering orsirr 1 orsreg 1 sherman3 sherman4 sherman5 finite element pde av4408 av11924 stiff ode fs 541 2 olmstead flow model olm5000 aeroelasticity tols4000 reservoir modelling pores 2 crystal growth simulation cry10000 power flow modelling gemat11 dielectric waveguide dw8192 eigenproblem astrophysics mcfe plasma physics utm5940 economics mahindas orani678 table 1 test matrices disciplines wu earth sciences division lawrence berkeley national laboratory figure 2 plots dimension nnza nnzl ie number nonzeros l u factors fillin matrices sorted increasing order factorization time matrices interest parallelization ones take time ie ones right graph figure clear matrices large dimension number nonzeros also require time factorize timing results reported subsection obtained sgi onyx2 machine running irix 64 system 8 195 mhz mips r10000 processors 5120 mbytes main memory use single processor since mainly interested numerical accuracy parallel runtimes reported section 3 detailed performance results section tabular format available httpwwwnerscgovxiaoyesc98 among 53 matrices would get wrong answers fail completely via division zero pivot without pivoting precautions 22 matrices contain zeros diagonal begin remain zero elimination 5 create zeros diagonal elimination therefore pivoting would fail completely 27 matrices 26 matrices would get unacceptably large errors due pivot growth experiment righthand side vector generated true solution x true vector ones ieee double precision used working precision machine epsilon 10 gamma16 figure 3 shows number iterations taken iterative refinement step matrices terminate iteration 3 steps 5 matrices require 1 step 31 matrices require 2 steps 9 matrices require 3 steps 8 matrices require 3 steps matrix present two error metrics figure 4 figure 5 assess accuracy stability gesp figure 4 plots error gesp versus error gepp implemented superlu matrix red dot green diagonal means two errors red dot diagonal means lu factorization time seconds dimension nonzeros nonzeros lu figure 2 characteristics matrices condition number number iterative refinement steps gesp red geppblue figure 3 iterative refinement steps gesp gesp accurate red dot means gepp accurate figure 4 shows error gesp little larger smaller 21 53 error gepp figure 5 shows componentwise backward error 7 also small usually near machine epsilon never larger 10 gamma12 although combination techniques steps 1 3 figure 1 works well matrices found matrices combinations better example fidapm11 jpwh 991 orsirr 1 errors large unless omit p r step 1 ex11 radrf1 cannot replace tiny pivots p therefore software provide flexible interface user able turn options evaluate cost step gesp figure 1 done respect serial implementation since parallelized numerical phases algorithm steps 3 4 timeconsuming particular large enough matrices lu factorization step 3 dominates steps measure times step respect step 3 simple equilibration step 1 computing r c using algorithm dgeequ lapack usually negligible easy parallelize row column permutation algorithms steps 1 2 computing p r p c easy parallelize parallelization future work fortunately memory requirement onnza 6 13 whereas memory requirement l u factors grows superlinearly nnza meantime run single processor figure 6 shows fraction time spent finding p r step 1 using algorithm 13 fraction factorization time time significant small problems drops 1 10 large matrices requiring long time factor problems interest parallel machines time find sparsitypreserving ordering p c step 2 much matrix dependent usually cheaper factorization although exist matrices ordering expensive nevertheless applications repeatedly solve system equations nonzero pattern different values ordering algorithm needs run cost amortized factorizations plan replace part algorithm error partial pivoting refine gesp figure 4 error jjx true gammaxjj 1 condition number backward error gesp figure 5 backward error lu factorization genp time seconds fraction genp time permute large diagonal triangular solve figure times factorize solve permute large diagonal compute residual estimate error bound 195 mhz mips r10000 order bbmat 38744 1771722 0224 5398 491 43 table 2 characteristics test matrices numsym fraction nonzeros matched equal values symmetric locations strsym fraction nonzeros matched nonzeros symmetric locations something faster outlined section 21 seen figure 6 computing residual sparse matrixvector multiplication cheaper triangular solve take small fraction factorization time large matrices solve time often less 5 factorization time algorithms parallelized see section 3 parallel performance data finally code ability estimate forward error bound true error jjx true gammaxjj 1 far expensive step factorization small matrices expensive factorization since requires multiple triangular solves therefore user asks 3 implementation mpi section describe design implementation performance distributed algorithms two main steps gesp method sparse lu factorization step 3 sparse triangular solve used step 4 implementation uses mpi 26 communicate data highly portable tested code number platforms cray t3e ibm sp2 berkeley report results 512 node cray t3e900 nersc illustrate scalability algorithms restrict attention eight relatively large matrices selected testbed table 1 representative different application domains characteristics matrices given table 2 31 matrix distribution distributed data structure distribute matrix twodimensional blockcyclic fashion distribution p processes restricted power 2 arranged 2d process grid shape p r theta p c matrix decomposed blocks submatrices blocks cyclically mapped onto process grid row column dimensions although 1d decomposition natural sparse matrices much easier implement 2d layout strikes good balance among locality blocking load balance cyclic mapping lower communication volume 2d mapping 2d layouts used scalable implementations sparse cholesky factorization 20 25 describe partition global matrix blocks partitioning based notion unsymmetric supernode first introduced 8 let l lower triangular matrix lu factorization supernode range columns l triangular block diagonal full row structure block identical row structure supernode stored dense format memory supernode partition used block partition row column dimensions n supernodes nbyn matrix matrix partitioned n 2 blocks nonuniform size size block matrix dependent clear diagonal blocks square full store zeros u upper triangle diagonal block whereas offdiagonal blocks may rectangular may full matrix figure 7 illustrates partitioning blockcyclic mapping mean block mapped onto process coordinate mod p r j mod p c process grid using mapping block li j factorization needed row processes blocks row similarly block ui j needed column processes blocks column j 2d mapping block column l resides one process namely column processes example figure 7 kth block column l resides column processes f0 3g process 3 owns two nonzero blocks contiguous global matrix schema right figure 7 depicts data structure store nonzero blocks process besides numerical values stored fortranstyle array nzval column major order need information interpret location row subscript nonzero stored integer array index includes information whole block column individual block note many offdiagonal blocks zero hence stored neither store zeros nonzero block lower upper triangles diagonal block stored l data structure process owns dnp c e block columns l needs dnp c e pairs indexnzval arrays matrix u use row oriented storage block rows owned process although numerical values within block still use column major order similarly l also use pair indexnzval arrays store block row u due asymmetry nonzero block u skyline structure shown figure 7 see 8 details skyline structure therefore organization index array different l omit showing figure since dynamic pivoting nonzero patterns l u determined symbolic factorization numerical factorization begins therefore block partitioning setup data structure performed symbolic algorithm much cheaper execute opposed partial pivoting size data structure cannot forecast must determined fly factorization proceeds 32 sparse lu factorization figure 8 outlines parallel sparse lu factorization algorithm use matlab notation integer ranges submatrices three steps kth iteration loop step 1 column processes participate factoring block column lk n k step 2 row processes participate triangular solves obtain block row ukk rankb update lk step 3 represents work also exhibits parallelism two steps b block size kth block columnrow ease understanding algorithm presented simplified actual implementation uses pipelined organization processes procc k step 1 iteration k soon rankb update step 3 iteration k block column index storage block column l blocks nzval block row subscripts full rows lda nzval block row subscripts full rows0000000001111111110000000001111111110000000000000000111111111111111111110000000000000001111111111111111111100001111000000000000000000001111111111111111111100000000011111111111100000011111100011111100011111100000011111100110011100110000111111001101100111000111000111 global matrix process mesh u figure 7 2d blockcyclic layout data structure store local block column l let mycol myrow process column row number process grid let procc k procr k column row processes block column row k block n obtain block column factor processes row need else receive need endif perform parallel triangular solves processes column need else receive need endif 3 n n endif end figure 8 distributed sparse lu factorization algorithm completing update trailing matrix ak owned procc k 1 pipelining alleviates lack parallelism steps 1 2 64 processors cray t3e instance observed speedups 10 40 nonpipelined implementation iteration major communication steps sendreceive lk n k across process rows sendreceive ukk process columns data structure see figure 7 ensures blocks lk n k ukk process contiguous memory thereby eliminating need packing unpacking sendreceive operation sending many smaller messages sendreceive pair two messages exchanged one index another nzval reduce amount communication employ notion elimination dags edags 18 send kth column l rowwise process owning jth column l exists path supernodes k j elimination dags done similarly columnwise communication rows u therefore block l may sent fewer p c processes block u may sent fewer p r processes words communication takes account sparsity factors opposed sendtoall approach dense factorization example af23560 4 theta 8 processes total number messages reduced 351052 302570 16 fewer messages reduction even processes sparser problems 33 sparse triangular solve sparse lower upper triangular solves also designed around distributed data structure forward substitution proceeds bottom elimination tree root whereas back substitution proceeds root bottom figure 9 outlines algorithm sparse lower triangular solve algorithm based sequential variant called inner product formulation formulation kth subvector xk solved update inner product must accumulated subtracted bk diagonal process coordinate k mod p r k mod process grid responsible solving xk two counters frecv fmod used facilitate asynchronous execution different operations frecvk counts number process updates xk received diagonal process owning xk needed distributed among row processes procr k due sparsity processes procr k contribute update frecvk becomes zero necessary updates xk complete xk solved fmodk counts number block modifications summed local inner product update stored lsumk xk fmodk becomes zero partial sum lsumk sent diagonal process owns xk execution program messagedriven process may receive two types messages one partial sum lsumk another solution subvector xk appropriate action taken according message type asynchronous communication enables large overlapping communication computation important communication computation ratio much higher triangular solve factorization algorithm upper triangular solve similar illustrated figure 9 however row oriented storage scheme used matrix u slight complication actual implementation namely build two vertical linked lists enable rapid access matrix entries block column u let mycol myrow process column row number process grid let procc k column processes block column k block k send xk column processes procc k endif end work receive message message lsumk send xk column processes procc k endif else message xk k li k 6 0 send lsumi diagonal process owns li endif end endif figure 9 distributed lower triangular solve l symbolic numeric table 3 lu factorization time seconds megaflop rate 512 node t3e900 34 parallel performance recall partition blocks based supernodes largest block size equals number columns largest supernode large matrices thousand especially towards end matrix l large granularity would lead poor parallelism load balance therefore occurs break large supernode smaller chunks chunk exceed preset threshold maximum block size experimenting found maximum block size 20 30 good cray t3e used 24 performance results reported section table 3 shows performance factorization cray t3e900 symbolic analysis steps 1 2 figure 1 yet parallel start copy entire matrix processor run steps 1 2 independently processor thus time independent number processors first column table 3 reports time spent symbolic analysis memory requirement symbolic analysis small store manipulate supernodal graph l skeleton graph u much smaller graphs l u subsequent columns table show factorization time varying number processors four large matrices bbmat ecl32 fidapm11 wang4 factorization time continues decreasing 512 processors demonstrating good scalability last column reports numeric factorization rate mflops 8 gflops achieved matrix ecl32 fastest published result seen implementation parallel sparse gaussian elimination table 3 starts processors examples could run fewer processors reference compare distributed memory code shared memory superlu mt code using small numbers processors example using 4 processor dec alphaserver factorization times superlu mt matrices af23560 ex11 19 23 seconds respectively comparable 4 processor t3e timings indicates distributed data structure message passing algorithm incur much overhead table 4 shows performance lower upper triangular solves altogether number processors continues increasing beyond 64 solve time remains roughly although triangular solves achieve high megaflop rates time usually much less factorization efficiency parallel algorithm depends mainly workload distributed much time spent communication one way measure load balance follows let processor one t3e processor except 4 mb tertiary cache bbmat 369 342 227 223 183 56 ecl32 295 260 166 157 117 128 table 4 triangular solves time seconds megaflop rate t3e900 comm fact sol table 5 load balance communication 64 processors cray t3e f denote number floatingpoint operations performed process compute load balance words b average workload divided maximum workload clear better load balance parallel runtime least runtime slowest process whose workload highest table 5 present load balance factor b factorization solve phases seen table distribution workload good matrices except twotone table also show fraction runtime spent communication numbers collected performance analysis tool called apprentice t3e amount communication quite excessive even matrices scale well bbmat ecl32 fidapm11 wang4 50 factorization time spent communication solve much smaller amount computation communication takes 95 total time expect percentage communication even higher processors total amount computation less constant although twotone relatively large matrix factorization scale well large matrices one reason present submatrix process mapping results poor load distribution another reason due long time communication look communication time using apprentice found processes idle 60 time waiting receive column block l sent process column left step 1 figure 8 idle 23 time waiting receive row block u sent process row step 2 figure 8 clearly critical path algorithm step 1 must preserve certain precedence relation iterations pipelining method shortens critical path extent expect length critical path reduced sophisticated dag task graph scheduling solve found processes idle 73 time waiting message arrive line figure 9 process much work large amount communication communication bottlenecks also occur matrices problems pronounced twotone another problem twotone supernode size block size small 24 columns average results poor uniprocessor performance low megaflop rate concluding remarks future work propose number techniques place partial pivoting stabilize sparse gaussian elim ination effectiveness demonstrated numerical experiments techniques enable static analysis nonzero structure factors communication pattern result scalable implementation becomes feasible largescale distributed memory machines hundreds processors preliminary software used quantum chemistry application lawrence berkeley national laboratory complex unsymmetric system order 200000 solved within 2 minutes 41 techniques numerical stability although current gesp algorithm successful large number matrices fails solve one finite element matrix av41092 pivot growth still large combination current techniques plan investigate complementary techniques stabilize algorithm example use judicious amount extra precision store matrix entries accurately perform internal computations accurately facility available free intel architectures performs arithmetic efficiently 80bit registers modest cost machines extra precision used factorization residual computation also mix static partial pivoting pivoting within diagonal block owned single processor smp within cluster smps enhance stability use aggressive pivot size control strategy step 4 algorithm instead setting tiny pivots delta jjajj may set largest magnitude current column incurs nontrivial amount rank1 perturbation original matrix end use shermanmorrisonwoodbury formula 7 recover inverse original matrix cost steps inverse iteration remains seen circumstances ideas employed practice also theoretical questions answered 42 high performance issues order make solver entirely scalable need parallelize symbolic algorithm case start matrix initially distributed manner symbolic algorithm determines best layout numeric algorithms redistributes matrix necessary also requires us provide good interface user knows input matrix distributed manner lu factorization investigate general functions matrixtoprocess mapping scheduling computation communication exploiting knowledge edags expected relax much synchrony current factorization algorithm reduce communication also consider switching dense factorization one implemented scalapack 4 submatrix lower right corner becomes sufficiently dense uniprocessor performance also improved amalgamating small supernodes large ones speed sparse triangular solve may apply graph coloring heuristic reduce number parallel steps 21 also alternative algorithms substitutions based partitioned inversion 1 selective inversion 24 however algorithms usually require preprocessing different matrix distributions one used factorization unclear whether preprocessing redistribution offset benefit offered algorithms probably depend number righthand sides 5 related work duff koster 13 applied techniques permuting large entries diagonal direct iterative methods direct method using multifrontal approach numeric factorization first proceeds diagonal pivots previously chosen analysis structure aa diagonal entry numerically stable elimination delayed larger frontal matrix passed later stage showed using initial permuta tion number delayed pivots greatly reduced factorization experimented iterative methods gmres bicgstab qmr using ilu preconditioners convergence rate substantially improved many cases initial permutation employed amestoy duff lexcellent 2 implemented multifrontal approach distributed memory machines host performs fillreducing ordering estimates frontal matrix structure statically maps assembly tree based symmetric pattern sends information processors numerical factorization frontal matrix factorized master processor one slave processors due possible delayed pivots frontal matrix size may different predicted analysis phase master processor dynamically determines many slave processors actually used frontal matrix showed good performance processors ibm sp2 mcsparse 16 parallel unsymmetric linear system solver key component solver reordering step transforms matrix bordered block upper triangular form reordering first uses unsymmetric ordering put relatively large entries diagonal algorithm modified version duff 11 12 unsymmetric ordering use several symmetric permutations preserve diagonal order matrix desired form large diagonal entries better chance obtaining stable factorization pivoting within diagonal blocks number pivots border thus reduced large medium grain parallelism exploited factor diagonal blocks eliminate bordered blocks implemented parallel factorization algorithm cedar experimental shared memory machine fu jiao yang 15 designed parallel lu factorization algorithm based following static information sparsity pattern householder qr factorization contains union sparsity patterns lu factors possible pivot selections used memory allocation computation conservatively possibly zero entries arbitrarily conservative particularly matrices arising circuit device simulations several matrices incur much overestimation showed good factorization speed 128 processors cray t3e interesting compare performance different approaches 6 acknowledgement grateful iain duff giving us access early version harwell subroutine mc64 permutes large entries diagonal r highly parallel sparse triangular solution multifrontal parallel distributed symmetric unsymmetric solvers scalapack users guide university florida sparse matrix collection approximate minimum degree ordering unsymmetric matrices applied numerical linear algebra supernodal approach sparse partial pivoting asynchronous parallel supernodal algorithm sparse gaussian elimination algorithm 575 algorithms obtaining maximum transversal design use algorithms permuting large entries diagonal sparse matrices users guide harwellboeing sparse matrix collection release 1 efficient sparse lu factorization partial pivoting distributed memory architectures solving large nonsymmetric sparse linear systems using mcsparse nested dissection regular finite element mesh elimination structures unsymmetric sparse lu factors matrix computations optimally scalable parallel sparse cholesky factorization scalable iterative solution sparse linear systems sparse gaussian elimination high performance computers modification minimum degree algorithm multiple elimination efficient parallel sparse triangular solution selective inversion efficient blockoriented approach parallel sparse cholesky factorization tr elimination structures unsymmetric sparse italicluitalic factors efficient blockoriented approach parallel sparse cholesky factorization scalable iterative solution sparse linear systems modification minimumdegree algorithm multiple elimination solving large nonsymmetric sparse linear systems using mcsparse applied numerical linear algebra users guide efficient sparse lu factorization partial pivoting distributed memory architectures algorithms obtaining maximum transversal algorithm 575 permutations zerofree diagonal f1 sparse gaussian elimination high performance computers asynchronous parallel supernodal algorithm sparse gaussian supernodal approach sparse partial pivoting ctr bergen f hulsemann u rude 17 x 1010 unknowns largest finite element system solved today proceedings 2005 acmieee conference supercomputing p5 november 1218 2005 laura grigori xiaoye li new scheduling algorithm parallel sparse lu factorization static pivoting proceedings 2002 acmieee conference supercomputing p118 november 16 2002 baltimore maryland mark baertschy xiaoye li solution threebody problem quantum mechanics using sparse linear algebra parallel computers proceedings 2001 acmieee conference supercomputing cdrom p4747 november 1016 2001 denver colorado olaf schenk klaus grtner twolevel dynamic scheduling pardiso improved scalability shared memory multiprocessing systems parallel computing v28 n2 p187197 february 2002 patrick r amestoy iain duff jeanyves lexcellent xiaoye li analysis comparison two general sparse solvers distributed memory computers acm transactions mathematical software toms v27 n4 p388421 december 2001 xiaoye li overview superlu algorithms implementation user interface acm transactions mathematical software toms v31 n3 p302325 september 2005 xiaoye li james w demmel david h bailey greg henry yozo hida jimmy iskandar william kahan suh kang anil kapur michael c martin brandon j thompson teresa tung daniel j yoo design implementation testing extended mixed precision blas acm transactions mathematical software toms v28 n2 p152205 june 2002 anshul gupta recent advances direct methods solving unsymmetric sparse systems linear equations acm transactions mathematical software toms v28 n3 p301324 september 2002 xiaoye li james w demmel superludist scalable distributedmemory sparse direct solver unsymmetric linear systems acm transactions mathematical software toms v29 n2 p110140 june