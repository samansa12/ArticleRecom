markov processes curves study classification problem arises two variablesone continuous x one discrete sevolve jointly time suppose vector x traces smooth multidimensional curve point variable attaches discrete label trace thus partitions curve different segments whose boundaries occur changes value consider learn mapping trace x trace examples segmented curves approach model conditional random process generates segments constant along curve x suppose variable evolves stochastically function arc length traversed x since arc length depend rate curve traversed gives rise family markov processes whose predictions invariant nonlinear warpings reparameterizations time show estimate parameters modelsknown markov processes curves mpcsfrom labeled unlabeled data apply models two problems automatic speech recognition x acoustic feature trajectories phonetic alignments b introduction automatic segmentation continuous trajectories poses challenging problem machine learning problem arises whenever multidimensional trajectory fxtjt 2 0 g must mapped sequence discrete labels segmentation performs mapping specifying consecutive time intervals attaching labels sk contiguous arcs along trajectory learning problem discover mapping labeled unlabeled examples paper study problem paying special attention fact curves intrinsic geometric properties depend rate traversed carmo 1976 properties include example total arc length maximum distance two points curve given multidimensional trajectory properties invariant reparameterizations monotonic function maps interval 0 put another way intrinsic geometric properties curve invariant nonlinear warpings time study curves requires simple notions differential geometry matter terminology refer particular parameterizations curves trajectories regard two trajectories x1t x2 equivalent curve exists monotonically increasing function f x1 precise mean oriented curve direction traversal matters follows adopt convention using xt denote entire trajectory opposed constantly writing fxtjt 2 0 g necessary refer value xt particular moment time use different index xt1 let us return problem automatic segmentation consider two variablesone continuous x one discrete sthat evolve jointly time assume vector x traces smooth multidimensional curve point variable attaches discrete label note trace yields partition curve different components particular boundaries components occur points changes value refer partitions segmentations regions constant segments see figure 1 goal paper learn probabilistic mapping trajectories xt segmentations st labeled unlabeled examples consider random process generates segments constant along curve traced x given trajectory xt let prst j xt denote conditional probability distribution possible segmentations suppose two equivalent trajectories xt xft identity eq 1 captures fundamental invariancenamely probability curve segmented particular way independent rate traversed paper study markov processes property call markov processes curves mpcs processes unambiguous write prs j x without providing explicit parameterizations trajectories xt st distinguishing feature mpcs variable evolves function arc length traversed along x quantity manifestly invariant nonlinear warpings time invariances symmetries play important role statistical pattern recognition encode prior knowledge problem domain duda hart 1973 work invariances focused spatial symmetries vision optical character recognition example researchers improved accuracy automatically trained classifiers incorporating invariances translations rotations changes scale simard lecun denker 1993 paper focuses invariance associated pattern recognition dynamical systems invariance nonlinear warpings time arises naturally problems involving segmentation continuous trajectories example penbased handwriting recognition invariance captures notion shape start tt figure 1 two variablesone continuous x one discrete sevolve jointly time trace partitions curve x different segments whose boundaries occur changes value markov processes curves model conditional distribution prsjx letter depend rate penned likewise automatic speech recognition rabiner juang 1993 invariance used model effects speaking rate thus addition mathematically interesting right principled handling invariance important consequences realworld applications machine learning main contributions paper postulate eq 1 fundamental invariance random processes ii introduce mpcs family probabilistic models capture invariance iii derive learning algorithms mpcs based principle maximum likelihood estimation iv compare performance mpcs automatic speech recognition versus hidden markov models ra biner juang 1993 terms previous work motivation closely resembles tishby 1990 several years ago proposed dynamical system approach speech processing organization paper follows section 2 begin reviewing basic concepts differential geometry introduce mpcs family continuoustime markov processes parameterize conditional probability distribution prs j x processes derived set differential equations describe pointwise evolution along curve traced x section 3 consider learn parameters mpcs supervised unsupervised settings settings correspond whether learner access labeled unlabeled examples labeled examples consist trajectories xt along corresponding segmentations ordered pairs eq 2 indicate st takes value sk times start end states used mark endpoints unlabeled examples consist trajectories xt boundary values fstart eq 3 specifies markov process starts time terminates later time case learner must infer target values st order update parameter estimates view types learning instances maximum likelihood estimation describe em algorithm general case unlabeled examples section 4 describe simple extensions mpcs significantly increase modeling power also compare mpcs probabilistic models trajectory segmentation hidden markov models argue mpcs distinguished two special proper ties natural handling invariance nonlinear warpings time emphasis learning segmentation model prsjx opposed synthesis model prxjs finally section 5 apply mpcs problem automatic speech recognition setting identify curves x acoustic feature trajectories segmentations phonetic alignments present experimental results two tasksrecognizing new jersey town names connected alphadigits tasks find mpcs generally match exceed performance comparably trained hidden markov models conclude section 6 posing several open questions future research markov processes curves markov processes curves based fundamentally notion arc length reviewing compute arc lengths along curves show used define random processes capture invariance eq 1 21 arc length let gx define theta matrix point x 2 r words point x associate particular theta matrix gx gx nonnegative definite x use metric compute distances along curves particular consider two nearby points separated infinitesimal vector dx define squared distance two points arc length along curve nondecreasing function computed integrating local distances thus trajectory xt arc length points xt1 xt2 given dt theta dt xt denotes time derivative x note arc length two points invariant reparameterizations smooth monotonic function time maps interval special case gx identity matrix x eq 5 reduces standard definition arc length euclidean space generally however eq 4 defines noneuclidean metric computing arc lengths thus example metric gx varies function x eq 5 assign different arc lengths trajectories xt constant displacement 22 states lifelengths problem segmentation map trajectory xt sequence discrete labels s1 labels attached contiguous arcs along curve x describe sequence piecewise constant function time st figure 1 refer possible values states follows introduce family random processes evolve function arc length traversed along curve traced x random processes based simple premisenamely probability remaining particular state decays exponentially cumulative arc length traversed state signature state particular way computes arc length formalize idea associate state following quantities metric g x used compute arc lengths eq 5 ii decay parameter measures probability per unit arc length makes transition state state iii set transition probabilities ij ij represents probability thathaving decayed state ithe variable makes transition state j thus ij defines stochastic transition matrix zero elements along diagonal rows sum one 1 note quantitiesthe metric g x decay parameter transition probabilities ij depend explicitly state associated together quantities used define markov process along curve traced x particular let p denote probability state time based history point time markov process defined set differential equations dt theta theta right hand side eq 6 consists two competing terms first term computes probability decays state second computes probability decays state probabilities proportional measures arc length combining gives overall change probability occurs time interval process markovian evolution p depends quantities available time thus future independent past given present eq 6 certain properties interest first note summing sides gives identity shows p remains normalized probability distribution ie times second suppose start state allow return visits ie j case second term eq 6 van ishes obtain simple onedimensional linear differential equation follows probability remaining state decays exponentially amount arc length traversed x arc length computed using matrix g x decay parameter controls typical amount arc length traversed state may viewed inverse lifetime orto precisean inverse lifelength fi nally noting arc length reparameterizationinvariant quantity therefore observe dynamics capture fundamental invariance eq 1 23 inference let a0i denote probability variable makes immediate transition start statedenoted zero indexto state put another way probability first segment belongs state given trajectory xt markov process eq 6 gives rise conditional probability distribution possible segmentations st consider segmentation st takes value sk times tk let dt theta denote arc length traversed state sk eq 6 know probability remaining particular state decays exponentially arc length thus conditional probability segmentation given k k1 used s0 sn1 denote start end states markov process first product eq 8 multiplies probabilities segment traverses exactly observed arc length second product multiplies probabilities transitions states sk sk1 leading factors k included normalize states duration model many important quantities computed distribution prsjx particular interest probable segmentation given particular trajectory xt eq 9 calls maximization piecewise constant functions time st practice maximization performed discretizing time axis applying dynamic programming forwardbackward procedure resulting segmentations optimal finite temporal resolution deltat example let ff denote loglikelihood probable segmentation ending state subtrajectory time starting initial condition theta discrete delta function also time step let psi j deltat record value maximizes right hand side eq 10 suppose markov process terminates time enforcing endpoint condition find likely segmentation backtracking recursions yield segmentation optimal finite temporal resolution deltat generally speaking choosing deltat sufficiently small one minimize errors introduced discretization prac tice one would choose deltat reflect time scale beyond necessary consider changes state example penbased handwriting recognition deltat might determined maximum pen velocity automatic speech recognition sampling rate frame rate types inferences made distribution eq 8 example one compute marginal probability pr markov process terminates precisely observed time simi larly one compute posterior probability end earlier moment time t1 variable state inferences made summing probabilities eq 8 segmentations terminate precisely time sum performed discretizing time axis applying forwardbackward procedure similar eqs 1011 algorithms essentially form counterparts hidden markov models rabiner juang 1993 3 learning examples learning problem mpcs estimate parameters f eq 6 examples segmented nonsegmented curves first step assume convenient parameterization metrics g x compute arc lengths show fit metrics along parameters ij maximum likelihood estimation 31 parameterizing metric variety parameterizations considered metrics g x simplest possible form euclidean metric g x dependence point x metric virtue simplicity powerful terms model paper consider general form phi x positive scalarvalued function x oe positivedefinite matrix joe 1 eq 12 conformal transformation wald 1984 euclidean metricthat noneuclidean metric dependence x captured scalar prefactor conformal transformation one locally preserves angles distances eq 12 strikes one possible balance confines euclidean geometry full generality riemannian manifolds determinant constraint joe imposed avoid degenerate solution 0 every trajectory assigned zero arc length note defined metric g x terms inverse oe turns simplify parameter reestimation formula oe given later section form metric determines nature learning problem mpcs choice eq 12 one must estimate functions matrices oe decay parameters transition probabilities ij section consider functions phi x fixed predetermined leaving parameters oe ij estimated training data later section 42 suggest particular choice functions phi x based relationship mpcs hidden markov models 32 labeled examples suppose given examples segmented trajectories fx ff ff tg index ff runs examples training set short hand let iff denote indicator function selects segments associated state iff also let iff denote total arc length traversed state ffth example z dt iff theta paper view learning problem maximum likelihood esti mation thus seek parameters maximize loglikelihood ff iff overall number observed transitions state state j eq 15 follows directly distribution segmentations eq 8 note first two terms measure loglikelihood observed segments isolation last term measures loglikelihood observed transitions eq 15 convenient form maximum likelihood estimation particular fixed oe closedform solutions optimal values ij given ff formulae easy interpret transition probabilities ij determined observed counts transitions decay parameters determined mean arc lengths traversed state general cannot find closedform solutions maximum likelihood estimates oe however update matrices iterative fashion guaranteed increase loglikelihood step denoting updated matrices oe consider iterative scheme derived appendix ff z dt iff x ff ff theta x ff2 constant c determined determinant constraint joe 1 reestimation formula oe involves sum integral segments assigned ith state mpc practice integral evaluated numerically discretizing time axis taking gradients eq 15 one show fixed points iterative procedure correspond stationary points loglikelihood proof monotonic convergence given appendix case labeled examples procedures maximum likelihood estimation invoked independently state one first iterates eq 18 estimate matrix elements oe parameters used compute arc lengths iff appear eq 14 given arc lengths decay parameters transition probabilities follow directly eqs 1617 thus problem learning given labeled examples relatively straightforward 33 unlabeled examples unsupervised setting learner access labeled ex amples available information consists trajectories x ff well fact process terminates time ff goal unsupervised learning maximize loglikelihood trajectory x ff probable segmentation found terminates precisely observed time appropriate marginal probability computed summing prstjxt allowed segmentations described end section 23 maximization loglikelihood defines problem hidden variable density estimation hidden variables states markov process variables known problem would reduce one considered previous section fill missing val ues one use expectationmaximization em algorithm baum 1972 dempster laird rubin 1976 roughly speaking em algorithm works converting maximization hidden variable problem weighted version problem segmentations ff known weights determined posterior probabil ities prs ff tjx ff ff ff derived current parameter estimates note eqs 1011 suffice implement extremely useful approximation em algorithm mpcs approximation compute based current parameter estimates optimal segmen tation ff trajectory training set one reestimates parameters markov process treating inferred segmen tations ff targets approximation reduces problem parameter estimation one considered previous section viewed winnertakeall approximation full em algo rithm analogous viterbi approximation hidden markov models rabiner juang 1993 essentially algorithm also applied intermediate case partially labeled examples setting state sequences specified segment boundaries words examples provided form ability handle examples important two reasons first provide information unlabeled examples sec ond complete segmentations form eq 2 may available example problem automatic speech recognition phonetic transcriptions much easier obtain phonetic align ments view learning problem examples one hidden variable density estimation knowledge state sequence incorporated em algorithm restricting forwardbackward procedures consider paths pass desired sequence 4 observations section present extensions mpcs discuss relate probabilistic models trajectory segmentation 41 extensions mpcs mpcs accomodate general measures distance one presented eq 4 example let xj denote unit tangent vector along curve x j simple extension eq 4 consider x gx u depends point x also tangent vector u extension enables one assign different distances timereversed trajectories opposed measure eq 5 depend whether curve traversed forwards backwards generally one may incorporate vector invariants dt distance measure vectors characterize local geometry point along curve particular eq 20 gives point x unit tangent vector u local curvature etc incorporating higherorder derivatives way enables one use fairly general distance measures mpcs invariance nonlinear warpings time also relaxed mpcs done including time coordinate right ie operating spacetime trajectories computing generalized arc lengths z gx z gx spacetime metrica d1dimensional square matrix point x effect replacing x z allow stationary portions trajectory contribute integral r admixture space time coordinates way old idea physics originating theory relativity though context metric negativedefinite wald 1984 note extension mpcs also combined previous onefor instance incorporating tangent vectors timing information distance measure 42 relation hidden markov models previous work hidden markov models hmms currently popular approach trajectory segmentation also based probabilistic methods models parameterize joint distributions form several important differences hmms mpcs sides trivial one hmms formulated discretetime pro cesses first predictions hmms invariant nonlinear warpings time example consider pair trajectories x created doubling operation ae trajectories trace curve half rate x general hmms assign trajectories like lihood guaranteed infer equivalent segmentations true even hmms sophisticated durational models ra biner juang 1993 contrast trajectories processed identically mpcs based eqs 56 states hmms mpcs also weighted differently inference procedures one hand hmms contribution state loglikelihood grows proportion duration time ie number observations attributed state hand mpcs contribution state grows proportion arc length naturally weighting arc length attaches important role shortlived states nonstationary trajectories consequences automatic speech recognition discussed section 5 hmms mpcs also differ try model hmms parameterize joint distributions form given eq 21 thus hmms parameter estimation directed learning synthesis model prxjs mpcs directed learning segmentation model prsjx direction conditioning x crucial difference hmms one generate artificial trajectories sampling joint distribution prs x mpcs hand provide generative model trajectories markov assumption also slightly different hmms mpcs hmms observe conditional independence state t1 independent observation x given previous state contrast mpcs evolution p given eq 6 depends explicitly trajectory time tnamely arc length x 12 mpcs provide generative model trajectories emphasize provide generative model segmentations particular one generate state sequence s0s1s2 start state sn1 end state sampling transition probabilities ij sequence length n fixed advance determined sampling procedure moreover state sk one generate arc length k sampling exponential distribution together sampled values sk k define segmentation grafted onto sufficiently long trajectory xt importantly interpretation mpcs allows combined hierarchically generative models language models automatic speech recognition finally note one essentially realize hmms special case mpcs done computing arc lengths along spacetime trajectories described section 41 setting one mimic predictions hmms setting oe matrices one nonzero element namely diagonal element deltatime contributions arc length defining functions phi x terms hmm emission probabilities prxji equation sets correspondence emission logprobabilities hmms arc lengths mpcs ignoring effects transition probabilities often negligible mpc initialized eq 23 singular choice oe reproduce segmentations parent hmm correspondence important allows one bootstrap mpc previously trained hmm also despite many efforts found effective way estimate functions phi x terms previous work motivation mpcs resembles tishby 1990 several years ago proposed dynamical systems approach speech processing mpcs exploit notion trajectories continuous also bear resemblance socalled segmental hmms ostendorf digalakis kimball 1996 mpcs nevertheless differ segmental hmms two important respects treatment arc lengthparticularly estimation metric g x hidden state markov process ii emphasis learning segmentation model prsjx opposed synthesis model prxjs even complicated one ordinary hmms 5 automatic speech recognition markov processes paper conceived models automatic speech recognition rabiner juang 1993 speech recognizers take input sequence feature vectors summarizes acoustic properties short window speech acoustic feature vectors typically ten components particular sequence feature vectors viewed tracing multidimensional curve goal speech recognizer translate curve sequence words generally sequence subsyllabic units known phonemes denoting feature vectors x phonemes view problem discretetime equivalent segmentation problem mpcs 51 invariances speech though hmms led significant advances automatic speech recog nition handicapped certain weaknesses one poor manner model variations speaking rate siegler stern 1995 typically hmms make errors fast speech slow speech related effect occurring phoneme level consonants confused often vowels generally speaking consonants shortlived nonstationary acoustic signatures vowels opposite thus phoneme level view consonantal confusions consequence locally fast speech tempting imagine hmms make mistakes incorporate invariance nonlinear warpings time oversimplifies problem clear hmms systemic biases hmms contribution state loglikelihood grows proportion duration time thus decoding procedures hmms inherently biased pay attention longlived states shortlived ones view suggests one plausible explanation tendency hmms confuse consonants often vowels mpcs quite different hmms weight speech signal mpcs contribution state determined arc length weighting arc length attaches important role shortlived nonstationary phonemes consonants course one imagine heuristics hmms achieve effect dividing states contribution loglikelihood observed inferred duration unlike heuristics however metrics g x mpcs estimated states training data words designed reweight speech signal way reflects statistics acoustic trajectories admittedly oversimplistic model effects speaking rate invariance nonlinear warpings time acoustic realization ie spectral profile phoneme depend extent speaking rate certain phonemes likely stretched shortened others invariance nonlinear warpings time also presupposes certain separation time scales one hand time scale acoustic features spectral energies formant bandwidths pitch extracted speech signal time scale features tend vary time scales need well separated mpcs meaningful interpretation whether true obviously depends choice acoustic features despite caveats feel mpcs provide compelling alternative traditional methods motivated mpcs appealing intrinsic geometric properties curves emphasize automatic speech recognition critically important relax invariance nonlinear warpings time done computing arc lengths along spacetime trajectories described section 41 extension allows mpcs incorporate movement acoustic feature space duration time measures phonemic evolution measures important speech recognition 52 experiments hmms mpcs used build connected speech recogniz ers training test data came speakerindependent databases telephone speech data digitized callers local switch transmitted form receiver feature extraction input telephone signals sampled 8 khz bandlimited 1003800 preemphasized blocked 30ms frames frame shift 10ms frame hamming windowed autocorrelated processed linear predictive coding lpc cepstral analysis produce vector 12 liftered cepstral coefficients rabiner juang 1993 feature vector augmented normalized log energy value well temporal derivatives first second order overall frame speech described 39 features features used differently hmms mpcs described recognizers evaluated two tasks first task recognizing new jersey town names eg hoboken training data task sachs et al 1994 consisted 12100 short phrases spoken seven major dialects american english phrases ranging two four words length selected provide maximum phonetic coverage test data consisted 2426 isolated utterances 1219 new jersey town names collected nearly 100 speakers note training test data task nonoverlapping vocabularies baseline recognizers built using 43 lefttoright continuousdensity hmms corresponding contextindependent english phone phones modeled threestate hmms exception background mixture components hmm error rate mpc error rate table 1 error rates task recognizing new jersey town names versus number mixture components per hidden state noise modeled single state state emission probabilities computed gaussian mixture models diagonal covariance trices different sized models trained using mixture components per hidden state particular model number mixture components across states mixture model parameters estimated viterbi implementation baumwelch algorithm transition probabilities assigned default values particular transitions allowed task grammar assumed equally probable assumption simplifies forwardbackward procedure large state spaces mpc recognizers built using overall grammar hidden state mpcs assigned metric g x functions phi x initialized fixed state emission probabilities hmms given eq 23 matrices oe estimated iterating eq 18 computed arc lengths along 14 dimensional spacetime trajectories cepstra logenergy time thus oe 14 theta 14 symmetric matrix applied tangent vectors consisting deltacepstra deltalogenergy deltatime note mpcs made use extensions discussed section 41 curiously best results mpcs obtained setting opposed estimating values decay parameters training data suspect due highly irregular ie nonexponential distribution arc lengths state representing silence background noise hmms transition probabilities assigned default values table 1 shows results experiments comparing mpcs hmms error rates experiments measure percentage town names incorrectly recognized various model sizes measured number mixture components found mpcs yield consistently lower error rates hmms graph figure 2 plots error rates versus number modeling parameters per hidden state graph shows mpcs outperforming hmms merely extra modeling parameters ie oe matrices beam widths decoding procedures experiments chosen corresponding recognizers activated roughly equal numbers arcs second task experiments involved recognition connected alphadigits eg n z 3 v j 2 training test data consisted 14622 7255 utterances respectively recognizers parameters per state error rate nj town names figure 2 error rates hmms dashed mpcs solid new jersey town names versus number parameters per hidden state mixture components hmm error rate mpc error rate table 2 word error rates task recognizing connected alphadigits versus number mixture components per hidden state built 285 subword hmmsmpcs corresponding contextdependent english phone recognizers trained evaluated way previous task except measured word error rates instead phrase error rates results shown table 2 figure 3 follow similar pattern mpcs outperforming hmms 6 discussion experimental results previous section demonstrate viability mpcs automatic speech recognition nevertheless several issues require attention one important issue problem feature selectionnamely extract meaningful trajectories speech signal work used cepstral features mpcs hmms done facilitate sidebyside comparison doubtful however cepstral trajectories particularly smooth provide meaningful type input mpcs intuitively one suspects pitch contours formant trajectories would provide smoother informative trajectories cepstra unfortunately types features difficult track noisy unvoiced speech work area needed another important issue mpcs learningnamely param parameters per state error rate alphadigits figure 3 word error rates hmms dashed mpcs solid connected alphadigits versus number parameters per hidden state eterize estimate metrics g x trajectories xt stress learning problem mpcs many degrees freedom corresponding one hmms particular whereas hmms one must learn distribution prxji hidden state mpcs one must learn metric g x former scalarvalued function acoustic feature space latter matrixvalued function fair say understand parameterize metrics nearly well probability distributions certainly mpcs potential exploit sophisticated metrics one studied paper moreover somewhat unsatisfactory metric eq 12 relies trained hmm initialization final note emphasize issues feature selection parameter estimation mpcs independent cepstral front end todays speech recognizers extremely well matched hmm back end indeed one might argue last decade research systematically honed compensate others failings seems likely future progress automatic speech recognition require concerted efforts ends thus hope besides providing alternative hmms mpcs also encourage fresh look signal processing performed front end reestimation formula appendix derive reestimation formula eq 18 show leads monotonic increases loglikelihood eq 15 recall mpcs probability remaining state decays exponentially function arc length follows maximizing loglikelihood state equivalent minimizing arc length choice metric eqs 12 23 learning problem reduces optimizing matrices oe simplicity consider arc length z figure 4 square root function concave upper bounded p bounding tangents shown single trajectory metric z dt theta written arc length oe explicitly function matrix oe suppressed state index notational convenience goal minimize oe subject determinant constraint 1 note matrix elements oe gamma1 appear nonlinearly right hand side eq 24 thus possible compute optimal values closed form alternative consider auxiliary z dt ae x theta oe ae theta positivedefinite matrix like oe follows directly definition eq 25 trivially observe qae ae qae oe positive definite matrices ae oe inequality follows concavity square root function illustrated figure 4 consider value ae minimizes qae oe subject determinant constraint denote value matrix elements ae gamma1 appear linearly qae oe minimization reduces computing covariance matrix tangent vector x distributed along trajectory xt particular oe z dt x constant proportionality determined constraint minimize oe respect oe consider iterative procedure step replace oe oe observe qoe oe due concavity qoe oe since equality generally holding words iterative procedure converges monotonically local minimum arc length oe extending procedure combined arc lengths multiple trajectories obtain eq 18 acknowledgements authors thank f pereira many helpful comments presentation ideas r inequality associated maximization technique statistical estimation probabilistic functions markov process maximum likelihood incomplete data via em algorithm differential geometry curves sur faces pattern classification scene analysis hmms segment models unified view stochastic modeling speech recognition fundamentals speech recognition united states english subword speech data effects speech rate large vocabulary speech recognition systems efficient pattern recognition using new transformation distance dynamical system approach speech process ing general relativity tr fundamentals speech recognition efficient pattern recognition using new transformation distance ctr yon visell spontaneous organisation pattern models music organised sound v9 n2 p151165 august 2004