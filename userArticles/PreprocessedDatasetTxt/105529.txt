multiple resolution segmentation textured images multiple resolution algorithm presented segmenting images regions differing statistical behavior addition algorithm developed determining number statistically distinct regions image estimating parameters regions algorithms use causal gaussian autoregressive model describe mean variance spatial correlation image textures together algorithms used perform unsupervised texture segmentation multiple resolution segmentation algorithm first segments images coarse resolution progresses finer resolutions individual pixels classified method results accurate segmentations requires significantly less computation previously known methods field containing classification pixel image modeled markov random field segmentation resolution performed maximizing posteriori probability field subject resolution constraint resolution posteriori probability maximized deterministic greedy algorithm iteratively chooses classification individual pixels pixel blocks unsupervised parameter estimation algorithm determines number textures parameters minimizing global criterion based aic information criterion clusters corresponding individual textures formed alternately estimating cluster parameters repartitioning data clusters concurrently number distinct textures estimated combining clusters minimum criterion reached b introduction objective texture segmentation separate image regions distinct statistical behavior implicit assumption process statistics region stationary region extends significant area therefore reasonable assume image pixels spatially close likely texture addition texture segmentation algorithm independently classifies pixel image likely perform poorly since locally may sufficient information make good decision rea sons segmentation algorithms either implicitly explicitly impose form smoothness resulting segmentation example may done dividing image arbitrary blocks classifying texture block separately however block size chosen small discriminating among similar textures may difficult alternatively block size large regions differing texture may lost either case resulting boundaries accurate since reason believe actual texture boundaries occurred along block boundaries alternatively number authors proposed natural methods imposing smoothness constraints segmentation image 6 4 3 2 1 11 methods use random field smooth spatial behavior model discrete valued field containing classification pixel image segmentation approached statistical estimation problem approach region image classified differently surrounding regions sufficient statistical evidence justify distinct region regardless size methods segmentation use markov random fields mrf model discrete field containing individual pixel classifications mrf point statistically dependent neighbors complexity model restricted image segmented finding approximate maximum posteriori map estimate unknown field since likely segmentation given observed data however resulting functions quite difficult maximize globally stochastic relaxation algorithms simulated annealing applied problems 6 2 shown proper annealing schedule sa converge global optimum 6 schedule converges much slowly practically useful practice annealing schedule chosen yield reasonable trade performance computation sa remains computationally intensive method minimization alternatively greedy minimization algorithm known estimation iterated conditional modes icm 4 computationally efficient method maximizes probability segmentation field deterministically iteratively changing pixel classifications textures discriminated small regions containing pixels icm yields good results however work indicates larger numbers pixels necessary discriminate among different textures likely higher resolution images icm prone trapped local minimum cost function algorithm developed paper first segments image coarse resolution proceeds progressively finer resolutions individual pixels classified resolution greedy algorithm used perform classification result used initial condition next finer resolution found multiple resolution segmentation mrs algorithm less likely trapped local minima icm since resolution regions appropriate size classified used guide finer resolutions consistent observation convergence mrf based segmentation algorithms improved varying local neighborhoods 3 addition mrs algorithm requires substantially less computation icm since constraints propagate rapidly across image coarse resolution advantages important high resolution images individual pixels contain relatively little information discrimination textures information may spread large regions approach developed paper analogous multigrid methods used computing solutions partial differential equations multigrid methods recently applied computer vision problems 13 substantially reduce computation since constraints propagate rapidly coarse resolution however solving linear partial differential equations multigrid methods improve quality solution since local algorithms guaranteed converge unique solution finite precision effects ignored contrast multiple resolution segmentation techniques reduce computation improve performance since local minima cost function may avoided allowing coarse resolution solutions guide finer solutions 11 12 order use algorithm unsupervised fashion method needed estimate number texture regions parameters approach first develop statistical model observed image based texture model used segmentation overall model parameterized number textures parameters individual textures rate texture occurs shown fixed number textures approximate mle parameter estimation may performed alternately reestimating texture parameters repartioning clusters associated texture operation analogous algorithms kmeans algorithm 17 maximizes log likelihood instead minimizing mean square deviation cluster means problem estimating number textures quite different estimating parameters since mle number textures may arbitrarily large suggested 15 16 reasonable alternative mle find number type regions minimize information criteria aic proposed akaike 14 aic contains additional term account bias due number parameters estimated adopt similar approach based modified criteria also show criteria may minimized selective agglomeration texture clusters application parameter estimation algorithm described scheme decision agglomerate two clusters based cost function equals change criteria nodes merged cost function shown function texture statistics associated two clusters final step segmentation performed coarse resolution data occurring along boundaries removed superfluous textures formed boundaries eliminated segmentation parameter estimation algorithms use nonhomogeneous gaussian autoregressive ar model individual textures model allows extraction texture statistic pixel averaged arbitrary regions pixels yield aggregate statistic region therefore well adapted use multiple resolution techniques advantages predictive texture model model uses mean variance shown particularly important estimating number distinct textures textures high spatial correlation statistical model throughout paper assume observed image random field defined rectangular grid n points value point 2 written necessary points explicitly written integer pairs j x denote field also n points contains classification pixel points x take values set f1 mg number textures classifications conditional density function given x assumed exist strictly positive denoted fyjx probability p using framework image may segmented estimating pixel classifications x given observed image particular use map estimate x since set possible segmentations discrete maximum minimum well defined function set must exist hence distributions fyjx px defined problem segmenting image reduce minimizing cost function 21 texture model given known segmentation x image modeled causal nonhomogeneous gaussian ar random field 7 model advantage extraction texture statistics estimation parameter values direct computationally efficient causal ar model parameterizes conditional distribution point field given past values points local neighborhood course concept past dependent specific ordering image pixels work raster ordering used question arises whether artificial introduction causality image restrict range textures may modeled accurately partial answer question found considering gaussian random field models simultaneous autoregressive sar gauss markov random field gmrf 8 require introduction causal ordering gaussian random fields relationship among ar sar gmrf models depends neighborhoods used ar model special case sar model obtained constraining prediction coefficients future points zero also sar model may equivalently described gmrf larger finite neighborhood however choosing sufficiently large neighborhood size models may used approximate arbitrarily closely gaussian random field well behaved spectral density function 7 therefore major practical difference among models complexity required achieve sufficiently accurate model sar gmrf models disadvantage maximum likelihood ml parameter estimation difficult choose use ar model difficulty parameter estimation arises due complex dependence distributions normalizing constant parameters 8 estimated image assumed consist textures described parameter index particular texture parameter vector contains mean prediction variance oe 2 prediction coefficients texture parameter set used point 2 xs assume raster ordering points mean denote element element subtraction components r basic property ar model prediction errors formed filtering prediction coefficients independent property used calculate conditional distribution given x forward prediction error p th order ar model given xs r p values r xs r may nonzero placement p values depends prediction window used assume use second quadrant predictor window although models could also used ignore effects image boundary components prediction error independent mean zero variance given information conditional distribution given x n independent gaussian random variables however jacobian transformation needed compute conditional distribution calculated arranging components vectors raster ordering possible write 2 form matrix containing prediction coefficients c constant vector also lower triangular diagonal entries 1s due fact raster ordering points used prediction precede point predicted facts imply jacobian transformation 1 conditional distribution xs exp xs results negative conditional log likelihood xs log likelihood function 3 form sum local functions x general methods used throughout paper applicable texture model log likelihood function form l yjx functions l depend x point cdelta arbitrary function specifically texture model local likelihood functions form l yjx xs 22 segmentation field model use mrf model x due isotropic nature restriction local interactions order define mrf concept neighborhoods must first defined neighborhood point 2 set points ae two properties 8s r 2 62 set ordered pairs fs sg s2s known neighborhood system use eightpoint neighborhood system neighbors interior points given neighbors boundary points depend whether toroidal free boundary specified mrf x conditional distribution point field given points dependent neighbors clique c subset points r two points contained c r neighbors notice set cliques induced neighborhood system given neighborhood system gibbs distribution defined distribution px expressed form z exp cliques c functions v c arbitrary functions values x clique c z normalizing constant constant physically analogous temperature argument exponential cliques c physically analogous energy hammersleyclifford theorem 5 states x mrf strictly positive distribution neighborhood system fs sg s2s distribution x written gibbs distribution cliques induced neighborhood system fs sg s2s therefore px formulated gibbs distribution know x properties mrf cliques used energy function px pairs horizontally vertically diagonally adjacent points functions cliques appear implicitly two functions number horizontally vertically neighboring points different value x 2 x number diagonally neighboring points different value x density function x assumed form z exp parameters model since 1 2 may written sum functions cliques know 6 distribution mrf eightpoint neigh borhood general model mrfs two point cliques results allowing different potential distinct combination unlike pixel pairs adding term one pixel cliques noted arguments used paper may easily generalized case however assume prior knowledge available behavior individual class regions substituting px 6 log fyjx 4 1 results final form minimization problem l yjx 3 minimization methods general problem minimizing 7 quite difficult since cost function convex contains complex local minima problem may formulated one dimensional dynamic programming problem associating state value x entire line resulting algorithm computationally infeasible derin elliott proposed finding approximate solution solving dynamic programming problem along thin strips 1 however computation required algorithm increases dramatically number textures also problem may solved one pass pass quite complex sequential nature approaches explored paper iterative generally require application simple local operations well suited highly parallel implementations 31 sa icm methods sa general purpose algorithm minimizing cost function cx function state x sa works generating irreducible markov chain states step chain transition distribution chosen limiting distribution gibbs distribution temperature energy function cx minimum cx found allowing drop slowly chain generated since sufficiently small lowest energy state gibbs distribution occurs high probability reasonable expect drops slowly samples concentrated minimum energy state advantage sa requires change cost computed consecutive states markov chain two distinct methods generating samples gibbs distribution consequently two distinct methods performing sa depend ergodic properties markov chains formed iteratively replacing points x metropolis algorithm 9 symmetric transition probability defined transitions either accepted rejected probability depending change energy alternative method used paper gibbs sampler 6 replaces points x samples conditional distribution point given remainder field notice cx energy function mrf p depend x segmentation problem formulated previous section cost function given uxjy functions l jdelta dependent individual points x easily verified udeltajy energy function gibbs distribution eightpoint neighborhood therefore applying hammersleyclifford theorem know conditional distribution 8 function eight neighbors x large transitions occur almost uniformly set possible values x small value x corresponding likely conditional probability chosen high probability shown 6 varied according schedule n correct starting value rate decrease n 1 distribution xn uniform values x minimize cost function practice annealing schedule much slow faster schedule used represents tradeoff performance computation faster schedule used algorithm guaranteed escape local minima example large region misclassified algorithm probably never explore possibility changing classifications region reason initial condition sa algorithm often critical importance besag proposed iterated conditional modes icm algorithm 4 alternative sa icm may regarded sa extreme annealing schedule replaces points x value maximizes conditional density function point since arg min xs clear replacement selects value x minimizes cost constraint fixing remaining values x let v 1 number horizontal vertical neighbors x equal k let v 2 number diagonal neighbors x equal k replacement explicitly expressed arg min xs icm algorithm differs sa argument minimizes unique case sa replace x value chosen uniform probability minimizing values however icm selects previous value x minimizes cost otherwise icm chooses pseudorandomly among minimizing values consequently time replacement operation changes value point x cost uxjy assured decrease besag suggested replacements performed fixed number iterations however approach continue replacing points x additional changes occur since udeltajy strictly positive finite domain convergence algorithm finite number steps assured minimizing state x obtained local minimum sense state differs x single point must energy greater equal uxjy icm perform backtracking initial condition critical reasonable choice initial condition maximum likelihood estimate mle equivalently estimate obtained dropping cost terms due priori probability functions l jdelta dependent individual points x mle easily written l yjk 10 algorithm icm may stated explicitly follows 1 calculate xmle initial segmentation using 10 2 perform local minimization defined 9 pixel specified order 3 changes occur repeat step 2 otherwise stop noted specific order replacements performed important raster scan ordering used current pixel replacements affected classification adjacent pixels raster ordering since prior distribution x encourages pixels adjacent classified similarly often effect biasing position boundaries direction raster ordering surprising solution obtained icm could affected update ordering since minimum reached local may depend details implementation another disadvantage raster ordering requires sequential replacements therefore allow parallel implementation solution problems break x groupings members grouping conditionally independent given points remaining groups property updating done within group depend ordering points grouping may updated parallel division x known coding 5 depends neighborhood system used eightpoint neighborhood results four groupings possible perform updating parallel using maximum n4 processors important aspect deterministic replacement algorithm icm neighbors point changed need updated scheme implemented keeping flag point indicates whether necessary update point start algorithm flags set time pixel updated flag cleared neighboring flags set pixels value changed consequently first iteration requires application replacement operation point later iterations computation concentrated regions significant change occurring substantially reduce computation 32 multiple resolution segmentation mrs paper shall perform segmentation using multiple resolution approach mrs algorithm begin segmentation coarsest resolution use result initial condition segmentation next finer level resolution resolution correspond level quad tree lattice point one resolution correspond four points next finer resolution group four points referred block x n denote field containing classification blocks resolution n general quantities vary function resolution superscripts parentheses denote level superscript 0 denote original quantity mapping pixel coordinates n th resolution level points previous coarser n th level formally written bdeltac denotes greatest smaller integer function also k denote composition function ddelta k times structure illustrated figure 1 may formulate local log likelihood functions resolution n assuming fine segmentation x interpolated version x n specifically 8s 2 x applying 4 results log fyjx n l n yjx n l n l ngamma1 r l 0 gamma1 r set points next finer resolution map r following section discuss specific methods computing decimated likelihood functions resolution approach solve analogous segmentation problem one formulated fixed resolution problem optimization criteria use finding best segmentation resolution n l n n count adjacent values different type x n fi n 1 fi n 2 parameters coarse resolution mrf criteria may minimized using local replacement operation arg min l n remaining question choose coarse resolution parameters one possible approach choose parameters cost functions coarse fine resolutions equal 11 fact may done observing x ngamma1 relation implies fine course resolution cost functions equal resolutions n using parameters minimization 12 corresponds minimization original cost criteria 7 constraint solution constant appropriate sized blocks coarse resolution minimization using parameters given 14 effectively minimize 7 correct segmentation approximately block constant however recursion parameters undesirable property implies mrf models coarser resolution segmentations progressively higher spatial correlation alternatively finer resolution segmentations lower correlation course runs counter normal assumptions spatial coherence images tend cause insufficient spatial correlation finer resolutions excessive correlation coarse resolutions reasonable approach assume spatial correlation independent resolution since avoids problem excessive correlation coarse resolutions also assumption appropriate prior information unavailable likely scale regions image therefore experimentation fix parameters mrf function scale l level mrs algorithm may defined follows 1 compute l n jdelta levels using 11 2 compute ml estimate x lgamma1 initial condition using 10 set 3 compute x n iteratively performing local minimization 13 changes occur 4 otherwise compute initial value x ngamma1 replicating values x n block x ngamma1 set return step 3 33 decimation likelihood functions question arises decimated likelihood functions l n jdelta calculated direct method explicitly calculate functions texture k sum functions proper blocks however number textures large may require considerable computation storage also presumes parameters textures known alternative method extract statistics block functions l n yjk may computed function k model parameters k parameters different resolutions may calculated decimation statistics fine coarse resolution let ae region na points likelihood function summed regions likelihood function l yjk may expanded using 4 5 parameters k th texture model may write log likelihood explicit function parameters defining three vectors let k vector prediction coefficients raster order let vector corresponding points let 1 vector equal dimension components 1 example quarter plane prediction window vectors following forms autocorrelation ra mean statistics region defined using vectors prediction error may written function statistics substituting formula prediction error expression l yjk may verify ra sufficient statistics l define statistic vector may define new function ldelta delta thus one find recursion calculating region statistics n fine coarse resolution corresponding likelihood functions l n yjk may calculated statistics let b two disjoint regions let may shown using 15 statistics c c may calculated b commutative operation combining region statistics denoted symbol phi therefore components c statistics coarse resolutions may calculated fine resolutions using operation use notation repeated combining region statistics equation similar 11 may defined operation combining region statistics also important agglomerative clustering used unsupervised parameter estimation finally require method calculating mle estimate parameter vector region statistic order simplify calculations approximate mle estimate average components therefore unit vector equal components autocorrelation decomposed following mle values components given unsupervised parameter estimation order estimate number type textures without supervision algorithm needed independent free parameters may vary image image approach taken base estimate statistical model much variation likely among different texture samples results algorithm somewhat analogous well known kmeans algorithm 17 incorporates additional step determining number distinct textures 41 clustering model first step developing clustering method calculate joint probability distributions image pixel classifications given texture parameters number textures gibbs distribution used previous section model pixel classifications easily applied problem unsupervised parameter estimation difficulty arises unsupervised estimation problem requires distribution x parameterized number textures however dependence quite complex since normalizing constant gibbs distribution complex function fact normalizing constants gibbs distributions calculated special cases eg homogeneous gaussian ising intractability introduces significant problem since normalizing constant critical comparing likelihood different clustering hypotheses however would still like use prior knowledge spatial correlation pixel classes determining number types textures present approach take first separate image blocks equal size normally coarsest resolution used mrs algorithm prior knowledge spatial correlation pixel classes incorporated assuming block contains single texture later remove effect blocks fall boundaries may contain mixture textures segmentation assumed classes blocks gibbs distribution form 6 properly chosen fi parameters gibbs distribution embodied prior knowledge adjacent blocks likely texture section ignore information regarding spatial ordering blocks conceptually ordering information may ignored assuming blocks randomly reordered approximation since may choose ignore information however ability distinguish similar textures diminished advantage approach weakest possible assumptions made prior distribution data clustering model may accurately describe observed data order image blocks ignored relevant sufficient statistics texture statistics block number blocks texture fixed gibbs distribution determines distribution number blocks texture however general choice fi simple form distribution known therefore approximate distribution using multinomial distribution unknown parameters emphasized equivalent assuming ordered block classifications independent random variables fact block classifications may high spatial correlation still result multinomial distribution number blocks texture clustering done using texture statistics extracted image blocks normally coarsest resolution level l gamma 1 let k ae lgamma1 set blocks level texture type k denote number elements set k w number elements lgamma1 j notice k random since function random field x lgamma1 w k turn random variable since function k sets fa k g partition field x lgamma1 groups uniform texture therefore random sets fa k g k1 contain information random field x lgamma1 two expressions generally interchanged probability distributions however chosen use partitioning sets since suggestive clustering techniques developed additional constraints make first gamma 1 values k w k unambiguous specification th value denote vector texture parameters appendix joint distribution unordered block classes represented k1 calculated described number blocks texture w k modeled multinomial distribution unknown parameters ae k information relative position blocks lattice ignored distribution given functions ldelta delta defined 17 abbreviated notation used place lgamma1 denote statistic block also shown appendix summing possible partitioning sets distribution given recognizable distribution w independent identically distributed random variables marginal distribution formed mixture distributions accurate closed form distribution function parameters ae problem estimating number parameters remains poorly defined example mle texture parameters estimated cases fit m1 texture case always good better since includes case textures reason ml estimation generally choose large infinite number textures achieve best fit observed data however solution unacceptable since segmentation intended partition image minimum number necessary distinct textures zhang modestino 15 16 suggested reasonable alternative mle find number type regions minimize aic information criteria proposed akaike14 aic contains additional term account bias due number parameters estimated condition identifiability necessitates fisher information matrix invertible 18 important application setting ae would seem result distribution one less parameter however case mean variance prediction coefficients corresponding texture longer identifiable therefore number identifiable parameters reduced p defined section 21 number ar prediction coefficients applying definition aic 21 yields optimization criteria number disadvantages calculation 22 requires use w statistics one block contrasts calculation 20 requires use statistics one texture minimization 22 fixed difficult due lack known partitioning sets approximate minimization may performed using em algorithm 19 20 equivalently baums algorithm 21 however minimization process still substantially complicated lack class information result algorithm simple agglomerative clustering interpretation reasons propose modified criteria estimates missing class information together number textures parameters criteria results replacing log likelihood given 21 joint log likelihood partitioning sets given 20 modified clustering criteria objective cluster techniques developed following sections minimize clustering criteria function parameters f aeg partitioning sets fa k k1 number textures finally make number observations regarding properties clustering criteria given 23 manipulation criteria requires use statistics seen resulting minimization techniques analogous traditional agglomerative clustering methods fixing number textures partitioning sets minimization criteria yields ml estimates parameters fixing parameters number textures minimization yields map estimate partitioning sets minimization 23 course unique permutation texture parameters corresponding partitioning sets make solution unique imposing ordering parameter sets ae k ae k1 equality holds finer ordering may imposed using k parameters fixed solution minimization problem may fall boundary parameter space corresponding ae corresponds situation texture never occurs blocks classified texture case optimum number textures less since minimization 0 also result ae 42 estimation number distinct textures parameters estimated using three procedures strictly reduces clustering criteria 23 procedures may thought minimizing respect one ae fa k g fixing remaining two quan tities first procedure reestimates parameters ae region statistics k second procedure repartitions set choosing sets fa k g minimize 23 two operations analogous centroid estimation data repartitioning kmeans algorithm context sets k contains members cluster corresponding texture k last procedure agglomerative hierarchical clustering algorithm reduces combining pairs sets k minimizes 23 terms involving ae separated log likelihood clustering criteria may minimized variable independently minimizing respect equivalent calculating ml estimate k respect k texture k algorithm given 19 ml estimate k denoted k ml estimate ae may found minimizing 23 respect faeg k1 subject constraint result choosing partitioning sets k equivalent choosing field x lgamma1 minimizes 23 fixed set parameters may done choosing class block likely texture given observed data therefore best partition blocks partition assigns block likely class third mechanism required minimize 23 respect number clusters approach take use agglomerative hierarchical clustering let ck l change 23 caused merging two clusters k l single cluster l assume remaining parameters ae fa held constant however order define ck l parameters new cluster c must also specified choose ml estimate corresponding c since parameter vector yields minimum value 23 merging clusters using definition ck l obtain following expression c ml estimate given statistics l new cluster c substituting mle 16 easily shown statistic ml estimate prediction error given statistic na number pixels complete clustering algorithm described end section functions ck l computed parameter values clusters k l take mle values therefore may apply 25 yield following simplified expression 24 na l log l formula see information required calculate ck l statistic vector regions k l since global objective minimize 23 justified merging two clusters ck l negative merging operation repeated ck l greater equal zero clusters k l practice often case many combinations clusters negative ck l since minimum found agglomeration procedure global order clusters merged affect final result steepest descent strategy first merges two clusters minimum value ck l brute force approach steepest descent requires 2 calculations ck l find two nearest clusters however computational complexity steepest descent may reduced using balanced trees 22 cluster k balanced tree used maintain list clusters order increasing ck l tree insertions deletions searches may performed ologm gamma 1 time since nodes tree therefore computation required locate two nearest clusters equal number trees multiplied computation required search smallest element tree yields total search time om logm gamma 1 two clusters merged new tree must created remaining trees must updated deleting old cluster entries inserting new entry together operations require om log computation therefore complete operation combining two clusters requires om log computation total complexity agglomerative clustering procedure om 2 log noted algorithm requires om 2 memory alternatively simpler algorithm assure steepest descent may also used algorithm shown figure 2 starts randomly ordering clusters linked list cluster selected list searched locate nearest neighbor selected cluster ck l negative clusters combined form new cluster replaces original two clusters new cluster selected otherwise cluster selected member list previously selected algorithm requires om 2 computation om memory direct method clustering data start image block individual cluster however initial number blocks ranging 1024 4096 experimental results often large easily handled addition stage many cluster pairs result negative value ck l therefore first separate data quantization bins based mean standard deviation sample statistics block efficiently groups together blocks similar statistics experiments used 64 quantization levels mean 64 quantization levels standard deviation total 4096 distinct cluster types coarse grouping reduced number clusters range 200 500 initial grouping merging clusters done strictly reduce clustering criteria 23 since initial number clusters large algorithm figure 2 first used merge clusters ck l 10 point steepest descent algorithm used strategy conservative since algorithms found yield similar usually equivalent results used individually initial assumption analysis block contains single texture however generally blocks fall region boundaries contain mixture textures zhang modestino proposed method testing variance pixels image block removing blocks excessively high variance 16 however texture segmentation method unreliable since normal image textures likely large variance approach take perform icm segmentation coarsest level resolution using estimated texture parameters blocks fall texture boundary identified boundary block defined block differs texture one four nearest neighbors boundary blocks removed associated clusters cluster statistics ml parameters recalculated clusters merged using steeped descent algorithm complete algorithm unsupervised parameter estimation given extract texture statistics block image using 18 group statistics coarsely mean prediction variance order generate tractable number clusters 3 calculate ml parameter estimates cluster 4 combine clusters negative values ck l occur 5 calculate cluster partitions fa k g k1 minimize 23 changes occurred return step 3 otherwise continue 6 perform icm segmentation coarsest resolution remove consideration blocks fall boundaries recalculate ml parameters combine remaining clusters negative values ck l occur verify performance mrs algorithm computer simulations done variety conditions stressed performance mrs icm sa algorithms computational requirements segmentation algorithms measured using two methods first method counts number visits per pixel full resolution case mrs usually fractional number since visit coarse resolution represents fraction visit full resolution however measure conservative deterministic algorithms since local operations require testing flag described end section 31 number actual replacement computations per pixels relevant measure computation implementations significantly fewer computing elements pixels plots terms parameter represents cost per unit length boundaries differing regions constants relating parameters fi chosen meet two constraints used conjunction square two dimensional lattice unit spacing samples axes first energy function diagonal horizontal vertical boundaries equal length second boundaries energy function equal unit boundary length multiplied constraints assume boundaries long ignore end effects neither holds boundaries fall angles diagonal horizontal vertical experiments sa used annealing schedule form n temperature used n th pass annealing schedule uniquely determined starting ending temperatures number iterations experiments sa algorithm started ml estimate x initial condition experimental evidence believe varying function number iterations improve performance applications sa icm example besag 4 found progressively increasing improved performance icm certain cases also number studies found varying parameters gibbs distribution icm sa improve results modeling continuously valued fields discontinuities 10 12 however case continuous valued fields parameters progressively reduced initially large values case schedule needed changing parameters introduces additional free parameters generally image dependent therefore approach fix parameter using icm sa described earlier fix parameter levels resolution using mrs series tests performed two simple 64x64 toroidal test patterns shown figure 3 determine relative performance icm mrs sa tests three levels resolution used 100 interactions sa performed 1t figures 4 5 plot mean number misclassified pixels value energy defined 7 function mrs icm sa set plots shown two different cases uses test pattern shown figure 3 added gaussian noise signaltonoise ratio defined plots mean error versus indicate performance mrs sa algorithms relatively insensitive choice dynamic range energy function plotted figure 5 makes relative differences pattern 1 difficult distingish curves sa mr fall close effectively overlap plot three additional tests done fixed compare performance mrs icm sa empirical density function error shown figures 6 7 8 together mean percentage error mean energy segmentation mean number visits per pixel three methods examples mrs algorithm requires less computation either icm sa empirical error distributions mean energy values indicate performance mrs comparable sa substantially better icm cases general advantages mrs greatest signaltonoise ratio low case information needed accurately classify pixels spread larger regions segmentation also performed six different tests shown figures 9 14 figure unsupervised parameter estimation used test images resolution 256x256 texture models used three point prediction model using three closest pixels prediction case free boundary assumed parameter value unsupervised parameter estimation performed done coarsest resolution used segmentation example contains segmentations resulting mrs icm sa 100 500 iterations starting temperature ending temperature possible correct segmentation also presented distinct texture displayed unique gray level outlined white tables summerize results unsupervised estimation algorithm table 1 lists number distinct texture types estimated five experiments shown figures 10 14 true number textures also listed three experiments number textures known case estimated number textures correct table 2 shows actual estimated parameter values synthetic image used figures 9 10 actual parameters used generating synthetic textures figures 9 12 used composite images formed piecing together natural synthetic textures composite images adjacent textures smoothly overlapped two pixel boundary order avoid visible cue four tests four levels resolution used noted unsupervised parameter algorithm requires least one region texture large enough guarantee region interior therefore detected pattern texture regions chosen true four levels resolution figures used synthetic image composite image formed synthetic textures generated using assumed texture model figure 9 shows result segmenting synthetic image using actual texture parameters figure 10 shows result segmenting image using parameters obtained unsupervised parameter estimation algorithm figures 11 12 used composite images formed natural texture types order area first image contains woolen cloth raffia grass second image contains woolen cloth grass beach sand figures 13 14 show segmentation aerial photograph three levels resolution used smaller regions distinct textures could identified images quality segmentation depends application possible vary amount detail segmentation varying boundary cost nevertheless superior algorithm reproducibly classify similar regions texture table 4 lists value energy function resulting three minimization algorithms mrs icm sa energy segmentation useful since provides objective measure performance computation required segmentation using mrs icm sa shown table 3 terms visits per pixel replacements per pixel every case mrs algorithm required less computation icm much less sa noted number replacements per pixel required mrs algorithm relatively independent image segmented 6 conclusion mrs algorithm proposed paper performed better required less computation icm cases tested also mrs algorithm generally performed comparably sa substantially less computation fact computation required sa ranged 10 1000 times mrs depending problem method comparison however number replacements per pixel found good basis comparison using measure sa typically required two orders magnitude computation range examples mrs algorithm seems yield substantial improvement images information per pixel low information larger regions must combined correctly segment image algorithm performing unsupervised parameter estimation proposed use conjunction segmentation algorithm algorithm based gaussian ar texture model estimates number textures statistically significant differences parameters textures algorithm composed three basic operations partitioning data distinct textures clusters estimation cluster parameters agglomeration similar clusters reduce number distinct textures operations performed context minimizing proposed statistical criteria algorithm guaranteed converge superfluous textures formed texture boundaries removed final step appendix section derive joint distribution fa k k1 assumptions relative order blocks set k ignored number blocks texture w k multinomial distribution unknown parameters ae k using function ldelta delta defined 17 rewrite conditional distribution given partitioning sets k1 explicitly function statistics k parameters k yields expression one method conceptualizing loss ordering information assume values x lgamma1 corresponding statistics reordered random mapping uniformly distributed possible reorderings case unique mapping occurs equal probability 1w pi random reordering condition knowledge fw k k1 value x lgamma1 probability particular group partitioning sets given sum probabilities mappings would result partitioning since exactly q unique mappings result group partitioning sets next assume fw k k1 multinomial distribution unknown parameters combining 28 29 final expression joint distribution fa k results distribution computed summing possible partitioning sets equivalently summing possible values x let sequence fs ordering points xs n 1 xs n 1 ae xs expfgammal yjx g ae xs expfgammal xs n 1 ae xs n expfgammal n yjx n g5 ae xs expfgammal yjx g x used place x lgamma1 exchanging products summations obtain recognizable distribution w independent identically distributed random variables marginal distribution formed mixture distributions r modeling segmentation noisy textured images using gibbs random fields segmentation noisy textured images using simulated annealing parallel image segmentation algorithm using relaxation varying neighborhoods mapping array processors statistical analysis dirty pictures spatial interaction statistical analysis lattice systems stochastic relaxation gibbs distributions bayesian restoration images advances mathematical models image processing estimation choice neighbors spatialinteraction models images equations state calculations fast computing machines computing motion using analog binary resistive networks segmentation textured images using multiple resolution approach multiple resolution approach regularization image analysis using multigrid relaxation methods new look statistical model modelfitting approach cluster validation application stochastic modelbased image segmentation unsupervised image segmentation using gaussian model pattern classification scene analysis chapman hall maximum likelihood incomplete data via em algorithm mixture densities maximum likelihood em algorithm maximization technique occurring statistical analysis probabilistic functions markov chains art computer programming volume tr image analysis using multigrid relaxation methods modeling segmentation noisy textured images using gibbs random fields parallel image segmentation algorithm using relaxation varying neighborhoods mapping computing motion using analog binary resistive networks art computer programming volume 3 ctr feng li jiaxiong peng xiaojun zheng objectbased semantic image segmentation using mrf eurasip journal applied signal processing v2004 n1 p833840 1 january 2004 walter g kropatsch yll haxhimusa zygmunt pizlo george langs vision pyramids grow high pattern recognition letters v26 n3 p319337 february 2005 feng li jiaxiong peng double random field models remote sensing image segmentation pattern recognition letters v25 n1 p129139 5 january 2004 zhuowen tu songchun zhu image segmentation datadriven markov chain monte carlo ieee transactions pattern analysis machine intelligence v24 n5 p657673 may 2002 olivier alata clarisse ramananjarasoa unsupervised textured image segmentation using 2d quarter plane autoregressive model four prediction supports pattern recognition letters v26 n8 p10691081 june 2005 wei fu guangzhong xing edgeoriented spatial interpolation error concealment consecutive blocks journal computer science technology v22 n3 p494497 may 2007 roland wilson changtsun li class discrete multiresolution random fields application image segmentation ieee transactions pattern analysis machine intelligence v25 n1 p4256 january lorette x descombes j zerubia texture analysis markovian modelling fuzzyclassification application urban area extraction fromsatellite images international journal computer vision v36 n3 p221236 febmarch 2000 majid mirmehdi maria petrou segmentation color textures ieee transactions pattern analysis machine intelligence v22 n2 p142159 february 2000 nikos paragios rachid deriche geodesic active regions level set methods supervised texture segmentation international journal computer vision v46 n3 p223247 februarymarch 2002 djamal boukerroui atilla baskurt j alison noble olivier basset segmentation ultrasound images multiresolution 2d 3d algorithm based global local statistics pattern recognition letters v24 n45 p779790 february alexandra pizurica wilfried philips ignace lemahieu marc acheroy application markov random field models waveletbased image denoising imaging vision systems theory assessment applications nova science publishers inc commack ny 2001 olivier alata christian olivier choice 2d causal autoregressive texture model using information criteria pattern recognition letters v24 n910 p11911201 01 june yue wang tlay adali chiming lau sunyuan kung quantitative analysis mr brain image sequences adaptive selforganizing finite mixtures journal vlsi signal processing systems v18 n3 p219239 april 1 1998 murilo p almeida anisotropic textures arbitrary orientation journal mathematical imaging vision v7 n3 p241251 june 1997 ayman elbaz aly farag georgy gimelfarb iterative approximation empirical greylevel distributions precise segmentation multimodal images eurasip journal applied signal processing v2005 n1 p19691983 1 january 2005 c c reyesaldasoro bhalerao bhattacharyya space feature selection application texture segmentation pattern recognition v39 n5 p812826 may 2006