learning onevariable pattern languages efficiently average parallel asking queries pattern finite string constant variable symbols langauge generated pattern set strings constant symbols obtained pattern substituting nonempty strings variables study learnability onevariable pattern languages limit respect update time needed computing new single hypothesis expected total learning time taken convergence correct hypothesis results follows first design consistent setdriven learner using concept descriptive patterns achieves update time on2logn n size input sample best previously known algorithm computing descriptive onevariable patterns requires time on4logn cf angluin j comput systems sci 21 1 1980 4662 second give parallel version algorithm requires time ologn on3logn processors erewpram third using modified version sequential algorithm subroutine devise learning algorithm onevariable patterns whose expected total learning time ol2logl provided sample strings drawn target language according probability distribution expected string length l probability distribution must strings equal length equal probability arbitrary otherwise thus establish first algorithm learning onevariable pattern languages expected total learning time provably differs update time constant factor finally show algorithm descriptive onevariable patterns used learning onevariable patterns polynomial number superset queries respect onevariable patterns query language b introduction pattern string constant symbols variable symbols language generated pattern set strings obtained substituting strings constants variables cf 1 pattern languages variations thereof widely investigated cf eg 17 18 19 continuous interest pattern languages many reasons among learnability full version paper available technical report cf 4 limit pattern languages text cf 1 2 moreover learnability pattern languages interesting respect potential applications cf eg 19 given efficiency becomes central issue however defining appropriate measure efficiency learning limit difficult problem cf 16 various authors studied efficiency learning terms update time needed computing new single guess really counts applications overall time needed learner convergence ie total learning time one show total learning time unbounded worstcase thus study expected total learning time purpose motivation shortly summarize known regard pattern languages learned outputting descriptive patterns hypotheses cf 1 resulting learning algorithm consistent setdriven learner setdriven iff output depends range input cf eg 20 22 general consistency setdrivenness considerably limit learning capabilities cf eg 14 22 polynomial time algorithm computing descriptive patterns known hence already update time practically infeasible moreover finding descriptive pattern maximum possible length npcomplete cf 1 thus unlikely polynomialtime algorithm computing descriptive patterns maximum possible length therefore natural ask whether efficient pattern learners benefit concept descriptive patterns special cases eg regular patterns noncross patterns unions k regular pattern languages k priori fixed studied cases descriptive patterns polynomialtime computable cf eg 19 thus learners achieve polynomial update time nothing known expected total learning time another restriction obtained priori bounding number k different variables occurring pattern kvariable patterns open polynomialtime algorithms computing descriptive kvariable patterns fixed k 1 cf 9 12 hand kvariable patterns paclearnable respect unions kvariable patterns hypothesis space cf 11 lange wiehagen 13 provided learner lwa pattern languages may output inconsistent guesses lwa achieves polynomial update time setdriven cf 21 even iterative thus beating angluins 1 algorithm respect space complexity lwa expected total learning time exponential number different variables occurring target pattern cf 21 moreover point convergence lwa definitely depends appearance sufficiently many shortest strings target lan guage algorithms mentioned least corresponding correctness proofs depend thus following problem arises naturally exist efficient pattern language learner benefiting concept descriptive patterns thereby depending presentation sufficiently many shortest strings target language still achieving expected total learning time polynomially bounded expected string length provide complete affirmative answer studying special case onevariable patterns believe case natural choice since nontrivial may exponentially many consistent patterns given sample since first case polynomial time algorithm computing descriptive patterns known cf 1 angluins 1 algorithm finding descriptive patterns runs time 4 log n inputs size n always outputs descriptive patterns maximum possible length aware possible improvements running time certain special cases hoped study would provide insight uniform improvement present improvement ie algorithm computing descriptive onevariable patterns 2 log n steps key idea achieve goal giving necessarily finding descriptive patterns maximum possible length note results concerning difficulty finding descriptive patterns depend additional requirement compute ones maximum possible length cf eg 1 12 thus result may least support conjecture efficient learners may arise one definitely trying find descriptive patterns maximum possible length descriptive ones instead moreover algorithm also efficiently parallelized using ologn time n processors erewpram previously efficient parallel algorithm learning onevariable pattern languages known main result version sequential algorithm still learning onevariable pattern languages expected total learning time 2 log sample strings drawn target language according probability distribution expected string length arbitrary except strings equal length equal probability particular shortest strings may probability 0 note expected total learning time differs constant factor time needed update actual hypotheses hand could prove olog many examples sufficient achieve convergence finally deal active learning learner gains information target object asking queries oracle show algorithm descriptive onevariable patterns used learning onevariable patterns asking polynomially many superset queries another algorithm learning pattern languages polynomial number superset queries known uses much powerful query language ie patterns one variable even target pattern onevariable pattern cf 3 11 pattern languages inductive inference set natural numbers let real numbers x define bxc greatest integer less equal x let finite alphabet containing least two elements denotes free monoid denote set finite nonnull strings ie denotes empty string let ing infinite set variables patterns strings length string 2 pattern denoted jsj jj respectively pat denote set patterns denote ith symbol eg called constant otherwise 2 x ie variable use si denote ith symbol var denote number different variables occurring x denotes number occurrences variable x kvariable pattern pat k denote set kvariable patterns case denote variable occurring x denotes string obtained substituting u j occurrence x j tuple called substitution every pattern 2 pat k define language generated pat k denote set kvariable pattern languages denotes set pattern languages note several problems decidable even efficiently solvable pat 1 undecidable npcomplete pat example general pattern languages word problem npcomplete cf 1 inclusion problem undecidable cf 10 problems decidable linear time onevariable pattern languages hand pat 1 still incomparable regular context free languages finite set called sample pattern consistent sample l onevariable pattern called descriptive consistent consistent one pattern l ae l next define relevant learning models start inductive inference languages text cf eg 15 22 let l language every infinite sequence strings text l textl denotes set texts l let text r 2 set r initial segment length r denote range r ie rg define inductive inference machine abbr iim algorithmic device taking inputs initial segments text outputting every input pattern hypothesis cf 7 definition 1 pat called learnable limit text iff iim every l 2 pat every 2 textl 1 r 2 r defined 2 2 pat almost r 2 r learnability onevariable pattern languages defined analogously replacing pat pat pat 1 pat 1 respectively dealing setdriven learners often technically advantageous describe dependence relevant set r ie sample obtained input let refer n size sample 3 study nonerasing substitutions erasing substitutions also considered variables may replaced leading different language class cf 5 pat well pat 1 constitute indexable class l uniformly recursive languages ie effective enumeration l j j2in l recursive function f j 2 2 fj except section 3 use prammodel assume model computation representation patterns 1 next define update time total learning time let iim every l 2 pat least number r stage convergence tm r denote time compute r call tm r update time total learning time taken iim successive input defined finally define learning via queries objects learned elements indexable class l assume indexable class h well fixed effective enumeration hypothesis space l clearly h must comprise l hypothesis h describes target language l iff source information target l queries oracle distinguish membership equivalence subset superset disjointness queries cf 3 input membership query string output yes 2 l otherwise queries input index j output yes query disjointness query otherwise reply counterexample returned ie string 2 l4h j symmetric difference l h j respectively always assume queries answered truthfully definition 2 angluin 3 let l indexable class let h hypothesis space learning algorithm exactly identifies target l 2 l respect h access certain type queries always halts outputs index j note learner allowed one hypothesis must correct complexity query learner measured number queries asked worstcase 2 improved sequential algorithm present algorithm computing descriptive pattern 2 pat 1 sample input without loss generality assume 0 shortest string algorithm runs time js 0 j log js 0 simpler much faster angluins 1 algorithm needs time log js 0 j angluins 1 algorithm computes explicitly representation set consistent onevariable patterns outputs descriptive pattern maximum possible length avoid find descriptive pattern maximum possible length thus work polynomialsize subset consistent patterns next review establish basic properties needed later obtained substituting pattern 2 pat x pattern consistent must strings ff obtained substituting ff x given consistent pattern set fff denoted ffs moreover sample called prefixfree jsj 1 string prefix strings note distinction prefixfree samples nonprefix free samples well pay lemma 2 prefixfree exists descriptive pattern 2 pat 1 least two strings ffs start different symbol proof let u denote longest common prefix strings pattern ux consistent u shorter every string since prefixfree consequently exists descriptive pattern 2 pat 1 know exists pattern 2 pat 1 longest common prefix strings conclude hence least two strings ffs ux must start different symbol 2 1g conss subset consistent patterns prefixfree lemma 3 let prefixfree sample conss 6 every maximum length descriptive proof let prefixfree according lemma 2 exists descriptive pattern belonging conss thus conss 6 suppose maximum length descriptive thus moreover exists pattern 2 pat 1 l well l ae l hence lemma 1 know obtained substituting pattern x since least two strings ffs start different symbol immediately get 1 2 x moreover least two strings ffs must also start different symbol hence 2 conss note jj 1 since otherwise contradicting l ae l finally jj 1 may conclude contradiction maximum length thus pattern exist hence descriptive 2 next explain handle nonprefixfree samples algorithm checks whether input sample consists single string happens outputs terminates otherwise tests whether 0 prefix strings case outputs ux 2 pat 1 u prefix 0 length js terminates clearly lux suppose pattern l l ae lux lemma 1 applies ie thus 6 l consequently ux descriptive otherwise jsj 2 0 prefix strings thus lemma 3 suffices find output longest pattern conss improved algorithm prefixfree samples based fact jconssj bounded small polynomial let k l 2 k 0 call patterns x l occurrences constants k lpatterns k lpattern length k thus k lpattern conss satisfying js refers length string substituted occurrences x relevant k lpattern obtain 0 therefore bjs 0 jkc possible values l fixed value k hence number possible k lpairs k lpatterns conss exist bounded algorithm considers possible k lpairs turn describe algorithm one specific k lpair k lpattern 2 conss strings ff 2 ffs must satisfy substring length starting first position input strings differ js 2 consistent k lpattern computation performed k lpair following lemma shows k lpattern conss unique exists lemma 4 let prefixfree sample every given k lpair one k lpattern conss proof lemma 4 directly yields algorithm 1 either returns unique k lpattern 2 conss nil k lpattern conss assume subprocedure taking input sample returning longest common prefix u algorithm 1 input k l u following l b k c l else l l return else return nil fi note minor modifications algorithm 1 perform consistency test even constructed putting lemma 4 fact ojs possible k lpairs together directly obtain lemma 5 prefixfree sample g using algorithm 1 subroutine algorithm 2 finding descriptive pattern prefixfree sample follows strategy exemplified thus simply computes patterns conss outputs one maximum length inputs size n overall complexity algorithm js 0 j log js 0 2 log n since ojs 0 j log js 0 tests must performed time complexity algorithm 2 input following pi k js0 output maximumlength pattern 2 p note number k lpairs processed often smaller ojs since condition js restricts possible values k strings equal length also advantageous process k lpairs order nonincreasing k l algorithm terminate soon finds first consistent pattern however worstcase complexity improved descriptive pattern x finally summarize main result obtained following theorem theorem 1 using algorithm 2 subroutine pat 1 learned limit setdriven consistent iim update time 2 log n input samples size n 3 efficient parallel algorithm whereas ram model generally accepted suitable model developing analyzing sequential algorithms consensus yet reached area parallel computing pram model introduced 6 usually considered acceptable compromise pram consists number processors local memory execute local program communicate exchanging data shared memory variants pram model differ constraints simultaneous accesses memory location different processors crewpram allows concurrent read accesses concurrent write ac cesses ease presentation describe algorithm crewpram model algorithm modified run erewpram however use standard techniques computing descriptive onevariable patterns known previously algorithm 2 efficiently parallelized using wellknown techniques including prefixsums treecontraction listranking subroutines cf 8 parallel algorithm handle nonprefixfree samples way algorithm 2 checking singleton 0 prefix strings requires time olog n using log n processors thus may assume input sample prefixfree additionally assume prefixtest returned first position input strings differ index parallel algorithm handle ojs 0 j log js 0 possible k lpairs par allel k lpair algorithm computes unique candidate k lpattern conss exists checks whether l suffices output obtained pattern maximum length next show efficiently parallelize two steps given k lpair algorithm uses strings 0 calculating unique candidate k lpattern conss reduces processor requirements modification lemma 4 shows candidate pattern remain unique position j said bcorresponding position j 0 0 k meaning bcorresponding positions follows suppose consistent k lpattern fs position j 0 0 corresponds 2 1 jj b occurrences x left corresponds position computing candidate pattern 0 algorithm calculates entries array equalj b boolean values first j ranges 1 js 0 j b 0 k equalj b true iff symbol position j 0 symbol bcorresponding position thus array defined follows equalj array equal okjs 0 entries calculated constant time thus using okjs 0 log n processors equal computed time olog n moreover directed graph g forest binary intrees built equal candidate pattern calculated g using treecontraction prefixsums listranking details omitted due space restrictions thus prove lemma 6 let sample n size given array equal unique candidate k lpattern conss nil pattern exists computed erewpram time ologn using okjs 0 processors algorithm either discovered k lpattern exists obtained candidate k lpattern latter case test whether consistent lemma 7 given candidate pattern consistency sample size n checked time ologn using log n processors crewpram putting together obtain following theorem theorem 2 exists parallel algorithm computing descriptive onevariable patterns time ologn using ojs n processors crewpram samples size n note product time number processors algorithm time spent improved sequential algorithm larger product exceeds time sequential algorithm factor less ojs 4 analyzing expected total learning time dealing major result present paper ie expected total learning time sequential learner let target pattern total learning time algorithm trying infer unbounded worst case since infinitely many strings l mislead however best case two examples ie two shortest strings 0x 1x always suffice learner outputting descriptive patterns guesses hence assume strings presented algorithm drawn l according certain probability distribution compute expected total learning time algorithm distribution must satisfy two criteria two strings l equal length must equal probability expected string length must finite refer distributions proper probability distributions design algorithm 1la inferring pattern 2 pat 1 expected total learning time 2 log advantageous calculate descriptive pattern time new string read instead algorithm 1la reads certain number strings starts perform computations waits length sample string smaller number sample strings read far least two different sample strings read first two phases outputs 1 first sample string guess sample strings read far x otherwise constant pattern correct hypothesis always output algorithm never reaches third phase otherwise algorithm uses modified version algorithm 2 calculate set p 0 candidate patterns enters phase 3 precisely calculate whole set p 0 instead uses function first cand obtain longest pattern p 0 function next cand repeatedly obtain remaining patterns p 0 order nonincreasing length substantially reduces memory requirements pattern obtained calling first cand used current candidate new string compared 2 l output otherwise next cand called obtain new candidate 0 0 current candidate output independently 2 l 0 longest common prefix sample strings including shorter sample strings excluding however first cand called new list candidate patterns considered thus algorithm 1la may output inconsistent hypotheses algorithm 1la shown figure 1 let defined follows string w fvxg otherwise denote u longest common prefix set patterns computed algorithms 1 2 omit consistency check hence p 0 conss conss defined section 2 p 0 necessarily contains pattern 0 2 l longest common prefix 0 longest constant prefix assuming first cand 0 returns returns i1 since omit consistency checks call first cand subsequent calls next cand either correct pattern found prefix changes performed time ojsj 2 log jsj show algorithm 1la correctly infers onevariable pattern languages text limit correctly infers onevariable pattern languages text probability 1 sample strings drawn l according proper probability distribution 4 theorem 3 let arbitrary onevariable pattern algorithm 1la correctly infers text limit 4 note learning language l limit learning l strings drawn l according proper probability distribution r 0 repeat r r string sr string sr f phase 3 g shortest string fs1 prefix fs1 string fs1 longer else 0 string fs1 differs position juj first candss 0 forever read string 00 u prefix 00 common prefix 00 first candss 0 else output hypothesis od fig 1 algorithm 1la theorem 4 let 2 pat 1 sample strings drawn l according proper probability distribution algorithm 1la learns probability 1 proof outputs reading first string converges otherwise let string gamma 1 constant symbols 2 pat 1 fg algorithm 1la read two strings differ position pattern one candidates set p 0 implicitly maintained algorithm string r r 1 satisfies 1 6 r probability jaj gamma 1jaj 12 event must happen probability 1 long current candidate differs probability next string read belong l least 12 cf lemma 8 hence candidate patterns discarded probability 1 current candidate output algorithm converges 2 lemma 8 let ux onevariable pattern constant prefix u arbitrary 0 juj let 6 pattern p 0 fs generate string drawn l according proper probability distribution probability least jaj gamma 1jaj analyze total expected learning time algorithm 1la obvi ously total expected learning time target pattern 2 hence assume following contains least one occurrence x next recall definition median establish basic property used later er denote expectation random variable r definition 3 let r random variable ranger median r number 2 ranger prr 1and prr 1proposition 1 let r random variable ranger median satisfies 2er lemma 9 let proper probability distribution let l random variable taking values length string drawn l respect let median l let expectation expected number steps performed algorithm 1la phase 1 proof let l random variable whose value length ith string read algorithm clearly distribution l l let r random variable whose value number strings algorithm 1la reads phase 1 let l sigma lr number symbols read algorithm 1la phase 1 let w 1 random variable whose value time spent algorithm 1la phase 1 obviously w claim 1 el el 1 l must least provided r equation 1 proved follows similarly shown el r furthermore clear el r rewrite el sigma el el sigma r el sigma using well equations 1 3 obtain el fi use equations 1 2 obtain el using r assumptions lemma 9 estimate expected number steps performed phase 2 follows lemma 10 phase 2 expected number steps performed algorithm 1la finally deal phase 3 let l lemma 9 average amount time spent phase 3 estimated follows lemma 11 phase 3 expected number steps performed calls functions first cand next cand 2 log lemma 12 phase 3 expected number steps performed reading strings log proof denote w rthe number steps performed reading strings phase 3 make distinction strings read correct set candidate patterns considered strings read afterwards end phase 3 former accounted random variable v 1 latter v 2 correct set candidate patterns ie set containing yet considered probability new string force correct set candidate patterns considered 12 denote k random variable whose value number strings read phase 3 correct set candidate patterns considered assume correct set candidate patterns p 0 contains patterns considered pattern pattern probability string drawn l according proper probability distribution language 12 either additional variable additional constant symbol lemma 8 denote v 2 steps performed reading strings ith pattern p 0 considered log r obtain oel log oelr log r hence ew r lemma 13 phase 3 expected number steps performed checking whether current candidate pattern generates newly read sample string log putting together arrive following expected total learning time required algorithm 1la theorem 5 sample strings drawn l according proper probability distribution expected string length expected total learning time algorithm 1la 2 log 5 learning superset queries pat learnable polynomially many queries equivalence membership subset queries allowed provided result may easily extended pat 1 however positive results also known first pat exactly learnable using polynomially many disjointness queries respect hypothesis space pat fin fin set finite languages cf 13 proof technique easily extends pat 1 second angluin 3 established algorithm exactly learning pat respect pat asking ojj 2 jjjaj many superset queries however requires choosing general patterns asking queries definitely work hypothesis space pat 1 hence natural ask exist superset query algorithm learning pat 1 respect pat 1 uses polynomially many superset queries using results previous sections able answer question af firmatively nevertheless whereas pat learned respect pat restricted superset queries ie superset queries returning counterexamples query algorithm needs counterexamples interestingly need counterexample every query answered negatively instead two counterexamples always suffice next theorem shows onevariable patterns learnable polynomial number restricted superset queries theorem 6 algorithm exactly identifying l 2 pat 1 generated pattern length n respect pat 1 using restricted superset queries restricted equivalence queries must make least jaj ngamma2 2 ngamma2 queries worst case furthermore show learning pat 1 polynomial number superset queries impossible algorithm may ask single counterexample theorem 7 algorithm exactly identifies onevariable pattern languages restricted superset queries one unrestricted superset query needs least 2 queries worst case k length counterexample returned new algorithm ql works follows see figure 2 assume algorithm learn pattern first ql asks whether l holds case iff answer yes ql knows right result otherwise ql obtains counterexample c0 2 l asking answer ql computes g know starts c0 ith position asks l lc0 determine case since would imply ql uses counterexample query l lc0 x construct set xg construction two counterexamples differ ith position coincide first positions else l lc0 x else fc0 cc0 xg r conss repeat maxr r r n fg l l fig 2 algorithm ql algorithm learns pattern superset queries queries form l l 2 pat 1 chosen algorithm answer query l l algorithm ask counterexample c w denote prefix w length maxr maximumlength element r algorithm 2 section 2 computes coincides first narrowed search set r candidates let length shortest counterexample log lemma 5 task left find among patterns r find removing patterns r using following lemma lemma 14 let implies ql tests l l maximum length pattern 2 r removes r l 6 l iterating process finally yields longest pattern l l lemma 14 guarantees thus theorem 8 algorithm ql learns pat 1 respect pat 1 asking superset queries query complexity ql ojjm log many restricted superset queries plus two superset queries first two queries answered every language l 2 pat 1 length shortest counterexample returned thus query complexity ojjm log learner compares well angluins 3 learner ojjjaj restricted learn pat 1 using much powerful hypothesis space pat long length shortest counterexample returned large acknowledgements substantial part work done second author visiting kyushu university visit supported japanese society promotion science grant 106011 fifth author kindly acknowledges support grantinaid scientific research c japan ministry education science sports culture grant 07680403 r finding patterns common set strings inductive inference formal languages positive data machine learning efficient learning onevariable pattern languages positive data relation two patterns comparable languages parallelism random access machines language identification limit introduction parallel algorithms time inference general pattern languages inclusion undecidable pattern languages polynomialtime algorithm learning kvariable pattern languages examples note twovariable patternfinding problem systems learn introduction learning theory cognitive computer scientists inductive inference patterns eatcs bulletin return patterns pattern inference formal principles language acquisition lange wiehagens pattern language learning algorithm averagecase analysis respect total learning time guided tour across boundaries learning recursive languages tr theory learnable systems learn introduction learning theory cognitive computer scientists complexity inductive inference pattern languages examples queries note twovariable patternfinding problem deterministic simulation idealized parallel computers realistic ones prudence conditions formal language learning polynomialtime algorithm learning italickitalicvariable pattern languages examples polynomialtime inference arbitrary pattern languages efficient pram simulation distributed memory machine introduction parallel algorithms lange wiehagens pattern language learning algorithm queries concept learning polynomial time inference extended regular pattern languages inclusion undecidable pattern languages polynomial time inference general pattern languages relation two patterns comparable languages pattern inference guided tour across boundaries learning recursive languages inductive inference unbounded unions pattern languages positive data monotonic nonmonotonic inductive inference functions patterns parallelism random access machines ctr john case sanjay jain rdiger reischuk frank stephan thomas zeugmann learning subclass regular patterns polynomial time theoretical computer science v364 n1 p115131 2 november 2006 thomas zeugmann learning limit stochastic finite learning theoretical computer science v364 n1 p7797 2 november 2006