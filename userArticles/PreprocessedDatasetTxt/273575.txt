spectral perturbation bounds positive definite matrices let h h positive definite matrices shown barlow demmel demmel veselic one takes componentwise approach one prove much stronger bounds lambdaihlambdaih components eigenvectors h h using standard normwise perturbation theory unified approach presented improves results barlow demmel veselic also shown growth factor associated error bound components eigenvectors computed jacobis method grows linearly rather exponentially number jacobi iterations required convergence b introduction positive definite matrix h written diagonal much better conditioned h eigenvalues eigenvectors h determined high relative accuracy entries matrix h determined high relative accuracy shown demmel veselic 2 building work barlow demmel 1 paper strengthen perturbation bounds 2 present unified approach proving results also show conjectured 2 growth factor arises bound accuracy components eigenvectors computed jacobis method linear rather exponential give outline paper main ideas define notation section 2 quickly reprove eigenvalue eigenvector perturbation bounds 2 perhaps unified way derive bounds sensitivity eigenvalues perturbations given entry matrix main idea section analysis reduced standard perturbation theory one express additive perturbations multiplicative perturbations respect approach similar eisenstat ipsen 4 except assume multiplicative perturbation go derive bounds whereas assume additive perturbation rewrite multiplicative perturbation performing analysis results 4 eigenvalues eigenvectors briefly compare approach relative perturbation bounds 1 2 4 section 21 also show relative gap associated eigenvalue good measure distance scaled norm nearest matrix repeated eigenvalue section 3 consider components eigenvectors graded positive definite matrix 1 key idea h graded positive definite matrix u orthogonal h u graded department mathematics college william mary williamsburg va 23187 email namathiasnanetornlgov research supported part national science foundation grant dms9201586 much done author visiting institute mathematics applications university minnesota say positive definite matrix h graded diagonal much better conditioned h roy mathias structure related h h 1 2 fact systematically applied obtain componentwise perturbation bounds eigenvectors graded positive definite matrices componentwise bounds accuracy eigenvectors computed jacobis method fact matrix eigenvectors graded observed 1 2 however results weaker papers exploit graded structure great extent basic results gradedness eigenvectors section 31 applications section 32 let mmn denote space theta n real matrices let mn j nn symmetric matrix h let 1 eigenvalues ordered decreasing order singular values norm use spectral norm 2norm denote k delta k ie x say matrix unit columns mean columns unit euclidean norm matrix vector x jxj denotes entrywise absolute value two matrices vectors x dimensions use minfx g denote entrywise minimum use x mean entry x smaller corresponding entry differentiate componentwise positive semidefinite orderings use b mean b positive semidefinite use e denote matrix ones e denote column vector onesthe dimension apparent context studying perturbation theory eigenvectors use two notions relative gap eigenvalues introduced 1 use different notation given positive vector define relgap one similarity two relative gaps sufficient take minimum either case however easy see relgap 1 relgap arbitrarily large show end section relgap unfortunately result perturbation relgap complicated sometimes complicates analysis results involving relgap see 2 proof proposition 26 instance 2 mean kd much larger 1 1 diagonal matrices diagonal elements 1 use quotes usual definition gradedness nonetheless related gradedness h h 1 perturbation bounds positive definite matrices 3 clear relative gap one use whether one use perhaps relative gap used 4 2 suggested relgaph appropriate measure relative gap h rest eigenvalues h relgapoeg appropriate measure relative gap oe g rest singular values g eigenvector results theorems 35 29 corollary 210 singular vector results theorem 28 suggest case luckily one interested relative gap small case doesnt make much difference definition one chooses example one check one also check left hand inequality always valid simple application arithmeticgeometric mean inequality let us prove 11 define f 0 1 2 relgap order prove 11 sufficient prove 1 must without loss generality bound 13 implies 1 2 since writing 16 one sees f function ff 1 ff 2 minimized substituting values ff 1 ff 2 substituting expressions 15 16 14 see sufficient prove 4 roy mathias equivalently equivalent left hand side 19 increasing function ffi order verify 19 sufficient verify ffi large possible straight forward algebra shows 19 holds equality one substitutes value ffi thus verified 11 bound 11 slight improvement 7 proposition 33 equation 38 case 2 unified approach section give unified approach inequalities 2 1 approach also allows one bound relative perturbation eigenvalues positive definite matrix caused perturbation particular entry key idea section express additive perturbation h deltah multiplicative perturbation h given multiplicative perturbation matrix quite natural perturbation eigenvalues eigenvectors also multiplicative small step multiplicative perturbation componentwise perturbation bounds desire two ways write deltah multiplicative perturbation possible choice h 1 one wants prove eigenvalue inequalities seems representations give bounds one uses representation 21 ostrowskis theorem 6 theorem 459 yields relation eigenvalues h h route taken 4 shall use 22 monotonicity principle theorem 21 proofs slightly quicker demmel veselic 2 barlow demmel 1 used courantfisher minmax representation eigenvalues hermitian matrix derive similar results jacobis method one encounters positive definite matrices diagonal much better conditioned h reason demmel veselic assumed matrices ddeltaad diagonal data work 2 consider slightly general situation assume h h deltah positive definite consider general setting firstly show one prove relative perturbation bounds positive definite matrices without assuming matrices graded secondly results slightly cleaner general case perturbation bounds positive definite matrices 5 example statement theorem 29 deals general case cleaned statement corollary 210 deals special case diagonal lemma 22 allows us derive results corollaries theorem 21 monotonicity principle 6 corollary 433 let b 2 mn following lemma useful applying general results special situations lemma 22 let h positive definite let deltah arbitrary let 2 mn furthermore proof since must orthogonal matrix q q thus second part lemma take 2 apply first part required 2 note diagonal applications using notation lemma 22 bounds terms j demmel veselic 2 terms larger quantity ka assumed diagonal elements 1 always necessary though good choice approximately minimizes ka assume diagonal elements 1 necessary 21 eigenvalues singular values main eigenvalue perturbation theorem theorem 23 let h h deltah 2 mn positive definite let k proof write 2 since 6 roy mathias monotonicity principle theorem 21 gives required bounds 2 using second part lemma 22 obtain result essentially 2 theorem 23 theorem 24 let positive definite assume diagonal let k another corollary monotonicity principle useful relation diagonal elements positive definite matrix eigenvalues 2 proposition 210 corollary 25 let mn positive definite matrix assume diagonal main diagonal entries 1 main diagonal entries h ordered decreasing order proof since n ai 1 ai follows n ad 2 dad 1 ad 2 matrix 2 diagonal eigenvalues diagonal elements n result follows monotonicity principle 2 one would expect eigenvalues h sensitive perturbations entries h less sensitive perturbations others stating bound terms allows one derive stronger bounds sensitivity eigenvalues h perturbation one entries two corresponding offdiagonal entries h replaced j us assume notation theorem unit nvector ith component equal 1 suppose relative perturbation ffl jth main diagonal entry fact say monotonicity principle know lower bound 25 taken 1 vice versa ffl 0 ka quite possible values j bound 25 much better 23 j replaced perturbation entries ij ji perturbation bounds positive definite matrices 7 taking one may hope prove bound ja instead 2 see bound possible consider case diagonal elements gamma1 0 clearly perturbing offdiagonal element change eigenvalues dad one obtain similar bounds perturbation eigenvectors singular values singular vectors caused perturbation one elements matrix case eigenvectors singular vectors one obtain normwise componentwise bounds bounds singular values singular vectors involve row b full rank rather one element inverse pseudoinverse 22 eigenvectors singular vectors let us see approach gives normwise perturbation bounds eigenvectors graded positive definite matrix terms relative gap eigenvalues let h positive definite let u orthogonal matrix jth column eigenvector h corresponding j h let diagonal matrix ii element h h first part lemma 22 implies u eigenvector 1 2 eigenvector h deltah vector eigenvector h normwise difference u u show u chosen ku gamma uk small must show u chosen close e j lemma 26 follows easily standard perturbation theory given 5 pp 3456 used fact u orthogonal 28 hence norm 1 obtain normwise bound u gamma u section 32 use componentwise bounds u derive componentwise bound u gamma u lemma 26 let diagonal elements ordered decreasing order assume j1 symmetric matrix let 8 roy mathias fflx ffl sufficiently small distinct one choose uffl eigenvector hffl take 2 lemma 26 one see coefficient ffl right hand side 211 bounded x element delta substituting j kdeltak 27 get relgap 28 follows bound ku gamma uk summarize argument following theorem theorem 27 let h 2 mn positive definite let k let assume j 0 simple eigenvalue h let u corresponding unit eigenvector h sufficiently small ffl eigenvector uffl hffl corresponding j ffl relgap mentioned earlier may replace j ka gamma1 kkdeltaak resulting bound improves 2 theorem 25 factor eisenstat ipsen also give bound perturbation eigenvectors involves relative gap 4 theorem 22 bound relates eigenvectors h khk k 2 mn nonsingular absolute bound first order bound obtain bound form 212 4 theorem 22 one must find bound kh form shown 9 213 hold n theta n h deltah h positive definite constant c must depend n must grow like log n direct application 4 theorem 22 present situation yield 212 however one derive 212 using idea behind proof 4 theorem 22 careful argument 3 perturbation bounds positive definite matrices 9 veselic author used ideas similar section prove nonasymptotic relative perturbation bound eigenvectors positive definite matrix 13 one apply lemma 26 gg g g thereby remove factor bound perturbation right left singular vectors given 2 theorem 216 note one must apply lemma 26 directly order obtain strongest result one applies theorem 27 g g resulting bound contains extra factor notice bound right left singular vectors bound right singular vectors potentially much smaller since relgap much larger relgap theorem 28 let g g deltag 2 mn let g pseudoinverse g assume g rank ng deltag deltagg g assume oe j g simple let u v left right singular vectors g corresponding oe j g sufficiently small ffl left right singular vectors gffl uffl vffl corresponding oe j gffl relgap oe 2 g proof let u sigmav singular value decomposition g u v square sigma rectangular first let us consider right singular vectors eigenvectors g g hence norm 2kdeltagg 211 one choose u jth eigenvector sigma differs norm e j first order ffl hence bound ku gamma v uk first order ffl vector v u eigenvector corresponding jth eigenvalue equal g fflgffl offl 2 terms since jth singular value gffl simple follows v u right singular vector gffl offl 2 let us consider left singular vectors show roy mathias u norm j 211 eigenvector differs e j norm first order ffl way deduce vector vffl distance v 2 23 distance nearest illposed problem shown 1 proposition 9 relgaph approximately distance h nearest matrix multiple ith eigenvalue case h scaled diagonally dominant symmetric matrix distances measured respect grading h show similar result positive definite matrices theorem 29 show relgaph exactly distance nearest matrix repeated ith eigenvalue use norm k strengthen 1 propostion 9 corollary 210 upper lower bounds distance differ factor 1 proposition 9 differ factor 4 potentially large difference bound considerably simpler 1 doesnt involve factors n although one could replace n validity doesnt depend value relative gap bound 1 requirement diagonal examples show every eigenvalue h maximum sensitivity gamma1 difference upper lower bounds expected say one cannot hope improve bound 217 factor n bound involves relgap bound 1 involves relgap reasons suggest relgap gamma1 relgap gamma1 right measure distance nearest problem repeated ith eigenvalue theorem 29 let h positive definite let h simple eigenvalue h relgap deltah multiple eigenvalue h proof first show ffi relgap h let deltah perturbation attains minimum definition ffi k let n theorem 23 know since deltah multiple ith eigenvalue index j 6 0 216 must perturbation bounds positive definite matrices 11 implies relgap h show choose value j one easily show possible set x x j unit eigenvectors h corresponding j one check deltah x x j eigenvectors h x x j orthogonal follows required 2 corollary 210 let positive definite assume diagonal main diagonal entries 1 let h simple eigenvalue h relgap deltah multiple eigenvalue h deltahg proof lemma 22 follows result follows theorem 29 2 3 eigenvector components shown 1 eigenvectors scaled diagonally dominant matrix scaled way matrix essentially proof yields 2 proposition 28 strengthen factor corollaries 32 33 section 32 strengthen many results 2 using stronger results section 31 show growth factor error bound eigenvectors computed jacobis method linear rather exponential theorem 38 also give improved componentwise bounds perturbation singular vectors theorems 36 37 essential diagonal section considering components eigenvectors roy mathias 31 gradedness eigenvectors give simple results graded structure orthogonal matrix transforms one graded positive definite matrix another use derive results eigenvectors graded positive definite matrix lemma 31 let h main diagonal entries 1 diagonal assume h 0 2 mn h 1 2 mm positive definite proof easy check using fact main diagonal entries 1 2 mm 1 first inequality monotonicity principle theorem 21 applied n 1 second taking square roots dividing 1n asserted bound 2 matrix c orthogonal h companion bound stated next result corollary 32 let h main diagonal entries 1 diagonal assume u orthogonal says orthogonal matrix u transforms h 0 h 1 n gamma1 small u graded structure special case u matrix eigenvectors obtain n useful bounds individual entries u state variety bounds 3537 note actually weaker normwise bounds 34 bounds 3537 stronger 2 proposition 28 1 proposition 6 factor 32 rather 1 2 right hand side result 1 however applicable scaled diagonally dominant symmetric matrices result positive definite matrices corollary 33 let positive definite assume diagonal main diagonal entries 1 let u orthogonal matrix diagonal diagonal entries r perturbation bounds positive definite matrices 13 r r first inequality stronger second third proof fact ju ij j larger first second quantity right hand side 35 follows first second inequality 34 remaining inequalities derived 35 using relations eigenvalues h main diagonal entries corollary 25 also shows weaker 35 2 another way state bound 35 minimum taken componentwise recall e matrix ones 32 applications graded eigenvectors use results section 31 give another proof fact components eigenvectors graded positive definite matrix determined high relative accuracy show relgap h good measure distance graded matrix nearest matrix multiple ith eigenvalue distance measured norm respects grading finally jacobis method indeed compute eigenvectors accuracy improving 2 theorem 34 combine lemma 26 general technique used section 2 obtain lemma useful proving componentwise bounds eigenvectors singular vectors lemma 34 let diagonal elements ordered decreasing order assume j1 symmetric matrix let u orthogonal matrix eigenvector h j h0 associated j let u upper bound jth eigenvector r g ffl sufficiently small simple one choose uffl unit eigenvector hffl corresponding j ffl relgap proof since u matrix eigenvectors h bound 35 gives r r g note vector u defined statement theorem jth column matrix u defined 310 lemma 26 follows eigenvector uffl 14 roy mathias r vector given element x let uffl must bound ur ith element ur min r r min r r min r final equality note quantity min r independent k defined statement lemma 2 result gives componentwise perturbation bounds eigenvectors singular vectors simple corollaries theorem 35 let positive definite let assume simple eigenvalue h let u corresponding unit eigenvector h let u upper bound jth unit eigenvector r g sufficiently small ffl j ffl simple unit eigenvector uffl hffl corresponding j ffl relgap perturbation bounds positive definite matrices 15 proof write u matrix eigenvectors h implies j asserted bound follows lemma 34 2 lemma 34 also yields componentwise bound singular vectors theorem 36 let mn rank n let assume oe j simple v corresponding unit right singular vector let v upper bound jth right unit singular vector g sufficiently small ffl oe j ffl simple unit right singular vector vffl gffl corresponding oe j ffl relgap oe proof let orthonormal columns sigma 2 mn positive diagonal v 2 mn orthogonal may write pseudoinverse b note kfk 2j since jth singular value g simple corresponding singular vector differentiable particular vffl jth singular vector gffl eigenvector gffl gffl v jth eigenvector v sigma differ offl 2 according lemma 34 know relgap oe hence bound vffl 2 improves 2 proposition 220 two ways firstly upper bound v j smaller 2 factor oe gamma1 n b secondly factor denominator 2 factor oe 2 bound smaller factor oe gamma2 b latter difference arises 2 authors used equivalent theorem 35 applied g g whereas use lemma 34 quantity relgap oe hard deal one perturbs g hence also singular values would convenient relgap oe bound easy check relgap oe worth stating stronger form inequality 311 natural g cholesky factor positive definite h case 12 case oe 2 roy mathias componentwise bound left singular vectors cannot get componentwise bound difference left singular vectors bd b deltabd give result componentwise perturbations singular vectors bound stronger 2 propositions 219 220 factor oe gamma3 upper bound v smaller 2 factor oe gamma2 n b denominator contains factor oe n b 2 contained oe 2 b could give improved bound eigenvectors also restrict case singular vectors important one uses one sided jacobi compute eigenvectors positive definite matrix high componentwise relative accuracy theorem 37 let rank n assume positive b unit columns choose ng let v unit right singular vector corresponding oe g let deltabd set vector v right singular vector g deltag proof statement v upper bound v follows 35 let tdeltag condition ensures oe gt simple differentiable vt right singular vector gt 312 componentwise bound dt vt bt unit columns dt positive diagonal bound jv gamma need bound quantities depend integrate bound using fact one show 2 0 1 relgap oegt one check condition necessarily less 1 use final inequality display perturbation bounds positive definite matrices 17 using 35 first inequality bounds oe j gt oe n bt subsequent inequalities substituting bounds 314 gives dt integrated yields asserted inequality 2 right handed jacobi one computes singular values g 0 2 mn generating sequence g orthogonal matrix chosen orthogonalize two columns g one stops one obtain right singular vectors g accumulating j demmel veselic show 2 theorem 34 implemented finite precision arithmetic algorithm gives individual components eigenvectors high accuracy relative upper bounds actually two sided jacobi proof essentially 1 sided jacobi however bound contains factor say linear growth far likely exponential growth next result show growth indeed linear one prove analogous result two sided jacobi applied positive definite matrix let us denote product j j j ik key idea allows us derive growth factor linear rather exponential bound j ik directly rather bound jj ik j jj jjj bounding terms right hand side idea used profitably 11 also theorem 38 let g unit columns diagonal assume j orthogonal assume columns gm almost orthogonal sense gm satisfies 315 tolerance tol let assume computed column j 0m gamma1 corresponding oe j g unit right singular vector u g roy mathias corresponding oe j g first order j ffl tol oe gamma2 min relgap oeg upper bound u 35 bound 316 first order bound proof would also yield bound takes account higher order terms resulting inequality would much complicated probably useful g j arise right handed jacobi applied g finite precision arithmetic precision ffl one take theorem 42 ensuing discussion let us compare bound relgap oeg min essentially bound computed right singular values 2 theorem 44 stated notation bound stronger several respects term qm n growth factor exponential bound linear mentioned earlier upper bound vector u 317 larger u factor oe gamma2 n b could quite significant also two terms one oe gamma2 min oe min delta relgap oeg quantities less oe 2 min relgap oeg occurs 317 weakness bounds contain factor oe gamma1 min defined statement theorem rather oe n b observed experimentally 2 section 74 oe min oe n b generally 1 close 1 rigorous proof fact known mascarenhas shown ratio large n4 8 one also see given ffl take tol stopping tolerance large ffloe gamma1 significantly increasing right hand side 316 typically suggested one take tol modest multiple ffl one wants compute eigenvectors eigenvalues high relative accuracy 2 thus larger value tol may useful practice save little computation earlier termination rest paper devoted rather lengthy proof theorem proof outline proof follows first bound ju gamma uj u value jth column j 0m gamma1 computed exact arithmetic next true exact singular vector g associated oe j g inequality 316 follows combining two bounds use facts oe j gm min consider uj bound depends scaling j ik independent relgap oeg j xy 2 mn multiplied floating point arithmetic precision ffl result xy using fact one show induction 3 perturbation bounds positive definite matrices 19 error multiplying j 0igamma1 j first order effect error computed value j 0m taking absolute values 318 gives componentwise error bound 0 jj noe noe noe noe min e recall e denotes matrix ones used first term 38 fact first order j i1m gamma1 diagonalizes g first inequality 32 twice third since gm orthogonal columns otol singular values gm g oj follows dm sigma least first order multiplying 0 sigma gamma1 first order min ffle way obtain first order bound min ffle two bounds combined give min minimum taken componentwise note jth column inequality desire u completes first step let us bound error u singular vector g columns gm orthogonal particular e j would right singular vector associated oe j gm addition deltag 0 would right singular vector associated oe j g neither hypotheses true though case almost true u close singular vector g 0 bound difference first consider fact columns gm exactly orthogonal write entry tol absolute value tol equation 320 implies orthogonal matrix q one check roy mathias use bound later consider effect deltag easy check induction example using assumption kdeltag first inequality 32 second together 321 yields combine two results show gm deltag right singular vector close e j hence g 0m gamma1 right singular vector close right singular vectors gm deltag qgm orthogonal matrix introduced equation 320 also jth right singular vector dm e j theorem 37 right singular vector v g deltag corresponding jth singular value let us drop second order terms 322 v term otoloe 1 bm may drop first order terms v ratio 322 particular may replace oe n bm 1 perturbation bounds positive definite matrices 21 assume relgap oeg j large respect j cannot replace relgap oeg however shown end introduction relgap oeg min hence relgap oeg substitutions obtain bound equivalent 322 first order relgap oeg convenience let coefficient v 323 denoted c construction right singular vector g 0 corresponding oe j g 0 complete proof bounding used slight generalization 35 penultimate inequality dropped second order terms last inequality similarly combine bound ju gamma u acknowledgment thank referee whose detailed comments including observation grading necessary relative perturbation bounds greatly improved presentation results paper r computing accurate eigensystems scaled diagonally dominant jacobis method accurate qr personal communication relative perturbation techniques singular value problems matrix computations matrix analysis relative perturbation theory eigenvalue singular value variations note jacobi accurate qr bound matrix square root application eigenvector perturbation matrix anal accurate eigensystem computations jacobi methods instability parallel prefix matrix multiplication fast accurate eigenvalue computations graded positive definite matrices relative perturbation bound positive definite matrices tr