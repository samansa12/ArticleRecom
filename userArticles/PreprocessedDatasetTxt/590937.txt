probabilistic pattern matching evolution stochastic regular expressions use genetic programming probabilistic pattern matching investigated stochastic regular expression language used language features statistically sound semantics well syntax promotes efficient manipulation genetic programming operators algorithm efficient string recognition based approaches conventional regular language recognition used attempting recognize particular test string recognition algorithm computes probabilities generating string prefixes given stochastic regular expression promote efficiency intermediate computed probabilities exceed given cutoff value preempt particular interpretation paths hence prune unconstructive interpretation experiments recognizing stochastic regular languages discussed application technology bioinformatics progress b introduction language inference classical problem machine learning continues important active research topic basic problem given set example behaviours strings automatically infer corresponding language grammar automata expression generates recognizes examples genetic algorithms ga genetic programming gp applied towards languages inference varying degrees success although successful inference possible generic inference problem entirely wellsuited solution evolutionary search number reasons example genome encodings preserve useful language characteristics crossover even small local changes genomes catas trophic lend well genetic reproduction evolutionary search even acute weakness nothing problems language inference problem entirely natural gp acceptable language inference minimally requires solution language correctly recognize positive test cases reject negative ones essential criteria may also supplemented efficiency con cerns relatively small number states grammar rules resulting search space difficult one navigate evolutionary techniques due stringent requirements language correctness completeness hand generally recognized gp community problems require acceptably close solution typically best candidates successful solution gp pragmatically speaking giving fitness function larger degree freedom evaluating successful solution substantially increase chances discovery acceptable solutions research addresses inference stochastic regular languages using genetic programming stochastic languages formal languages probability distribution associated language set stochastic language inference problem similar classical inference problem additional requirement distribution strings recognized stochastic language conform desired target distribution first may seem intuitively complex nonstochastic language inference since unclear impact determination probability distributions turns however inclusion string distributions simplify inference problem hypothesized languages allowed generate erroneous strings long fall within acceptably small probability occurence words use language distributions introduces generous degree freedom generated solutions ideal gp setting simplifies search space substantially evolutionary search target language used probabilistic regular expression language henceforth called stochastic regular expressions sre although theoretically weaker stochastic contextfree languages studied elsewhere nevertheless chosen due amenability concise gp representation ability naturally solve substantial number problems regular language domain stochastic regular expression language closely related stochastic regular grammars stochastic finite automata latter commonly referred hidden markov models literature sre language implementation issues addressed gp could successfully applied stochastic regular expression problems firstly efficient implementation sre interpretation necessary interpretation sre expression requires probability recognizing given string generated since intermediate probabilities would computed interpretation string values used terminate prune unproductive interpretation paths whose probabilities smaller supplied cutoff probability given extensive testing necessary fitness evaluation pruning greatly increases speed gp runs sre language implemented grammatical gp system permitted use syntactic language constraints enhance evolution efficiency two example experiments proved probabilistic language inference indeed possible sre gp complex experiments indicated complex search space often resulted premature convergence minor language enhancement experiment resulted failed inferences gp system experience deduced fitness evaluation strategy used general purpose solution stochastic language problems rather suitable class stochastic regular languages whose members structurally related one another outline paper follows related work reviewed section 2 section 3 defines syntax semantics stochastic regular expression language discusses algorithm processing sre expressions section 4 outlines genetic programming system used two example experiments discussed section 5 discussion future directions conclude paper section 6 formal language induction long history fundamental problem machine learning booth 1975a fu booth 1975b angluin 1992 sakakibara 1997 specialized topic stochastic languages also studied time fu huang 1972 stochastic grammar differs conventional grammar grammar rule marked probability associated use set probabilities grammar encode probability distribution resulting derived language fu 1982 extensive treatment stochastic grammars derivation application pattern recognition stochastic grammars also complex nonstochastic kin distributions inherent language introduce new dimension membership criteria example context free languages also stochastic context free languages probabilities 1 however may many stochastic context free languages essentially membership set vastly different distributions set language equivalence issues therefore discriminating nonstochastic setting stochastic context free languages enjoy expressitivity tractable properties example existence useful inference algorithms lari young 1990 also found practical use language processing charniak 1993 stochastic regular languages albeit descriptively weaker stochastic contextfree languages also found practical niche applications regular languages definable finite automata regular grammars regular expressions hopcroft ullman 1979 similarly stochastic regular languages defined stochastic versions three representations examples work stochastic grammar inference maryanski booth 1977 van der mude walker 1978 carrasco forcada 1996 carrasco oncina 1998 stochastic finite automata defined terms hidden markov models hmm rabiner juang 1986 hmm finite automaton probabilities marking transition links nodes node connected nodes network maximally connected particular transitions required probabilities associated nodes set zero hmms found extensive use language speech processing rabiner 1989 charniak 1993 strangely enough stochastic regular expressions extensively studied one example paper garg et al 1996 language inference successfully done using genetic algorithms ga genetic programming gp distinguishing difference ga gp approaches one denotation pure ga uses binary encoding genome gp uses variablesized parse tree following use encodings characteristics approaches respect regular languages early work evolving finite automata zhou grefenstette 1986 used ga binary encoding automata set state transitions capped size 8 states weakness encoding represented automata susceptible destructive effects crossover mutation unspecified fitness function scores language performance ability accept positive strings reject negative examples automata size dunay et al 1994s approach similar zhou grefenstette 1986 except finite automata denoted gpstyle nested sexpression notation dupont 1994 uses automatatheoretic partition representation regular languages advantage preserving language properties chromosomes ga reproduction unlike fragile fa represention zhou grefenstette 1986 fitness function scores language performance automata size successfully evolved large set regular languages including benchmark tomita languages tomita 1982 brave 1997 uses abstract cellular encoding representation deterministic fas builds network structure fa interpretation intention denotation preserve structural properties language evolutionary production automata embellished boolean operators permit automata composition fitness function tallied number correctly classified sentences one tomita languages successfully inferred using technique longshaw 1997 uses straightforward statetransition representation au tomata however ga uses population seeded correct overly general au tomata specialized reproduction operators manipulate automata duplicating refining states overall intention refine general automata specific one language question fitness function scores example classification performance automata size svingen 1998 uses gp regular expressions regular expressions directly encoded program trees fitness based correct example classification successfully evolved tomita languages contextfree languages also studied wyard 1991 uses ga evolve contextfree grammars chromosomes takes form lists production rules guarantees correctness times fitness function scores example classification per formance two simple cfgs successfully evolved lankhorst 1994 uses vector encoding represent productions fitness function involved others scores example classificaton perfor mance length substrings examples correctly classified degree determinism grammars ability grammar generate correct strings included example set additional evaluation considerations give ga information drive evolution applied ga number cfg regular languages lucas 1994 uses binaryencoded normal form cfg productions preserves language properties reproduction promotes convergence fitness strategy scores example classification grammar size sen janakiraman 1992 applies ga towards inferring deterministic pushdown automata alternative grammar representation cfgs fitness scores example recognition performance whether pda attempts erroneously pop empty stack lankhorst 1995 extends idea towards nondeterministic pushdown automata fitness additionally considers prefix sizes stack size string consumed dunay petry 1995 use turing machine representation ga exper iments although powerful notation denote entire set languages chomsky necessary mean search easy accomplish given inherent enormity search space question solve relatively simple examples regular contextfree contextsensitive languages used compositional approach ga access tm building blocks evolved earlier runs finally evolution stochastic languages studied schwehm ost 1995 uses ga evolving stochastic regular languages two different encodings studied production rules probabilities quotient automata fitness function uses grammar complexity number productions modified 2 test distribution conformance measure grammars ability accept prefixes target grammar experiments performed ga performance compares well standard regularlanguage inference algorithms kammeyer belew 1997 uses ga evolve stochastic contextfree gram mars use liberal representation grammars correct grammars parsed genome evaluated permits intron junk material included chromosomes fitness function evaluates size test example prefixes consumed grammar uses crossentropy evaluate distribution conformance also use local search technique finding production probabilities evolution couple cfgs successfully evolved 31 language definition target language gp system stochastic regular expressions sre language similar one garg et al 1996 used modeling qualitative behaviour stochastic discrete event systems amongst properties prove probabilistic regular language operations choice concatenation kleeneclosure forms closed language hence algebra although basic properties illustrated reader referred garg et al 1996 details assumed reader familiar basic concepts formal language theory hopcroft ullman 1979 two language variations sre guarded sre gsre used first define sre let ff range alphabet positive integers 0 n 1000 f range decimal values precision 2 decimal places 0 f 100 syntax sre recursively defined without loss generality empty string ffl included alphabet operators following meaning 1 atomic action ff action ff generated 2 choice denotes probabilistic choice terms choice expression chosen probability example given expression e 5 term e 1 chosen probability 38 e 2 probability 58 3 concatenation e followed e 2 4 kleene closure repeatedly executed 0 times iteration occurs probability f probability e terminating execution 5 closure repeatedly executes 0 times using probability scheme kleene closure closure abbreviation following guarded sre language identical sre except guarded choice operator used instead general choice 2 6 guarded choice term choice expression either prefixed unique atomic action found nowhere else expression consists unique action makes guarded choice deterministic unlike sres nondeterministic choice note even guarded choice gsre still nondeterministic language since closure operators nondeterministic rest discussion section pertains equally sre gsre derivation conventional regular expression e set sentences strings alphabet derivable defines language le e notion language derivation similarly applicable sre except string probability value associated hence language associated probability distribution members alternatively intuitive way consider sre expressions every expression defines specific probability function strings sigma using denotational semantics style representation stoy 1977 probability function sre expression e denoted e application particular string denoted es denotes probability associated string language le probability function model sre given let ffl atomic actions 1 ffl choice including guarded choice 2 since every term might recognize overall probability choice expression sum term probabilities respect ffl concatenation first summation decomposed two substrings may consumed concatenated expression even though one term may recognize substring argument term recognize respective substring term returns probability 0 overall probability instance decomposition 0 rest formula represents cases one entire expression consumes consumes ffl cases succeed return 0 ffl kleene closure first formula accounts empty strings way iterated expression recognize empty string iterating formula recursively defines general case one iteration e consume portion rest consumed iterations final term formula represents first iteration consumes entire string assumed iteration loop always consumes nonempty string otherwise semantic model would account kleene closure iterating indefinitely argument useful behaviour ffl closure similar nonempty argument formula kleene closure except expression e consume part string iterations commence seen lack f value formula nondeterministic nature regular expressions modeled multiple argument decomposition concatenation closure operators nondeterminism also arise nonguarded choice operator membership sre reflected sre expressions returning nonzero probabilities particular strings definition 31 probability functions pf must adhere following two characteristic subrahmaniam 1979 x sample space experiment ii every event 7consequently sre expressions define wellformed probability functions expressions must similarly respect requirements theorem 31 sre operators wellformed probability functions proof proof uses structural induction sre expressions show conditions ii defn 31 hold operators let 2 sigma atomic actions trivially b choice equations 2 induction hypothesis x thus x ii equation 2 greatest value sum occurs k case sum reduces equivalently summation zero 0 resulting summation fraction 0 1 hence probability c concatenation equation using equation 3 ranges sigma becomes translates induction hypothesis simplifies ii given concatenation induction hypothesis e 1 hence product must likewise probability kleene closure starting equation using equations 4 translates follows inductive hypothesis algebraic manipulation f note division f gamma 1 permitted f 1 definition ii induction length string shown base case ffl case probability f first equation 4 1 arbitrary 6 ffl probability second equation 4 incorporating second term first terms summation rewritten inductive hypothesis probability furthermore structural induction expressions probability hence product f probability closure similar c above32 implementation sre processor given regular expression determining whether particular strings members language tractable problem sipser 1996 hopcroft ullman 1979 different ways may performed one technique convert regular expression equivalent nondeterministic finite automaton done polynomial time done graphsearching algorithm reads string character charac ter marking states fa still elligible paths towards acceptance state advantage fa approach nondeterministic fa polynomially time translated deterministic fa efficient recognition characteristics language recognition alternatively regular expressions symbolically interpreted directly behaviour regular expression operator corresponding operational semantics used define regular expression interpreter may done perspective either language generation language acceptance one technical requirement expression interpretation approach interpreter must able handle nondeterministic nature expression derivations since regular expressions naturally nondeterministic nature expression interpretation similar fa approach mapping states translated fa derivation paths used interpreter processing expression stochastic regular language recognition uses basic recognition schemes conventional regular languages additional requirement probabilities computed strings example fa derived stochastic language links marked probabilities overall probability accepting given string computed computing product transition probabilities used start state final accepting state probabilistic fa known hidden markov model hmm rabiner juang 1986 therefore given stochastic regular language defined sre formulae section 31 incorporated translated fa sre recognition system uses expression interpretation approach described operational semantics use two relations one relation gamma e theta sigma p theta e p probability represents single action transitions expressions relation denoted relation e theta sigma p theta e transitive closure gamma models generation strings figure contains transitional rules relations define structural operational semantics sre operators hennessy 1990 inference rules define abstract interpreter sre expressions basis sre recognizer fact languages prolog rules compiled prolog statements directly interpreted using prologs inference engine clocksin mellish 1994 furthermore multiple solutions obtained nondeterministic sre expressions using prologs builtin backtracking mechanism actual implementation sre processor uses fundamental ideas operational semantics implemented superset rules figure 1 action ff f ffp closure figure 1 transitional semantics sre implementation uses logical grammar definition sre part dctggp system ross 1999 see section 4 prologs backtracking advantageously used investigate different paths expressions derivation addition string recognition performed pattern matching argument string generated string shown transitional semantics match occurs current derivation path correct mismatches cause current derivation backtrack test another possible nondeterministic path example one instance backtracking may try different terms choice expression another may unwrap iterative expression varying number times backtracking assured terminating finite size input strings checked well assertion within sre semantics empty strings ffl never generated within generative component iterative operators generated iteration terminates one advantage stochastic language computed probabilities strings used efficiency mechanism expression recognition implementation sre recognizer probability intermediate strings always known throughout interpreter current probability becomes smaller usersupplied threshold current derivation path forced terminate prunes derivations expression yield probabilities small conse quence course setting threshold large results inaccurate probability values recognized strings may even erroneously reject legal strings however many experiments especially large strings recognized speeds processing significantly 41 grammatical sre gsre gp system used sre experiments dctggp system ross 1999 dctggp performs grammarbased genetic programming target language evolved program population defined terms contextfree grammar lucas 1994 whigham 1995 wong leung 1995 geyershulz 1997 major advantage grammatical gp systems search space syntactically constrained evolution given helpful push towards program structures sensible problem hand grammar used dctggp definite clause translation grammar dctg abramson dahl 1989 dctg logical version contextfree attribute grammar dctg production syntactic component defines contextfree syntax rule addition production included one semantic components semantic component defines characteristic syntactic component attached example one important sre characteristic defined dctg grammar string recognition algorithm section 32 since operational semantics sre operators modular nature recognition behaviours encoded grammar rules define syntax operators overall result compact definition sre language syntax semantics conveniently unified together one syntactic constraint applied sre gsre experiments section 5 following although specified grammar sre gsre grammatical definition sre disallows iterative operators directly nested within one another words expressions allowed reason restriction pragmatic one gp performed without restriction many programs multiply nested iterative expressions expressions relatively expensive interpret due variety nondeterministic paths possible interpreting addition nested iteration typically results strings low probabilities since probabilistic factor f associated executing every nested iterative expression moreover expense nested iteration justified results since expressions replaced semantically equivalent expression uses one iterative operator restriction imply expression like illegal since concatenation operator means iteration operators directly nested gsre also encoded syntactic constraint sre rather permitting sre expression term within choice operator uniquely guarded terms permitted 42 gp system details dctggp uses standard gp strategies tournament roulettewheel selection steadystate generational evolution relevant experimental parameters illustrated section 5 system implemented sicstus prolog 3 windows silicon graphics platforms 51 general strategy inference stochastic language considered involve two different objec tives given training set positive possibly negative examples one task infer language correctly classifies training examples equivalent nonstochastic language inference additional task required stochastic language ference however ascertain stochastic distribution training examples one might naively presume statistical analysis training set could performed results applied inferred language unfortunately situation typically complicated representation stochastic language used hypothesis likely permit straightforward application final string distributions internal encoding example hmm representation used hypotheses finding appropriate probability values intermediate links network correspond example set distribution challenging task significance problem determining distributions hmms contextfree languages spawned specialized training algorithms lari young 1990 charniak 1993 inference strategy undertaken gp experiments let evolution determine stochastic distributions concert example classification since sre incorporates probability values directly expressions treating numeric probability fields straightforward gp found approach sufficient many experiments undertaken fact discovered evolution using local search finetuning probability parameters lent advantage simple evolution parameters training sets used gp experiments consist sets positive examples target language inferred member set string along frequency respect total number strings set typically 1000 since format target languages already known via stochastic regular expression gram mar generating sets straightforward unlike conventional language inference probability distributions training example sets permits stochastic languages forgo need negative examples inference distribution matches training set automatically account negative examples 0 probability distribution stochastic language inference incorporates implicit degree error inferred solution ramifications gp fitness evaluation described also used boost efficiency computations performed inference detailed section 32 string recognition preempted intermediate probabilities become smaller threshold limit set experiment similarly test set pruned strings whose frequency limit set user limit parameter set recognition threshold mind example threshold set 0001 test set limit could likewise set 1 test set size 1000 course may many nondeterministic derivations expression recognizing string probababilities derivations summed overall probability string less discriminating recognition threshold test set limit precise albeit slower results since gp experiments use steadystate algorithm discrete generations convenience however new generation said occurred every k reproductions k population size generations test set regenerated prevents overfitting one set test data reflects nature stochastic languages test set reflects sampling actual distribution one disadvantage however discrete test set approximation real distribution language hence introduces unavoidable measure noise noise compensated fact multiple test sets used successive generations cumulative effect reflect accurate model target distribution however population reevaluated newly generated test set fitnesses much population may legacy values earlier generations acceptable test sets used generations presumed statistically valid generation fitness evalution strategy used experiments modified 2 test press et al 1992 known distribution taken set test examples experimental set results sre recognition algorithm member test set example string given sre processor overall probility string computed nonmembership reflected probability 0 fitness formula frequency example test set maximum prefix recognized first term 2 formula used example string completely recognized second formula used prefix recognized value inversely proportional size prefix recognized none recognized value becomes 2 delta normal formula would use prefix scoring gives credit expressions recognize portions examples helps drive evolution towards expressions recognize complete examples 52 experiment 1 stochastic iteration first experiment uses simple stochastic regular language naturally encoded sre main intention experiment test evolvability stochastic kleene closure modeled sre target language stochastic rendition regular language suggested tomita 1 popular benchmarks machine learning tomita 1982 target language written sre nontrivial language especially stochastic domain overall distribution b term strings conform given probability 05 terms may also generate empty strings iterations terminate immediately parameters experiment figure 1 selfexplanatory fitness function strategy discussed earlier section 51 initial population oversampled running population pruned using tournament selection replacement done using reverse tournament selection sample k members randomly selected member lowest fitness selected replaced language b b table 1 parameters parameter value target language gsre fitness function modified 2 generation type steadystate initial population size 750 running population size 500 unique population members yes maximum generations 50 probability crossover 090 probability mutation 010 probability internal crossover 090 probability terminal mutation 075 probability numeric mutation 050 mutation range sigma01 max reproduction attempts 3 initial population shape ramped halfhalf depth initial popn 6 12 max depth offspring 24 tournament size 5 test set size 1000 test string size approx 20 min test example frequency 3 probability limit 00001 mutation performed either terminal nonterminals nonterminal mutated 05 probability numeric field numeric field selected mutation current value perturbed sigma10 entire range numeric type range sigma100 integers sigma01 probabilities test set generated every generation initially 1000 strings generated l 1 frequencies tallied maximum string size approximately 20 may exceed length less 3 instances given string pruned test set means typically 55 strings test set particular frequency particular sample language number unique strings test set important 2 analyses equivalent bin size 2 formula table 2 summary l 1 total runs 15 unique examples avg test set 2 14222 50 cases fitness min 894 2 888 generation fitness average figure 2 fitness curves avg 15 runs summary statistics best solutions 15 runs given figure 2 values obtained using common test set since run used different test set prior evolution average 2 test test sets included order better evaluate expression results 50 pairs random test sets generated one pair fixed independent variable dependent variable sets filtered frequencies minimal test example frequency figure 1 2 computed resulting 50 2 values averaged performance chart best average population fitness averaged 15 runs figure 2 seen convergence local optimum largely occured generation 10 best solution found nearly perfect solution iterative probabilities within range might expected given stochastic error inherent random test sets second best solution 2 8963 last term interesting erroneous choice acute problem given low probability choosing 011 one poorer solutions 2 13285 inaccuracy occurs first term erroneously permits b occur fre quently even though low probability 025 enclosing iteration helps reduce likelihood worst solution obtained 2 20375 note repetition particular numeric fields 320 004 sign population convergence simplifying expression removing iterative probabilities less 010 expanding closure terms becomes obviously suboptimal solution example shows nature introns within expressions virtually expression intron code long associated choice iterative probability low enough 04 figure 3 target language 53 experiment 2 stochastic regular grammar second experiment evolves complex stochastic regular language target language l 2 taken carrasco forcada 1996 defined stochastic regular grammar figure 3 production probability right denotes probability rule selected respect productions nonterminal table 3 summary l 2 total runs 50 unique examples 35 avg test set 2 9975 50 cases fitness min 6639 2 6506 experimental parameters runs identical figure 1 summary 50 runs figure 3 performance plot best fitness average population fitness averaged 50 runs figure 4 best solution simplifying expanding closures removing terms probabilities less 003 generation fitness average figure 4 fitness curves avg 50 runs becomes difficult see expression maps target grammar figure 3 intuitive mapping may even exist however 2 impressive compared test set average 54 limitations many language inference algorithms easily thwarted target languages characteristics antagonistic peculiarities algorithm question often languages subtlely different ones algorithms problems inferring gp paradigm suffers similar limitation variation language section 53 tried language l 2 additional string bbaaabab probability 10 50 runs performed using parameters figure 1 none runs found acceptably close solution best solution fitness 259 40 one reason gp problems evolving l 0 2 attributed linguistic characteristics sre even though definition l 0is concise statement language evolutionary process tries unify term bbaaabab l 2 together regular expression difficult string anomaly respect strings l 2 considering stochastic regular grammar used generate clear strings derived progressively incrementally one another strings l 2 equal length bbaaabab natural extensions smaller strings language anomalous string however derivable l 2 hence natural model union languages sre cannot inferred especially true given bbaaabab 10 probability makes populous member smaller probability might ignored noise must considered light linguistic nature formal lan guages representations naturally model particular languages others even though regular expressions finite automata regular grammars expressive power languages naturally concisely denoted regular expressions finite automata vice versa could case another representation language example hmms may naturally denote l 0 sre 6 conclusion paper presented new means evolving stochastic regular languages using probabilistic version regular expressions language evolution genetic programming capable evolving accurate expressions stochastic regular languages however stochastic regular languages amenable successful evolution others speculated languages members structural similarities one another suitable paradigm complex languages sophisticated evolutionary techniques may required found experimentation sre evolutionary advantage gsre respect quality solutions discovered hand sre expressions less efficient process runs took much longer gsre ones use sre genetic programming context presents advantages evolutionary experiments stochastic languages one advantage sre akin programming language operators syntactic semantic definitions akin conventional languages since gp typically applied towards languages lisp encoding processing sre within gp environment straightforward importantly however sre linguistic advantages finite automata regular grammars stochastic languages naturally encoded sre representations l 1 experiment clear example point linguistic clarity l 2 less apparent although solution overly complex compared target grammar like svingen 1998 work uses regular expression language directly gp work required fairly large populations parallel populations order evolve tomita languages fitness strategy used similar used lankhorst 1994 schwehm ost 1995 language recognition performance prefix consumption taken consideration many directions future work gp strategies used fairly conventional sophisticated approaches may applicable stochastic languages experiments wide degree qualitative variations runs indicates evolution quickly gets stuck suboptimal solutions parallel subpopulations may help regard although found local search using hillclimbing numeric fields advantageous evolution worth investigating utility sophisticated local search techniques akin used stochastic contextfree languages eg insideoutside algorithm currently applicability sre bioinformatics problems investi gated fundamental problem dna protein sequencing determine common pattern shared amongst family sequences brazma et al 1995 used search analytical purposes number techniques hmms regular pattern languages used purpose sre natural vehicle problem area since regular expression basis conforms pattern languages commonly used eg used prosite database hofmann et al 1999 stochastic features conveniently model probabilistic characteristics dna sequences acknowledgement thanks tom jenkyns helpful discussions probability research supported nserc operating grant 1384671998 r logic grammars computational learning theory survey selected bibliography evolving deterministic finite automata using cellular encoding approaches automatic discovery patterns biosequences learning deterministic regular grammars stochastic samples polynomial time inferring stochastic regular grammars recurrent neural networks statistical language learning programming prolog 4th solving complex problems genetic algorithms regular grammatical inference positive negative samples genetic search gig method syntactic pattern recognition applications stochastic grammars languages probabilistic language framework stochastic discrete event systems next 700 programming languages stanford university semantics programming languages elementary introduction using structural operational semantics database introduction automata theory stochastic contextfree grammar induction genetic algorithm using local search grammatical inference genetic algorithm genetic algorithm induction pushdown automata estimation stochastic contextfree grammars using insideoutside algorithm evolutionary learning large grammars structuring chromosomes contextfree grammar evolution inference finitestate probabilistic grammars ieee transactions computers c26 numerical recipes c introduction hidden markov models recent advances grammatical inference inference stochastic regular grammars massively parallel genetic algorithms introduction theory computation denotational semantics primer probability learning regular languages using dynamic construction finite automata examples using hill climbing inference stochastic regular grammars information control learning programs different paradigms using context free grammar induction using genetic algorithms induction finite automata genetic algo rithms ieee press tr ctr ashok argentkatwala jeremy bradley nicholas j dingle expressing performance requirements using regular expressions specify stochastic probes process algebra models acm sigsoft software engineering notes v29 n1 january 2004 rolv seehuus amund tveit ole edsberg discovering biological motifs genetic programming proceedings 2005 conference genetic evolutionary computation june 2529 2005 washington dc usa brian j ross evolution stochastic regular motifs protein sequences new generation computing v20 n2 p187213 april 2002