scalable marksweep garbage collector largescale sharedmemory machines work describes implementation marksweep garbage collector gc sharedmemory machines reports performance simple parallel collector processors cooperatively traverse objects global shared heap collector stops application program collection assumes uniform access cost locations shared heap implementation based boehmdemersweiser conservative gc boehm gc experiments done ultra enterprise 10000 ultra sparc processor 250 mhz 64 processors wrote two applications bh nbody problem solver cky context free grammar parser parallel extension cthrough experiments observe load balancing key achieving scalability naive collector without load redistribution hardly exhibits speedup fourfold speedup 64 processors performance improved dynamic load balancing exchanges objects scanned processors still observe straightforward implementation severely limits performance first large objects become source significant load imbalance unit load redistribution single object performance improved splitting large object small pieces pushing onto mark stack next processors spend significant amount time uselessly serializing method termination detection using shared counter problem suddenly appeared processors implementing nonserializing method termination detection idle time eliminated performance improved careful implementation achieved average speedup 280 bh 286 cky 64 processors b introduction sharedmemory architecture attractive platform implementation generalpurpose parallel programming languages support irregular pointerbased data structures 4 20 recent progress scalable sharedmemory technologies also making architectures attractive highperformance massively parallel computing one important issues yet addressed implementation generalpurpose parallel programming languages scalable garbage collection gc technique sharedheaps previous work gc sharedmemory machines concurrent gc 6 10 17 mean collector dedicated processor runs concurrently application programs perform collection parallel focus shortening pause time applications overlapping collection applications different processors large number processors however collectors may able catch allocation speed applications achieve scalability parallelize collection paper describes implementation parallel marksweep gc largescale 64 processors multiprogrammed sharedmemory multiprocessor presents results empirical studies performance algorithm least conceptually simple allocation requests collection application program stopped processors dedicated collection despite simplicity achieving scalability turned challenging task empirical study found number factors severely limit scalability appear number processors becomes large show eliminate factors demonstrate speedup collection implemented collector extending boehmdemersweiser conservative garbage collection library boehm gc 2 3 two systems 64processor ultra enterprise 10000 16processor origin 2000 heart extension dynamic task redistribution exchanging contents mark stack ie data live yet examined collector present achieved 1428fold speedup ultra enterprise 10000 37 63fold speedup origin 2000 rest paper organized follows chapter 2 compares approach previous work chapter 3 briefly summarizes boehm gc collector based chapter 4 describes parallel marking algorithm solutions performance limiting factors chapter 5 describes experimental conditions chapter 6 shows experimental results conclude chapter 7 chapter 2 previous work previous published work gcs sharedmemory machines dealt concurrent gc 6 10 17 one processor performs collection time focus work scalability largescale mediumscale sharedmemory machines shortening pause time overlapping gc application utilizing multiprocessors gc parallelized collector may fail finish single collection cycle application exhausts heap figure 21 occur largescale machines amount live data large cumulative speed allocation correspondingly high therefore much interested parallel garbage collectors single collection performed cooperatively processors several systems use type collectors 7 16 believe many unpublished work relatively published performance results knowledge present paper first published work examines scalability parallel collectors real largescale multiprogrammed sharedmemory machines previous publications reported preliminary measurements uzuhara constructed parallel mark sweep collector symmetric multiprocessors 22 amount free space shared heap becomes smaller threshold processors start collection processors continue application execution collector processors cooperatively mark reachable objects dynamic load balancing using global task pool sweep heap time memory region reused gc time time application pes concurrent gc parallel gc approach figure 21 difference concurrent gc approach one dedicated processor performs gc collection cycle becomes longer proportion number processors join application workers approach advantage concurrent gc prevent single collection cycle becoming longer largescale machines ichiyoshi morita proposed parallel copying gc shared heap 11 assumes heap divided several local heaps single shared heap processor collects local heap individually collection sharedheap done cooperatively asynchronously collection live data shared heap called fromspace collection copied another space called tospace processor initiative copies data reachable local heap tospace processor copied data reachable local heap resume application processor works new sharedheap ie tospace collector much simpler uzuharas collector ichiyoshi moritas collector simply synchronizes processors collection processors dedicated collection reachable objects marked although ichiyoshi morita mentioned explicitly believe potential advantage method lower susceptibility load imbalance collection idle time would appear collector effectively filled application performance measurement chapter 6 shows good speedup maximum configuration 64 processors indicates urgent need consider using application fill idle time prefer method interfere spmdstyle applications global synchronizations frequent 1 uzuharas method ichiyoshi moritas method may interact badly applications exhibits long marking cycle applications cannot utilize processors taura also reached similar conclusion distributedmemory machines 21 collector algorithm similar imai ticks parallel copying collector 12 study processors perform copying tasks cooperatively memory object one shared heap copied processor dynamic load balancing 1 global synchronization occurs even programming language provide explicit barrier synchronization primitives implicitly occurs many places reduction termination detection achieved exchanging memory pages scanned tospace among proces sors speedup calculated simulation assumes processors become idle load imbalancethe simulation overlooks sources performance degrading factors spintime lock acquisition show chapter 6 factors become quite significant especially largescale multiprogrammed environments chapter 3 boehmdemersweiser conservative gc library boehmdemersweiser conservative gc library boehm gc marksweep gc library c c interface applications simple simply replaces calls malloc calls gc malloc collector automatically reclaims memory longer used application lack precise knowledge types words memory conservative gc necessarily marksweep collector move data boehm gc supports parallel programs using solaris threads current focus seems support parallel programs minimum implementation efforts serializes allocation requests gc parallelized 31 sequential marksweep algorithm marksweep collectors work find garbage objects unreachable root set machine registers stacks global variables via pointer paths free objects tell whether object live reachable garbage object mark bit shows 0 unmarked collection cycle mention boehm gc maintains mark bits section 32 collection cycle consist two phases mark phase collector traverse objects reachable root set recursively sets marks mark bits 1 marked mark objects recursively boehm gc uses data structure called mark stack shown section 33 sweep phase collector scans mark bits frees objects whose mark bits still unmarked sweeping method heavily depends free objects managed describe aspects relevant sweep phase section 34 32 heap blocks mark bitmaps boehm gc manages heap units 4kb blocks called heap blocks objects single heap block must size wordaligned block separate header record heap block header allocated contains information block size objects also kept header mark bitmap objects block single bit allocated word 32 bits experimental environments thus mark bitmap 128byte length j th bit th byte mark bitmap describes state object begins blockaddr blockaddr start address corresponding heap block put differently word mark bitmap describes states consecutive words corresponding heap block may contain multiple small objects therefore parallel gc algorithms visiting marking object must explicitly done atomically otherwise two processors simultaneously mark objects share common word mark bitmap either may marked properly 33 mark stack boehm gc maintains marking tasks performed vector called mark stack keeps track objects marked may directly point unmarked object entry represented two words ffl beginning address object ffl size object figure 31 shows marking process pseudo code iteration pops entry mark stack scans specified object 1 possibly pushing new entries onto precisely specified object large 4 kb collector scans first 4 kb keeps rest stack push roots registers stack global variables onto mark stack mark stack empty f size f oi pointer nothing else mark bit oi marked nothing else f mark bit pushoi mark stack figure 31 marking process mark stack mark stack mark phase finishes mark stack becomes empty 34 sweep sweep phase boehm gc free garbage object actually instead distinguish empty heap blocks heap blocks boehm gc examines mark bitmaps heap blocks heap heap block contains marked object linked list called reclaim list prepare future allocation requests 2 heap blocks found empty linked list called list heap blocks sorted addresses adjacent ones coalesced form large contiguous block heap block free list examined allocation cannot served reclaim list 2 system free garbage objects nonempty heap blocks program requests objects proper size lazy sweeping order find garbage objects heap blocks mark bitmaps preserved next collection chapter 4 parallel gc algorithm collector supports parallel programs consist several unix processes assume processes forked initialization program added application dynamically interface application program original boehm gc provides gc malloc returns pointer shared memory acquired mmap system call could alternatively support solaris threads choice arbitrary somewhat historical simply thought private global variables makes implementation simpler claim one better 41 basic algorithm 411 parallel marking processor local mark stack gc invoked application processes suspended sending signals signals delivered every processor starts marking local root pushing objects onto local mark stack object marked corresponding word mark bitmap locked mark bit read purpose lock twofold one ensure live object marked exactly atomically set appropriate mark bit word reachable objects marked mark phase finished naive parallel marking hardly results recognizable speedup objects marked pe2 objects marked pe1 pe1s root pe2s root heap figure 41 simple algorithm nodes shared tree marked one processor imbalance marking tasks among processors load imbalance significant large data structure shared among processors small number externally visible objects example significant imbalance observed large tree shared among processors root object case root node tree marked one processor internal nodes figure 41 improve marking performance collector performs dynamic load balancing exchanging entries stored mark stacks 412 dynamic load balancing marking besides local mark stack processor maintains additional data structure named stealable mark queue tasks entries mark stacks exchanged 42 marking processor periodically checks stealable mark queue empty processor moves entries local mark stack tasks mark stack lock stealable mark queue figure 42 dynamic load balancing method tasks exchanged stealable mark queues except entries point local root processed local processor stealable mark queue processor becomes idle ie mark stack becomes empty tries obtain tasks stealable mark queues processor examines stealable mark queue first processors finds nonempty queue finds one steals half entries 1 queue stores mark stack several processors may become idle simultaneously testandsteal operation must acquire lock queue mark phase terminated mark stacks stealable mark queues become empty termination detected using global counter maintain number empty stacks empty queues counter updated whenever processor becomes idle obtains tasks 1 queue n entries n odd number n 12 entries stolen 413 parallel sweeping parallel algorithm processors share single heap block free list processor maintains local reclaim list sweep phase processor examines part heap links empty heap blocks heap block free list nonempty ones local reclaim list since processor local reclaim list inserting blocks reclaim list straightforward inserting blocks heap block free list however far difficult heap block free list shared blocks must sorted addresses adjacent blocks must coalesced reduce contention overhead shared list make unit work distribution sweep phase larger single heap block perform tasks locally possible processor acquires large number 64 current implementation contiguous heap blocks time processes locally empty blocks locally sorted coalesced within blocks acquired time accumulated local list called partial heap block free list processor repeats process blocks examined finally lists empty blocks accumulated partial heap block free lists chained together form global heap block free list possibly coalescing blocks joints sweep phase finished restart application 42 performance limiting factors solutions basic marking algorithm described previous section exhibits acceptable speedup smallscale systems eg approximately fourfold speedup eight processors see chapter 6 however several factors severely limit speedup basic form never yields 12fold speedup list factors describe address turn load imbalance large objects often found large object became source significant load imbalance recall smallest unit task distribution single entry stealable mark queue represents single object memory still large often found processors busy scanning large objects processors idle behavior prominent applications used many stacks large arrays one parallel applications input data single 800kb array caused significant load imbalance basic algorithm unusual processors idle entire second half mark phase address problem splitting large objects objects larger 512 bytes small 512byte pieces pushed onto mark stack experiments described later refer optimization slo split large object delay testing mark bitmap observed cases processors consumed significant amount time acquiring locks mark bits simple way guarantee single object marked lock corresponding mark bit precisely word contains mark bit reading ever may unnecessarily delay processors read mark bit object know object already marked improve sequence replaced lockandtest operation optimistic synchronization tests mark bit first quit bit already set otherwise calculate new bitmap word write new bitmap original location location originally read bitmap operation done atomically compareswap instruction sparc architecture loadlink storeconditional instructions mips architecture retry location overwritten another processor operations eliminate useless lock acquisitions mark bits already set refer optimization mos marking optimistic synchronization experiments another advantage algorithm nonblocking algorithm 8 18 19 hence suffer untimely preemption major problem basic algorithm however locking word bitmap every time check object marked causes contention even absence preemption confirmed testandlockandtest sequence checks mark bit locking works equally well though blocking algorithm serialization termination detection number processors becomes large found gc speed suddenly dropped revealed processors spent significant amount time acquire lock global counter maintains number empty mark stacks empty stealable mark queues updated counter time stack queue became empty tasks thrown empty stack queue serialized update operation counter introduced long critical path collector implemented another termination detection method two flags maintained processor one tells whether mark stack processor currently empty tells whether stealable mark queue processor currently empty since processor maintains flags locations different flags processors setting flags clearing flags done without locking termination detected scanning flags turn guarantee atomicity detecting process maintain additional global flag detectioninterrupted set collector recovers idle state detecting processor clears detectioninterrupted flag scans flags finds nonempty queue finally checks detection interrupted flag queues empty retries process interrupted processor must take care order updating flags lest termination detected mistake example processor steals tasks processor b need change flags following order 1 stack empty flag cleared 2 detectioninterrupted flag set 3 queue empty flag b set refer optimization nsb nonserializing barrier chapter 5 experimental conditions implemented collector two systems ultra enterprise 10000 origin 2000 former uniform memory access uma architecture latter nonuniform memory access numa architecture implementation based source code boehm gc version 410 used four applications written c bh nbody problem solver cky context free life game simulator rna program predict rna secondary structure 51 ultra enterprise 10000 ultra enterprise 10000 symmetric multiprocessor sixtyfour 250 mhz ultra processors processors memories connected crossbar interconnect whose bandwidth 107 gbs l2 cache block size 64 bytes 52 origin 2000 origin 2000 distributed shared memory machine machine used experiment sixteen 195 mhz r10000 processors system consists eight modules two processors memory module modules connected hypercube interconnect whose bandwidth 25 gbs memory bandwidth module 078 gbs l2 cache block size 128 bytes default configuration memory page whose size 16 kb placed node processor accessed page first therefore processors desired pages local touching pages initializing phase program used two physical memory allocation policies experiment local allocator la heap block corresponding mark bitmap local processor allocate heap block first roundrobin rr home node heap block determined address rather allocator block heap block local processor p processors home mark bitmap determined rule 53 applications used following four applications written c bh cky parallel ap plications wrote enterprise version applications parallel extension c 14 extension allows programmer create user level threads dynamically runtime implicitly uses fork system call beginning program since extension work origin wrote applications origin using fork system call explicitly life rna sequential applications even sequential applications utilize parallel collection facility bh simulates motion n particles using barneshut algorithm 1 time step bh makes tree whose leaves correspond particles calculates acceleration speed location particles using tree experiment simulate 10000 particles 50 time steps cky takes sentences written natural language syntax rules language input outputs possible parse trees sentence cky calculates nonterminal symbol substring input sentence bottomup experiment given 256 sentences consists 10 words life solves conways game life simulates cells square board cell either two states state cell determined states adjacent cells previous time step program takes list contains cells initial state number initial cells 5685 experiment simulate 150 time steps rna predicts secondary structure rna sequence input data set stack regions stack region position energy set stack regions called feasible pair elements fulfills certain condition problem find feasible subsets given stack regions whose total energy smaller threshold size input stack regions 119 experiment 54 evaluation framework ideally speedup collector measured using various numbers processors applying algorithm snapshot heap difficult however reproduce snapshot multiple times indeterminacy application programs amount data large cannot simply dump entire image heap even dumping feasible would still difficult continue dumped image different number processors thus feasible approach formulate amount work needed finish collection given heap snapshot calculate fast work processed occurrence collection generally accepted estimation workload marking given heap configuration amount live objects equivalently number words scanned collector however ignores fact load word differs depending whether pointer density pointers live data may differ one collection another given word heap boehm gc first performs simple test rules nonpointers examines word elaborately measure speedup accurately define workload w collection 4 x 4 5 x 5 number marked objects x 2 number times scan already marked objects x 3 number times scan nonpointers x 4 number empty heap blocks x 5 number nonempty blocks 1 x n totaled processors gc speed defined elapsed time collection gc speedup n processors ratio n processors single processor measure single processor eliminate overhead parallelization constants determined preliminary experiment determine 3 example created 1000word object contained nonpointers measured time scan object ran measurement several times used shortest time took 20 us scan 1000word object enterprise 10000 means 0020 us per word result let 0020 constants determined similarly intention preliminary experiment measure time workload without cache misses experiment constants set 1 016 3 4 20 5 13 enterprise 10000 1 013 3 4 20 5 13 origin 2000 1 marking workload derived x1 x2 x3 sweeping workload x4 x5 chapter 6 experimental results 61 speedup gc figures 61616 show performance gc using four applications two systems measured several versions collectors sequential refers original boehm gc simple refers algorithm processor simply marks objects reachable root processor without task distribution basic refers basic algorithm described section 41 following three versions refer ones implement one optimizations described section 42 noxxx stands version implements optimizations xxx full fully optimized version measured additional version origin 2000 fullla full takes different physical memory allocation policy fullla takes local allocator policy versions roundrobin policy applications executed four times configuration invoked collections 40 times table shows average performance invocations used almost processors machine occasionally observed invocations performed distinguishably worse usual ones typically times worse usual ones frequency unusually bad invocations every five invocations used processors yet determined reason invocations might effect processes purpose study exclude cases figure 6164 68611 compare three versions namely simple basic full graphs show simple exhibit recognizable speedup application figure 6164 show basic enterprise 10000 performs reasonably certain point scale beyond exception rna see difference basic full saturation point basic depends application basic cky reaches peak 32 processors bh reaches saturation point 8 processors peak life 48 processors full achieved 28fold speedup bh cky 14fold speedup life rna 64 processors 16processor origin 2000 difference basic full little except bh performance problems basic however appear number processors becomes large observed enterprise 10000 thus full would significant larger system full achieved 3763fold speedup processors 62 effect optimization figure 6567 show optimization affects scalability enterprise 10000 especially bh cky removing particular optimization yields sizable degradation performance large number processors without improved termination detection nonserializing barrier nsb neither bh cky achieves 17fold speedup without nsb life scale 48 processors sensitivity optimizations differs among applications splitting large objects slo marking optimistic synchronization mos significant impacts bh applications slo important large object application bh use single array named particles hold particles data whose size 800 kb experiments large array became bottleneck omitted slo optimization phenomenon noted origin 2000 figure 612 indicates generally mos significant effects objects big reference counts objects cause many contentions collectors try visit experiment revealed array particles source problem one collection cycle observed 70000 pointers array caused significant contentions big reference count produced stack user threads bh implementation computes forces particles parallel thread references responsible particles although references directed distinct addresses example ith thread references particlesi regarded pointers single object particles mos optimization effectively alleviate contentions case improve performance observe significant impact nsb optimization gc speed three applica tions see rna even 64 processors although reason difference understood well general nsb important collectors tend become idle frequently collectors often update idle coun ters implement nsb rna frequency task shortage may low investigate whether hypothesis case future 63 effect physical memory allocation policy figure 613616 compare two memory placement policies local allocator la roundrobin rr origin 2000 described section 52 full adopts rr policy fullla la see easily collection speed rr significantly faster la three applications bh cky rna adopt la policy gc speed improve eight processors fully analyzed conjecture mainly due imbalance amount allocated physical pages among nodes la policy access objects mark bitmaps mark phase contend nodes allocated many pages actually bh significant memory imbalance one processor construct tree particles objects rna naturally allocated one processor rna sequential program 1 investigate case life also sequential program 64 discussion optimized performance seen section 61 gc speed fully optimized version always get faster number processors increases applications considerably differ gc speed instance 28fold speedup bh cky 14fold life rna enterprise 10000 order try find cause difference examined processors spend time mark phase 2 figure 617620 show breakdowns figures say biggest problem life load imbalance processors spend significant amount time idle performance improvement may possible refining load balancing method hand currently specify reasons relatively bad performance rna processors busy 90 mark phase collection cycles sweep phase five ten times shorter mark phase therefore focus mark phase sequential sequential code without overhead parallelization simple parallelized load balancing done basic load balancing done optimizations slo splitting large object done optimizations mos marking optimistic synchronization done optimizations nsb nonserializing barrier done optimizations slo splitting large object done full optimizations done fullla origin 2000 full physical memory allocation policy local allocator table 61 description labels following graphs except fullla physical memory allocation policy origin 2000 roundrobin number processors speedup full basic simple linear figure 61 average gc speedup bh enterprise 10000 number processors speedup full basic simple linear figure 62 average gc speedup cky enterprise 10000 number processors speedup full basic simple linear figure 63 average gc speedup life enterprise 10000 number processors speedup full basic simple linear figure 64 average gc speedup rna enterprise 10000 number processors speedup full linear figure 65 effect optimization bh enterprise 10000 number processors speedup full linear figure 66 effect optimization cky enterprise 10000 number processors speedup full linear figure 67 effect optimization life enterprise 10000 bh origin 20002610 number processors speedup full basic simple linear figure 68 average gc speedup bh origin 2000 cky origin 20002610 number processors speedup full basic simple linear figure 69 average gc speedup cky origin 2000 life origin 20002610 number processors speedup full basic simple linear figure 610 average gc speedup life origin 2000 rna origin 20002610 number processors speedup full basic simple linear figure 611 average gc speedup rna origin 2000 bh origin 20002610 number processors speedup full linear figure 612 effect optimization bh origin 2000 bh origin 20002610 number processors speedup full fullla linear figure 613 effect physical memory allocation policy bh origin 2000 cky origin 20002610 number processors speedup full fullla linear figure 614 effect physical memory allocation policy cky origin 2000 life origin 20002610 number processors speedup full fullla linear figure 615 effect physical memory allocation policy life origin 2000 rna origin 20002610 number processors speedup full fullla linear figure 616 effect physical memory allocation policy rna origin 2000 bh enterprise 10000 0 20 40 80 100 number processors busy lock balance idle figure 617 breakdown mark phase bh enterprise 10000 shows busy waiting lock moving tasks idle cky enterprise 10000 0 20 40 80 100 number processors busy lock balance idle figure 618 breakdown mark phase cky enterprise 10000 life enterprise 10000 0 20 40 80 100 number processors busy lock balance idle figure 619 breakdown mark phase life enterprise 10000 rna enterprise 10000 0 20 40 80 100 number processors busy lock balance idle figure 620 breakdown mark phase rna enterprise 10000 chapter 7 conclusion constructed highly scalable parallel marksweep garbage collector sharedmemory machines implementation evaluation done two systems ultra enterprise 10000 symmetric sharedmemory machine 64 processors origin 2000 distributed sharedmemory machine 16 processors collector performs dynamic load balancing exchanging objects mark stacks experiments largescale machine found number factors severely limit scalability presented following solutions 1 unit load balancing single object large object cannot divided degraded utilization processors splitting large objects small parts pushed onto mark stack enabled better load balancing 2 observed processors spent significant time lock acquisitions mark bits bh useless lock acquisitions eliminated using optimistic synchronization instead lockandtest operation 3 especially 32 processors processors wasted significant amount time serializing operation used termination detection global counter implemented nonserializing method using local flags without locking long critical path eliminated origin 2000 must pay attention physical page placement default policy places physical page node first touches gc speed scalable improved performance distributing physical pages round robin fashion conjecture default policy causes imbalance access traffic nodes since nodes much physical pages allocated nodes accesses highlyloaded nodes tend contend hence latency remote accesses accordingly increases enough tools conclude using solutions achieved 14 28fold speedup 64processor enterprise 10000 37 63fold speedup 16processor origin 2000 chapter 8 future work would like improve gc performance section 64 seen collectors applications still spend significant amout time idle investigate improve load balancing method instead using buffers communication stealable mark queues stealing tasks victims mark stack directly may enable faster load distributing also noticed cannot explain relatively bad performance rna load imbalance alone may due number cache misses included busy figure 620 capture number cache misses using performance counters recent processors equipped use r10000s counters proc file system origin 2000 constructed simple tool use ultra sparcs counters enterprise 10000 tools planning examine often processors meets cache misses section 63 mentioned obtain better performance rr roundrobin physical memory allocation policy la local allocator policy far focus discussion speed gc alone matter complicated take account locality application programs la policy may advantageous memory region tends accessed allocator ideal situation would accesses application local collection task balanced well r hirarchical log n forcecalculation algorithm space efficient conservative garbage collection garbage collection uncooperative environment concurrent garbage collection c concurrent generational garbage collector multithreaded implementation ml language concurrent symbolic computa tion methodology implementing highly concurrent data ob jects concurrent copying garbage collector languages distinguish immutable data sharedmemory parallel extension klic garbage collection evaluation parallel copying garbage collection sharedmemory multiprocessor garbage collection sgi origin ccnuma highly scalable server garbage collection multischeme preliminary version concurrent replicating garbage collection remarks methodology implementing highly concurrent data objects methodology implementing highly concurrent data objects effective garbage collection strategy parallel programming languages large scale distributedmemory machines parallel garbage collection sharedmemory multiprocessors uniprocessor garbage collection techniques tr multilisp language concurrent symbolic computation garbage collection uncooperative environment garbage collection multischeme space efficient conservative garbage collection concurrent copying garbage collector languages distinguish immutable data concurrent generational garbage collector multithreaded implementation ml methodology implementing highly concurrent data objects concurrent replicating garbage collection remarks methodology implementing highly concurrent data notes myampersandldquoa methodology implementing highly concurrent data objectsmyampersandrdquo garbage collection effective garbage collection strategy parallel programming languages large scale distributedmemory machines lockfree garbage collection multiprocessors evaluation parallel copying garbage collection sharedmemory multiprocessor uniprocessor garbage collection techniques iccac dialect high performance parallel computing ctr guy e blelloch perry cheng bounding time space multiprocessor garbage collection acm sigplan notices v34 n5 p104117 may 1999 guy e blelloch perry cheng bounding time space multiprocessor garbage collection acm sigplan notices v39 n4 april 2004 david siegwart martin hirzel improving locality parallel hierarchical copying gc proceedings 2006 international symposium memory management june 1011 2006 ottawa ontario canada toshio endo kenjiro taura reducing pause time conservative collectors acm sigplan notices v38 n2 supplement february h gao j f groote w h hesselink lockfree parallel concurrent garbage collection marksweep science computer programming v64 n3 p341374 february 2007 david detlefs christine flood steve heller tony printezis garbagefirst garbage collection proceedings 4th international symposium memory management october 2425 2004 vancouver bc canada yoav ossia ori benyitzhak irit goft elliot k kolodner victor leikehman avi owshanko parallel incremental concurrent gc servers acm sigplan notices v37 n5 may 2002 katherine barabash ori benyitzhak irit goft elliot k kolodner victor leikehman yoav ossia avi owshanko erez petrank parallel incremental mostly concurrent garbage collector servers acm transactions programming languages systems toplas v27 n6 p10971146 november 2005 yossi levanoni erez petrank onthefly referencecounting garbage collector java acm transactions programming languages systems toplas v28 n1 p169 january 2006 yossi levanoni erez petrank onthefly reference counting garbage collector java acm sigplan notices v36 n11 p367380 11012001 guy e blelloch perry cheng bounding time space multiprocessor garbage collection acm sigplan notices v39 n4 april 2004