flexible adaptable buffer management techniques database management systems abstractthe problem buffer management database management systems concerned efficient main memory allocation management answering database queries previous works buffer allocation based either exclusively availability buffers runtime access patterns queries paper first propose unified approach buffer allocation considerations taken account approach based notion marginal gains specify expected reduction page faults allocating extra buffers query extend approach support adaptable buffer allocation adaptable buffer allocation algorithm automatically optimizes specific query workload achieve adaptability propose using runtime information load system buffer allocation decisions approach use simple queuing model predict whether buffer allocation improve performance system thus paper provides theoretical basis buffer allocation simulation results show methods based marginal gains predictive methods consistently outperform existing allocation strategies addition predictive methods added advantage adjusting allocation changing workloads b introduction relational database management systems buffer manager responsible operations buffers including load control buffers become available manager needs decide whether activate query waiting queue many buffers allocate query figure 1 outlines major components involved issue buffer allocation buffer pool area common resource queries queries currently running queries waiting queue compete buffers like competitive environment principle supply demand well protection starvation unfairness must employed hence principle number buffers assigned query determined based following factors 1 demand factor space requirement query determined access pattern query shown path 1 figure 1 2 buffer availability factor number available buffers runtime shown path 2 figure 1 3 dynamic load factor characteristics queries currently system shown path 3 figure 1 based factors previous proposals buffer allocation classified following groups summarized table 1 allocation algorithms first group consider buffer availability factor include variations firstinfirstout fifo random leastrecentlyused lru clock workingset6 10 15 however focus adapting memory management techniques used operating systems database systems fail take advantage specific access patterns exhibited relational database queries performance satisfactory3 allocation strategies second group consider exclusively demand factor specifically access patterns queries include proposal kaplan8 implementation ingres16 hotset model designed sacca schkolnick13 14 strategy used cornell yu5 integration buffer management query optimization approach buffer allocation culminated work chou dewitt3 introduce the2output queries buffer manager cpu buffer pool figure 1 buffer manager related components access patterns availability dynamic queries demand buffers runtime workload fifo random lru etc p hotset dbmin p flexible algorithms proposed p p adaptable algorithms proposed p p p table 1 classification buffer allocation algorithms notion locality set query ie number buffers needed query without causing many page faults propose dbmin algorithm makes allocation equal size locality set dbmin also allows different local replacement policies simulation results 2 3 show dbmin outperforms hotset strategy algorithms referred first group strength dbmin algorithms referred second group lies consideration access patterns queries weakness arises oblivion runtime conditions availability buffers imposes heavy penalties performance whole system deficiency leads us study propose unified approach buffer allocation simultaneously takes account access patterns queries availability buffers runtime objective provide best possible use buffers maximize number page hits basis approach notion marginal gains specify expected number page hits would obtained allocating extra buffers query shall see later simulation results show allocation algorithms based marginal gains gives better performance dbmin however one characteristic common algorithms static nature cannot adapt changes system loads mix queries using system rectify situation second half paper propose new family buffer management techniques adaptable workload system basic idea approach use predictors predict effect buffer allocation decision performance system predictions based availability buffers runtime characteristics particular query also dynamic workload system two predictors considered paper throughput effective disk utilization simulation results show buffer allocation algorithms based two predictors perform better existing ones section 2 present mathematical models derive formulas computing expected number page faults different types database references introduce section 3 notion marginal gains present flexible buffer allocation algorithms based marginal gains section 4 introduce predictors present policies adaptable allocation algorithms finally present section 5 simulation results compare performance algorithms dbmin mathematical models relational database references section first review taxonomy proposed chou dewitt2 3 classifying reference patterns exhibited relational database queries analyze detail major types references present mathematical models formulas calculating expected number page faults using given number buffers models help provide formulas computing marginal gains predictive estimates sections 3 4 21 types reference patterns 2 3 chou dewitt show page references relational database queries decomposed sequences simple regular access patterns focus three major types references random sequential looping random reference consists sequence random page accesses selection using nonclustered index one example following definitions type references reference ref length k relation sequence p relation read given order 2 random reference r kn length k relation size n reference p 1 uniformly distributed set pages accessed relation p independent p j 6 j 2 sequential reference selection using clustered index pages referenced processed one another without repetition definition 3 sequential reference kn length k relation size n reference 1 sequential reference performed repeatedly nested loop join reference called looping reference looping reference l kt length k reference p ii p subsequence called loop called length loop 2 following three types references give formulas computing expected number page faults using given number buffers table 2 summarizes symbols used section denote expected number page faults caused reference using buffers ref l kt r kn kn 2 symbols definitions k length reference number buffers f number page faults n number pages accessed relation length loop looping reference lkt looping reference length k loop length rkn random reference length k relation size n skn sequential reference length k relation size n expected number faults reference ref buffers table 2 summary symbols definitions 22 random references throughout section use p f k n denote probability f faults k accesses relation size n using buffers 1 0 f k thus random reference expected number page faults given model random reference set markov chain following way state markov chain form f k indicating f faults k accesses f k setting transitions states states two cases deal first case number f faults exceed number allocated buffers thus must f distinct pages kept buffers f faults consider state f k chain two possibilities f faults k accesses last access cause page fault probability fn must f faults accesses words arc state f k gamma 1 state f k transition probability fn arc state f k state f gamma transition probability corresponds case f gamma 1 faults accesses last page accessed one f gamma 1 pages kept buffers hence case f summarized following recurrence equation second case number f faults exceeds number allocated buffers local replacement must taken place always pages kept buffers note however since reference random choice local replacement policies irrelevant analysis case f almost identical case f except transition probabilities must changed following sn accessing page already buffers n gamma sn otherwise hence situation f summarized following recurrence equation addition recurrence equations 2 3 base case p0 0 n 1 1 expected number page faults efr computed according equation 1 except case simple closed form formula efr kn fortunately formula gives close approximations actual values expected number page accesses fill buffers thus top row formula corresponds case none buffers filled needs replaced first case uses cardenas formula1 calculates expected number distinct pages accessed k random pages selected n possible ones replacement accurate results may obtained yaos assumes replacement formulas make uniformity assumption effects discussed in4 second row corresponds case local replacement occurred faults generated fill buffers take k 0 page accesses average remaining chance finding page buffer pool sn 23 sequential references recall definition 3 page sequential reference kn accessed thus probability page rereferenced 0 hence sequential reference viewed degenerate random reference following formula obvious 24 looping references recall condition definition 4 within loop looping reference l kt strictly sequential thus based equation 5 page faults generated first iteration loop two cases firstly number allocated buffers less length loop pages loop retained buffers page faults generated remainder reference choice local replacement policy irrelevant case second case number allocated buffers less length loop local replacement policy plays major role determining number page faults generated looping reference among local replacement policies difficult see looping reference l kt mru replacement requires fewest number faults key observation looping reference mru identical policy looks ahead keeps pages used immediate future cf table example wellknown result mattson et al11 optimal page replacement operating systems applied show optimality mru thus paper present analysis mru best explained example example 1 consider looping reference loop b c e suppose buffers available reference following table summarizes situation mru b c e b c e b c e b c e b c e b c e b c e b c e b c e b c e b b b e e e e c b c c c b b b b e e e e c c c b b b c first row table indicates numbers page accesses second row shows order pages accessed five iterations loop page hit occurs access marked asterisk last three rows table indicate pages kept buffers page access page frequently used top row example demonstrates important properties mru first note five minicycles length four may align iterations loop separated vertical lines table minicycles also follow cyclic pattern namely twentysixth access table exactly sixth access furthermore within minicycle two resident pages swapped minicycle instance first minicycle resident pages e note resident pages pages begin next minicycle avoiding page faults accesses property exactly reason mru optimal 2 general given loop length minicycles length words iterations loop different minicycles furthermore minicycles recur every iterations loop minicycle resident pages thus minicycle hence average iteration loop thus equation follows immediately marginal gains flexible allocation methods mgxy section first review dbmin introduce notion marginal gains finally propose flexible buffer allocation algorithms mgxy designed maximize total marginal gains utilization buffers 31 generic load control dbmin order classify study various allocation methods break problem load control two load control buffer manager determines whether waiting reference activated decides many buffers allocate reference throughout paper use term admission policy refer first decision term allocation policy refer second one admission allocation policies chosen buffer allocation algorithm adopting firstcomefirstserve policy outlined follows algorithm 1 generic whenever buffers released newly completed query whenever query enters empty queue perform following 1 use given admission policy determine whether query q head waiting queue activated 2 feasible use allocation policy decide number buffers q notice q write buffers returned buffer pool termination q activate q go back step 1 3 otherwise halt queries must wait buffers released 2 note allocation algorithms considered paper dbmin proposed methods alike query consists one reference given number buffers equal sum buffers allocated relation accessed query allocation relation determined reference pattern described previous section relation uses allocated buffers throughout see 2 detailed discussion ongoing work study allocate buffers per query basis describe dbmin using general framework outlined algorithm 1 let us define symbols used throughout rest paper use term denote number available buffers terms min max denote respectively minimum maximum numbers buffers buffer allocation algorithm willing assign reference dbmin admission policy simply activate query whenever specified number buffers available min allocation policy depends type reference looping reference locality set size total number pages loop 2 pp 52 since dbmin requires entire locality set allocated 2 pp 50 ie length loop 1 random reference proposed 2 3 random reference may allocated 1 b yao buffers b yao yao estimate average number pages referenced series random record accesses18 practice yao estimates usually high allocation example blocking factor 5 yao estimate accessing 100 records 1000record relation 82 pages thus dbmin almost always allocates 1 buffer random reference ie preview algorithms may also make use yao estimate important difference unlike dbmin allocates either 1 82 buffers example algorithms may allocate buffer within 2 chou remarks mru best replacement policy looping reference suboptimal allocation however far know method proposed 2 3 allocate suboptimally range 1 82 depending conditions buffer availability dynamic workload finally sequential reference dbmin specifies note dbmin improves traditional algorithms like workingset lru etc flexible enough make full use available buffers inflexibility illustrated fact range degenerates point words dbmin allow suboptimal allocations looping references allow random references luxury allocated many buffers even buffers available problems lead us development notion marginal gains flexible buffer allocation algorithms mgxy discussed next 32 marginal gains concepts marginal gain marginal utility widely used ecomonics theory since 18th century9 apply approach database buffer allocation definition 6 2 marginal gain reference ref use buffers defined ref l kt r kn kn 2 given reference ref marginal gain value mgref specifies expected number extra page hits would obtained increasing number allocated buffers note values take account reference patterns availability buffers simultaneously essence marginal gain values specify quantitatively efficiently reference uses buffers moreover quantification granularity level finer locality set sizes used dbmin thus dbmin allocate per localitysetsize basis allocation algorithms based marginal gains flexible allocate per buffer basis analyze marginal gain values different types references vary number buffers analysis crucial designing flexible algorithms presented looping reference l kt equation 6 dictates allocation extra page hits would obtained allocating buffers reference loop fully accommodated buffers allocation optimal allocation generates fewest page faults furthermore allocation certainly wasteful extra buffers used graph looping references figure 2 summarizes situation typical marginal gain values looping references order magnitude o10 o10 2 example reference goes loop 50 pages 20 times marginal gain value buffers 50 194 similarly based equations 2 3 4 easy check marginal gain values random references positive strictly decreasing number allocated buffers increases shown figure 2 eventually marginal gain value becomes zero allocation exceeds number accesses number pages accessed relation note unlike dbmin buffer allocation algorithm based marginal gains may allocate idle mg looping l kt mg random r kn mg ssequential kn figure 2 typical curves marginal gain values buffers random reference long marginal gain values reference indicate benefits allocate buffers reference fact even number idle buffers exceeds yao estimate may still beneficial allocation beyond yao estimate however worth pointing marginal gain values random reference normally lower looping reference highest marginal gain value random reference typically order magnitude o1 o10 gamma1 example random reference discussed earlier ie accessing 100 records 200 pages highest marginal gain value 05 finally shown equation 5 marginal gain values sequential references always zero indicating benefit allocate one buffer references cf figure 2 33 mgxy shown marginal gain values reference quantify benefits allocating extra buffers reference thus system queries compete fixed number buffers marginal gain values provide basis buffer manager decide queries get buffers others ideally given n free buffers best allocation one exceed n maximizes total marginal gain values queries waiting queue however optimization expensive complicated buffer allocation purposes furthermore ensure fairness favor buffer allocation firstcomefirstserve basis following present class mgxy allocation algorithms achieve high marginal gain values maximizes buffer utilization fair easy compute follows generic framework outlined algorithm 1 like dbmin allocation policy mgxy presented allocates per reference basis allocation policy 1 mgxy let r reference head waiting queue 0 number available buffers moreover let x parameters mgxy explained detail shortly case 1 r looping reference l kt 1 number available buffers exceeds length loop ie buffers reference 2 otherwise number available buffers low ie x allocate buffers reference 3 otherwise ie x give buffers reference r case 2 r random reference r kn 1 long marginal gain values r positive allocate r many buffers possible exceeding number available buffers ie allocation minimum case 3 r sequential reference kn 1 allocate 1 buffer 2 mgxy two parameters x x parameter used determine allocations looping references described case 1 mgxy first checks see number available buffers exceeds length loop looping reference recall previous section figure 2 allocation accommodates whole loop minimizes page faults corresponds highest total marginal gain values reference thus enough buffers like dbmin mgxy gives optimal allocation however enough buffers mgxy checks determine whether suboptimal allocation beneficial via use parameter x general response time query two components waiting time processing time former time arrival query time query activated latter time activation completion processing time minimized optimal allocation obtain optimal allocation waiting time may become long hand suboptimal allocation may result longer processing time may end give response time shorter optimal allocation reduction waiting time offsets increase processing time hence trying achieve fine balance waiting time processing time mgxy uses heuristic suboptimal allocation allowed total marginal gain values allocation far away optimal requirement translates condition shown case 1 suboptimal allocation must least x optimal one constrast dbmin mgxy may allocate extra buffers random reference long extra buffers justified marginal gain values reference however pitfall simply considering marginal gain values random reference example suppose random reference followed looping reference waiting queue situations buffers scarce giving one buffer random reference implies one fewer buffer give looping reference since marginal gain values looping reference usually higher random reference desirable save buffer allocation allocation policy admission algorithms looping random sequential policy predictive methods fload fload b yao 1 1 min table 3 characteristics buffer allocation algorithms random reference allocate buffer looping reference instead since mgxy operates firstcomefirstserve basis mgxy uses heuristic imposing maximum number buffers allocated random reference purpose parameter mgxy first two rows table 3 summarize similarities differences dbmin mgxy recall previous section min max denote respectively minimum maximum numbers buffers buffer allocation algorithm willing assign reference fact easy see mgxy generalizes dbmin mg1001 ie x100 y1 dbmin shall see section 6 allow flexible values x dbmin mgxy performs considerably better note obtain best performance x parameters need determined according mix queries use system may involve experimenting different combinations values x 2 clearly kind experimentation expensive moreover optimal values vulnerable changes mix queries thus next section explore idea flexible buffer allocation develop adaptable allocation algorithms dynamically choose min max values using runtime information basis approach use queueing model give predictions performance system make min max parameters vary according state queueing model next section describe proposed queueing model well ways model used perform buffer allocation fair fcfs robust adaptable way 4 adaptable buffer allocation 41 predictive load control described previous section dbmin mgxy static nature admission policy simply min min predefined constant type reference propose adaptable methods use dynamic information min function workload denoted fload table 3 thus considering admissions methods consider characteristics reference number available buffers 2 good starting point experience symbols definitions number available buffers smin minimum number buffers assigned reference smax maximum number buffers assigned reference number buffers usable reference tp throughput multiprogramming level number active queries ncq number concurrent queries active waiting buffers tci cpu load ref tdi disk load ref time one disk access time process one page main memory geometric average cpu loads td harmonic geometric average disk loads ae relative load disk vs cpu ud disk utilization udi disk utilization due ref edu effective disk utilization number buffers assigned ref portion avoidable wasted page faults ref table 4 summary symbols definitions queueing model also take account dynamic workload system specifically waiting reference activated buffers admission predicted improve performance current state system precise notations suppose pf denotes performance measure eg gamma cur represents references ie queries ref currently system gamma buffers respectively ref reference consideration admission min smallest improve pf predictor pf gamma new gamma cur gamma gamma cur ref gamma symbol pf denotes performance system r active references buffer allocations thus reference ref admitted degrade performance system 3 paper consider two performance measures predictors throughput tp effective disk utilization edu analyze predictors discuss motivation behind choices outline queueing model forms basis predictors end section discuss predictors incorporated various allocation policies give different adaptable buffer allocation algorithms section 6 present simulation results comparing performance adaptable algorithms mgxy dbmin 42 queueing model assume closed queueing system two servers one cpu one disk figure 3 shows system table 4 summarizes symbols used queueing model within system n references jobs ref whose cpu disk loads ci tdi respectively 3 however one exception see section 44 discussion queue buffers disk cpu figure 3 queueing system furthermore ref allocated buffers therefore every disk access costs eg 30 msec processing page brought core costs c eg 2 msec following equations number pages accessed ref efref computed using formulas listed section 3 general solution network calculated see example 17 pp 451452 involves nclass model job class gives accurate performance measures throughput utilizations solution expensive compute since requires exponential time number classes ease computation essential load control approximate singleclass model assume jobs come one class overall cpu load tc overall disk load td averages respective loads individual references tc td may harmonic geometric means depending predictors introduced following proceed propose two performance predictors allocation note paper focus singledisk system mainly show effectiveness proposed buffer allocation schemes multiple disk system would introduce issue data placement decided could extend queueing model multiple disks queueing systems multiple servers studied 17 43 predictor tp since ultimate performance measure throughput system natural predictor estimate throughput directly general two ways try increase throughput system increase multiprogramming level mpl decrease disk load jobs allocating buffers jobs however two requirements normally conflict total number buffers system fixed hence first predictor tp propose following admission policy admission policy 1 tp activate reference maximal allocation possible otherwise activate reference increase throughput 2 policy described maximal allocation one assigns many buffers reference reference needs many number buffers available implement policy provide formulas compute throughput solution single class model given 17 ud utilization disk given ae ae ratio disk load versus cpu load derive average loads tc td use harmonic means respective loads reason equations queueing systems based concept service rate inverse load thus using harmonic means loads equivalent using arithmetic means rates ie notice calculation throughput requires o1 operations buffer managers keeps track values td tc 44 predictor edu although intuitive using estimated throughput criterion admission may lead anomalies consider situation long sequential reference head waiting queue short maximally allocated random references currently running system admitting sequential reference may decrease throughput increases average disk load per job however optimal allocation sequential reference one buffer activating sequential reference reasonable exactly reason admission policy 1 patched admit reference max buffers even admission decreases throughput anomaly throughput predictor leads us development second predictor effective disk utilization edu consider following point view problem queue jobs ie references system one cpu one disk buffer pool help decrease page faults jobs assuming disk bottleneck case experiments usually case practice reasonable objective make disk work efficiently possible two sources inefficient uses disk 1 disk sitting idle jobs 2 disk working page requests could avoided enough buffers given references causing page faults following concept captures observations wasten idle udn 1nud ud figure 4 effective disk utilization definition 7 effective disk utilization edu portion time disk engaged page faults could avoided even references assigned optimal number buffers infinite equivalently opt maximum number buffers usable reference 2 hence second predictor edu use following admission policy admission policy 2 edu activate reference increase effective disk utilizationmathematically effective disk utilization expressed udi represents disk utilization due ref w portion avoidable page faults caused practical calculations use opt instead clearly opt 1 b yao sequential looping random references respectively note equation relates notion edu marginal gain values introduced previous section term rewritten ps opt intuitively represents portion avoidable page faults also regarded form normalized marginal gain values informally equation 12 specifies every unit time disk serves ref udi units time ref wastes w udi units time summing jobs get equation 12 figure 4 illustrates concept effective disk utilization horizontal line corresponds 100 disk utilization dotted portion stands idle time disk dashed parts correspond wasted disk accesses sum solid parts corresponds effective disk utilization note io bound jobs every job approximately equal share total disk utilization ud even though jobs may different disk loads thus formula simplifies equation 12 notice yet used singleclass approximation need approximation calculate disk utilization ud using exact nclass model 17 find geometric averages give better approximation disk utilization thus average cpu disk loads given based equations disk utilization ud computed according equations 10 11 like calculating tp predictor calculation edu requires o1 steps buffer manager keeps track loads total wasted disk accesses 45 adaptable buffer allocation algorithms thus far introduced two predictors tp edu presented admission policies based predictors provided formulas computing predictions complete design adaptable buffer allocation algorithms propose three allocation policies rules determine number buffers allocate reference reference passed admission criterion allocation policy 2 optimistic give many buffers possible ie smina allocation policy 3 pessimistic allocate buffers necessary random references ie min many possible sequential looping references 2 optimistic policy tends give allocations close optimal possible however may allocate many buffers random references even though extra buffers may otherwise useful references waiting queue pessimistic policy thus designed deal problem weakness policy unfairly penalizes random references particular abundant buffers available reason let buffers sit idle allocate buffers random references allocation policy 4 2pass assign tentatively buffers first references waiting queue following pessimistic policy eventually either end waiting queue reached m1 th reference waiting queue cannot admitted perform second pass distribute remaining buffers equally random references admitted first pass 2 essence 2pass policy makes allocation decisions consider reference head waiting queue also takes account many references possible rest queue query query selec access path join access path reference type type operators tivity selection method join data pages clustered index s50500 nonclustered index r3015 nonclustered index r30150 sequential scan index join nonclustered index b r10015 sequential scan index join nonclustered index b r30150 clustered index nested loop sequential scan b l30015 table 5 summary query types relation 10000 tuples relation c 3000 tuples tuple size 182 bytes page size 4k table details relations follwing generic framework described algorithm 1 three allocation policies used conjunction tp edu giving rise six potential adaptable buffer allocation algorithms naming convention algorithm denoted pair predictorallocation predictor either tp edu allocation one p 2 representing optimistic pessimistic 2pass allocation policies respectively instance eduo stands algorithm adopting edu admission policy optimistic allocation policy 5 simulation results section present simulation results performance mgxy adaptable methods multiuser environment chou dewitt shown 2 3 dbmin performs better hotset algorithm firstinfirstout clock leastrecentlyused workingset compare algorithms dbmin 51 details simulation order make direct comparison dbmin use simulation program chou dewitt used dbmin experiment types queries table 5 summarizes details queries chosen represent varying degrees demand cpu disk memory 2 3 table 6 table 7 show respectively details relations query mixes used simulation number concurrent queries varies 2 16 24 concurrent queries generated query source cannot generate new query last query source completed thus simulation program simulates closed 50500 r 3015 r 30150 r 10015 r 30150 l 30015 table 7 summary query mixes 16100110120130140150number concurrent queries r r tpo eduo mg506 figure 5 relative throughput mix 1 mainly looping references data sharing system 4 see 2 3 details 52 effectiveness allocations looping references first mix queries consists 70 queries type vi looping references 10 queries types ii iv sequential random random references respectively purpose mix evaluate performance mgxy adaptable algorithms situations many looping references executed x parameter mgxy set one following 100 85 70 50 parameter one 1 6 12 15 figure 5 shows throughputs dbmin mg10012 mg50ys adaptable algorithms running 4 besides buffer management concurrency control transaction management another important factor affecting performance whole database system simulation package consider transaction management see 2 discussion transaction lock manager integrated buffer manager using dbmin since algorithms differ dbmin load control integration proposed also applies buffer manager using algorithms tpo eduo edu2 mg506 tpo eduo edu2 mg50ys number concurrent queries r e 16405060708090number concurrent queries b u f r l figure average waiting time buffer utilization mix 1 different number concurrent queries using 35 buffers results mg70ys mg85ys similar mg50ys omitted brevity results pessimistic approach typically slightly better dbmin thus performance figures plotted graphs brevity major reason pessimistic approach gives poor performance approach aggressive allowing many queries get system note obtain throughput values run simulation package repeatedly values stabilized 2 discusses simulation package used obtain results within specified confidence interval figure 5 also includes throughputs ideal algorithm infinitely many buffers therefore support number concurrent queries requiring number buffers furthermore highlight increase decrease relative dbmin values normalized values dbmin effectively showing ratio throughput let us focus attention mgxy algorithms first four mg50y algorithms show considerable improvement compared dbmin particular since allocations random sequential references mg501 mg1001 ie dbmin improvement exhibited mg501 relative mg1001 due solely effectiveness allocating buffers suboptimally looping references whenever necessary value increases 1 15 throughput increases gradually becomes 15 increase throughput attributed fact random queries benefited allocation buffers many buffers eg allocated random query buffers used efficiently thus throughput mg5015 lower mg5012 finally adaptable algorithms tpo eduo edu2 perform comparably best mgxy scheme mg5012 case note certain extent algorithm mg10012 represents algorithm allocates 28 29 total number buffers r r figure 7 relative throughput vs total buffers mix 1 buffers minimize number page faults however optimal allocations may induce high waiting time 5 queries low buffer utilization throughput system two graphs figure 6 demonstrate situation graph left shows average waiting time queries values normalized values dbmin graph right shows average percentage buffers utilized thus far seen performance mgxy varies different values x figure 7 shows relative throughput varies number total buffers used running mix queries 8 concurrent queries graphs multiprogramming levels exihibit similar patterns figure 7 shows situations suboptimal allocations allowed mg5012 mg7012 mg8512 instance number total buffers becomes 30 mg5012 allows suboptimal allocations looping references throughput system increases significantly compared algorithms total number buffers increases mg7012 mg8512 follow mg5012 perform better dbmin discrepancy explained considering looping reference head waiting queue dbmin insists giving optimal allocation reference 18 case reference blocking queries using buffers reference finally manages get optimal number buffers ie total number buffers becomes 36 dbmin performs much worse others case difference throughput due effective allocations random references mgx12 algorithms graph extends higher numbers total buffers expect similar pattern divergence throughput 5 waiting time query time arrival activation tpo 24100110120130140150number concurrent queries r r figure 8 relative throughput mix 2 mainly random references data sharing appears every multiple 18 though magnitude probably decrease 53 effectiveness allocations random references second mix queries consists 45 queries type ii 45 queries type iv random references 10 queries type sequential references purpose mix evaluate effectiveness mgxy adaptable schemes allocating buffers random references since looping references mix x parameter mgxy irrelevant simply set 100 parameter one following 1 8 13 15 figure 8 shows ratio throughputs dbmin mg100ys adaptable algorithms running different number concurrent queries using 35 buffers results pessimistic policies explicitly included figure mix queries algorithms adopting pessimistic policies behave exactly dbmin ie mg1001 allocating one buffer random reference let us focus attention mgxy algorithms first compared dbmin ie mg 1001 three mg100y algorithms show significant increases throughput value increases 1 15 throughput increases gradually becomes 15 increase throughput attributed fact random queries benefited allocation buffers explained previous section becomes 15 buffers allocated random queries longer used efficiently thus throughput mg10015 drops mg10013 even mg1008 adaptable algorithms eduo tpo perform comparably mg10013 tpo eduo edu2 mg5012 mg5015 r r number concurrent queries figure 9 relative throughput mix 1 full data sharing ideal algorithm edu2 though better dbmin perform well others every time first pass allocations cf allocation policy 4 edu2 tendency activating many random references result number buffers per random reference allocated edu2 lower allocated algorithms thereby causing page faults degrading overall performance 54 effect data sharing simulations carried far every query access data buffers however algorithms support sharing data among queries exactly way dbmin specifically page requested query algorithm first checks see page already buffers owned query data allowed shared system algorithm tries find page buffers query allowed share page found page given query without changing original ownership page see 2 3 details examine effect data sharing relative performance algorithms relative dbmin also run simulations varying degrees data sharing figure 9 shows relative throughputs dbmin mg50ys adaptable algorithms running first mix queries buffers query read access buffers queries ie full data sharing compared figure 5 case data sharing figure 9 indicates data sharing favors algorithms query mixes used behaviour occurs fact b tpo 16100110120130number concurrent queries r eduo tpo number concurrent queries r figure 10 switching mixes stage 1 mix 4 b stage 2 mix 3 phenomenon surprising suboptimal allocations looping references give even better results data sharing allowed obvious data sharing higher buffer utilization higher throughput likely words inflexibility dbmin buffer allocation becomes even costly case data sharing 55 comparisons mgxy adaptability among simulations shown thus far adaptable allocation algorithms tpo edu perform comparably best mgxy reason fixed mix queries types queries selected carefully x parameters best suited specific mix simulations described shall see one set statically chosen values x creates problems mgxy first problem mgxy due fact mgxy scheme one x one value kinds looping random references consider situation two kinds random references first one low yao estimate high selectivity one high yao estimate low selectivity example consider query type ii respectively query type ii r 3015 yao estimate 12 selectivity making random accesses 15 pages hand query type v r 30150 yao estimate 27 selectivity making random accesses 150 pages query first type beneficial allocate close yao estimate possible query second type worthwhile allocate many buffers query thus mgxy scheme using one value sufficient handle diversity queries problem demonstrated running simulation third query mix consists two kinds random references mentioned query type ii query type v figure 10b shows relative throughput tpo dbmin075085095105115125simulation time n n e t075085095105115125simulation time t075085095105115125simulation time n n e figure mix 4 mix 3 instantaneous throughput switching running mix queries buffers compared best result mgxy ie mg5016 case adaptable algorithms perform better handling diversity queries effectively second weakness mgxy inability adjust changes query mixes figure 10 shows result running simulation consists two stages first stage query mix ie mix 4 consists random references shown figure 10a best result mgxy ie mg5018 case performs comparably adaptable algorithms second stage comes query mix changes mix 4 mix 3 mg5018 cannot adapt changes illustrated figure 10b contrast adaptable algorithms adjust appropriately figure shows instantaneous throughputs dbmin mg5018 tpo fluctuate switching mixes instantaneous throughput values obtained calculating average throughputs within 10second windows thin line graph plots fluctuation instantaneous throughputs solid line represents overall average throughput mix indicates moment switching mixes figure indicates time switching instantaneous throughputs dbmin fluctuate greatly eventually tapering lower average throughput mg5018 fluctuation switching mixes greater tpo adaptable schemes since designed sensitive characteristics queries currently running system fluctuation expected 56 summary simulation results show mgxy algorithms effective allocating flexibly queries compared dbmin mgxy algorithms give higher throughput higher buffer utilization lower waiting time queries increase performance even higher data sharing allowed simulation results also indicate adaptable allocation algorithms effective flexible dbmin without data sharing capable making allocation allocation average time taken average response ratio load control time algorithms load control msec time sec response time table 8 costs running algorithms mix 2 data sharing decisions based characteristics queries runtime availability buffers dynamic workload compared mgxy algorithms adaptable changes behaving flexibly mgxy schemes moreover sensitivity analysis needed adaptable methods advantages adaptable schemes listed seem indicate adaptable algorithms used situations concern amount time take make load control decisions table 8 lists average time query took load control average response time query running query mix 2 4 concurrent queries cf figure 8 figures obtained running simulation package unix environment dec 2100 workstation easy see mgxy algorithms take much less time execute adaptable ones thus situations query mixes expected change often sensitivity analysis performed inexpensively find good values x parameters beneficial use mgxy algorithms instead adaptable ones case adaptable algorithms desirable even though computation algorithms take much longer time static ones extra time worthwhile 3 milliseconds ie worst case edu2 offset saving one disk read 3 milliseconds constitute less 1 total response time query two predictors tp edu perform quite well edu probably accurate single disk system tp extendible multidisk systems slightly easier compute cf table 8 allocation policies winners 2pass approach optimistic one pessimistic approach generally give poor results 2pass approach hand performs well situations exception heavy workloads consisting primarily random references case 2pass policy degenerates pessimistic one normally buffers left distributed second pass another practical disadvantage 2pass policy cannot activate queries instantaneously queries admitted first pass may wait second pass additional buffers thus slower algorithms require one pass finally optimistic allocation policy performs well situations addition optimistic policy simple easy implement unlike 2pass approach capable making instantaneous decisions 6 conclusions principal contributions reported paper summarized following list 1 proposed studied flexible buffer allocation ffl unified approach buffer allocation access patterns queries availability buffers runtime taken consideration achieved notion marginal gains give effective quantification buffers used efficiently ffl mgxy allocation algorithms designed achieve high total marginal gains maximize buffer utilization generalizing dbmin mg1001 allocate buffers flexibly ffl simulation results show flexible buffer allocation effective promising mgxy algorithms give higher throughput higher buffer utilization lower waiting time queries dbmin 2 proposed studied adaptable buffer allocation ffl extending flexible buffer allocation approach incorporates runtime information buffer allocation based simple accurate singleclass queueing model predicts impact buffer allocation decision ffl two performance predictors tp edu proposed general waiting query activated activation degrade performance system estimated predictors addition three different allocation policies stud ied optimistic pessimistic 2pass combined two predictors six different adaptable buffer allocation algorithms considered ffl simulation results indicate adaptable algorithms effective flexible dbmin compared flexible algorithms mgxy adaptable ones capable adapting changing workloads performing flexibly mgxy though costly compute extra time well paid finally simulation results show performance predictors tp edu perform equally well optimistic 2pass allocation policies effective taking implementation complexity account tpo seems best choice 3 set mathematical models analyze relational database references models provide formulas compute marginal gains performance predictions based tp edu ongoing research investigating extend predictors systems multiple disks set analytic models references data sharing also studying whether flexible predictor approach incorporated framework proposed cornell yu5 order improve quality query plans generated query optimizer finally interested deriving formulas computing marginal gains complex queries like sortmerge joins acknowledgements would like thank h chou dewitt allowing us use simulation program dbmin direct comparison made would also like thank anonymous referees many valuable suggestions comments r analysis performance inverted data base structures buffer management database systems evaluation buffer management strategies relational database systems implication certain assumptions data base performance evalua tion integration buffer management query optimization relational database environment principles database buffer management predictive load control flexible buffer allocation buffer management policies database environment history marginal utility theory database buffer paging virtual storage systems evaluation techniques storage hierarchies flexible buffer allocation based marginal gains mechanism managing buffer pool relational database system using hot set model buffer management relational database systems performance database manager virtual memory system design implementation ingres probability statistics reliability approximating block accesses database organizations tr ctr wenguang wang richard b bunt simulating db2 buffer pool management proceedings 2000 conference centre advanced studies collaborative research p13 november 1316 2000 mississauga ontario canada donghee lee jongmoo choi jonghun kim sam h noh sang lyul min yookun cho chong sang kim existence spectrum policies subsumes least recently used lru least frequently used lfu policies acm sigmetrics performance evaluation review v27 n1 p134143 june 1999 peter scheuermann junho shim radek vingralek watchman data warehouse intelligent cache manager proceedings 22th international conference large data bases p5162 september 0306 1996 jongmoo choi sam h noh sang lyul min eunyong ha yookun cho design implementation performance evaluation detectionbased adaptive block replacement scheme ieee transactions computers v51 n7 p793800 july 2002 database disk buffer management algorithm based prefetching proceedings seventh international conference information knowledge management p167174 november 0207 1998 bethesda maryland united states jongmoo choi sam h noh sang lyul min yookun cho implementation study detectionbased adaptive block replacement scheme proceedings annual technical conference 1999 usenix annual technical conference p1818 june 0611 1999 monterey california lee j choi j h kim h noh l min cho c kim lrfu spectrum policies subsumes least recently used least frequently used policies ieee transactions computers v50 n12 p13521361 december 2001 jongmoo choi sam h noh sang lyul min yookun cho towards applicationfilelevel characterization block references case finegrained buffer management acm sigmetrics performance evaluation review v28 n1 p286295 june 2000 jong min kim jongmoo choi jesung kim sam h noh sang lyul min yookun cho chong sang kim lowoverhead highperformance unified buffer management scheme exploits sequential looping references proceedings 4th conference symposium operating system design implementation p99 october 2225 2000 san diego california ronald p doyle jeffrey chase omer asad wei jin amin vahdat modelbased resource provisioning web service utility proceedings 4th conference usenix symposium internet technologies systems p55 march 2628 2003 seattle wa chris gniady ali r butt charlie hu programcounterbased pattern classification buffer caching proceedings 6th conference symposium opearting systems design implementation p2727 december 0608 2004 san francisco ca