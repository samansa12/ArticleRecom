efficient address generation affine subscripts dataparallel programs address generation compiling programs written hpf executable spmd code important necessary phase parallelizing compiler paper presents efficient compilation technique generate local memory access sequences blockcyclically distributed array references affine subscripts dataparallel programs memory accesses array reference affine subscript within twonested loop exist repetitive patterns outer inner loops use tables record memory accesses repetitive patterns according tables new startcomputation algorithm proposed compute starting elements processor outer loop iteration complexities table constructions oks2 k distribution block size s2 access stride inner loop tables constructed generating starting element outer loop iteration run o1 time moreover also show repetitive iterations outer loop pkgcdpks1 p number processors s1 access stride outer loop therefore total complexity generate local memory access sequences blockcyclically distributed array affine subscript twonested loop opkgcdpks1ks2 b introduction generally speaking dataparallel languages support three regular data distributions block cyclic blockcyclic data distributions block distribution distribute contiguous array elements evenly onto processors cyclic distribution distribute array element onto processors one time roundrobin fashion distribution blocks size k distributed onto processors roundrobin fashion blockcyclic distribution denoted cyclick blockcyclic distribution known general data distribution block cyclic distributions represented blockcyclic distribution cyclicd na e cyclic1 respectively na number array elements p number processors address generation problems compiling array references block cyclic distributions studied thoroughly 5 12 13 21 general problems compiling array references blockcyclic distribution also studied extensively 3 7 9 11 11 14 16 19 20 22 finite state machine fsm approach proposed traverse local memory access sequence processor 3 method tablebased approach table construction needs solve k linear diophantine equations incurs sorting operation work improving fsm approach 3 proposed 10 11 20 efficient fsm table generation proposed improved work enumerates local memory access sequences viewing accessed elements integer lattice sorting step 3 avoided improved work 7 authors use virtual processors generate communication sets processor different viewpoint blockcyclic distribution virtual processor approach actually contains two approaches one termed virtual block approach termed virtual cyclic approach virtual block approach views blockcyclic distribution block distribution set virtual processors cyclically mapped processors con trary virtual cyclic approach views blockcyclic distribution cyclic distribution set virtual processors blockwise mapped processors approaches similar either fsm approach virtual processor approach except modifications however consider simple array subscript array subscripts contain one induction variable recently several efforts compiling array references affine array subscripts proposed 1 10 11 15 17 22 affine array subscript means array subscript linear combination multiple induction variables mivs 1 authors use linear algebra framework generate communication sets affine array subscripts complex loop bounds local array subscripts generated code incur significant overhead tablebased approach proposed 22 authors classify blocks classes use class table record memory accesses first repetitive pattern using class table derived communication sets processor 1 22 addressing compilation array references affine subscripts within multinested loop ever proposed methods efficient enough dealing special case compiling array references affine sub scripts researchers pay attention array reference enclosed within twonested loop find better result 10 11 17 based fsm approach 3 kennedy et al proposed another approach solving compilation array references affine subscripts within twonested loop 10 11 memory accesses array reference affine subscript within twonested loop exist repetitive patterns outer inner loops moreover fix iteration outer loop affine subscript reduced simple subscript therefore iteration outer repetitive pattern use fsm approach generate local memory access sequences inner loop use fsm approach inner loop start computations decide initial state fsm iteration outer loop necessary proposed opk algorithm startcomputation p number processors k distribution block size outer loop found repetitive iterations pk iterations hence total complexity generate local memory access sequence array reference affine subscript within twonested loop op ramanujam et al proposed improved work find local starting elements processor 17 first find factor basis vectors jump global start processors space traverse lattice hitting starting element enddo enddo fig 1 hpflike program model considered paper since traverse step incurred complexity startcomputation algorithm ok thus total complexity ramanujams algorithm turned opk 2 paper also propose another new start computation algorithm preprocessing step required compute starting elements complexity preprocessing step ok 2 access stride inner loop preprocessing done time complexity generate starting element processor o1 addition also discover outer loop repetitive iterations pk gcdpk 1 instead pk 1 access stride outer loop therefore total complexity proposed approach opk gcdpk 1 asymptotical opk proposed approach correct also efficient paper organized follows section 2 formulates problem describes traditionally techniques generate local memory access sequences compiling array references affine subscripts within twonested loop efficient approach finding starting elements global start proposed section 3 performance analyses comparisons related work demonstrated section 4 section 5 concludes paper address generation affine subscripts 21 problem formulation specifically fig 1 illustrates program model considered paper array distributed onto processors cyclick distribution array reference contains two induction variables 1 2 access strides array reference respect 1 2 1 2 respectively access offset array reference fig 2 example program model shown fig 1 fig 2a layout array elements processors colored elements array elements accessed array reference twonested loop fig 2b shows global addresses array elements accessed every processor however data distribution transfers global addressing space processors local spaces therefore care generate local addresses processor accessed ele ments thus miv address generation problem generate local addresses array elements accessed processor like fig 2c shows 22 table based address generation affine subscripts case array reference containing single induction variable siv wellknown memory accesses processors repetitive pattern 3 finite state machine fsm built orderly iterate local memory access sequence pro cessor similarly array reference containing multiple induction variables mivs extend technique used siv orderly enumerate local memory access sequences processor consider program model shown fig 1 outer iteration miv address generation problem reduced siv problem thus utilize fsm approach generate local memory access sequence siv problem generating local memory access sequence miv problem therefore easily solved enumerating sequence outer loop iteration reaching outer loop bound example consider example illustrated fig 2 suppose outer loop iteration 1 0 twonested loop reduced single nested loop array reference turns a2i 2 thus finite state machine fsm built enumerate local memory access sequences siv problem fig 3 illustrates fsm generate local memory access sequences array reference contains single induction variable access stride 2 fig 3a shows fsm table next records next transition state deltam records local memory gaps successive array elements current state transmitting next state fig 3b transition diagram fsm initial state fsm depends position starting array element block processor instance starting element processor p 0 0 position b c fig 2 miv address generation example layout array elements processors b global addresses array elements accessed every proces sor c local addresses array elements accessed every processor state next deltam b fig 3 finite state machine fsm generate local memory access sequences siv problem access stride 2 block 0 thus initial state fsm case 1 0 state 0 addition initial state fsm also need know local address starting element since fsm records local memory gaps successive array elements allocated processor fsm enough information show start terms local address words although fsm initial state still know starting local address case exam ple besides initial state fsm state 0 know figure local address starting element obviously local address starting element 0 processor p 0 0 therefore local memory access sequence processor p 0 0 2 4 likewise twonested loop reduced single nested loop array reference simplified 37 finite state machine built case 1 still reused since memory access stride still 2 example 1 1 starting element processor 49 position block 1 thus initial state fsm case 1 1 state 1 however start terms local address key point 1 1 starting element processor p 0 local address processor p 0 13 therefore local memory access sequence processor p 0 13 15 similarly done likewise outer loop iteration accordingly obtain local memory access sequences processors fig 2c shows actually need iterate outer loop iterations 0 n 1 found iterating outer loop iterations pk gcdpk 1 times enough repetitive pattern outer loop discovery save lot time due avoidance recomputation repetitive patterns following theorem demonstrates repetitive period outer loop pk gcdpk 1 iterations since space lim ited proof theorem omitted paper one refer 18 details theorem 1 program model shown fig 1 memory accesses array reference repetitive pattern repetitive period respect outer loop iteration pk gcdpk 1 tions 2 according description evidently determining local address starting element outer loop iteration primary step solve miv address generation problem problem find local address starting element outer loop iteration described next section new approach generating local addresses starting elements presented next section well generating starting elements findings starting elements outer loop iterations important solving miv address generation problem obvious given outer loop iteration memory accesses depend inner loop access stride 2 therefore section use indicate inner loop access stride 2 except otherwise notified method find starting elements case k found 10 17 o1 com plexity however methods find starting elements case k opk ok spectively propose new approach find starting elements case k time complexity algorithm o1 problem solution described follows 31 problem description given overall description miv address generation problem section 22 finding starting element processor given outer loop iteration plays important role dealing miv address generation problem following formally describe induced problem let accessed element fixed outer loop iteration b fig 4 starting elements every processor example shown fig 2 global addresses starting elements every processor b local addresses starting elements every processor global start g local address global start specifically given global start g processor p g allocated processor q would like find starting element problem figure q local address starting element processor q example consider example shown fig 2a gray colored elements elements accessed array reference deepcolored shaded elements global starts corresponding every outer loop iterations lightcolored shaded elements processor starting elements corresponding every global starts suppose given global start 37 local address 9 processor starting elements processors p 0 p 3 49 41 45 respectively terms global addresses problem figure local addresses starting elements 13 9 9 respectively starting elements every outer loop iteration shown fig 4 global local addresses starting elements every processor listed figs 4a 4b respectively goal induced problem obtain table containing local addresses starting elements required processor every global start fig 4b shows required processor 32 preprocessing given global start g propose new approach find local address starting element q processor q case k proposed approach tablebased approach approach necessary precompute tables order evaluate starting elements given global start section describe character fig 5 onelevel mapping example illustrate ideas tables used paper assumes array elements distributed 4 processors cyclic4 distribution access stride 5 istics tables works proposed approach complexities time space analyzed section 4 sake space limi tation constructions tables omitted paper details please refer 18 321 c2p p2c offset tables wellknown accessed elements blocks repetitive pattern 22 blocks classified classes according positions accessed elements block note blocks class format blocks numbered terms class according rule b mod b block number block repetitive pattern contains blocks class 0 class 1 termed class cycle 22 addition since k one accessed element block therefore use table record position accessed element every class different 22 assume accessed element block class 0 first position position 0 blocks accessed element recorded table easily efficiently get position accessed element block class number block given therefore easily deduce q g since blocks classified classes denote table recording position accessed element every class c2p table example let us suppose array elements distributed 4 processors cyclic4 distribution access stride 5 layout array elements processors illustrated fig 5 figure accessed elements elements white text black background obviously blocks classified 5 classes class colored gradations gray color class number block labeled bottom block blocks bounded dashed line indicate repetitive pattern accessed elements class positions 0 1 2 3 respec tively therefore values c2p0 1 2 3 0 1 2 3 respectively moreover accessed element class 4 c2p4 thus obtain c2p table fig 7a illustrates c2p table example shown fig 5 get position accessed element block according class number block using c2p table contrast c2p table p2c table record class number according position accessed element block thus obtain class number block according position accessed element block generally speaking obtain class number block according position accesses element block using c2p table however requires search operation cases recognize class number block according c2p table instance number classes smaller block size possible position class accessed element position would cause confusion recognize class number posi tion therefore p2c table necessary p2c table constructed c2p table example considering c2p table shown fig 7a one scan table obtain p2c table p2c table illustrated fig 7b note case number classes smaller block size position class number correspond assume class number position class number previous po sition example suppose distribution block size 4 access stride 6 blocks classified 3 classes accessed elements class 0 1 2 positions 0 2 respectively result positions 0 2 corresponding classes 0 1 spectively p2c0 1 obviously positions 1 3 suitable class numbers correspond assumption class number corresponded position 1 0 previous position similarly class number corresponded position 3 1 thus p2c0 0 1 1 basically c2p p2c tables sense like hash table reason make assumption p2c table construction solve offset prob lem offset problem solved assumption conjunction another table offset generally speaking global start g position block however constructing c2p table assumed accessed element block class 0 position 0 construct p2c table position class number correspond assign class number previous value current value nevertheless according c2p table class number real position correspond consequently difference real position assumed position make assumption order make use c2p table every case use another table record difference order make shortcomings c2p table table denoted asoffset table paper discussed number classes larger block size position suitable class number correspond case offset table use fig 5 example condition offset table shown fig 7c hand number classes smaller block size follow example used explanation p2c table assumes distribution block size 4 access stride 6 c2p p2c tables 0 2 0 0 1 1 respectively since position 1 suitable class number correspond assign class number corresponded position 0 position 1 although position 1 correspond class 0 real position accessed element block class 0 position 0 according c2p ta ble thus 1 difference assumed value real value result offset11 similarly offset31 problem positions 0 2 since suitable class numbers correspond consequently offset table 0 1 0 1 322 nextactive jump tables previously described block contains one accessed element access stride larger block size course block may contain accessed element case thus name block elements accessed active block otherwise termed empty block tables nextact jump would like introduced used jumping empty b fig one group ordered sequence b multiple groups ordered sequence blocks active block one important observation processors viewpoint blocks processors repetitive pattern terms classes explain concretely let us take look example shown fig 5 blocks processor classes 0 4 3 2 1 repeat class 0 similar situation also happens processors sequence class numbers p 1 1 0 4 3 2 p 2 2 1 0 4 3 p 3 3 2 1 0 4 interesting sequence class numbers processor except initial class number processor sequence class numbers processor viewed sequence 0 4 initial class numbers p 0 p 3 0 1 2 3 respectively use notation 0 4 3 2 1 denote ordered sequence clearly class numbers appeared ordered sequence thus say one group ordered sequence fig 6a illustrates one group ordered sequence example addressed possible sequence class numbers processor may different may one group ordered sequence however groups mutually disjoint processor belong one one group give example illustrate phenomenon suppose array elements distributed 2 processors cyclic3 distribution access stride 12 4 classes example sequence class numbers 2 p 1 1 3 ordered sequence represented 0 21 3 obviously ordered sequence contains two groups one 0 2 another 1 3 0 2 1 mutually disjoint processor p 0 belongs group 0 belongs group 1 3 fig 6b illustrates multiple groups ordered sequence c offset nextact fig 7 tables used starting elements findings example important discovery since obtain class number next block processor current block class number current block known based discovery use one table record class number next active block current block processor another record many empty blocks need skip named nextact jump respectively constructions two tables based ordered sequence c2p table current block empty block need jump block thus value nextact table block recorded class number jump table recorded 0 otherwise traverse ordered sequence find active block value nextact table block recorded class number active block jump table recorded number blocks traversed find active block values nex tact jump tables recorded example array elements distributed 4 processors cyclic4 distribution access stride 8 nextact jump tables 0 0 respectively although processor belong one one group nextact table suitable processors since construction table based class number group nextact jump tables example shown fig 5 illustrated fig 7d e respectively 33 algorithm tables evaluate starting element given global start o1 time complex ity fig 8 illustrates algorithm evaluate starting element given global start term algorithm start computation algorithm basic concept start computation algorithm algorithm start computation algorithm case k input g global start p processor global start allocated q processor would like find starting element q 6 p k distribution block size p number processors access stride c number classes c2p p2c offset nextact jump tables output q starting element processor q steps 1 pos 2 dist 3 4 pos 5 pos 7 return starting element q 8 else pos jumpclambdak 9 endif 10 endif 12 q p 13 14 endif 16 return q fig 8 start computation algorithm case viewpoint global start try figure distance starting element global start distance therefore get local address starting element adding distance local address global start details algorithm described follows given g local address global start g allocated step 1 calculate position block global start obtained value stored pos g step 2 measure distance processors p q stored dist step 3 p2cpos g obtain class number block global start thus step 3 get class number block processor q may contain starting element class number obtained step 3 represented c according c2p table c2pc get position accessed element block class c ever therefore step 4 obtain position block starting element processor q obtained position represented pos pos equal means accessed element block course accessed element starting element go direct step 11 evaluate distance starting element global start q p still need add size block distance since starting element must one course global start steps 1214 done result local address starting element obtained step 15 shows hand pos means accessed element block use nextact table obtain class number next active block nextactc implies exists active block processor certainly starting element pro cessor otherwise means find active block processor get number blocks required jump current block next active block position accessed element active block jump nextact tables respectively since pos step 4 represents position starting element block course global start hence distance caused number blocks required jump active block added pos case thus step 8 steps 11 explained previous paragraph let us take fig 9 example assumes given global start 37 whose local address 9 processor first find starting element processor input start computation algorithm tables used example shown fig 7 following steps 1 4 algorithm obtain pos 2 since pos equal go direct step 11 obtain offset 1 due invalidation condition step 12 go direct step 15 2 corresponds array element 42 terms global address input except take finding starting element processor p 0 fig 9 layout array elements processors case 2 k another miv example another example executing step 4 pos 4 pos since pos equals means block contains accessed element go step 6 according nextact jump tables active block one block empty block processor p 0 step 8 pos 7 step 11 offset 6 needs add 4 size block turns corresponds array element 67 terms global address evidently time complexity start computation algorithm o1 complexity analyses tables used algorithm performance comparisons existing methods discussed section 4 performance analyses comparisons performance analyses tables used start computation algorithm shown table 1 performance comparisons method existing methods shown table 2 sake space limitation please refer 18 details conclusions paper presented efficient approach evaluation starting element processor given global start key step solve miv address generation problem table 1 performance analyses table complexity time space offset nextact oc c table 2 performance comparisons 10s 17s start comp o1 o1 o1 preprocess o1 o1 os 2 start comp opk ok o1 outer loop repetitive pk pk pk iterations total dataparallel programs assuming array block cyclically distributed subscript affine approach tablebased approach constructions tables require os plexity 2 access stride inner loop tables start computation algorithm run o1 time addition discovered exists repetitive pattern every iterations therefore miv address generation problem solved access stride outer loop future would like apply address generation approach evaluate communication sets furthermore address generation communication sets evaluation general affine subscripts also investigation r linear algebra framework static hpf code distribution programming vienna fortran generating local addresses communication sets data parallel programs automatic parallelization distributedmemory multiprocessing systems concrete mathematics compiling array expressions efficient execution distributedmemory machines high performance fortran forum efficient address generation blockcyclic distri butions lineartime algorithm computing memory access sequence dataparallel programs compiling global namespace parallel loops distributed execu tion local iteration set computation blockcyclic distributions computing local iteration set blockcyclically distributed reference affine subscripts optimizing representation local iteration sets access sequences blockcyclic distributions code generation complex subscripts dataparallel programs generating communication array state ments design efficient computation address sequences data parallel programs using closed forms basis vectors optimizing fortran compiler mimd distributedmemory machines compiling array references affine functions dataparallel programs tr concrete mathematics foundation computer science compiletime generation regular communications patterns vienna fortranmyampersandmdasha fortran language extension distributed memory multiprocessors high performance fortran handbook generating communication array statements compilation techniques blockcyclic distributions optimizing fortran compiler mimd distributedmemory machines generating local addresses communication sets dataparallel programs lineartime algorithm computing memory access sequence dataparallel programs efficient address generation blockcyclic distributions compiling array expressions efficient execution distributedmemory machines efficient computation address sequences data parallel programs using closed forms basis vectors empirical study fortran programs parallelizing compilers compiling global namespace parallel loops distributed execution code generation complex subscripts dataparallel programs