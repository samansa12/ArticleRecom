incremental syntactic parsing natural language corpora simple synchrony networks abstractthis article explores use simple synchrony networks ssns learning parse english sentences drawn corpus naturally occurring text parsing natural language sentences requires taking sequence words outputting hierarchical structure representing words fit together form constituents feedforward simple recurrent networks great difficulty task part number relationships required specify structure large number unit outputs available ssns representational power output necessary on2 possible structural relationships ssns extend incremental outputs simple recurrent networks entity outputs provided temporal synchrony variable binding article presents incremental representation constituent structures allows ssns make effective use dimensions experiments learning parse naturally occurring text show output format supports effective representation effective generalization ssns emphasize importance generalization ability article also proposes shortterm memory mechanism retaining bounded number constituents parsing mechanism improves on2 speed basic ssn architecture linear time experiments confirm generalization ability ssn networks maintained b introduction article explores use simple synchrony networks ssns learning parse english sentences drawn corpus naturally occurring text ssn defined previous work 17 23 extension simple recurrent network srn 6 7 ssn extends srns temporal synchrony variable binding tsvb 33 enables ssn represent structures generalise across structural constituents apply ssns syntactic parsing natural language provides standard task real world data requires structured output parsing natural language sentences requires taking sequence words outputting hierarchical structure representing words fit together form constituents noun phrases verb phrases stateoftheart techniques tackling task statistical language learning 2 3 5 20 basic connectionist approach learning language based around srn network trained predict next word sentence 7 8 30 else trained assess whether sentence grammatical 24 25 however simple srn produced results comparable statistical parsers basic output representation flat unstructured reason simple srn produce structured output representations lies required number relationships must output specify structure parse tree srn relationships may output n number words sentence however parse tree may specify structural relationship word word requiring 2 outputs possible simple srn length sentence unbounded number output units fixed fundamentally even scheme devised bounding required number outputs stm mechanism discussed using large numbers output units means learning large number distinct mappings thus generalising across distinctions thus article focus representation syntactic structures networks output crucially demonstration resulting networks generalise appropriate way learning one example connectionist parser uses multiple groups output units represent multiple structural relationships hebbian connectionist network 13 network explicitly enforces generalisation across structural constituents requiring group output units trained random selection constituents however amount nontrainable internal structure required enforce generalisation represent possible forms structure severe limitation particular component network would need grow length complexity sentences network tested toy sublanguage demonstrated scale requirements naturally occurring text two alternatives increasing number output units increase amount information represented units activation level increase amount time used output structure approaches exemplified confluent preorder parser 18 holistic parsers confluent preorder parser first encodes sentence distributed representation representation uses bounded number units use continuous activation values allows theory encode sentence unbounded length distributed representation decoded different sequence represents sentences syntactic structure particular preorder traversal structure output sequence long needs output required structural relationships thereby avoiding restriction srn outputs decoding stage previous encoding stage miss important generalisations manifested explicitly structural representation thus scale well naturally occurring text decoding stage structural constituents close together structure may end far apart sequential encoding structure thus important regularities relationship constituents easily learned regularities constituents happen occur next sequence learned 1 encoding stage sentences get longer ability fixed sized distributed representation encode entire sentence degrades indeed 18 point representational capacity approaches limited preventing scale beyond toy grammars approach represent structural constituents directly rather using one indirect encodings thus connectionist architecture must able output 2 structural relationships parse tree achieve ssn extends incremental outputs srn entity outputs provided tsvb enough simply provide representation point using direct encoding enable network learn regularities motivated use structured representation first place 2 ssn use tsvb means learned regularities inherently generalise structural constituents 15 17 thereby capturing important linguistic properties systematicity 15 generalisation ability allows ssn parser presented article scale naturally occurring text article demonstrate ssn represent structured outputs necessary natural language parsing way allows ssn learn parse corpus real natural language text emphasise generalisation ability required learn task also introduce extension ssn namely shortterm memory stm mechanism places bound number words involved structural relationships given time improves 2 speed basic ssn architecture linear time also means relationships output however unlike previous connectionist approaches parsing bound affect ability ssns generalise across bounded dimension thus generalise linguistically appropriate way indeed performance ssn parser actually improves addition stm simple synchrony networks section provide summary simple synchrony networks ssns 17 23 begin describing basic principles temporal synchrony variable binding tsvb 33 extend standard connectionist networks pulsing units pulsing units enable network provide output entity word encountered current one standard connectionist unit briefly summarise main equations defining operation tsvb networks describe training algorithm finally give three example ssn architectures ssns defined restriction space possible tsvb networks 1 methods long shortterm memory 19 help learn regularities distant items sequence cannot totally overcome unhelpful bias 2 note argument applies domain structured representations found useful express important regularities thus although article focuses requirements parsing natural language ssn architecture would relevant task best thought mapping sequence inputs structure 21 trainable tsvb networks tsvb 33 connectionist technique solving binding problem use synchrony activation pulses binding problem arises multiple entities represented multiple properties mechanism required indicate properties bound entities example seeing two objects properties red green square triangle mechanism needed indicate colour relates shape one method provide variables x stand two objects scene may unambiguously described redx greeny squarex triangle another mechanism use synchrony binding two units representing properties bound entity pulsing synchronously proposal originally made biological grounds malsburg 36 earlier implementations tsvb 14 33 used nondifferentiable binary threshold units could trained using standard connectionist techniques section describe different implementation tsvb one based standard sigmoid activation units yields trainable implementation tsvb networks order implement tsvb network central idea divide time period number phases phase associated unique entity correspondence phases entities means phases analogous variables units active phase representing information entity units predicates variable analogy variables phase numbers play role determining unit activations within network two kinds unit first pulsing unit computes individual phases independent phases number phases time period nt may vary pulsing units output activation ntplace vector ie activation pulsing unit j time formed nt values fo p activation unit j phase p time second type unit nonpulsing unit computes across phases equally current time period output activation across every phase time define net input output activation separately type unit within tsvb network based type unit receiving activation index pulsing units network set integers u ae nonpulsing units set integers u noninput unit j receives activation units indexed set inputs j recurrent links handled without loss generality adding context units network context units activation value another unit previous time step function c maps unit associated context unit units linked realvalued weights link unit unit j weight w ji given definitions output activation pulsing unit j 2 u ae phase p time defined follows net p i2inputs j w p input unit standard sigmoid function note net function phase p takes activation pulsing units phase p nonpulsing units whose activation across phases achieved function r p represents activation unit phase p time nonpulsing units 2 u constant activation across time period activation pulsing units 2 u ae output separate activation phase time period activation definition output activation nonpulsing unit complicated possibility nonpulsing unit inputs pulsing units case activations unbounded number phases would combined single input value discussed 22 including links necessary decreases effectiveness architecture thus allowed simple synchrony networks given simplification output activation nonpulsing unit j 2 u time defined net i2inputs j j j input unit note nonpulsing units act like standard unit within srn order train tsvb networks use novel extension backpropagation time bptt 31 applying bptt standard recurrent network one copy network made time step input sequence extending bptt networks requires copy network every phase time period unfolding procedure copies pulsing nonpulsing units per time period pulsing units copied additionally per phase standard bptt unfolded network feedforward network trained using backpropagation however unfolded network set copies original training progresses changing weights must constrained ensure copy link uses weight achieved summing individual changes copy link 22 ssn architectures three example ssn architectures illustrated figure 1 figure depicts layers units rectangles blocks layer containing however many units system designer chooses rectangles denote layers nonpulsing units blocks denote layers pulsing units links layers solid lines indicate every unit source layer connected every unit target layer discussed recurrence implemented context units srns 7 dotted lines indicate activation unit source layer copied corresponding context unit target layer three architectures possess layer pulsing input units separate layer nonpulsing input units procedure inputting information ssns little different standard connectionist networks consider sequence inputs b c different pattern activation defined different input symbol example activating one input unit represent symbol rest input units inactive ie localist representation srn sequence input patterns would presented network consecutive time periods thus time period 1 output nonpulsing pulsing input input nonpulsing pulsing output nonpulsing pulsing input output figure 1 three simple synchrony networks rectangles layers nonpulsing units blocks layers pulsing units srn would receive pattern symbol input time period 2 would receive pattern symbol b input etc ssns nonpulsing input units operate way input symbol time period simply presented nonpulsing units pulsing units input symbol introduced new phase ie one unused input sequence point thus time period 1 ssn would receive pattern symbol pulsing inputs phase 1 time period 2 pattern symbol b would presented pulsing inputs phase 2 three architectures illustrated figure 1 cover three different options combining information nonpulsing inputs information pulsing inputs essentially nonpulsing units contain information relevant sentence whole pulsing units information relevant specific constituents information combined three possible ways recurrence type type b type c given constraint discussed section 21 ssns links pulsing nonpulsing units three types partition possible architectures combination layer types b c recurrent layers output layer optional empirically compare three illustrated architectures experiments section 4 3 syntactic parsing syntactic parsing natural language center great deal research theoretical cognitive practical importance purposes provides standard task real world data requires structured output syntactic parser takes words sentence produces hierarchical structure representing words fit together form constituents sentence section discuss structure represented ssn implications representation 31 statistical parsers traditionally syntactic parsing addressed devising algorithms enforcing syntactic grammars define isnt possible constituent structure sentence recent work focused incorporate probabilities grammars 2 20 estimate probabilities corpus naturally occurring text output structure taken structure highest probability according estimates example probabilistic contextfree grammars pcfgs 20 contextfree grammars probabilities associated rewrite rules probability entire structure multiplication probabilities rewrite rules used derive structure straightforward translation contextfree grammars statistical paradigm rule probabilities simply multiplied assumed independent contextfree assumption means rules applied independently work statistical parsing focuses finding good independence assumptions often takes starting point linguistic claims basic building blocks syntactic grammar ssn parser considered statistical parser network form statistical model however clear differences firstly grammar traditional sense networks grammatical information held implicitly pattern link weights fundamentally fewer independence assumptions network decides information pay attention ignore statistical issues combining multiple estimators smoothing sparse data handled network training usually case onesizefitsall machine learning techniques domain knowledge gone design ssn parser first apparent particular general inputoutput representation designed make linguistic generalisations easy network extract example ssn incrementally processes one word time output required time related word reflects incremental nature human language processing also biases network towards learning wordspecific generalisations wordspecific nature linguistic generalisations manifested current popularity lexicalised grammar representations 5 also shortterm memory mechanism discussed later section motivated psycholinguistic phenomena particular motivations discussed arise 32 structured output representations ssns syntactic structure natural language sentence hierarchical structure representing sentences words fit together form constituents noun phrases relative clauses structure often specified form tree constituents nodes tree parentchild relationships representing hierarchy words included child constituent also included parent constituent thus output syntactic structure outputting set constituents set parentchild relationships words difficulty outputting structure arises number parentchild relationships linguistic grounds safe assume number constituents linear number words 3 thus introduce bounded number entities word one entity constituent 4 problem still leaves 2 quadratic possible parentchild relationships linear constituents solution make full use times words presented network entities given time thus cannot wait words input output entire structure done symbolic parser instead must incrementally output pieces structure time whole sentence input whole structure output 3 simply means arent unbounded numbers constituents contain set words unbranching constituents 4 alternatively could introduce one entity word one entity represent bounded number constituents discussed assuming bound 1 two alternatives equivalent first aspect solution new constituents introduced ssn word input words period new phases ie entities must added set phases processed illustrated figure 1 ssns two banks input units pulsing nonpulsing nonpulsing input units hold part speech tag word input current period simplicity use words using pulsing input units wordtag also input new phases introduced current period way number constituents represented network grows linearly number words input experiments discussed make slightly stronger assumption reasons detailed namely one constituent added wordtag means network output parse trees contain many constituents words sentence simplification requires adjustments made preparsed corpus naturally occurring text linguistically unmotivated result equivalent form dependency grammar 27 grammars long linguistic tradition return adjustments required training set considering corpus used experiments section 4 constituents need ensure enough information structure output period entire structure specified end sentence need make use pulsing output units illustrated figure 1 description parsing process refers example figure 2 illustrates sentence mary saw game bad sentence represented sequence wordtags np vvd nn vbd jj sentence structure contains separate constituents subject noun n object clause contains noun phrase n note parse tree constituents conflated comply constraint one constituent per relation standard parsetree representations covered section 4 pulsing units network n th time period provide output n constituents represented ssn period mentioned obviously want outputs relate n th wordtag input period one thing want output time parent wordtag within constituent structure thus simply parent output unit pulses period phase constituent parent periods wordtag examples parent relationships shown figure 2 examples outputs shown table 1 experiments discussed assume constituent identified first wordtag attaches way first word tag attach given constituent parent constituent introduced wordtag np vvd vbd example otherwise wordtags parent constituent introduced leftmost sibling word nn jj example cases newly introduced constituent simply play role constituent structure parent output unit enough specify parentchild relationships constituents wordtags leaving parentchild relationships constituents take maximally incremental approach parentchild relationship needs output soon constituents involved introduced ssn two cases parent introduced child child introduced parent first case covered adding grandparent output unit output specifies grandparent constituent current wordtag illustrated figure 2 table 1 vbd grandparent constituent must parent constituent parent current wordtag words marysawthegamewasbad f parent sibling grandparent figure 2 sample parse tree solid lines indicate parse tree dotted dashed lines relationships words nodes time pulsing inputs nonpulsing structure outputs period phase inputs phase table 1 input information corresponding grandparentparentsibling outputs sample structure shown figure 2 combination parent output grandparent output specifies parentchild relationship wordtags grandparent parent second case covered similarly sibling output unit output specifies constituent shares parent current wordtag illustrated figure 2 table 1 vvd vbd combination parent output sibling output specifies parentchild relationship wordtags parent siblings possibly one experiments grandparent sibling outputs used period wordtags parent first introduced interpretation outputs best described reference detailed trace activation input output units provided table 1 structure figure 2 first two columns table period numbers show inputs ssn first column shows activation presented bank pulsing input units row column represents separate time period column divided separate phases 6 sufficient example cell indicates information input network indicated time period phase wordtag appearing cell indicates pulsing input units active time period phase second column shows activation presented bank nonpulsing units phase information relevant units wordtag given simply indicates nonpulsing input units active time period third column table shows activation present output units every period parent p output active phase constituent periods wordtag attached period 1 np proper noun input attached constituent number 1 constituent introduced period 1 specified wordtags parent previous wordtags parent previous wordtags case thing happens parent outputs periods 2 3 5 period 4 wordtag nn noun input attached constituent 3 constituent introduced input article previous period specifies nn parent namely constituent 3 relationship specified period 6 jj adverbadjective input preceding vbd verb given set constituents identified parent outputs grandparent g sibling outputs specify structural relationships constituents specify outputs periods new constituent identified periods 2 3 5 outputs period 1 constituents specify relationships period 2 constituent 1 specified sibling vvd wordtag input thus since constituent 2 parent vvd constituent must also parent constituent 1 constituent 2 parent root tree structure period 3 siblings grandparent specified constituent 3 constituent children parent yet introduced period 5 parent constituent 3 specified constituent 5 sibling output also period 5 grandparent output unit pulses phase 2 specifies constituent 2 grandparent vbd wordtag input period 5 thus constituent 2 parent constituent 5 since constituent 5 parent vbd note use phases represent constituents means special mechanisms necessary handle case one constituent f embedded within another addition structural relationships natural language syntactic structures typically also include labels constituents relatively straightforward ssns achieve use additional set pulsing output units one label network indicates label given constituent pulsing labels unit phase new constituent introduced parse tree far described target output ssn units either pulsing based srns actual unit outputs course continuous values 0 1 variety ways interpret patterns continuous values specification constituency experiments discussed simply threshold units activations 06 treated indicated set structural relationships converted set constituents may evaluated using precision recall measures standard statistical language learning precision percentage output constituents correct recall percentage correct constituents output important characteristic ssns outputting structures three output units grandparent parent sibling sufficient specify structures allowed assumptions one constituent needs introduced word 5 call gps representation ssn proceeds incrementally sentence wordtag outputs wordtags parent parents first wordchild parentchild relationships parent previous constituents sentence time ssn reaches last word sentence cumulative output specify parentchild relationships constituents thus specifying entire hierarchical structure sentences constituency 33 inherent generalisations section 2 described ssn training algorithm variety possible ssn architectures definition tsvb units retains augments properties standard feedforward recurrent connectionist networks ssns retain advantages distributed representations ability generalise across sequences ssn also shown support class structured representation although representation important extending range domains connectionist networks may applied ssns use phases identify structural constituents also confers powerful generalisation ability specifically ability generalise learnt information across structural constituents example kind generalisation consider required generalise sentence john loves mary sentence mary loves john sentences network needs output john mary noun phrases noun phrase preceding verb subject noun phrase verb object order generalise must learn four things independently yet particular sentence must represent binding constituents word syntactic position ssn achieves generalisation ability using temporal phases represent bindings using link weights represent generalisations link weights used phase information learned one phase inherently generalised constituents phases thus network learned input mary correlates noun phrase produce noun phrase output regardless features syntactic position bound phase 5 mentioned set allowable structures expanded either increasing number entities introduced word expanding number structural relationships allow entity represent one constituent either case number constituents still must linear number words constraint could relaxed allowing unbounded computation steps per word input similarly network learn noun phrase preceding verb correlates noun phrase subject verb regardless features word mary bound phase even network never seen mary subject application two independent rules phase produce pattern synchronous activation represents mary subject henderson 15 shown inherent ability tsvb networks generalise across constituents relates systematicity 9 34 shortterm memory learningbased systems systems ability generalise training sets testing sets determines value implies real value ssn ability generalise constituents ability output 2 structural relationships suspicion confirmed consider specific characteristics domain natural language sentences long known constraints peoples ability process language put bound constructions centre embedding 4 constructions would actually require allowing 2 structural relationships example rat cat dog chased bit died almost impossible understand without pencil paper dog chased cat bit rat died easy understand basic reason difference first case noun phrases need kept memory relationships later verbs determined second case noun phrase forgotten soon verb following seen motivated observation work showing many domains people keep small number things active memory one time 28 added shortterm memory stm mechanism basic ssn architecture mechanism improves ssns efficiency time definition ssn far stated wordtag input network input new phase network information computed phases every subsequent time period however bound depth centre embedding implies given time period relatively small number phases referred later parts parse tree trick work phases going relevant later processing compute information phases idea use simple one based idea audioloop proposed baddeley 1 instead computing phases current time period instead compute stm queue queue maximum size bound stm referred new phase introduced network phase added head queue phase referred one output units phase moved head queue simple mechanism means unimportant phases ie referred output move end queue forgotten note training target outputs used determine phases moved head queue actual outputs thereby ensuring network learns relevant phases also information held network wordtags constituents specific phases position stm input word order therefore items cannot confused reordering process occurs stm indeed precisely use phases represent constituents allows ssn keep ability generalise constituents still parse time 4 experiments syntactic parsing ssns section describe experiments training range ssns parse sentences drawn corpus real natural language experiments demonstrate ssn may used connectionist language learning structured output representations also fact ssns performance evaluated terms precision recall constituents means ssns performance may directly compared statistical parsers first describe corpus used provide results give analysis training data 41 natural language corpus use susanne corpus source preparsed sentences experiments susanne corpus subset brown corpus preparsed according classification scheme described 32 order use susanne corpus convert provided information format suitable presentation parser susanne scheme provides detailed information every word symbol within corpus use wordtags input network due time constraints limited size corpus wordtags susanne detailed extension tags used lancasterleeds treebank 12 experiments simpler lancasterleeds scheme used wordtag two three letter sequence eg john would encoded np articles encoded verbs encoded vbz wordtag input network setting one bit three banks input bank representing one letter position set bit indicating letter space occupies position order construct set constituents forming target parse tree first need extract syntactic constituents wealth information provided classification scheme includes information metasentence level discarded semantic relations constituents finally described gps representation used experiments requires every constituent least one terminal child limitation violated constructions though one svp division common example sentence mary loves john typical encoding would np mary vp v loves np john linguistic head verb loves within vp tags immediate children address problem collapse vp single constituent producing np mary v loves np john done constructions include adjective noun determiner prepositional phrases corpus used number changes fairly minor 6 42 results one susanne genres genre press reportage chosen experiments training crossvalidation test sets selected random total ratio 411 sentences fewer 15 wordtags selected training set form set 265 sentences containing 2834 wordtags average sentence length 107 similarly crossvalidation set selected containing 38 sentences 418 wordtagsout 1580 constituents 265 lost svp change 28 similar changes made relative clauses 12 adjustments required nonverb constructions verb clauses could artificially reintroduced output leaves around irrecoverable changes corpus structure average sentence length 110 test set 34 sentences containing 346 words average sentence length 102 three ssn types b c tested twelve networks trained type consisting four sizes network 20 100 units layer size tested three different stm lengths 3 6 10 network trained training set 100 epochs using constant learning rate j 005 table 2 gives figures five networks network performance three datasets training crossvalidation test given three categories number correct sentences measure number correct constituents precision recall percentage correct responses output unit measure precision recall used constituent evaluation standard measure used statistical language learning 20 precision number correct constituents output parser divided total number constituents output parser recall number correct constituents divided number target parse constituent counted correct contains set words target label 7 presence measure results significant confirms similarity inputoutput representations used ssn used statistical parsers therefore direct comparisons made return point section 5 considering figures table type networks particularly successful rare sentence correctly parsed results best performing type network given first row table type b c networks much successful type results two networks given first best average precisionrecall measure second better results individual outputs ie g p label type b c networks produce similar ranges performance around 25 sentences correct 7080 scored average precisionrecall better networks particular percentage correct constituent labels p output exceed 90 also notable percentage results networks similar across three datasets indicating network learnt robust mapping input sentences output parse trees level generalisation around 80 average precisionrecall similar achieved pcfg parsers 20 although fair comparison identical experiments must performed algorithm return point section 5 43 analysis results basic experimental results provided detailed values performance network specific output relationships well combined performance terms constituentlevel measures precision recall results broken compared see progress learning networks time comparison effect stm queue table actual dependencies present data 431 effects stm length effects stm length seen plotting performance one type network varying sizes stm done figure 3 performance type c network 80 units every hidden layer shown three sizes stm ie 3 6 andprecision may compared standard measure errors commission recall standard errors omission test type stm 10 100 units train 1265 0 347 313 356 659 208 893 cross 038 0 318 297 305 652 187 847 units layer train 63265 24 699 687 741 943 730 975 cross 1038 26 715 712 748 956 707 964 test units layer train 62265 24 660 644 752 923 846 975 cross 1238 32 653 642 824 926 853 962 test units layer train 64265 24 726 717 708 957 721 984 cross 938 24 744 738 718 973 707 978 test units layer train 56265 15 655 637 681 928 823 971 cross 838 21 636 611 687 915 813 955 test table 2 comparison network types learning parse 10 separate graphs show constituentlevel performance network terms average precisionrecall performance separate output units grandparent parent sibling constituent label latter though group units treated single output graphs demonstrate constituentlevel shorter stm lengths perform better however longer stm lengths achieve greater accuracy specific outputs particular respect sibling output expected longer lengths preserve information greater likelihood containing phase referred specific output 432 dependency lengths data set important concern connectionist language learning length dependency srn learn 8 section provide analysis data set see exactly dependency lengths present corpus naturally occurring text table 3 contains analysis lengths dependency contained sentences maximum length words length dependency number words current word indicated output table lists separately lengths output units final two columns providing total number percentage dependency length across whole corpus surprising result table dependencies almost 70 relate current word predecessor sharp tailing frequency consider longer dependencies table shows shortest lengths lengths tail gradually length 25 words stm network process limited number words one time trainingepochs percentagecorrect trainingepochs percentagecorrect grandparentrelationship80400 trainingepochs percentagecorrect trainingepochs percentagecorrect parentrelationship80400 trainingepochs percentagecorrect constituentlabel figure 3 comparison effects stm length type c network 80 units every hidden layer dependency length g p total length length mean length 27 08 33 16 table 3 dependencies type length sentences fewer words dependencies shown greatest length 25 words number sentences 716 number words 13472 average stm dependency length g p total length length 4 284 127 78 489 25 961 mean length 20 07 27 12 table 4 dependencies type length across stm sentences table 3 length dependency network handle altered stm length dependency number places queue phase progressed required table 4 provide similar analysis time instead counting length number intervening phases count length dependency position phase occupies stm thus phase third position stm required length dependency given three table shows similar effect range dependency lengths table 3 although greater concentration shortest lengths desired limited number longer dependencies shown still extend length 25 isolated words punctuation symbols referred 433 conclusions experiments impact stm quite considerable respect training times reducing least order magnitude discussed actual lengths dependencies encountered network changed much addition stm experiments show longer stms achieve better performance specific outputs network however shorter stm still yields best level constituent accuracy difference interest choice stm length depends ones measure performance better performance achieved specific outputs longer stm desired output likely appear stm better performance achieved constituent level based competition different outputs smaller stm reduces likelihood spurious outputs competing correct ones note also domain specificity last point smaller stm works natural language bias towards shorter dependencies article focussed ssns use incremental representation constituent structure order learn parse addition shown arguments generalisation abilities necessary learn parse distinct arguments bounds abilities introduction stm mechanism enhances efficiency basic ssn without harming ability learn parse section consider brief importance results connectionist language learning model compares extensions srns handling structured representations first experiments described demonstrate connectionist network successfully learn generate parse trees sentences drawn corpus naturally occurring text standard task computational language learning using statistical methods performance measures precisionrecall applied output ssn typical statistical method simple probabilistic context free grammar pcfg direct comparisons made two approaches instance simple pcfg achieve around 72 average precisionrecall 20 parsing sequences wordtags comparison ssn experiments achieves 80 average precisionrecall trained tested sentences fewer words however fair comparison corpora sizes contents dissimilar extension work henderson 16 presented slight variant basic ssn model compared performance directly pcfgs identical corpora results pcfg due restricted size training set able parse half test sentences precisionrecall figure 5429 comparison ssn able parse sentences yielded performance 6565 even count parsed sentences pcfg performance 5458 compared ssns performance 6867 subset variations introduced henderson 16 ssn mostly affect input layer article pulsing inputs ssn receive input newly introduced phases requiring network remember previous periods input 16 pulsing input previous period carried forward particular phase additional pulsing input unit used distinguish newly introduced phase others change input representation results 16 achieved type ssn noted introduction article experiments natural language using srns typically used restricted form input representation either predicting next word sentence 6 8 30 assessing whether grammatical 24 25 extension srn ssn corrects limitation enhancing range output representations include structured parse trees approach designed generate structured representation given sequence input data generation aspect task largely distinguishes approach extensions srns handling structured data example backpropagation structure bpts algorithm 35 11 assumes network trained process structured input data either classification 10 transformation 11 transformation task closer training parser conclusion 11 makes clear use bpts relies input output structural form prevents networks directly applicable task generating parse tree sequence input wordtags however relationship bpts ssn terms ssns temporal structure ssn trained using extension backpropagation time network unfolded two temporal dimensions period phase one specific instance bpts structure question temporal structure however mapping temporal structure structures domain done part interpretation networks output activations using gps representation thus fall within bpts framework broader context transforming structured data ssn incremental parse tree representation described article thereby offer one way connectionist network generate structured output data unstructured input apart models based srns forms connectionist network proposed handling types structured information required language learning instance hadley hayward 13 propose highly structured network learns generalise across syntactic structures accordance systematicity 9 however approach limited due amount nontrainable internal structure required enforce appropriate generalisations discussed section 33 henderson 15 ssn relies temporal synchrony produce similar effect renders generalisation ability ssns largely independent specific architectural details indeed experiments training type b ssn recursive grammar hadley hayward 13 similar ability generalise across syntactic structures demonstrated 21 22 discussion makes clear identical ssn network learns effectively specific toy grammar corpus naturally occurring text added ability generalise constituents allows ssn generalise linguistically appropriate way thus deal high variability naturally occurring text transfer toy domain real corpus sentences sets ssn apart number proposals connectionist language learning tend limited applications involving toy grammars alone include approaches encode sentences recursively distributed representation holistic parsers 18 labelledraams 34 number cycles recursive encoding depends size parse tree means performance degrades complexity sentences increases makes difficult apply approaches naturally occurring text ssn address specific problem attempting encode everything distributed representation prior extracting parse tree incrementally outputting pieces parse tree incremental approach parsing presents different model connectionist parsing one similar classical deterministic parsers described marcus 26 example 6 conclusion article described use simple synchrony networks ssns learning parse samples english sentences drawn corpus naturally occurring text described inputoutput representation enables ssn incrementally output parse tree sentence representation important demonstrating connectionist architecture manipulate hierarchical recursive output representations also introduced important mechanism improving 2 speed basic ssn architecture linear time mechanism based concept shortterm memory stm enables ssn retain necessary constituents processing experiments demonstrated number ssn architectures provide reliable generalisation performance domain theoretical experimental results article go beyond language learning apparent ssn architectures modelled simple recurrent network specifically adapted natural language thus ability learn manipulate structured information general one although specific inputoutput representation used experiments carefully tailored target domain underlying principles incremental output recursively defined structures may applied widely also stm queue although defined based cognitive limitations language processing may also used domains appropriate acknowledgements authors would like thank fernand gobet reviewers editors helpful comments article work carried whilst first author department computer science university exeter funded engineering physical sciences research council uk acknowledge roles economic social research council uk sponsor university sussex grantholder providing susanne corpus used experiments reported article r working memory statistical language learning statistical techniques natural language parsing certain formal properties grammars finding structure time distributed representations learning development neural networks importance starting small critical analysis efficient classification data structures neural networks general framework adaptive processing data structures computational analysis english corpusbased approach strong semantic systematicity hebbian connectionist learning description based parsing connectionist network connectionist architecture inherent systematicity neural network parser handles sparse data connectionist architecture learning parse design connectionist holistic parser long shortterm memory pcfg models linguistic tree representations simple synchrony networks learning generalisations across syntactic constituents simple synchrony networks new connectionist architecture applied natural language parsing simple synchrony networks natural language grammatical inference comparison recurrent neural networks machine learning methods natural language grammatical inference recurrent neural networks theory syntactic recognition natural language dependency syntax theory practice magical number seven enriched lexical representations learning internal representations error propagation english computer simple associations systematic reasoning connectionist representation rules stability properties labeling recursive autoassociative memory supervised neural networks classification structures tr ctr james henderson neural network parser handles sparse data new developments parsing technology kluwer academic publishers norwell 2004 james henderson segmenting state entities implication learning emergent neural computational architectures based neuroscience towards neuroscienceinspired computing springerverlag new york inc new york ny 2001 marshall r mayberry iii risto miikkulainen broadcoverage parsing neural networks neural processing letters v21 n2 p121132 april 2005 james henderson discriminative training neural network statistical parser proceedings 42nd annual meeting association computational linguistics p95es july 2126 2004 barcelona spain james henderson inducing history representations broad coverage statistical parsing proceedings conference north american chapter association computational linguistics human language technology p2431 may 27june 01 2003 edmonton canada james henderson neural network probability estimation broad coverage parsing proceedings tenth conference european chapter association computational linguistics april 1217 2003 budapest hungary mladen stanojevi sanja vrane knowledge representation soul expert systems applications international journal v33 n1 p122134 july 2007