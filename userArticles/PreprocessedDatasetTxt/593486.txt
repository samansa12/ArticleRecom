effect data distribution parallel mining associations association rule mining important new problem data mining crucial applications decision support marketing strategy proposed efficient parallel algorithm mining association rules distributed sharenothing parallel system efficiency attributed incorporation two powerful candidate set pruning techniques two techniques distributed global prunings sensitive two data distribution characteristics data skewness workload balance prunings effective skewness balance high implemented fpm ibm sp2 parallel system performance studies show fpm outperforms cd consistently parallel version representative apriori algorithm agrawal srikant 1994 also results validated observation effectiveness two pruning techniques respect data distribution characteristics furthermore shows fpm nice scalability parallelism tuned different business applications b introduction association rule discovery attracted lot attention research business communities 1 2 4 association rule rule implies certain association relationships among set objects occur together one implies database intuitive meaning association set items transactions database contain x tend contain classical example 98 customers purchase tires automobile accessories department store also automotive services carried example typical association basket database sounds like common sense knowledge however could lot associations among data may able deduce common knowledge therefore efficient automated technique discover type rules important area research data mining 2 7 8 15 17 applications association rule mining range decision support product marketing consumer behavior prediction previous studies examined efficient mining association rules many different angles influential association rule mining algorithm apriori 2 developed rule mining large transaction databases scope study also extended efficient mining sequential patterns 20 generalized association rules 19 multiplelevel association rules 9 quantitative association rules 21 constrainted association rules 14 etc although studies sequential data mining techniques algorithms parallel mining association rules also proposed 3 16 22 23 development parallel systems mining association rules unique importance databases data warehouses 18 used often store huge amount data data mining databases require substantial processing power parallel system possible solution observation motivates us study efficient parallel algorithms mining association rules large databases work study problem parallel system distributed sharenothing memory ibm sp2 11 model database partitioned distributed across local disks processors processors communicate via fast network well studied major cost mining association rules computation set large itemsets ie frequently occurring sets items see section 21 database 1 2 itemset set items large percentage transactions containing items greater given threshold representative parallel algorithm mining association rules cd algorithm count distribution designed sharenothing parallel systems 3 extends directly basic technique aprori parallel system proposed algorithm fpm fast parallel mining following distinct feature comparison cd fpm explored important property locally large itemsets large respect partition processor globally large itemsets large respect entire database develop two powerful pruning techniques distributed pruning global pruning reduce number candidate sets individual processor since number candidate sets dominant parameter computation cost substantially smaller candidate sets fpm performs much better cd another contribution work discovery effectiveness two aforementioned pruning techniques hence performance parallel mining depends data distribution characteristics database partitioning captured distribution characteristics two factors data skewness workload balance two factors orthogonal properties intuitively partitioned database high data skewness globally large itemsets locally large partitions hand partitioned database high workload balance processors similar number locally large itemsets partitions precise definitions skewness workload balance given sections 3 4 defined metrics measure data skewness workload balance found distributed global prunings super performance best case high data skewness high workload balance combination high balance moderate skewness second best case hand high skewness moderate balance combination provide moderate improvement cd combination low skewness low balance worst case marginal improvement found implemented fpm ibm sp2 parallel machine processors extensive performance studies carried results confirm observation relationship pruning effectiveness data distribution rest paper organized follows section 2 overviews parallel mining association rules techniques distributed global prunings together fpm algorithm described section 3 section also investigated relationship effectiveness prunings two data distribution characteristics data skewness workload balance section 4 define metrics measure data skewness workload balance data partition section 5 reports result extensive performance study section 6 discuss issues including possible extensions fpm enhance scalability section 7 conclusion 2 parallel mining association rules 21 sequential algorithm mining association rules set items let database transactions transaction consists set items given itemset x transaction contains x x association rule implication form x 2 association rule holds confidence c probability transaction contains x also contains c association rule x support probability transaction contains x task mining association rules find association rules whose support larger minimum support threshold whose confidence larger minimum confidence threshold itemset x support percentage transactions contains x support count denoted x sup number transactions containing x itemset x large precisely frequently occurring support less minimum support threshold itemset size k called kitemset shown problem mining association rules reduced two subproblems 1 1 find large itemsets given minimum support threshold 2 generate association rules large itemsets found since 1 dominates overall cost mining association rules research focused developing efficient methods solve first subproblem 2 interesting serial algorithm apriori 2 proposed computing large itemsets mining association rules transaction database outlined follows 2 large itemsets computed iterations iteration apriori scans database finds large itemsets size kth iteration apriori creates set candidate sets c k applying candidate set generating function apriori gen l kgamma1 l kgamma1 set large 1itemsets found k gamma 1st iteration apriori gen generates kitemset candidates whose every 1itemset subset l kgamma1 22 count distribution algorithm parallel mining cd count distribution parallel version apriori one earliest proposed representative parallel algorithms mining association rules 3 describe briefly steps comparison purpose database partitioned distributed across n processors program fragment cd processor kth iteration outlined figure 1 convenience use x supi represent local support count itemset x partition step 1 every processor computes candidate set c k applying aprior gen function l kgamma1 set large itemsets found k gamma 1th iteration step 2 local support counts support candidates c k found steps 3 4 local support counts exchanged processors get global support counts support globally large itemsets large respect l k computed independently processor cd repeats steps 1 4 candidate found implemented cd ibm sp2 using mpi message passing interface 13 scan partition find local support count x supi processors get global support counts x sup figure 1 count distribution algorithm pruning techniques fpm algorithm 31 distributed pruning important observe interesting properties related large itemsets parallel environments since properties may substantially reduce number candidate sets preliminary form results section developed 6 extended first important relationship large itemsets processors database every globally large itemsets must locally large processors itemset x globally large locally large processor p x called gllarge processor p set gllarge itemsets processor form basis processor generate candidate sets second gllarge itemset processor following monotonic subset relationship property itemset gllarge processor p subsets also gllarge p combining two properties following results lemma 1 itemset x globally large exists processor n x subsets gllarge processor p 1 use gl denote set gllarge itemsets processor p gl ik denote set gllarge kitemsets processor p follows lemma 1 x 2 l k exists processor p gllarge processor p ie belong gl ikgamma1 straightforward adaptation apriori set candidate sets kth iteration denoted ca k stands sizek candidate sets apriori would generated applying apriori gen function l kgamma1 processor p let cg ik set candidates sets generated applying apriori gen gl ikgamma1 ie cg stands candidate sets generated gllarge itemsets hence cg ik generated gl ikgamma1 since gl ikgamma1 l kgamma1 cg ik subset ca k following use cg k denote set n theorem 1 every k 1 set large kitemsets l k subset cg result stronger 17 result states globally large itemset locally large partition states subsets must locally large together partition proof let follows lemma 1 exists processor n subsets x gllarge processor p hence x 2 cg ik therefore indicates cg k subset ca k could much smaller ca k taken set candidate sets sizek large itemsets effect set candidates ca k pruned cg k called technique distributed pruning result forms basis reduction set candidate sets algorithm fpm first set candidate sets cg ik generated locally processor p kth iteration exchange support counts gllarge itemsets gl ik cg ik found end iteration based gl ik candidate sets processor p 1st iteration generated according theorem 1 according performance studies number candidate sets generated distributed pruning substantially reduced generated cd example 1 illustrates effectiveness reduction candidate sets using distributed pruning example 1 assuming 3 processors parallel system database partitioned suppose set large 1itemsets computed first iteration l g hg b c locally large processor p 1 b c locally large processor p 2 e f g h locally large processor p 3 therefore gl g hg based theorem 1 set size2 candidate sets processor p 1 cg fabbcacg similarly cg egehfgfhghg hence set candidate sets large 2itemsets cg candidates however apriori gen applied l 1 set candidate sets ca would 28 candidates shows effective apply distributed pruning reduce candidate sets 2 32 global pruning result count exchange local support counts x supi processor also available every processor information another powerful pruning technique called global pruning developed let x candidate kitemset partition x supi supi ae x therefore local support count x x supi bounded value minfy 1g since global support count x x sup sum local support count processors value upper bound x sup x maxsup minsup theta jdj x pruned away technique called global pruning note global pruning requires additional information except local support counts resulted count exchange previous iteration table 1 gives example show global pruning pruning away candidates cannot pruned distributed pruning suppose global support count threshold 15 local support count threshold local support processor 1 13 local support processor 2 3 3 12 34 1 4 local support processor 3 global support gllarge processor 1 theta theta gllarge processor 2 theta theta gllarge processor 3 theta theta theta theta table 1 high data skewness high workload balance case processor 5 distributed pruning cannot prune away cd c gllarge processor 2 whereas global pruning prune away cd cd also pruned ef would survive global pruning example clear global pruning effective distributed pruning ie pruned away distributed pruning pruned away global pruning three pruning techniques one apriori gen distributed global prunings increasing pruning power latter ones subsume previous one 33 fast parallel mining algorithm fpm present fpm algorithm section enhancement cd simple support counts exchange scheme cd retained fpm main difference incorporation distributed global prunings fpm reduce candidate set size first iteration fpm cd processor scans partition find local support counts size1 itemsets use one round count exchange compute global support counts end 1st iteration addition l 1 processor also finds gllarge itemsets gl 1i kth iteration fpm k 1 program fragment executed processor 1 n described figure 2 compute candidate sets cg distributed pruning prune candidates cg k global pruning scan partition find local support count x supi remaining candidates processors get global support counts x sup return figure 2 fpm algorithm similar cd fpm also implemented collective communication operations mpi sp2 order compare effects distributed global pruning also implemented variant fng fpm global pruning fpm fng perform global pruning ie procedure figure 2 except step 2 removed 4 data skewness workload balance database partition two data distribution characteristics data skewness workload balance orthogonal effects prunings hence performance fpm intuitively data skewness partitioned database high supports large itemsets clustered partitions low supports large itemsets distributed evenly across processors table 1 clear itemsets high skewness partition high skewness even though support large itemset clustered processors clusterings different large itemsets may distributed evenly across processors concentrated first case clusterings large itemsets distributed evenly among processors hence processor would similar number locally large itemsets characterise case high workload balance second case clusterings would concentrated processors hence processors would much locally large itemsets others low workload balance case example itemsets table 1 high skewness also good workload balance b locally large processor 1 c processor 2 whereas e f processor 3 follows discussion pruning techniques high data skewness would increase chance candidate sets pruning however factor workload another critical factor following see given good data skewness distribution clusterings amount processors even pruning effects would reduced significantly aggravate problem work computing large itemsets would concentrated processors troublesome issue parallel computation example 2 explained table 1 case high data skewness high workload balance supports itemset distributed mostly one partition hence skewness high hand every partition number locally large itemsets therefore workload balance also high case cd generate candidates second iteration whereas distributed pruning generate three candidates ab cd ef shows pruning good effect distribution local support processor 1 13 33 12 34 2 1 local support processor 2 local support processor 3 global support gllarge processor 1 gllarge processor 2 theta theta theta theta theta theta gllarge processor 3 theta theta theta theta table 2 high data skewness low workload balance case table 2 example high data skewness low workload balance thresholds table 1 ie global support threshold 15 local support threshold processor 5 support count distribution item table 1 except items b c locally large together processor 1 instead distributed processors 1 2 lower workload balance case distributed pruning generate 7 size2 candidates namely ab ac ad bc bd cd ef cd still 15 candidates thus distributed pruning remains effective good high workload balance case table 1 local support processor 1 6 12 4 13 5 12 local support processor 2 6 12 5 12 4 13 local support processor 3 global support gllarge processor 1 theta gllarge processor 2 theta gllarge processor 3 theta table 3 low data skewness high workload balance case table 3 example low data skewness high workload balance support counts items b c e f almost equally distributed 3 processors hence data skewness low hand workload balance high number locally large itemsets processor almost case cd distributed pruning generate 15 candidate sets hence restrict pruning distributed pruning advantage cd case however global pruning prune away candidates ac ae ce words fpm still 20 improvement cd pathological case low skewness high balance 2 following example 2 observed global pruning effective distributed pruning perform significant candidates reduction even moderate data skewness low workload balance cases note low skewness low balance cannot occur together also according analysis distributed pruning prune away almost n n number partitions size2 candidates generated cd high data skewness high workload balance case summary distributed pruning effective database partitioned high skewness high balance hand worst cases high skewness low balance high balance low skewness effect distributed pruning degraded level cd however global pruning may still perform better cd strengthen studies investigated problem defining metrics measure skewness balance 41 data skewness metric developed skewness metric based well established notion entropy 5 given random variable entropy measurement even uneven probability distribution values database partitioned n processors value px xsup regarded probability occurrence itemset x partition n entropy logp x measurement distribution local supports x partitions example x skewed completely single partition k 1 k n ie occurs k px k value minimal case hand x evenly distributed among partitions px value logn maximal case therefore following metric used measure skewness data partition table local count processor 1 13 local count processor 2 1 3 12 34 1 4 0348 high local count processor 3 data skewness sx 0452 0633 0429 0697 0429 0586 high tsd 0494 workload balance tbd 0999 table local count processor 1 13 33 12 34 2 1 0601 local count processor 2 high local count processor 3 data skewness sx 0452 0633 0429 0697 0429 0586 low tsd 0494 workload balance tbd 0789 table 3 local count processor 1 6 12 4 13 5 12 0329 local count processor 2 6 12 5 12 4 13 0329 low local count processor 3 data skewness sx 0015 0001 0012 0001 0012 0001 high tsd 0005 workload balance tbd 0999 table 4 data skewness workload balance given database partition 1 n skewness sx itemset defined logn skewness sx following properties skewness lowest value x distributed evenly partitions px skewness highest value x occurs one partition cases follows property entropy sx increases respect skewness x hence suitable metric skewness individual itemset table 4 shows skewness large itemsets tables 1 2 3 addition measuring skewness itemset also need metric measure skewness database partition define skewness database partition weighted sum skewness large itemsets words skewness partition measurement total skewness large itemsets given database partition 1 n skewness tsd partition defined x2ls sx theta wx ls set large itemsets 2i ysup weight support x large itemsets ls sx skewness x tsd properties similar sx skewness itemsets minimal value skewness itemsets maximal value cases table 4 skewness tsd partitions three situations computed illustration purpose computed tsd respect skewness size1 large itemsets note ignored small itemsets computation skewness partition since purpose task investigate effect data skewness candidate sets pruning involves large itemsets restriction would fact make metric relevant candidate set pruning 42 workload balance metric workload balance measurement distribution support clusterings large itemsets partitions processors based definition wx definition 2 px definition 1 define x2ls wx theta px itemset workload partition l set large itemsets intuitively workload w partition ratio total supports large itemsets partitions note partition high workload balance w partitions hand distribution w partitions uneven workload balance low pointed workload balance important bearing pruning performance parallel mining parallel metric data skewness also define metric workload balance factor measure workload balance partition based also entropy definition 3 database partition database workload balance factor tbd partition given logn metric tbd following properties workload across processors workload concentrated one processor cases table 4 workload w first last cases tables 1 3 high balance values almost equal 1 second case table 2 workload processor 2 shifted processor 1 hence created unbalance case value tbd thus reduced 0789 indicates moderate workload balance data skewness metric workload balance factor independent theoretically one could values range 0 1 however combinations values admissable first let us consider boundary cases theorem 2 let partition database 1 admissable values tbd range 0 1 2 admissable values tsd range 0 1 proof 1 definition 0 tbd 1 need prove boundary cases admissable implies large itemsets x therefore large itemset large one one processor large itemsets large processor 1 n hand processor number large itemsets w large itemsets x implies w 1 n hence 2 follows first result theorem admissable 1 therefore first part proved furthermore exists partition implies large itemsets locally large hence shown theorem 2 possible combinations admissable general admissable combinations subset unit square one figure 3 always contains two segments figure 3 figure 3 defining metrics studying characteristics validate observation relationship data skewness workload balance candidates pruning effect performance studies performance studies order confirm analysis proposed fpm efficient algorithm mining associations parallel system implemented algorithms ibm sp2 carried substantial performance evaluation comparison following three goals studies 1 verify fpm faster representative algorithm cd confirm major performance gain two pruning techniques 2 confirm observation data skewness workload balance two critical factors performance fpm 3 demonstrate fpm good parallelism scalability parallel algorithm implemented fpm variant fng cd ibm sp2 parallel system used processors 667mhz 64 mb main memory running aix operating system communication figure 3 admissable combinations skewnesss balanceb processors high performance switch aggregated peak bandwidth 40 mbps latency 40 microseconds data allocated local disk processor database partition node 100mb size order able control experiments test different data distributions scenarios many works 2 3 9 15 16 mining association rules adopted standard technique introduced 2 generate database enhanced technique generation database partitions introduced parameters control skewness workload balance table 5 list parameters used synthetic databases details data generation technique appendix number transactions partition average size transactions average size maximal potentially large itemsets l number maximal potentially large itemsets n number items partition skewness n number partitions table 5 synthetic database parameters 51 relative performance order compare performance fmp fng cd databases twenty data sets generated data sets generated skewness balance factors listed table 6 number partitions case size partition 100mb name partition denoted dxtyizsrbl x number transactions partitions average size transactions z average size itemsets two parameters sr bl two important parameters used control skewness workload balance data generation bl values listed separately table table 6 also computed measured skewness tsd balance factor tbd partitions generated important note measured skewness workload close values controlled parameters ie values b good approximations values tsd tbd addition cover wide range skewness balanace thus synthesized data partitions good simulation data partitions various distribution characteristics believe technically valuable even real data may general enough cover possible distributions name table attributes synthetic databases ran fpm fng cd database partitions table 6 minimum support threshold 05 improvement fpm fng cd response time recorded table 7 result encouraging fpm fng consistently faster cd cases following analyze performance gain fpm fng three aspects 1 improvement workload balance high skewness varies high moderate 2 improvement skewness high workload balance varies high moderate 3 desirable undesirable combinations skewness workload balance values response time fpmcd fngcd table 7 performance improvement fpm fng cd figure 4 relative performance fpm fng cd partitions different skewness high balance value performs much better cd skewness relatively high 05 response timesec relative performanced3278kt5i2sb100n16 minsup05 cd figure 4 relative performance databases high balance different skewness hand fpm outperforms cd significantly even skewness moderate range 01 05 b90 result table 7 shows fpm much faster cd demonstrates fpm outperforms cd consistently given high workload balance least moderate skewness1000300050000 01 03 05 07 09 1 response timesec relative performanced3278kt5i2s90bn16 minsup05 cd figure 5 relative performance databases high skewness different workload balance figure 5 relative performance given high skewness different workload balance fpm fng perform much better cd workload balance relatively high b 05 however improvement range moderate balance 01 b 05 marginal confirms observation workload balance essential requirement shows high skewness accompany high workload balance effect high skewness moderate balance may good high balance moderate skewness figure 6 vary skewness balance together low value range high value range response timesec skewness balancesb relative performanced3278kt5i2sbn16 minsup05 cd figure relative performance databases skewness balance change trend shows improvement fpm cd increases faster values approach high value range combining observations three cases together results table 7 divide area covering admissable combinations skewness balance experiments four performance regions shown figure 7 region favorable region balance high skewness varies high moderate fpm general 50 100 faster cd region b workload balance value degraded moderately skewness remains high case gain fpm cd around 50 region covers combinations low workload balance gain fpm falls moderate range 30 region contains undesirable combinations fpm marginal performance gain figure 8 provides us another view understand candidates pruning effects shows ratio number candidate sets fpm fng cd experiments figure 4 reduction ratios runs database d3278kt5i2srb100 first graph skewness high distributed pruning 792 reduction candidate sets comparing cd global pruning 939 reduction skewness low distributed pruning 66 reduction global pruning 307 reduction confirm observation effect high balance combined high moderate skewness 52 scalability parallelism speedup scaleup order study efficiency fpm parallel algorithm investigate speedup scaleup cd speedup reduction response time vs number processors given total size database remains unchanged processors used faster computation ideal speedup linear function number processors scaleup performance vs number processors database size scaled proportional number proccesors algorithm high efficiency low overhead performance would maintain uniform number processors size database scaled proportionally speedup experiment execute algorithms fixed size database various number processors figure 7 performance regions fpmcd admissible combinations skewness workload number candidate sets pruning effectd3278kt5i2sb100n16 minsup05 apriorigenno pruning distributed pruningapriorigen global pruningapriorigen figure 8 pruning effect databases figure 4 response timesec number processors cd fpm14122028 number processors relative speedupd3278kt5i2s90b100 cd figure 9 speedup database partitions selected database high skewness balance presentative perform study database listed table 8 total size 16gb first divided 16 partitions subsequently combined partitions form databases 8 4 2 zero partitions figures 9 execution times speedups fpm fng cd databases speedups also shown table 8 fng linear speedup fpm achieved remarkable superlinear speedup reason behind fpms superlinear speedup increase skewness number partitions increases scaleup experiment database size number processors scaled proportionally number processors involved increased 1 16 sizes databases increased correspondingly 100mb 16gb database partitioned according number processors every partition maintained size 100mb performed experiment based database d3278kt5i2s90b100 ie databases generated parameters figure 10 shows result experiment surprisingly fpm fng maintain performance response time databases speedup fpm speedup fng speedup cd table 8 speedup five databases different distribution characteristics fact gone database scaled prime reason increase pruing capability number partitions increases 6 discussion restrict search large itemsets small set candidates essential performance mining association rules pruning techniques proposed theoretically interesting effect parallel case serial case distributed global pruning provide significant amount pruning power particular data distribution favorable situation ie workload balance high skewness least moderate level important study partition techniques deliver good data distribution random approaches general deliver partitions good balance however skewness would difficult guarantee together good workload balance clustering technique kmeans algorithm 12 give good skewness remains open problem modify clustering technique generate partitions good skewness also good workload balance 7 conclusion parallel algorithm fpm mining association rules proposed performance study carried ibm sp2 sharednothing memory parallel system shows fpm consistently outperforms cd also nice scalability terms speedup scaleup gain performance fpm due mainly pruning techniques incorporated discovered effectiveness pruning techniques depend highly data distribution characteristics measured two data skewness workload balance analysis experiment results show pruning techniques sensitive workload balance though good skewness also important positive effect techniques effective best case high balance high skewness combination high balance moderate skewness second best case worst case low balance low skewness fpm deliver performance close cd since mining associations many interesting applications important future works would include fine tuning proposed parallel techniques real business cases r mining association rules sets items large databases fast algorithms mining association rules parallel mining association rules design dynamic itemset counting implication rules market basket data elements information theory fast distributed algorithm mining association rules maintenance discovered association rules large databases incremental updating technique advances knowledge discovery data mining discovery multiplelevel association rules large databases scalable parallel data mining association rules scalable powerparallel systems methods classification analysis multivariate observations message passing interface forum exploratory mining pruning optimizations constrainted association rules effective hashbased algorithm mining association rules efficient parallel data mining association rules efficient algorithm mining association rules large databases database achievements opportunities 21st century mining generalized association rules mining sequential patterns generalizations performance improvements mining quantitative association rules large relational tables hash based parallel algorithms mining association rules parallel data mining association rules sharedmemory multiprocessors tr ctr frans coenen paul leng partitioning strategies distributed association rule mining knowledge engineering review v21 n1 p2547 march 2006 vipin kumar mohammed zaki high performance data mining tutorial pm3 tutorial notes sixth acm sigkdd international conference knowledge discovery data mining p309425 august 2023 2000 boston massachusetts united states