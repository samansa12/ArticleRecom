maximum variance cluster algorithm abstractwe present partitional cluster algorithm minimizes sumofsquarederror criterion imposing hard constraint cluster variance conceptually hypothesized clusters act parallel cooperate neighboring clusters order minimize criterion satisfy variance constraint order enable demarcation cluster neighborhood without crucial parameters introduce notion foreign cluster samples finally demonstrate new method cluster tendency assessment based varying variance constraint parameter b introduction data clustering extensively investigated problem many algorithms reported 18 26 roughly cluster algorithms categorized hierarchical partitional algorithms hierarchical algorithms deliver hierarchy possible clusterings partitional cluster algorithms divide data number subsets partitional cluster analysis algorithms assume number clusters known priori many cases number clusters known advance additional validation studies used find optimal partitioning data 6 8 11 16 paper propose algorithm partitional clustering minimizes within cluster scatter constraint cluster variance accordingly contrast many cluster algorithms method finds number clusters automatically clearly proper value variance constraint parameter selected present way discover cluster tendencies find significant values variance parameter case information available problem domain first formally define cluster problem authors department mediamatics faculty information technology systems delft university technology pobox 5031 2600 ga delft netherlands email fcjveenman mjtreinders data set vectors pdimensional metric space cluster problem find clustering x set clusters number clusters clusters c homogeneous union clusters inhomogeneous widely used criterion quantify cluster homogeneity sumofsquarederror criterion simply squareerror criterion x 2 2 expresses cluster homogeneity x 3 cluster mean straight minimization 1 leads trivial clustering n clusters one sample therefore additional constraints imperative instance one could fix number clusters priori known number like among others widely used kmeans model 23 image segmentation domain maximum variance per cluster sometimes used addition spatial connectivity constraint eg 1 15 paper present algorithm based model proposed intensitybased image segmentation 28 constraint imposed square error criterion 1 within model states variance union two clusters must higher given limit 2 consequence model variance resulting cluster generally 2 however imply could impose maximum variance constraint individual cluster instead would replace joint variance constraint 4 constraint individual clusters c minimization 1 would lead trivial solution one sample per cluster clearly since model imposes variance constraint instead fixing number clusters resulting optimal clustering different kmeans result even final number clusters 1 usually sumofsquarederror criterion averaged whole data set defined j e expresses average distance cluster centroids instead total distance aa b c figure 1 illustration cluster neighborhood 2 nearestneighbors method neighbor ranking sample shown b c display neighborhood grey colored cluster respectively 3 4 samples c set expansion candidates empty algorithm optimization cluster model propose stochastic optimization algorithm literature stochastic clustering algorithms reported generally optimize kmeans model fuzzy cmeans model either using simulated annealing techniques 7 20 25 using evolutionary computation techniques 10 13 22 29 accordingly stochastic approaches focus optimization known cluster models algorithm propose however shows resemblance distributed genetic algorithm dga image segmentation introduced andrey tarroux 2 3 also dynamically apply local operators gradually improve set hypothesized clusters contrast dga approach consider statistics whole cluster optimization process describing algorithm first elaborate neighborhood relationships samples play crucial role proposed algorithm effectiveness efficiency algorithm exploits locality feature space namely promising candidates cluster expansion clusters close feature space similarly distant cluster members least reliable hence first candidates removal computational performance also profit feature locality cluster update operations executed parallel optimization process applies locally reasons consider optimization process individual clusters point view ie cluster execute number actions order contribute improvement criterion well satisfy variance constraint order collect expansion candidates cluster need find neighboring samples cluster common way define neighborhood sample collect k nearest neighbors using eucledian distance measure k predefined number way neighborhood cluster would union neighbors samples cluster accordingly set expansion candidates cluster consists samples neighborhood excluding figure 2 illustration cluster neighborhood 8 nearestneighbors method neighbor ranking sample shown b c display neighborhood grey colored cluster respectively 3 4 samples samples cluster problem approach value k becomes integral part cluster model eg 12 19 24 k set low even small clusters k nearest neighbors cluster expansion candidates left see fig 1 hand k set high neighborhood always large clusters major part samples expansion candidates clearly violates idea locality consequence set expansion candidates mix good bad candidate samples without preference see fig 2 take another approach collect expansion candidates first call set expansion candidates cluster c outer border b cluster c introduce notion foreign samples define neighboring samples cluster accordingly kth order outer border b cluster c union k nearest foreigners samples c leading xc fx k c x set k nearest foreigners x according n f x c nearest foreigner sample x c x defined yyc consequently outer border cluster always limited number samples never becomes empty unless one cluster left fig 3 illustrate secondorder outer border evolves growing cluster appropriate value order outer border depends constellation clusters actual data figure 3 figures illustrate cluster border construction respect sample cluster distance ranking sample shown b c display secondorder border grey colored cluster respectively 3 4 samples besides expansion candidates also need collect candidates removal cluster order impose variance constraint end introduce qth order inner border cluster c inner border consists samples furthest cluster mates samples c accordingly qth order inner border expressed follows xc gx q c set q furthest cluster mates x words q furthest neighbors x c according f nx gx q 1 f nx q 0 f nx furthest neighbor x since set foreigners sample changes every time cluster updated efficiency reasons introduce rank list r per sample x containing indices samples x order distance given sample rank list r n tuple defined r according concatenate operator nnx nearest neighbor x k nearest foreigners cluster sample easily found scanning rank list starting head skipping elements already cluster end bookkeeping needed clusters samples definitions inner outer border cluster describe maximum variance cluster mvc algorithm since optimization model defined 1 4 certainly intractable problem exhaustive search alternatives question order prevent early convergence consequent approximate optimization process introduce sources nondeterminism algorithm 4 algorithm starts many clusters samples sequence epochs every cluster possibility update content conceptually epoch clusters act parallel alternatively sequentially random order update process cluster c performs series tests causes different update action cluster 1 isolation first c checks whether variance exceeds predefined maximum 2 randomly takes number candidates inner border proportional total number samples isolates candidate furthest cluster mean c takes restricted number candidates control greed operation isolated sample forms new cluster resulting increase number clusters 2 union hand c homogeneous variance 2 checks unite neighboring cluster neighboring cluster cluster contains foreign sample c end computes joint variance neighbors lowest joint variance remains 2 corresponding neighbor merges c resulting decrease number clusters 3 perturbation finally none actions applies cluster c attempts improve criterion randomly collecting number candidates b outer border b control greed restricted number candidates selected proportional size border c ranks candidates respect gain squareerror criterion moving neighboring cluster c b c define criterion gain c c b respect x c b b figure 4 r15 data set shown generated 15 similar 2d gaussian distributions b j e displayed function 2 r15 data set best candidate positive gain candidate moves neighbor c otherwise small probability p occasional defect forces best candidate move c irrespective criterion contribution occasional defect true convergence algorithm exists therefore certain number epochs e max set p since possible minimum constrained optimization problem 1 4 variance clusters exceeds 2 exceptions general rule mentioned section 1 e max epochs also isolation longer allowed order prevent algorithm oscillations precautions algorithm certainly converge since overall homogeneity criterion decreases always greater equal zero case two clusters unite criterion may increase number clusters finite still wait number epochs clusters changed due stochastic sampling border 3 experiments section demonstrate effectiveness proposed maximum variance cluster mvc algorithm artificial real data sets first however show maximum variance constraint parameter used cluster tendency assessment clearly since clustering result depends setting 2 squareerror criterion j e also changes function 2 accordingly cluster tendencies read trends j e consider instance data set shown fig 4a corresponding squareerror curve resulting varying cumulative distribution maximum plateau strength max 5 points points points points points figure 5 cumulative distribution maximum plateau strength differently sized random 2d data sets distributions calculated 1000 independent draws seen fig 4b figure shows prominent plateaus squareerror crite rion clearly plateaus caused hierarchical cluster structures random sample patterns real structure data variance constraint increased moment true clusters lumped together hand resulting clustering random character clusters easily rearranged variance constraint increased starting point j e plateau 2 b end point plateau example fig 4b define strength plateau ratio accordingly strength plateau gives indication whether corresponding clustering represents real structure data intuitively expect two real clusters lumped together variance constraint higher roughly twice individual variance implies strength plateau j e greater 2 order significant plateau represent real structure test hypothesis experiments uniform random data various numbers samples data set measured maximum plateau strength subsequently computed distribution max fig 5 show cumulative distribution max different sizes random data sets n low n 50 significant plateaus occasionally found expected low numbers samples hand experiments structured data among ones describe section indeed resulted significant j e plateaus advantage cluster tendency assessment approach model used clustering cluster tendency detection view usual approach different criteria used clustering detection cluster tendency undesirable like instance 6 8 11 16 cluster algorithm may able find clustering corresponding local minimum knee cluster tendency function 2 additional remarks need made significance j e plateaus first noted fractal like effects 2 must higher certain value order rule extremely small significant plateaus second case multiple significant plateaus plateaus represent scales data considered user select appropriate scale corresponding plateau view best scale cases selection fully subjective experiments compared performance mvc algorithm kmeans algorithm 23 gaussian mixtures modeling gmm method likelihood maximisation 18 using em algorithm 9 kmeans gmm method numbers clusters set resulting number found mvc algorithm since mvc kmeans algorithm prefer circular shaped clusters constrained gaussian models gmm circular order reduce number parameters mvc algorithm set p noted parameter values appeared critical experiments included merely serve tune convergence behavior similar nondeterministic optimization algorithms like instance mutation rate population size parameters genetic algorithms 14 since algorithms nondeterministic components 3 ran 100 times data set display result lowest squareerror mvc kmeans highest likelihood gmm ie best solution found measured average computation time number times best solution found hit rate mvc algorithm add computation time rank lists since time depends size data set structure moreover lists computed data set used tendency assessment subsequent runs find optimal clustering start already mentioned data set fig 4a consisting 15 similar 2d gaussian clusters positioned rings r15 though know estimate variance clusters first varied 2 constraint order discover cluster tendency fig 4b shows resulting curves j e number found clusters figure shows number prominent plateaus j e first 420165 strength significant plateau corresponds originating structure 15 clusters large plateau 675122 strength corresponds clustering inner clusters merged one cluster plateau however significant according definition additional criteria used discover cluster tendencies usually called cluster validity functions indices 3 kmeans gmm algorithm initialized randomly chosen cluster models figure results applying clustering algorithms r15 data set results mvc kmeans algorithm 15 clusters shown b results gmm method 15 clusters shown method parameter hit time ms kmeans kmeans table 1 statistical results applying algorithms r15 data set total variance clusters lumped center much higher variance outer clusters resulting clusterings mvc fig 6 also mvc table 1 shows mvc algorithm clearly robust converging towards possibly local minimum criterion hit rate mvc algorithm much higher kmeans gmm algorithm kmeans algorithm known efficient indeed fastest next artificial data set consists three clusters additional outliers o3 see fig 7a first varied 2 parameter mvc algorithm order discover cluster tendencies although roughly know variance clusters case certainly useful search proper 2 value since outliers may disrupt original cluster variances fig 7b clearly shows one prominent plateau 220820 plateau significant strength corresponding clustering result three outliers put separate clusters leading total six clusters shown fig 8a kmeans b figure 7 o3 data set shown generated 3 similar 2d gaussian distributions additional outliers b j e displayed function 2 parameter algorithm impose variance constraint could find lower squareerror minimum corresponding clustering mvc seen fig 8b algorithm split one cluster instead putting outliers separate clusters supports statement using one model detection cluster tendencies another clustering undesirable also gmm algorithm able find mvc solution see fig 8b though mvc solution indeed higher likelihood kmeans gmm algorithm merged two true clusters put outliers one clusters table 2 shows statistics experiment kmeans gmm algorithm clearly less robust finding respective local criterion optimum mvc kmeans fastest repeated experiment several times different generated clusters outliers results generally described ie difference cluster results algorithms mvc handled outliers better putting separate clusters converged often criterion optimum last synthetic experiment used larger data set d31 consisting 31 randomly placed gaussian clusters 100 samples see fig 9a tendency curve resulting varying mvc algorithm shows one significant plateau 0003000062 corresponds original 31 clusters remarkably kmeans gmm algorithm able find originating cluster structure even 10000 trials statistical results table 3 show mvc algorithm consistently found real structure difference c figure 8 results applying clustering algorithms o3 data set results mvc algorithm shown resulting 6 clusters b c show results kmeans gmm algorithm respectively gmm puts remote cluster sample separate cluster method parameter hit time ms kmeans kmeans table 2 statistical results applying algorithms o3 data set hit rate gmm method certainly refers local maximum 0040812 b figure 9 d31 data set shown generated 31 similar 2d gaussian distributions b j e displayed function 2 constraint parameter method parameter hit time ms kmeans table 3 statistical results applying algorithms d31 data set kmeans gmm algorithm able find originating structure hit rate refers local optimum computation time algorithms becomes small next applied algorithms real data sets started german towns data set consists 2d coordinates 59 german towns prewende situation order find significant clustering result varied 2 parameter mvc algorithm resulting curves j e displayed fig 10a two plateaus 12901840 19402810 respectively although plateaus significant show clustering results first plateau 4 clusters fig 10b equals result kmeans algorithm 4 gmm algorithm came different solution consisting three main clusters one cluster containing single sample visually inspect data fig 10b conclude certainly arguable data set contains significant structure table 4 shows similar hit rates kmeans algorithm fastest finally processed wellknown iris data set algorithms iris data set actually labeled data set consisting three classes irises characterized four features fig 11 illustrates cluster tendencies resulting varying 2 mvc algorithm figure displays several plateaus 076139 140453 strongest figure 10 shows j e function 2 constraint parameter german towns data set b clustering result mvc kmeans algorithm 4 clusters displayed method parameter hit time ms kmeans table 4 statistical results applying algorithms german towns data set figure 11 j e function 2 constraint parameter iris data set method parameter hit time ms kmeans kmeans table 5 statistical results applying algorithms iris data set plateaus strengths correspond three two clusters respectively hence latter significant three algorithms found similar results number clusters since known three classes cannot separated based given features surprising clustering correspond given labels however clustering corresponding significant plateau one cluster almost perfectly matches samples class cluster matches samples class iiiii iris class labels statistics table 5 show similar differences mvc kmeans gmm algorithm experiments presented maximum variance cluster algorithm mvc partitional clustering contrast many algorithms mvc algorithm uses maximum variance constraint instead number clusters parameter experiments showed method effective finding proper clustering compared results widely used kmeans algorithm gaussian mixtures modeling gmm method likelihood maximisation em algorithm contrast proposed mvc method kmeans gmm method need number clusters known priori showed mvc method copes better outliers kmeans algorithm gmm method principle able separate outliers problems optimization process leading convergence local criterion optima mvc algorithm robust finding optimum criterion kmeans gmm algorithm must note better optimization schemes kmeans model eg 5 21 gaussian mixtures modeling eg 17 27 developed however improved optimization algorithms achieved cost considerable additional computation time algorithm complexity mvc algorithm 100 times slower efficient kmeans algorithm especially small data sets low number clusters partially caused fact adjust maximum number epochs parameter e max size data set larger data sets higher number clusters differences computation time algorithms almost disappear advantage mvc algorithm respect computational efficiency implemented parallel distributed computer architectures relatively easily ac cordingly large data sets mvc algorithm may advantageous also efficiency reasons distributed computing environment clusters maintained separate processes clusters neighbors communicate main point consideration balance cluster processes available computers clusters merge samples isolated new clusters interesting property proposed method enables assessment cluster ten dencies generally curve resulting varying maximum variance constraint parameter function squareerror displays prominent plateaus reveal structure data indicated way find significant structure data rating strength plateaus ac cordingly able find proper settings maximum variance constraint parameter model parameter drawback mvc algorithm may uses distance rank list every sample size rank list grows proportional square number samples amount storage needed become substantially main problem however lies computation rank lists since lists sorted construction costs logn operations order prevent rank list becoming bottleneck application mvc algorithm maximum distance constraint max imposed addition maximum cluster variance constraint eg samples need ranked within max range reference sample r seeded region growing unsupervised image segmentation using distributed genetic algo rithm unsupervised segmentation markov random field modeled textured images using selectionist relaxation competitive environments evolve better solutions complex tasks stochastic connectionist approach global optimization application pattern clustering new indexes cluster validity practical application simulated annealing clustering cluster separation measure maximum likelihood incomplete data via em algorithm well separated clusters optimal fuzzy partitions agglomerative clustering using concept mutual nearest neighborhood adaptation natural artificial systems picture segmentation tree traversal algorithm comparing partitions comparison simulated annealing em algorithms normal mixtures decompositions algorithms clustering data clustering using similarity measure based shared near neigh bors experiments projection clustering simulated annealing genetic kmeans algorithm evolutionary fuzzy clustering methods classification analysis multivariate observations finding salient regions images nonparametric clustering image segmentation grouping simulated annealing algorithm clustering problem pattern recognition statistical analysis finite mixture distri butions cellular coevolutionary algorithm image segmentation genetic algorithm fuzzy clustering tr ctr venkatesh rajagopalan asok ray symbolic time series analysis via waveletbased partitioning signal processing v86 n11 p33093320 november 2006 venkatesh rajagopalan asok ray rohan samsi jeffrey mayer pattern identification dynamical systems via symbolic time series analysis pattern recognition v40 n11 p28972907 november 2007 kuoliang chung jhinsian lin faster robust point symmetrybased kmeans algorithm pattern recognition v40 n2 p410422 february 2007 jos j amador sequential clustering statistical methodology pattern recognition letters v26 n14 p21522163 15 october 2005 j veenman marcel j reinders nearest subclass classifier compromise nearest mean nearest neighbor classifier ieee transactions pattern analysis machine intelligence v27 n9 p14171429 september 2005