multicategory classification support vector machines examine problem discriminate objects three classes specifically investigate twoclass discrimination methods extended multiclass case show linear programming lp approaches based work mangasarian quadratic programming qp approaches based vapniks support vector machine svm combined yield two new approaches multiclass problem lp multiclass discrimination single linear program used construct piecewiselinear classification function proposed multiclass svm method single quadratic program used construct piecewisenonlinear classification function piece function take form polynomial radial basis function even neural network k 2class problems svm method originally proposed required construction twoclass svm separate class remaining classes similarily k twoclass linear programs used multiclass problem performed empirical study original lp method proposed k lp method proposed single qp method original k qp methods discuss advantages disadvantages approach b introduction investigate problem discriminating large realworld datasets two classes given examples points known come k classes construct function discriminate classes goal select function eciently correctly classify future points classification technique used data mining pattern recognition example united states postal service interested ecient yet accurate method classifying zipcodes actual handwritten digits zipcodes collected united states postal service used study digit represented 16 16 pixel grayscale map resulting 256 attributes sample number given enormous quantities mail postal service sorts day accuracy eciency evaluation extremely important paper combine two independent related research directions developed solving twoclass linear discrimination problem first linear programming lp methods stemming multisurface method mangasarian 12 13 method later extension robust linear programming rlp approach 6 used highly successfully breast cancer diagnosis system 26 second direction quadratic programming qp methods based vapniks statistical learning theory 24 25 statistical learning theory addresses mathematically problem best construct functions generalize well future points problem constructing best linear twoclass discriminant posed convex quadratic program linear constraints resulting linear discriminant known support vector machine svm function subset training data known support vectors specific implementations generalized optimal plane gop method proven perform well practice 8 throughout paper refer two dierent approaches rlp svm primary focus paper two research directions diered approach solving problems k 2 classes original svm method multiclass problems find k separate twoclass discriminants 23 discriminant constructed separating single class others process requires solution k quadratic programs applying k classifiers original multicategory dataset multiply classified points unclassified points may occur ambiguity avoided choosing class point corresponding classification function maximized point lp approach directly construct k classification functions point corresponding class function maximized 5 6 multicategory discrimination method 5 6 constructs piecewiselinear discriminate k class problem using single linear program call method mrlp since direction extension rlp approach show two dierent approaches combined two yield two new methods krlp msvm section 2 provide background existing rlp svm methods kclass cases quite dierent twoclass linear discrimination methods svm rlp almost identical dier regularization term used objective use regularized form rlp proposed 3 equivalent svm except dierent norm used regularization term twoclass linear discrimination rlp generalizes equally well computationally ecient svm rlp exploits fact stateoftheart lp codes far ecient reliable qp codes primary appeal svm simply elegantly applied nonlinear discrimination minor changes svm methods construct wide class twoclass nonlinear discriminants solving single qp 24 basic idea points mapped nonlinearly higher dimensional space dual svm problem used construct linear discriminant higher dimensional space nonlinear original attribute space using kernel functions dual svm problem svm eciently eectively construct many types nonlinear discriminant functions including polynomial radial basis function machine neural net works successful polynomialtime nonlinear methods based lp use multistep approaches methods roy et al 20 19 18 use clustering conjuction lp generate neural networks polynomial time another approach recursively construct piecewiselinear discriminants using series lps 13 2 15 approaches could also used svm limit discussion nonlinear discriminants constructed using svm kerneltype approaches introduction existing multiclass methods mrlp k svm show idea used mrlp adapted construct multiclass svm using single quadratic program adapt problem formulation similar twoclass case twoclass case initially problem construct linear discriminant data points transformed higher dimensional feature space linear discriminant constructed higher dimension space results nonlinear classification function original feature space section 3 k 2 class case begin constructing piecewiselinear discriminant function regularization term added avoid overfitting method extended piecewise nonlinear classification functions section 4 variables mapped higher dimensional space piecewiselinear discriminant function constructed new space results piecewisenonlinear discriminant original space section 5 extend method piecewise inseparable datasets call final approach multicategory support vector machine msvm depending choice transformation pieces may polynomials radial basis functions neural networks etc concentrate research polynomial classifier leave computational investigation classification functions future work figure 1 shows piecewisesecond degree polynomial separating three classes two dimensions msvm requires solution large quadratic program transforming data points higher dimension feature space number figure 1 piecewisepolynomial separation three classes two dimensions variables grow exponentially example second degree polynomial classifier two dimensions requires original variables x 1 x 2 well variables x 2 2 x 1 x 2 primal problem problem size explode degree polynomial increases dual problem however remains tractable number dual variables k 1 times number points regardless transformation selected dual problem transformation appears inner product high dimensional space inexpensive techniques exist computing inner products dual variable corresponds point original feature space point corresponding positive dual variable referred support vector goal maintain high accuracy using small number support vectors minimizing number support vectors important generalization also reducing computational time required evaluate new examples section 6 contains computational results comparing two lp approaches k rlp mrlp two qp approaches ksvm msvm methods compared terms generalization testing set accuracy number support vectors computational time following notation used throughout paper mathematically abstract problem follows given elements sets 1 k ndimensional real space r n construct discriminant function determined separates points distinct regions region contains points belonging almost class let j set points ndimensional real space r n cardinality j let j j n matrix whose rows points j th point j th row j denoted j let e denote vector ones appropriate dimension scalar 0 vector zeros represented 0 thus x r n x 0 implies x 0 similarly n set minimizers fx set denoted arg min xs fx vector x r n x denote vector r n components x n step function x denote vector 0 1 n components n vector x r n matrix r nm transpose x denoted x respectively dot product two vectors x denoted x background section contains brief overview rlp svm methods clas sification first discuss twoclass problem using linear classifier svm two classes defined rlp reviewed finally piecewiselinear function used multicategory classification mrlp reviewed 21 two class linear discrimination commonly method discrimination two classes points involves determining linear function consists linear combination attributes given sets simplest case linear function used separate two sets shown figure 2 function separating plane x psfrag replacements figure 2 two linearly separable sets separating plane w normal plane distance origin let 1 2 two sets points ndimensional real space r n cardinality 1 2 respectively let 1 1 n matrix whose rows points 1 let 2 2 n matrix whose rows points 2 let x r n point classified follows two sets points 1 2 linearly separable e vector ones appropriate dimension two classes linear separable infinitely many planes separate two classes goal two choose plane generalize best future points mangasarian 12 vapnik chervonenkis 25 concluded best plane separable case one minimizes distance closest vector class separating plane separable case formulations mangasarians multisurface method pattern recognition 13 vapniks optimal hyperplane 24 25 similar 3 concentrate optimal hyperplane problem since basis svm validated theoretically statistical learning theory 24 according statistical learning theory optimal hyperplane construct linear psfrag replacements class 1 class 2 figure 3 two supporting planes resulting optimal separating plane discriminants high dimensional spaces without overfitting reader consult 24 full details statistical learning theory covered paper problem canonical form vapnik 24 becomes determine two parallel planes margin distance two planes maximized margin seperation two supporting planes 2 w example plane shown figure 3 problem finding maximum margin becomes24 min general always possible single linear function completely separate two given sets points thus important find linear function discriminates best two sets according error minimization criterion bennett mangasarian 4 minimize average magnitude misclassification errors construction following robust linear programming problem rlp min subject z 2 w misclassification costs avoid null solution cardinalities 1 2 respectively rlp method eective practice functions generated rlp generalize well many realworld problems additionally computational time reasonably small solution involves single linear program note however rlp method longer includes notion maximizing margin statistical learning theory indicates maximizing margin essential good generalization svm approach 8 23 multiobjective quadratic program minimizes absolute misclassification errors maximizing separation margin minimizing w 2 min fixed constant note problem 6 equivalent rlp addition regularization term linear programming version 6 constructed replacing norm used minimize weights w 3 recall svm objective minimizes square 2norm w w 1norm w w used instead absolute value function removed introducing variable constraints w svm objective modified substituting e w w optimality k resulting lp min wyzs refer problem rlp since yields original rlp method svm method rlp method minimizes average distance misclassified points relaxed supporting planes maximum classification error main advantage rlp method svm problem rlp linear program solvable using robust algorithms simplex method 17 svm requires solution quadratic program typically much computationally costly size problem 3 rlp method found generalize well linear svm much less computational cost ecient computationally solve dual rlp svm prob lems dual rlp problem min uv e paper use 1 2 may positive weights misclassification costs dual svm problem extension nonlinear discriminants given next section 22 nonlinear classifiers using support vector machines primary advantage svm 6 rlp 7 dual form used construct nonlinear discriminants using polynomial separators radial basis functions neural networks etc basic idea map original problems higher dimensional space construct linear discriminant higher dimensional space corresponds linear discriminant original space example construct quadratic discriminant two dimensional problems input attributes x 1 x 2 mapped linear discriminant function constructed new fivedimensional space two examples possible polynomial classifiers given figure 4 dual svm applied mapped points regularization term primal objective helps avoid overfitting higher dimensional space dual svm provides practical computational approach use generalized inner products kernels figure 4 two examples second degree polynomial separations two sets dual svm follows follows min formulate nonlinear case convenient rewrite problem summation notation let set points 1 2 define 2 total number points let x construct nonlinear classification function original data points x transformed higher dimension feature space function x n dot product original vectors x replaced dot product transformed vectors x first term objective function written sum using notation simplifying problem becomes min st support vector machine svm vapnik replaces inner product inner product hilbert space kx x symmetric function kx x must satisfy theorem 53 23 theorem ensures inner product feature space choice kx x determines type classifier constructed possible choices include polynomial classifiers figure 4 kx x degree polynomial radial basis function machines k xx x x distance two vectors width parame ter twolayer neural networks kx x sigmoid function 23 variants svm 10 proven quite successful paractice 21 22 7 note number variables program 10 remains constant increases dimensionality additionally objective function remains quadratic thus complexity problem increase fact size problem dependent number nonzero dual variables points x corresponding variables called support vectors according statistical learning theory best solution given misclassification error uses minimum number support vectors final classification function generalized kernel function kx x support vectors psfrag replacements 3 figure 5 piecewiselinear separation sets 1 2 3 convex piecewiselinear function fx 23 multicategory discrimination multicategory classification piecewiselinear separator used discriminate points examine two methods accomplishing first used svm 24 two construct discriminate function separate one class remaining k 1 classes process repeated k times separable case linear discriminant class must satisfy following set inequalities find w classify new point x compute f one clearly point belongs class one f x 0 class ambiguous thus general rule class point x determined w finding maximized figure 5 shows piecewiselinear function r separates three sets note either svm 10 rlp used construct k twoclass discriminants clarity call method used svm 10 k svm denote method used rlp 8 ksvm advantage ksvm used piecewisenonlinear discriminants krlp limited piecewiselinear discriminants ksvm krlp attain perfect training set accuracy following inequalities must satisfied inequality used definition piecewiselinear separability definition 21 piecewiselinear separability sets points 1 k represented matrices piecewise linearly separable exist w equivalent definition 21 finding piecewiselinear separator involves solving equation w e rewritten 0 w e figure 6 shows example piecewiselinear separator three classes two dimensions linear separating functions represented quantities psfrag replacements 3 figure three classes separated piecewiselinear function w 1 k mrlp method 1 proposed investigated 5 6 used find w min mrlp 15 optimal objective value zero dataset piecewiselinearly separable dataset piecewise linearly separable positive values variables ij l proportional 1 method originally called multicategory discrimination magnitude misclassified points plane x w program 15 generalization twoclass rlp linear program 5 multicategory case like original rlp 5 mrlp include terms maximizing margin directly permit use generalized inner products kernels allow extension nonlinear case next section show mrlp svm combined including margin maximization generalized inner products mrlp 3 formulation msvm piecewiselinear separable case propose construct piecewiselinear piecewisenonlinear svm using single quadratic program analogous two class case start formulating optimal piecewiselinear separator separable case assume k sets points piecewiselinearly separable ie exist class point x determined w finding maximized piecewiselinearly separable problem infinitely many w exist satisfy 16 intuitively optimal w provides largest margin classification approach analogous two class support vector machine svm approach add regularization terms dashed lines figure 7 represent margins piece w piecewiselinear separating function margin separation classes j ie distance 2 would like minimize w also add regularization term 1k objective piecewiselinearly separable problem get following min st w simplify notation formulation piecewiselinear svm rewrite matrix notation see appendix complete matrix definitions general k three class problem following matrices let psfrag replacements w 1 3 w 1 w 2 w 1 w 1 figure 7 piecewiselinear separator margins three classes r nn identity matrix let vector ones using notation fixed k 2 program becomes min w k dual problem written u eliminate variables w problem first show matrix c nonsingular proposition 31 nonsingularity c inverse matrix c k 2 nk1 n 1 nk1 n 1 nk1 n 1 nk1 n n indicates n n identity matrix proof show c nonsingular k 2 calculate inverse matrix c defined appendix size n 1 kn recall n indicates dimension feature space k 1i n size kn kn therefore kn kin kin simple calculations shown inverse matrix nk1 n 1 nk1 n 1 nk1 n 1 nk1 n using proposition 31 following relationship results 22 follows problem 20 equation 22 u 23 using relationship eliminate w dual problem additionally removed simplification new dual problem becomes u construct multicategory support vector machine convenient problem summation notation let dual vector u 1k u kk1 resulting dual problem piecewiselinear datasets l li l l l 0 number points class recall piecewiselinear classification function class point x determined finding maximized equation 23 u solving w summation notation get therefore 4 formulation msvm piecewisenonlinearly separable case like twoclass case msvm generalized piecewise nonlinear functions construct separating functions f x higher dimension feature space original data points x transformed 8 function f x related sum dot products vectors higher dimension feature space according 23 symmetric function kx x theorem 9 replace dot product x x mercers theorem guarantees eigenvalue j expansion kx x positive sucient condition function kx x define dot product higher dimension feature space therefore let kx x returning dual problem 25 objective function contains sum dot products j two points original feature space transform points j p higher dimension feature space replace dot products resulting msvm piecewiselinearly separable datasets l li l l l l 0 points l corresponding nonzero dual variables u ij l referred support vectors possible l correspond figure 8 piecewisepolynomial separation three classes two dimensions support vectors indicated circles one nonzero variable l figure 8 support vectors represented circle around point points double circles indicate two dual variables u ij l 0 complementarity within kkt conditions 14 l l w consequently support vectors located closest separating func tion fact remainder points support vectors necessary construction separating function resulting nonlinear classification problem point x find classification function support vectors support vectors maximized 5 formulation msvm piecewise inseparable case proceeding sections provided formulation piecewiselinearly piecewisenonlinear separable cases construct classification function piecewise linearly inseparable dataset must first choose error minimization crite rion technique used preceeding sections formulating msvm piecewiselinearly separable datasets combined 1norm error criterion used problem 15 bennett mangasarian 6 result msvm piecewiselinearly inseparable problems using matrix notation section 3 add terms 1 objective problem 15 resulting primal problem follows min wy 13 solving dual substituting u simplifying produces following problem u shown proposition 51 problem 30 maximizes concave quadratic objective bounded polyhedral set thus exists locally optimal solution globally optimal proposition 51 concavity objective function u e 1 u concave proof matrix always positive semidefinite symmetric thus hessian matrix 1 negative semidefinite therefore objective concave function problem 30 identical problem 24 piecewiselinearly separable case except dual variables bounded 1 therefore transforming data points l proceed identically section 4 using function denote dot product feature space final msvm results l li l l 1 sections 3 4 class point x determined finding maximum function support vectors support vectors determine threshold values solve primal problem w fixed aw transformed higher dimension feature space problem follows min l st r l l r li r r l l r r l 0 right side constraints constant thus problem 33 linear program easily solved 6 computational experiments section present computational results comparing msvm 32 rlp 15 ksvm using svm 10 krlp using rlp 8 several experiments realworld datasets reported description datasets follows paragraph methods implemented using minos 54 17 solver quadratic programming problems svm ksvm solved using nonlinear solver implemented minos 54 solver uses reducedgradient algorithm conjunction quasinewton method msvm ksvm mrlp selected values given better solutions may result dierent choices addition ally necessary value used methods kernel function piecewisenonlinear msvm ksvm methods degree desired polynomial wine recognition data wine dataset 1 uses chemical analysis wine determine cultivar 178 points 13 features three class dataset distributed follows 59 points class 1 71 points class 2 48 points class 3 dataset available via anonymous file transfer protocol ftp uci repository machine learning databases domain theories 16 ftpftpicsuciedupubmachinelearningdatabases glass identification database glass dataset 11 used identify origin sample glass chemical analysis dataset comprised six classes 214 points 9 features distribution points class follows 70 float processed building windows 17 float processed vehicle windows 76 nonfloat processed building windows 13 containers 9 tableware 29 headlamps dataset available via anonymous file transfer protocol ftp uci repository machine learning databases domain theories 16 ftpftpicsuciedupubmachinelearningdatabases us postal service database usps database 10 contains zipcode samples actual mail database comprised separate training testing sets 7291 samples training set 2007 samples testing set sample belongs one ten classes integers 0 9 samples represented 256 features two experiments performed first datasets normalized 1 1 10fold cross validation used estimate generalization future data second experiment conducted two subsets united states postal service usps data data contains handwriting samples integers 0 9 objective dataset quickly eectively interpret zipcodes data separate training testing sets consist 10 integer classes compiled two individual training subsets usps training data first subset contains 1756 examples belonging classes 3 5 8 call set usps1 training data second subset contains 1961 examples belonging classes 4 6 7 call set usps2 training data similarly two subsets created testing data datasets data values scaled 1 testing set accuracies reported four methods total numbers unique support vectors resulting classification functions msvm ksvm methods given table contains results mrlp krlp msvm ksvm wine glass datasets anticipated adding regularization term msvm 9719 9719 9775 9663 9663 glass mrlp 6495 ksvm 4346 5561 6495 7056 7243 table 1 percent testing set accuracies total number support vectors msvm ksvm05 krlp msvm ksvm degree one problem msvm produced better testing generalization mrlp wine dataset wine dataset piecewiselinearly separable therefore mrlp method infinitely many optimal solutions ever testing accuracy msvm degree one glass data much lower mrlp accuracy may indicate choice large however degree increases accuracy msvm method improves exceeds mrlp results ksvm method generalized surprisingly well testing accuracies reported ksvm wine dataset higher msvm linear krlp method performed well quadratic ksvm program wine dataset better msvm mrlp methods glass data degree creases methods msvm ksvm improve dramatically testing accuracy using higher degree polynomials msvm ksvm methods surpass accuracies mrlp krlp demonstrates potential polynomial piecewisepolynomial classification functions linear piecewiselinear functions table contains results four methods usps data subsets similar observations made datasets piecewise linearly separable solution mrlp found datasets tests significantly lower methods ksvm method generalizes slightly better msvm krlp method reports similar accuracies ksvm method additionally solving linear programs rather quadratic programs computational training time significantly smaller methods changing parameter may improve gener alization msvm method consistently finds classification functions using fewer support vectors ksvm fewer support vectors sam msvm 9126 9187 9228 9207 9228 ksvm 9167 9228 9289 9268 9248 msvm 9458 9497 9536 9497 9400 ksvm 9613 9652 9613 9516 9458 table 2 percent testing set accuracies total number support vectors msvm svm05 ksvm degree table 3 total computational training time seconds mrlpkrlp msvm ksvm usps1 ple classified quickly since dotproduct sample support vector must computed thus msvm would good method choose classification time critical cpu times training four methods usps1 dataset reported table 3 times datasets listed programs run using batch system clusters machines timing reliable however trends clear krlp method significantly faster methods msvm ksvm methods degree increased computational time would decrease certain degree reached would increase degree polynomial starts increase varies dataset surprisingly usps datasets ksvm method faster mrlp method case wine glass datasets mrlp method faster training times ksvm datasets times reported ibm rs6000 model 590 workstations 128 mb ram conclusions examined four methods solution multicategory discrimination problems based lp methods mangasarian qp methods svm vapnik twoclass methods rlp svm dier norm regularization term past two dierent approaches used k 2 class case method called ksvm constructed k twoclass discriminants using k quadratic programs resulting classifier piecewiselinear piecewise nonlinear discriminant function depending kernel function used svm original multicategory rlp k classes constructed piecewiselinear discriminant using single linear program proposed two new hybrid approaches like ksvm method k rlp uses lp construct k twoclass discriminants also formulated new approach msvm began formulation adding regularization terms mrlp like ksvm piecewisenonlinear discriminants nonlinear pieces found mapping original data points higher dimension feature space transformation appeared dual problem inner product two points higher dimension space generalized inner product used make problem tractable new msvm method requires solution single quadratic program performed computational study four methods four datasets general found k svm krlp generalized however msvm used fewer support vectors counterintuitive result since twoclass class statistical learning theory predicts fewer support vector result better generalization theoretic justification better generalization ksvm krlp svm mrlp open question krlp method provided accurate ecient results piecewiselinear separable datasets ksvm also tested surprisingly well requires solution k quadratic programs thus providing solutions smaller classification time piecewise linearly inseparable dataset polynomial piecewisepolynomial classifiers provided improvement mrlp krlp methods datasets krlp method found solutions generalized best nearly best less computational time matrix representations multicategory support vector machines appendix contains definitions matrices used general kclass svm formulation 18 min let 0 0 0 0 0 0 0 0 0 r nn identity matrix matrix c n 0 0 0 0 0 0 0 0 0 0 matrix 0 0 0 0 0 0 0 0 0 0 vector ones matrix r comparison classifiers high dimensional settings decision tree construction via linear programming geometry learning neural network training via linear programming multicategory discrimination via linear programming serial parallel multicategory discrimination support vector networks methods mathematical physics rule induction forensic science linear nonlinear separation patterns linear programming nonlinear programming mathematical programming machine learning uci repository machine learning databases minos 54 users guide algorithm generate radial basis function rbflike nets classification problems polynomial time algorithm construction training class multilayer perceptrons pattern classification using linear program ming incorporating invariances support vector machines comparing support vector machines gaussian kernels radial basis function classifiers nature statistical learning theory nature statistical learning theory theory pattern recognition multisurface method pattern separation medical diagnosis applied breast cytology tr polynomial time algorithm construction training class multilayer perceptrons algorithm generate radial basis function rbflike nets classification problems nature statistical learning theory networks feature minimization within decision trees feature selection via concave minimization support vector machines incorporating invariances support vector learning machines comparison viewbased object recognition algorithms using realistic 3d models comparing support vector machines gaussian kernels radial basis function classifiers ctr tieyan liu yiming yang hao wan huajun zeng zheng chen weiying support vector machines classification largescale taxonomy acm sigkdd explorations newsletter v7 n1 p3643 june 2005 koby crammer yoram singer algorithmic implementation multiclass kernelbased vector machines journal machine learning research 2 312002 rong jin jian zhang multiclass learning smoothed boosting machine learning v67 n3 p207227 june 2007 glenn fung l mangasarian multicategory proximal support vector machine classifiers machine learning v59 n12 p7797 may 2005 ryan rifkin aldebaro klautau defense onevsall classification journal machine learning research 5 p101141 1212004 yiguang liu zhisheng liping cao novel quick svmbased multiclass classifier pattern recognition v39 n11 p22582264 november 2006 ping zhong masao fukushima secondorder cone programming formulations robust multiclass classification neural computation v19 n1 p258282 january 2007 andreas albrecht chakkuen wong approximation boolean functions local search computational optimization applications v27 n1 p5382 january 2004 isabelle guyon jason weston stephen barnhill vladimir vapnik gene selection cancer classification using support vector machines machine learning v46 n13 p389422 2002 fabien lauer ching suen grard bloch trainable feature extractor handwritten digit recognition pattern recognition v40 n6 p18161824 june 2007