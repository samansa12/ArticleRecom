guide literature learning probabilistic networks data abstractthis literature review discusses different methods general rubric learning bayesian networks data includes overlapping work general probabilistic networks connections drawn statistical neural network uncertainty communities different methodological communities bayesian description length classical statistics basic concepts learning bayesian networks introduced methods reviewed methods discussed learning parameters probabilistic network learning structure learning hidden variables presentation avoids formal definitions theorems plentiful literature instead illustrates key concepts simplified examples b introduction probabilistic networks probabilistic graphical models representation variables problem probabilistic relationships among bayesian net works popular kind probabilistic network used different applications including fault diagnosis medical expert systems software debugging 1 review learning focus mainly bayesian networks based directed graphs probabilistic networks increasingly seen convenient highlevel language structuring otherwise confusing morass equations explicit representation dependencies independencies variables ignores specific numeric functional details depending interpretation also represent causality 2 3 4 5 probabilistic networks broad sense independently developed number communities 6 genetics 7 social science statistics factor multidimensional contingency tables artificial intelligence model probabilistic intelligent systems 8 decision theory model complex decisions 9 area considered review graphical modeling social science rich development application strong interactions artificial intelligence statistical communities 10 3 11 12 networks general play role highlevel language seen artificial intelligence statistics lesser degree neural networks biological views offer alternative interpretation see survey ripley 13 networks used build complex models simple components networks broader sense include prob current address thinkbank 1678 shattuck avenue suite berkeley ca 94709 email wraythinkbankcom url httpwwwthinkbankcomwray abilistic graphical models kind considered well neural networks 14 decision trees 15 probabilistic networks distinguishing characteristic specify probability distributionthey therefore clear semantics allow processed order diagnosis learning explanation many inference tasks necessary intelligent systems stance new research area considered briefly last section probabilistic network input specification compiler generates learning algorithm compilation made easier network defines probability distribution learning probabilistic networks particular terest earlier work artificial intelligence building expert systems involved tedious process manual knowledge acquisition 16 tedium spurred two developments less continued independently recently machine learning originally focused learning rule based systems 17 18 uncertainty artificial intelligence focused developing coherent probabilistic knowledge structures whose elicitation suffered less pitfalls instance henrion cooley give detailed case study 19 heckerman developed similarity networks 20 allow complex network elicited simply one would expect interest artificial intelligence learning probabilistic networks result marriage machine learning uncertainty artificial intelligence neural network learning developed concurrently based almost exclusively learning data networks computational side neural networks terested information processing opposed biological modeling increasingly moving direction probabilistic models therefore overlap learning probabilistic networks neural networks 21 22 23 statistics many general inference techniques 24 25 26 developed applied learning probabilistic networks computer scientists instance artificial intelligence often contributed terms combining scaling techniques generalizing classes representations examples variety probabilistic networks applications learning given 23 27 learning probabilistic networks includes number complications learning structure parameters given structure hidden variables whose values never present data values variable sometimes missing review describes current literature addressing various tasks reviews major methodolo ieee transactions knowledge gies applied describes major algorithms available software learning bayesian networks discussed review extensive list software general inference probabilistic networks maintained world wide web 28 list relevant online tutorial articles slides several mentioned also available 29 another area considered review empirical evaluation learning algorithms probabilistic networks empirical evaluation learning algorithms fraught difficulties 30 notwithstanding interesting empirical studies appear 31 32 33 34 35 36 37 38 ii introduction probabilistic networks section introduces bayesian networks general probabilistic networks tutorial articles bayesian networks see 39 40 41 introduction artificial intelligence perspective see 8 statistical introduction graphical models general see 42 tutorial introduction see 43 introduction bayesian networks bayesian methods learning see 44 kinds networks include markov undirected networks markov random fields considered widely image analysis spatial statistics 45 neural networks 14 section introduces bayesian networks simple example illustrates richness representation additional examples consider bayesian networks discrete variables simplest form consist network structure associated conditional probability tables example adapted 39 structure network structure represented directed acyclic graph dag given fig 1 network occupation climate age disease symptoms fig 1 simple bayesian network definition equivalent following functional decomposition joint probability full variable names abbreviated turn equivalent following set conditional independence statements symptoms fage occ climg two five probability tables age 45 054 disease symptoms stomach myocardial neither ulcer infarction stomach pain 080 chest pain 015 090 010 neither abjc reads b independent given c 8 46 take node symptoms exam ple node one parent disease three ancestors age occ clim one reads assumption symptoms dependent age occupation climate indirectly influence disease network substructure definition translates third independence statement bayesian networks therefore simplify full joint probability distribution set variables show independencies variables b conditional probability tables parameters conditional probability tables needed specify probability distribution based network structure fig 1 see equation 1 tables page pocc pclim pdiseasejage occ clim psymptomsjdisease need specified tables may specified form implicitly parametric probability distribution explicitly ta bles two tables given page psymptomsjdisease notice age real valued variable discretized create binary vari able symptoms three valued discrete variable disease without assumptions network leads equation 1 instead five smaller tables one large joint table five variables would required networks provide way simplifying representation probability distribution c extensions variables treated simple discrete variables conditional probabilities example simple tables general variety variables functions used bayesian networks variables could real valued integer valued multivariate realvalued variable may probability density function gaussian instead giving probability table mean variance gaussian would given functions parent variables buntine guide literature learning graphical models constructions allow bayesian networks represent standard statistical models regression gaussian error loglinear models 42 furthermore graphical models restricted directed undirected arcs used problems diagnosis association symptoms might represented image analysis associations regions image combination directed undirected graphical models developed lauritzen wermuth 47 forms rich representation language introduction combinations see 48 example richness consider feedforward neural networks next connections feedforward neural networks fig 2 shows transformation feedforward neural network predicting real valued variables probabilistic network fig 2a shows feedforward network sigmoid sigmoid sigmoid sigmoid gaussian gaussian b fig 2 feedforward network bayesian network form used 14 fig 2b shows corresponding probabilistic network bivariate gaussian error distribution grafted onto output nodes network feedforward neural network three lower nodes filled indicate input nodes bivariate gaussian represented probabilistic network two nodes directed arc equivalent representation would use undirected arc transformation bayesian network needs qualified several ways notice interior nodes bayesian network labeled simoids transfer function typically used feedforward network nodes also double ovals rather single ovals shorthand say variable deterministic function puts rather probabilistic function neural networks usually weight associated arc giving sense strength association probabilistic networks arc indicates form probabilistic dependence correlation weights instead associated node used parameterize functions node instead furthermore probabilistic network explicitly includes measured output variables network neural network includes predicted output variables 1 probabilistic network therefore explicitly represents error function whereas neural network leaves unspecified summary bayesian network indicates class fig 3 simple clustering model output variables gaussian distribution based variables 1 2 deterministic sigmoid functions hidden variables sophisticated dynamic networks recurrent neural networks 49roughly might thought flexible nonlinear extension probabilistic models like kalman filters hidden markov models networks based feedforward neural networks relationship probabilistic networks still development e connections statistics pattern recognition whittaker 42 wermuth lauritzen 50 provide rich set examples modeling statistical hypotheses using graphical models using mixed graphs incorporating undirected directed networks consider clustering style unsupervised learning bayesian network drawn clustering algorithm autoclass 51 assumed observed variables independent given hidden class clustering cases grouped coherent manner probabilistic network fig 3 suggests way discrete variable class introduced termed latent hidden variable value never appears data indicates unknown class case belongs advantage construction class value known case probability distribution becomes simple one b c independent needing 3 real valued parameters define model called mixture model joint probability mixture data obtained different classes visual illustration power mixture models consider real valued variables xy bivariate gaussian places oval shaped cloud centered point mixture four bivariate gaussians illustrated fig 4 four clouds points mixture contains many classes density become quite complex popular models used pattern recognition speech recognition control kalman filter hidden markov model hmm also modeled bayesian networks 52 53 simple hidden markov model given fig 5 sequence observations made phonemes utterance indicated shaded nodes observe 1 observe observe i1 shading indicates variables observed observations dependent hidden states hidden 1 hidden hidden i1 underlying system observations phonemes hidden states may letters underlying word spoken course hidden observer kinds models fig 4 data 2dimensional mixture gaussians observe 1 observe hidden observe fig 5 simple hidden markov model dynamic sense network set repeated units expanded time instance used forecasting 54 f causal networks useful trick used elicitation bayesian networks assume arcs represent causality consider network 39 reproduced fig 1 one could imagine environmental variables causing disease turn causes symptoms nice way explaining particular graph expert bayesian networks interpretation sometimes referred causal networks 2 3 4 55 causality fundamental importance science notion intervention 55 5 identifying observed probabilities relating smoking sex lung cancer interesting task real goal study establish act changing someones smoking habits change susceptibility lung cancer kind action external intervention variables causal model expected stable acts external conclusions drawn still valid probabilistic interpretation networks used elsewhere review assumption cases got passive observation independently identically distributed examples networks used represent causality manner networks different interpretation probabilistic networks considered causality networks learning causlity covered review learning identification causality considered 56 3 57 58 59 ii sample database relational table case b c iii simple examples basic concepts example learning consider data three binary variables b c data would take form table given simple example table ii 4 rows table give 4 cases might different patients typically hundreds thousands cases would exist relational database table ii case three variables measured values recorded values variable either true indicated false indicated f variable could also value represents missing value means value variable unknown missing values common domains especially variables expensive measure hypothesis space example bayesian networks might match problem given fig 6 first consider structure c fig 6 bayesian networks three variables b c denote represents three variables independent structure probability tables pa pb pc needed since variables binary three probabilities specified three real numbers 0 1 denote tables parameter set 2 3 structure c denoted c probability tables pa pb pcjb denoted c needed parameter set 4 pa pb specified one value pcjb specified two values instance consider conditional probability distributions complete network sm probability table pxjy subset real space number values variable x fully connected network matching table ii every two variables connected buntine guide literature learning graphical models 7 real values 7 calculated 2 3 gamma 1 network k binary variables needs k 2 values specify conditional probability tables realvalued node whose conditional probability distribution gaussian k parents require kk 12 real values specify mean covariance matrix gen eral real values used specify conditional probability tables either explicitly table implicitly referred parameters network simple counting argument shows 25 different networks three variables fig 6 however happens several equivalent sense represent equivalent independence statements networks 11 different equivalence classes networks three variables instance consider last three networks given fig 6 e f networks following functional decompositions respectively labeled e f basic algebra using laws conditional probability show bayesian networks e equivalent functional decompositions therefore equivalent independence properties bayesian network f different structures e said equivalent probability models properties equivalence relation worked general bayesian networks 2 discussed section v since kk gamma 12 different undirected arcs one place network k variables means 2 kkgamma12 different undirected networks k variables variables ordered ahead time arc point towards variable later ordering 2 kkgamma12 different directed networks would many ordering allowed vary although equivalent probability models b sample likelihood maximum likelihood approach starting point statistical theory introduced first fix structure sm parameters model matching problem table ii calculate likelihood sample follows pcase case probabilities pcase calculated using probability tables given formulation assumes case independent others given true model sm independently identically distributed true model unknown model believed represent process generating data assumed exist purposes modeling perhaps reasonable approximation exists perhaps instance structure fig 6 pcase 1 js three terms right equation found corresponding entries probability tables quantity equation 2 called sample likelihood maximum likelihood approach fixed structure sm chooses parameters maximize sample likelihood important notice structure maximum likelihood calculation probability appearing likelihood case 1 function parameters used conditional probability table variable parameters bayesian network structure partitioned different parameters node b c db represents parameters conditional probability table variable b sample likelihood becomes notice product separate terms da db likelihood optimization decomposed maximum likelihood optimization three different variable sets individually represented show three local maximum likelihood problems one node sample likelihood said decompose bayesian networks neither deterministic variables missing hidden values undirected arcs decomposition also applies network incrementally modified instance search 23 60 parameters describe probability tables binary variables table ii equation 3 corresponds product binomials instance counts pa na give occurences respectively data case binomial maximum likelihood given observed frequency napa likewise variables entries tables important common assumption used computing sample likelihood complete data assumption holds case missing values 6 ieee transactions knowledge unrealistic assumption instance data comes historical medical database likely expensive measurements would taken recorded considered critical diagnosis complete data assumption simplifies calculation sample likelihood network instance consider model fig 6f consider likelihood case 3 suppose variable c missing value pcase 3 js c2ftfg three terms right equation simply corresponding entries probability tables f however notice summation outside many summations longer simple closed form solution maximizing sample like lihood furthermore optimization problem longer decomposes demonstrated equation 3 hidden variables lead problem violate complete data assumption summations always appear sample likelihood concept central subsequent techniques family statistical distributions known exponential family 26 63 introduction context probabilistic networks appears 23 family includes gaussian bernoulli poisson general functional form lends many convenient computational properties including compact storage training sample simple calculation derivatives fitting guaranteed linear size sample one needs become familiar features exponential family order understand many recent developments learning probabilistic models many properties sample likelihood impact complete data assumption exact solutions maximum likelihood equations forth follow directly standard results exponential familythe effort usually expended formulating probabilistic network member exponential family standard results exponential family follow 26 63 c basic statistical considerations suppose structure sm network discrete gaussian variables fixed remains learn probability tables considered earlier enough data sample likelihood wellbehaved differentiable function parameters often called parametric problem nonparametric prob lem contrast potentially infinite number pa rameters coherent likelihood function defined unparameterized always clear literature cases model presented nonparametric manner whereas given parametric basis classification trees example 64 15 consider problem learning structures well remember finite number fixed network structure distinct set parameters allowing set different structures parameters full probability density single natural global realvalued parameterization different parameterizations depending structure used problems sometimes referred semi parametric qualifications apply course clever mathematician coerce full specification network parameters single real number however would artificial construct complex noncontinuous derivatives furthermore structures fig 6 probability distributions represented structure set measure zero probability distributions structure b set measure zero within e 1 offering structures valid alternatives set measure zero ignored refer combination detailfor given structure neat parametric model structures form nested hierarchies subset measure zero othersas parametric structure problem learning network structures data sometimes termed model selection problem sense network corresponds distinct model one selected based data nonparametric methods model selection active research areas modern statistics 65 25 66 recently researchers statistics focused model uncertainty accepted selection single best model exponentialsized family modelsas case learning bayesian networksis often infeasible 67 68 25 rather selecting single best model one looks subset reasonable models attempting quantify uncertainty complexity learning network learning involves choosing possibly exponential number network structures giving values possibly exponential number real values problem basic results computational learning theory show difficult terms number cases required training time space required optimization two aspects referred sample complexity computational complexity respectively learning roughly three distinct phases cases obtained learn small sample medium sample large sample phases initially 1 purposes paper subspace measure zero integrated area relative full space zero usually means space lower dimension line measure zero finite plane rectangle finite plane nonzero measure twodimensional slice cube measure zero full threedimensional cube buntine guide literature learning graphical models small sample learning corresponds going ones biases priors large sample learning close true model possible high probability close measured according reasonable utility criteria meansquare error kullbackleibler dis tance learning possible many reasonable algorithms asymptotically converge truth small large sample phase medium sample phase algorithms perform better others depending well particular biases align true model use term biases loose sense cases obtained learn performance may increase gradually sometimes jumps algorithm better approximates truth illustrated learning curve fig 7 plots error idealized algorithm gains cases represented sample size n asymptotic error00 bayes optimal error small sample medium sample large sample fig 7 idealized learning curve error example approaches bayes optimal error rate without prescience lower bound error rate achieved algorithm instance predicting coin tosses fair coin bayes optimal error rate 50 theory learning curves developed instance 69 suppose hypothesis space family probabilistic networks k results computational learning theory 70 show many conditions transition large sample phase made sample size given sample size sample complexity discrete bayesian networks discussed earlier first term exponential k number variables second term quadratic course ignores issue computational com plexity given exponential number networks surprising formula tions learning bayesian network npcomplete problem 71 72 36 formulations learning viewed maximization problem find network maximizing quality measure case sample likelihood scores usually decompose often based sample likelihood see instance 61 23 37 62 optimization problem find network variables x maximizing function qualityxjparents x sample network influences quality measure parents function parents quality measure may logprobability loglikelihood complexity measure minimized measures discussed section viii maximization problem instance maximum branchings problem see discussion 37 general allowing quality function nodes npcomplete even variables network restricted 2 par ents polynomial variable 1 parent another variation problem discussed 37 find best l networks terms quality measure bayesian networks search problem also confounded existence equivalent networks nevertheless experience existing systems shows standard search algorithms greedy algorithms iterated local search algorithms often perform well basic greedy search explored 35 furthermore search problem adapts nicely branch bound using standard methods information theory provide bounds 73 savings exhaustive search appear many orders magnitude iv parameter fitting fixed graphical structure sm parameter fitting problem learn parameters data mathematics fitting parameters bayesianmarkov network extension standard fitting procedures statistics fitting algorithms exist bayesian networks general probabilistic networks cases complete missing data 74 42 75 76 see whittaker extensive discussion review methods theory case bayesian network complete data distributions nodes discrete probability tables gaussians fast close form solutions exist computed time proportional size data set example consider fitting model fig 6a data table 6 probabilities oe model occurs sample likelihood form oe maximum nm maximum likelihood solution parameters therefore equal observed frequency relevant probabilities cases variety iterative algorithms exist make use fast closed form solutions subrou tine common techniques shall explain expectation maximization em algorithm 77 iterative proportional fitting ipf algorithm 75 exponential family important maximum likelihood approaches suffer socalled sparse data instance may become undefined whenever table counts total zero consider model fig 6e consider estimating instances sam ple maximum likelihood estimate probability undefined since sample likelihood exist k binary variables fully connected bayesian network every two variables directly connected clearly need greater 2 cases sample maximum likelihood estimate defined related problem problem overfitting suppose sparse data problem observe maximum likelihood estimate equal 10 data observed cases variable c value based four cases would seem reasonable true value could 09 chance data estimate 10 must upper bound probability definition maximum likelihood value 10 4 must overestimate true sample likelihood 09 4 sample size gets larger larger overestimate gradually converge true value assured cases large sample properties maximum likelihood theory introduction see 78 however small samples maximum likelihood value may much larger true likelihood general maximum likelihood solution attempt fit data well possiblefor instance regression using 10 degree polynomials fit 11 data points exactly whereas 11 data points one might reasonably attempt fit 2 3 degree polynomial assume remaining lack fit due noise data maximum likelihood parameter values therefore said overfit data wellknown problem supervised learning instance addressed pruning methods classification trees 64 15 bayesian maximum aposterior map approach extends maximum likelihood approach introducing prior probability good introductions simplified bayesian approach extensions found 79 80 approach places probability distribution unknown parameters reasons using axioms probability theory likelihood augmented prior gives initial belief seeing data consider column data table ii consider parameter giving probability bayes theorem psample numerator contains sample likelihood prior denominator obtained integrating numerator z computations become simplified cases exponential family mentioned previously gaussians bernoulli forth example given fig 8 fig 8 priors likelihoods posteriors left graph shows two different priors priors beta distributions parameters ff marked plot second prior mild preference 0625 whereas prior agnostic middle graph shows likelihoods 3 different samples 01 2 counts sample size 4 right graph shows resulting posterior 2 theta posteriors resulting cluster three peaks top three posteriors prior agnostic prior influenced likelihood whereas three posterior peaks mild prior reflect shape prior quite strongly maximum posterior value value maximum curve notice effected prior likelihood many general algorithms exist addressing parameter fitting problems probabilistic networks missing latent variables large samples recursive incremental tech niques special nodes subjective priors 26 24 81 25 23 42 table iii lists major techniques application references given introductions new extensions examples use means thorough list references area common versions em ipf algorithms mean field theory based exponential family although generalizations exist used conjunction methods large number optimization techniques finding map computing various quantities used laplace approximation several optimization techniques specific parameter fitting learning includes fisher scoring method 89 approximate newtonraphson algorithm stochastic optimization computes gradients subsamples individual cases time 90 variations method popular neural networks 91 feature early methods 92 proven yield computational buntine guide literature learning graphical models algorithm problems refs map general 25 laplace 2ndorder approx 25 82 em missing hidden values 77 76 83 ipf undirected network 75 mean field approximate moments 84 22 gibbs approximate moments 85 86 mcmc approximate moments 87 88 iii general algorithms parameter fitting savings many studies extension parameter fitting handle sequential online learning missing data described 93 uses bayesian methods overcome problems sparse data defining dirichlet prior entries probability tables full implementation described 94 extensions made gaussians popular nodes types bayesian network 95 combined structure elicitation techniques parameter fitting prove powerful applications instance dynamic models medical domain 96 54 v structure identification methods ignoring issue sample size moment difficult question whether particular network structures without latent variables identifiable limit probability 1 assuming large amounts data accurately estimate various probabilities true probabilistic network reconstructed sense learning algorithm given sufficiently large sample invariably return hypothesis graphical structure parameters close truth question formalized addressed several angles computational learning theory 97 name identification learnability well statistics 78 26 name consistency situation n 1 fig 7 bayesian networks question confounded existence equivalence classes graphs one example redundant model 78 use hidden latent variables instance consider networks given fig 6 bayesian networks e equivalent probability models bayesian network f different therefore bayesian networks e equivalent sample likelihoods cannot distinguished data without additional criteria knowledge whereas bayesian network f could identified data alone theoretical tool used analyze identifiability equivalence graphical models latent variables 98 56 99 without 100 101 2 102 recently involving causality variables manipulated 57 thorough treatment issues equivalence latent variables causality appears 3 cases class equivalent graphs reconstructed data cases latent variables properties cannot identified uniquely identification methods lead earliest algorithms learning structure data 103 56 related approach also combines cross validation address model selection 104 identification methods also used tetrad ii successor tetrad 12 theory network identification data network equivalence precursor techniques learning medium sized samples fig 7 network equivalence important concept used bayesian techniques learning bayesian networks data used advanced work priors bayesian networks 105 37 discussed later vi diagnostics elicitation assessment day day practice learning data analysis may learning algorithm core lot work involves modeling assessment building model trying find going data experts opinions work relevant learning comes statisticians generally experience 106 107 decision analysts use methods constructing systems working experts 41 108 basic problem elicitation twist problem knowledge acquisition expert systems ffl medium sample regime applies fre quently data complemented prior knowledge constraints reliable useful results obtained ffl prior knowledge often obtained domain experts manual process knowledge elicitation ffl domain experts poor judging limitations capabilities estimating probabilities 109 one common mistakes beginners assume experts claims valid applications issues crucial learning problem come prepackaged neat wrapper instructions assembly heres data use five variables try c45 tree program learning problem usually embedded larger prob lem domain expert may needed circumscribe learning component variables might used predicted forth sometimes crucial success learning algorithm used almost incidental 110 number techniques exist interface learning knowledge acquisition diagnostics measures used evaluate particular model assumptions 111 112 113 sensitivity analysis 114 measures sensitivity results study model assumptions using techniques taught engineers everywhere wiggle inputs model case learning means constraints priors watch output model wiggles assessment elicitation usual process discussed manual knowledge acquisition interviewing expert order obtain prior estimates relevant quantities elicitation evaluation probabilistic networks well developed area refinement networks via learning made possible discussed later priors vii learning structure data earliest result structure learning chow liu algorithm learning trees data 115 algorithm learns bayesian network whose shape tree k variables ok 2 trees much less exponential number bayesian networks sample complexity thus o2 log sample complexity tree ok thus learning feasible small samples furthermore computational complexity searching tree shaped network requires quadratic number network eval uations herskovits cooper 116 demonstrated problem significant size complex structure learning possible quite reasonable sample sizes case 10000 despite faced potentially exponential sample complexity npcomplete search problem early work structure learning often based identification results discussed previous section instance 103 56 104 117 problems like learning structure bayesian network suffer samples smaller happens overfitting structure space similar overfitting parameter space discussed previously maximum likelihood hypothesistesting methods provide techniques comparing one structure another shall add arc model c better model f done instance using likelihood ratio test 42 43 repeated use test lead problems chance hypothesis tests 95 confidence level fail 1 20 times hundreds tests may need made learning network structure data comparable problem statistics literature variable subset selection regression prob lem one seeks find subset variables base linear regression pitfalls hypothesis testing context discussed 67 basic problem model selection focuses choosing single best model discrete variables least problem learning bayesian networks complete data related problem learning classification trees exemplified cart algorithm 64 statistics id3 c4 artificial intelligence 15 relationship holds sample likelihood binary classification tree represented product independent binomial distribu tions like sample likelihood bayesian networks binary variables described section iii problems also similar parametric structure classification tree problem long history studied perspective applied statistics 64 artificial intelligence 15 bayesian statistics 118 minimum description length mdl 119 120 genetic algorithms computational learning theory adaptation successful tree algorithm algorithm learning bayesian networks appears 121 relationship two approaches discussed 122 another adaptation quite direct constructor algorithm 104 adapts cost complexity technique cart algorithm trees variety heuristic techniques developed trees including handling missing values 123 discretization realvalued attributes 124 yet find way algorithms probabilistic networks viii statistical methodology work learning structure researchers applied standard statistical methodology fitting models handling overfitting therefore appropriate discuss standard methodologies done sec tion problem overfitting encountered addressed earliest methods important note role statistical methodology convert learning problem optimization problem statistical methodologies despite wide philosophical differences reduce learning problem kind optimization problem practitioner could well left wondering differences also important note structure learning built around form parameter learning subproblem general many different structure learning methods extensions general algorithms summarized table iii cases simple placing model selection wrapper around parameter fitting system 125 cases sophistication layered top perhaps unfortunate many different competing statistical methodologies exist address essentially problem partly stems apparent impossibility handling smaller sample learning problems objective manner difficulty establishing basis statistical methodology judged see instance efforts made compare different learning algorithms 30 consider statistical methodology higher level abstraction learning algorithm discussion bayesian perspective issues learning appears 26 touching prior probabilities subjective statistical analysis different disciplines addressed problems parallel attempted extend classical maximum likelihood hypothesis testing approaches statistics methodology comes cast staunch protagonists antagonists litany standard claims dogma paradoxes counterclaims useful become familiar different approaches mappings approximations better understand differences however difficult given confusing state literature methodology particular strengths make suitable certain conditions ease implementation adequate large samples buntine guide literature learning graphical models appropriate engineer availability software training forth believe one methodology superior respects comments review colored bayesian perspective tried keep comments realm generally believed knowledgeable area rather merely repeating dogma community also section introduction methodologies include appropriate tutorial references finally really hundreds different methodologies one small cluster researchers list presents different corners continuum maximum likelihood minimum cross entropy method maximum likelihood approach says find network structure sm whose maximum likelihood parameters largest minimum cross entropy approach says find structure whose minimum cross entropy data smallest two approaches equivalent 126 also well known suffer overfitting discussed section iv true model one single equivalent representative hypothesis space maximum likelihood approach consistent sense limit large sample converge truth 78 maximum likelihood method also viewed simplification approaches important starting point everyone large sample regime best strategy use maximum likelihood approach avoid mathematical implementation details complex approaches results computational learning theory bounding onset large sample phase useful deciding bayesian networks maximum likelihood approach applied 127 116 paper herskovits cooper major breakthrough learning bayesian networks clear paper mdl bayesian methods extend maximum likelihood approach could applied detail b hypothesis testing approaches hypothesis testing standard model selection strategy classical statistics probabilistic networks methods well developed variety statistical software exists 28 43 13 mentioned problem viable approach small number hypotheses tested clever greedy search techniques help 128 reducing number hypothesis tests required another way thinking deal multiple hypotheses let hypothesis testing return set possible models rather expecting isolate single one 128 strategy resembles bayesian approach multiple models considered discussed context probabilistic networks c extended likelihood approaches number extensions maximum likelihood approach proposed overcome problem overfitting overcome problems inherent hypothesis testing approaches replace sample likelihood modified score maximized examples include penalized likelihood akaike information criteria aic bayesian information criteria bic others 66 129 typically involves minimizing formula bic formula bicsm jsample c maximum likelihood estimate fixing structure sm n sample size dimm dimensionality bic criteria related variations asymtotically bayesian avoid specification prior similar variations minimum information complexity approaches described examples undirected probabilistic networks bic criteria appear 67 minimum information complexity approaches several different schools general rubric minimizing information complexity measure code length instance minimum description length mdl 130 minimum message length 131 minimum complexity 132 simple approximation mdl equivalent bic variations involve statistical quantities fisher information hypothesis dependent complexity measures chosen particularly domain approaches popular among engineers computer scientists learn coding information theory undergraduates one perspective methods related bayesian map methods although subtle differences 133 one advantage proponents claim approach particularly mdl school requires prior hence objective instances corresponding implicit prior constructed code authors use approach use bayesian methods disguise without ridiculed antibayesian colleagues search bounds instance 134 one area information complexity approach takes advantage techniques developed information theory suzuki developed branch bound technique learning bayesian networks based informationtheoretic bounds 73 bayesian networks mdl applied 61 135 136 resampling approaches modern statistics developed variety resampling schemes addressing overfitting parametric situations like learning networks resampling refers fact pseudosamples created original sample popular approach cross validation applied 104 resampling schemes used great success applied multivariate statistics see instance tutorial 137 strength lies fact reliable black box method used without requiring complex mathematical treatment found bayesian minimum complexity methods 138 resampling schemes therefore provide good benchmark comparison complex schemes additional mathematical implementation pitfalls theoretical justification large sample although empirical successes small sample case wide range problems f bayesian approaches rich variety bayesian methods depending approximations shortcuts made previous methodologies reproduced form bayesian approximation full form bayesian approach requires specification prior probability tutorial list references see 139 good general introduction bayesian methods learning bayesian networks found 79 advanced introductions reviews bayesian methods learning found 25 26 24 bayesian approach many different approxima tions simplest map approach seeks find structure sm maximizing logprobability log term psamplejsm called evidence differs likelihood psamplejsm evidence average sample likelihood rather maximumsample likelihood used earlier techniques z sometimes relative value calculated instead base structure 0 called bayes factor variety techniques approximations exist computing 25 26 23 basic technique bayesian learning bayesian network structures complete data uses standard bayesian methods worked one form many 140 35 121 111 112 68 141 142 143 37 38 certainly techniques use standard bayesian manipulations obvious students bayesian theory general case exponential family worked 105 good summaries line work found 111 68 144 37 23 thesis covering many issues 36 full bayesian approach predictive one rather returning single best network aim might perform prediction estimate probabilities new cases instance one might interested probability new cases based sample pnewcasejsample general estimated averaging predictions across possible networks using probability identity sm situation represented fig 9 approaches gibbs new data sample pnew fig 9 averaging multiple bayesian networks matches intuition five different networks seem quite reasonable lets hedge bets combine practice full summation possible approximations used bayesian methods learning probabilistic networks general sense found 121 68 143 144 145 35 146 147 computational aspects finding best l networks discussed 37 related concern combine posterior network probabilities efficiently compute conditional posterior probabilities 148 111 32 general bayesian algorithm family inference applies context parameter fitting structure learn ing markov chain monte carlo mcmc family algorithms introduction given 149 23 extensive review given 87 family uses following kind trick suppose wish sample distribution pa b c general might complex distribution convenient sampling algorithm may known complete data assumption violated instance discussed section iiib quite easy get intractible sample likelihood distribution network parameters hence posterior distribution network parameters may convenient functional form sample fromthis exactly kind problem mcmc methods designed even used instance estimate posterior predictions learning complex parametric systems sigmoidal feedforward neural networks 88 sample pa b c using gibbs sampler simplest kind mcmc method start 0 repeatedly resample variable turn according current conditional distribution read buntine guide literature learning graphical models sampled probabilistic networks ideal framework developing mcmc methods conditional distributions generated automatically network mcmc methods used parameter fitting sample different network parameters structure learn ing sample different possible probabilistic network structures use mcmc methods learning probabilistic networks discussed 85 144 147 146 23 madigan gavrin raftery 146 refer use mcmc methods averaging multiple probabilistic networksthe full predictive approachas markov chain monte carlo model composition mc 3 key distinction bayesian nonbayesian methods use priors priors unfortunately complex mathematically poorly chosen priors make bayesian method perform poorly methodsa real danger case bayesian networks semiparametric nature informative priors 68 111 121 37 35 38 146 147 noninformative priors used fundamental assumption equivalent network structures equivalent priors parameters 121 60 37 150 instance consider structures e fig 6 prior probability p js virtue equivalence converted prior e using change variables jacobian transformation notice prior constructed prior necessarily equal prior actually used e assumption prior equivalence sets two priors equal something applicable network causal interpretation 58 gives set functional equations prior satisfy basic theory properties priors bayesian networks discussed 105 extending techniques presented 37 ability use variety informative subjective priors bayesian networks one strengths informative priors include constraints preferences structure network 121 37 well preferences probabilities even using expert generate imaginary data 146 example language chain graphs extension bayesian networks given 38 potential using bayesian networks basis knowledge refinement suggested 121 37 111 146 applications offers integrated approach development maintenance intelligent systems long considered one potential fruits artificial intelligence ix learning structure exact algorithm handling incomplete data missing values found 151 problems involved exact methods previously explained 35 impractical larger problems could serve tool benchmark nontrivial sized problems many approximate algorithms exist instance mentioned table iii simple clustering algorithms learn bayesian networks single latenthidden variable root net work kinds problems addressed limited sense many years ai statistics community 152 bayesian method 153 51 likewise missing values handled well known em algorithm 76 accurately gibbs sampling 85 recent versions clustering algorithms search possible structures well 51 algorithms fit neatly categories learning markov undirected networks data related early boltzmann machine neural networks 21 also earlier bayesian methods seemed require input strict ordering variables 35 121 whereas identification algorithms require one thought combination bayesian identification algorithms 33 bayesian methods equivalent things large sample case independence tests used identification algorithms strict ordering entirely necessary bayesian algorithms 32 37 variety hybrid algorithms exist 59 104 12 73 provide rich source ideas future development x constructing learning software variety network structures latent variables different parametric nodes logistic poisson forms bugs program generate gibbs samplers automatically 154 86 effectively allows data analysis algorithms compiled specifications given probabilistic network technique addresses number nontrivial data analysis problems 155 86 unfortunately gibbs sampling without much thought domain specific optimization time intensive convergence may slow methods need developed make approach widely applicable algorithm schemas table iii applied within compilation framework well may possible construct efficient algorithms automatically exposition techniques used algorithms learning bayesian networks exact bayes factors differentiation readily automatedcan found 23 156 r realworld applications bayesian networks introduction equivalence synthesis causal models definition graphical representation causality graphical models causality intervention local computations probabilities graphical structures application expert systems discussion correlation causation probabilistic reasoning intelligent systems influence diagrams attitude formation models insights tetrad inferring causal structure among unmeasured variables network methods statistics introduction theory neural computation building expert systems current developments expert systems ductive knowledge acquisition case study experimental comparison knowledge engineering expert systems decision anal ysis probabilistic similarity networks connectionist learning belief networks mean field theory sigmoid belief networks operations learning graphical models tools bayes factors model uncer tainty bayesian theory graphical models discovering knowl edge software belief networks machine learning diagnos tic systems created model selection methods case study properties bayesian belief network learning algorithms algorithm construction bayesian network structures data evaluation algorithm inductive learning bayesian belief networks using simulated data sets bayesian method induction probabilistic networks data networks inference construction learning bayesian networks combination knowledge statistical data recursive models induced relevant knowledge observa tions statistical techniques thinking backwards knowledge acquisition bayesian networks without tears decision analysis expert systems graphical models applied multivariate statis tics introduction graphical modelling bayesian networks knowledge representation learning spatial statistics independence properties directed markov fields graphical models associations variables qualitative quantitative chain graphs learning finite state machines recurrent neural networks automata dynamical systems approaches substantive research hy potheses conditional independence graphs graphical chain models bayesian classification correlation inheritance planning control decision analysis continuous discrete variables mixture distribution approach uncertain reasoning forecasting causal diagrams empirical research theory inferred causation identification nonparametric structural equations bayesian approach learning causal net works causal inference presence latent variables selection bias hyper markov laws statistical analysis decomposable graphical models using causal information local measures learn bayesian networks learning bayesian networks unification discrete gaussian domains information exponential families statistical theory classification regression trees smallsample largesample statistical model selection criteria bayesian model selection social research discussion gelman rubin hauser rejoiner model selection accounting model uncertainty graphical models using occams window rigor ous learning curve bounds statistical mechanics decision theoretic generalizations pac model neural net learning applications learning bayesian networks npcomplete learning robust learning product dis tributions efficient mdl learning procedure using branch bound technique hierarchical interaction models effective implementation iterative proportional fitting procedure em algorithm graphical association models missing data maximum likelihood incomplete data via em algorithm tutorial learning bayesian networks decision analysis perspectives inference decision experimentation decision theoretic subsampling induction large databases laplaces method approximations probabilistic inference belief networks continuous variables accelerated quantification bayesian networks incomplete data factorial learning em algorithm markov chain monte carlo methods hierarchical bayesian expert systems language program complex bayesian modelling probabilistic inference using markov chain monte carlo methods bayesian learning neural networks chapman hall stochastic optimizationmethod efficient training feedforward neural net works mcclelland pdp research group sequential updating conditional probabilities directed graphical structures ahugin systems creating adaptive causal probabilistic networks parameter adjustment bayesian networks generalized noisy orgate tradeoffs constructing evaluating temporal influence diagrams learning limit nonuniform ffl ffilearning equivalence causal models latent variables algorithm deciding set observed independencies causal explanation chain graph markov property identifying independence bayesian networks markov equivalence chain graphs undirected graphs acyclic digraphs algorithm fast recovery sparse causal graphs system induction probabilistic models characterization dirichlet distribution application learning bayesian net works quantification judgment methodological suggestions assessment criticism improvement imprecise subjective probabilities medical expert system uncertainty guide dealing uncertainty quantitative risk policy analysis judgement un certainty heuristics biases applications machine learning rule induction bayesian analysis expert systems learning probabilistic expert systems sequential model criticism probabilistic expert systems sensitivity analysis probability assessments bayesian networks approximating discrete probability distributions dependence trees kutato entropydriven system construction probabilistic expert systems databases automated construction sparse bayesian networks learning classification trees stochastic complexity statistical enquiry coding decision trees theory refinement bayesian networks classifiers theoretical empirical study unknown attribute values induction multivalued interval discretization continuousvalued attributes classification learning mlc machine learning library c information theory statistics entropybased learning algorithm bayesian conditional trees fast model selection procedure large families models three approaches probability model selection stochastic complexity estimation inference compact encoding minimum complexity density estimation mml bayesianism similarities differences admissible stochastic complexity models classification problems learning bayesian belief networks approach based mdl principle construction bayesian networks databases based mdl scheme statistical data analysis computer age study cross validation bootstrap accuracy estimation model selection prior probabilities bayesian method induction probabilistic networks data influence diagram approach medical technology assessment learning probabilistic expert systems bayesian methods analysis misclassified incomplete multivariate discrete data bayesian graphical models discrete data strategies graphical model selection eliciting prior information enhance predictive performance bayesian graphical models estimation proportion congenital malformations using double sam pling incorporating covariates accounting model un certainty minimal assumption distribution propogation belief networks john wiley learning bayesian networks combination knowledge statistical data method learning belief networks contain hidden variables statistical analysis finite mixture distributions bayesian classification bugs program perform bayesian inference using gibbs sampling modelling complexity buntine guide literature learning graphical models applications gibbs sampling medicine networks learning uncertainty artificial telligence proceedingsproceedings eleventh conferenceproceedings selecting models data artificial intelligence statistics iv uncertainty artificial intelligence uncertainty artificial intelligence uncertainty artificial intelligence 5 bayesian statistics 4 artificial intelligence frontiers statistics tr ctr marek j druzdzel linda c van der gaag building probabilistic networks numbers come guest editors introduction ieee transactions knowledge data engineering v12 n4 p481486 july 2000 peter l spirtes data mining tasks methods probabilistic casual networks mining probabilistic networks handbook data mining knowledge discovery oxford university press inc new york ny 2002 sajjad haider belief functions based parameter structure learning bayesian networks presence missing data international journal hybrid intelligent systems v1 n34 p164175 december 2004 xiaoming zhou cristina conati inferring user goals personality behavior causal model user affect proceedings 8th international conference intelligent user interfaces january 1215 2003 miami florida usa wei yi liu ning song fuzzy functional dependencies bayesian networks journal computer science technology v18 n1 p5666 january david maxwell chickering optimal structure identification greedy search journal machine learning research 3 p507554 312003 jie cheng david bell weiru liu learning belief networks data information theory based approach proceedings sixth international conference information knowledge management p325331 november 1014 1997 las vegas nevada united states jiaying shen victor lesser communication management using abstraction distributed bayesian networks proceedings fifth international joint conference autonomous agents multiagent systems may 0812 2006 hakodate japan sajjad haider hybrid approach learning parameters probabilistic networks incomplete databases design application hybrid intelligent systems ios press amsterdam netherlands peggy wright knowledge discovery databases tools techniques crossroads v5 n2 p2326 winter 1998 marina meila michael jordan learning mixtures trees journal machine learning research 1 p148 912001 nir friedman dan geiger moises goldszmidt bayesian network classifiers machine learning v29 n23 p131163 novdec 1997 padhraic smyth david heckerman michael jordan probabilistic independence networks hidden markov probability models neural computation v9 n2 p227269 feb 15 1997 thomas nielsen finn v jensen learning decision makers utility function possibly inconsistent behavior artificial intelligence v160 n1 p5378 december 2004 peter l spirtes data mining tasks methods probabilistic casual networks methodology probabilistic networks handbook data mining knowledge discovery oxford university press inc new york ny 2002 rong chen edward h herskovits bayesian network classifier inverse tree structure voxelwise magnetic resonance image analysis proceeding eleventh acm sigkdd international conference knowledge discovery data mining august 2124 2005 chicago illinois usa clifford thomas catherine howie leslie smith new singly connected network classifier based mutual information intelligent data analysis v9 n2 p189205 march 2005 helge langseth thomas nielsen fusion domain knowledge data structural learning object oriented domains journal machine learning research 4 1212003 david j miller lian yan approximate maximum entropy joint feature inference consistent arbitrary lowerorder probability constraints application statistical classification neural computation v12 n9 p21752207 september 1 2000 russell greiner xiaoyuan su bin shen wei zhou structural extension logistic regression discriminative parameter learning belief net classifiers machine learning v59 n3 p297322 june 2005 david w albrecht ingrid zukerman e nicholson bayesian models keyhole plan recognition adventure game user modeling useradapted interaction v8 n12 p547 1998 david maxwell chickering learning equivalence classes bayesiannetwork structures journal machine learning research 2 p445498 312002 john binder daphne koller stuart russell keiji kanazawa adaptive probabilistic networks hidden variables machine learning v29 n23 p213244 novdec 1997 jie cheng russell greiner jonathan kelly david bell weiru liu learning bayesian networks data informationtheory based approach artificial intelligence v137 n12 p4390 may 2002 david maxwell chickering david heckerman efficient approximations marginallikelihood bayesian networks hidden variables machine learning v29 n23 p181212 novdec 1997 luc de raedt kristian kersting probabilistic logic learning acm sigkdd explorations newsletter v5 n1 july david heckerman bayesian networks data mining data mining knowledge discovery v1 n1 p79119 1997 paolo frasconi marco gori giovanni soda data categorization using decision trellises ieee transactions knowledge data engineering v11 n5 p697712 september 1999 rebecca f bruce janyce wiebe decomposable modeling natural language processing computational linguistics v25 n2 p195207 june 1999 paul j krause learning probabilistic networks knowledge engineering review v13 n4 p321351 february 1999 p bidyuk n terentev gasanov construction methods learning bayesian networks cybernetics systems analysis v41 n4 p587598 july 2005 anthony hunter hybrid argumentation systems structured news reports knowledge engineering review v16 n4 p295329 december 2001 nuria oliver barbara rosario alex p pentland bayesian computer vision system modeling human interactions ieee transactions pattern analysis machine intelligence v22 n8 p831843 august 2000 sreerama k murthy automatic construction decision trees data multidisciplinary survey data mining knowledge discovery v2 n4 p345389 december 1998