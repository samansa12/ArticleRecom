finite precision error analysis neural network hardware implementations parallel processing low precision fixed point hardware used build high speed neural network computing engine low precision results drastic reduction system cost reduced silicon area required implement single processing unit taken advantage implementing multiple processing units single piece silicon operating parallel important question arises much precision required implement neural network algorithms low precision hardware theoretical analysis error due finite precision computation undertaken determine necessary precision successful forward retrieving backpropagation learning multilayer perceptron analysis easily extended provide general finite precision analysis technique neural network algorithms set hardware constraints may evaluated b introduction high speed desired implementation many neural network algorithms backpropagation learning multilayer perceptron mlp may attained use finite precision hardware finite precision hardware however prone errors method theoretically deriving statistically evaluating error presented could used guide details hardware design algorithm implementation paper devoted derivation techniques involved well details backpropagation example intent provide general framework neural network algorithms set hardware constraints may evaluated section 2 demonstrates sources error due finite precision computation statistical properties general error model also derived equation error output general compound operator may written example error equations derived section 3 operations required forward retrieving error backpropagation steps mlp statistical analysis simulation results resulting distribution errors individual step mlp also included section error equations integrated section 4 predict influence finite precision computation several stages early middle final stages backpropagation learning finally concluding remarks given section 5 2 sources error finite precision computation finite precision computation nonlinear operation multiple variables several sources error exist example computation oewx two input variables w x input errors ffl w ffl x respectively whose sources prior finite precision data manipulations errors generated due finite precision computation involved operators specifically finite precision multiplication two variables generates one error ffl 3 similarly finite precision nonlinear operator oe generates error ffl oe therefore resulting finite precision result equal assume error product ffl w ffl x negligible first order taylor series approximation used input errors propagated operators example multiplication two variables finite precision errors propagates error ie wffl x propagated error along generated finite precision multiplication error ffl 3 propagates nonlinear operator resulting total finite precision error total finite precision error ffl imposed become input finite precision error variable future operations 21 error generation propagation successive operators compound operator produced successive operators oe shown figure 1 error input ffl x error generated operator ffl oe propagated remaining operators output approximate output error ffl terms ffl x ffl oe oe 8 figure 1 figure 1 successive operators generating propagating error defined intermediate result first successive operators 5 carrying similar expansion intermediate values rewrite k 1 defined 1 product shown chain rule derivative approximated derivative without error note approximation equivalent approximation already made first order taylor series therefore x 22 error generation propagation general compound operators effects finite precision error output general system compound operators multiple input variables calculated extension previous analysis successive operators single variable 8 following steps employed 1 break computation calculation graph see example given figure 2 general calculation graph made n operators foe g system inputs g 2 number operators oe intermediate generated inputs fy k g operator oe lower indices operator output extending eq 8 multiple inputs total finite precision error ffl given 8 3 using calculation graph partial derivatives evaluated substituted eq 9 give equation ffl 4 statistical methods discussed used evaluate error eq 9 methods include computation mean variance various functions random variables well approximations using central limit theorem 23 common techniques finite precision computations three common techniques used finite precision computations truncation jamming rounding truncation operator simply chops q lowest order bits number leaves new lowest order bit 2 r th place unchanged jamming operator chops q lowest order bits number forces new lowest order bit 2 r th place 1 q removed bits 1 otherwise new lowest order bit retains value operation equivalent replacing 2 r th bit logical 2 r th bit q bits chopped jamming operator advantage generating error zero mean generates error higher variance compared truncated one rounding operator also chops q lowest order bits number new lowest order bit 2 r th place q bit value chopped greater equal 2 r01 resulting value incremented 2 r otherwise remains unchanged 1 error generated truncating jamming rounding techniques may considered discrete random variable distributed range determined specific technique employed statistical view error desirable know mean variance error generated three techniques discrete random variable x mean given variance usually assumed uniformly distributed truncation truncation generates error uniformly distributed range 02 r 2 q possible error values equal probability therefore 1 mean variance may computed jamming error generated jamming uniformly distributed probability error zero twice probability error holding possible values range error x results 2 q1 0 1 possible error values mean variance jamming error rounding rounding generates error uniformly distributed range 02 r01 2 q possible error values equal probability therefore 1 mean variance computed nonlinear functions discrete random variables nonlinear function discrete random variable x mean variance given 24 statistical properties independent random variables two independent random variables x means x variances oe 2 constant following properties mean variance shown 7 1 multiplication constant 2 sum two independent random variables 3 product two independent random variables 25 statistical properties sum independent random variables expected squared error expected squared error written terms mean variance error consider set errors independent random variables fffl g mean variance oe 2 expected value average sum squares fffl g written n noting oe expected value average sum squared errors equal central limit theorem central limit theorem 7 states fx g independent random variables density sum properly normalized tends normal curve n 1 discrete random variables probabilities tend samples normal curve normalization achieved couple different ways discrete random variables mean variance oe 2 sum x results invoking central limit theorem probability sum random variables x equal discrete value x k approaches sample normal curve n large oe x sum products independent random variables central limit theorem extended cover case random variable summed product random variables say independent random variables fx g fy g probability density random variable xy approaches normal curve large n mean variance equal xy 28 3 application neural network retrieving learning shown operations retrieving learning phases neural network models formulated linear affine transformation interleaved simple linear nonlinear scalar operation terms hardware implementations formulations call mac multiply accumulation processor hardware 4 5 1 without loss generality specifically discuss multilayer perceptron neural network model backpropagation learning 10 9 31 forward retrieving back propagation mlp given trained fixedweight mlp retrieving phase receives input test pattern fx 0i g propagates forward network compute activation values output layer fx lj g used indicator classification regression purposes hand commonly used learning phase mlp input training pattern fx 0i g first propagated forward network activation values fx lj g computed according forward operations used retrieving phase output activation values fx lj g compared target values g value output delta fffi lj g neuron output layer derived error signals propagated backward allow recursive computation hidden deltas fffi lj g well update values weights f1w lij g layer many methods proposed accelerate learning estimating local curvature training error surface using second order derivative information discussion methods beyond scope paper referred 3 operations forward retrieving llayer perceptron formulated forward affine transformation interleaved nonlinear scalar activation function x li denotes activation value th neuron l th layer w l1ij denotes synaptic weight interconnecting th neuron l th layer j th neuron th layer nonlinear activation function f1 usually taken sigmoidal learning mlp follows iterative gradient descent approach following update presentation training data pair 10 9 computation backpropagated error ffi lj formulated backward affine transformation interleaved postmultiplication derivatives nonlinear activation function initial outputlayer propagated error 32 finite precision analysis forward retrieving explicitly following procedure discussed section 22 calculation graph forward retrieving operation simplified notation see eq 29 mlp shown figure r r09 0x n w nj f figure 2 calculation graph forward retrieving mlp 3 denotes truncation jamming rounding operator carry analytical formula given eq 9 forward retrieving mlp several partial derivatives need computed eq 33 w ij i3 i3 substituting values partial derivatives eqs 34 37 generated propagated errors variables operators eq 9 ffl i3 33 finite precision analysis output eq 32 calculation graph computation backpropagated error output neuron simplified notation shown figure 3 r 00 r 023 j0 figure 3 calculation graph output neuron carry analytical formula given eq 9 output delta computation mlp partial derivatives eq 39 evaluated j0 substituting partial derivatives individual error terms overall finite precision error output delta computation 34 finite precision analysis hidden eq 31 calculation graph computation backpropagated error hidden layer neuron simplified notation shown figure 4 following similar partial derivative evaluations using eq 45 compute finite precision error hidden delta see eq 9 ffl k3 ffl k r 023 figure 4 calculation graph hidden neuron 35 finite precision analysis weight update eq 30 calculation graph computation weight update without momentum term simplified notation shown figure 5 r 023 j3 figure 5 calculation graph following similar partial derivative evaluations using eq 47 compute finite precision error weight update see eq 9 36 statistical evaluation finite precision errors given analytical expressions finite precision errors associated forward retrieving backpropagation mlp statistical evaluation errors undertaken evaluation based mean variance analysis using truncation jamming rounding techniques also based statistical properties independent random variables sums independent random variables sums products independent random variables discussed sections 23 24 25 first step choose precision component employed step problem practical limited precision implementation mlp algorithm might use precisions follows 2 1 neurons 8bit 8 bits right decimal inputs outputs targets range 00 10 2 weights biases use 16 bits one sign bit 3 bits left 12 bits right decimal range 080 80 3 output ffi 8 bits one sign bit 7 bits right decimal range 005 05 4 hidden ffi uses 16 bits one sign bit 3 bits left 12 bits right decimal range 080 80 finite precision error forward retrieving expected forward retrieving error calculated single layer neurons multiple layers neurons propagating upward finite precision errors lower layers first simplification may made equation 38 multiply accumulate steps computed without generating error enough bits eg 24 bits used intermediate steps i3 case ffl i3 practical since expense accumulator precision small 3 operator reduces final 24bit sum 8bit one sign bit 3 bits left 4 bits right decimal value used input sigmoid lookup table equation 38 may rewritten invoking central limit theorem sums necessary know distributions random variables w ij ffl w ij table 1 shows statistically evaluated values contributing component eq 49 based assumptions bit sizes given evaluation starts 16bit weights uniformly distributed across entire range 08 8 weight error comes truncating 24bit weights 16 bits input neuron x 8bit value uniformly distributed 00 10 truncated 24bit value therefore ffl x truncation error 16 distribution f 0 j approximated function normally distributed random variable ffl 3 error generated accumulated 24bit value jammed become 8bit value ffl f j error generated lookup table approximated rounding 2 bits place rv type q r oe 2 rounding 8 12 truncation jamming rounding 2 5 table 1 mean variance variables forward retrieving calculation based precision assumptions given table 1 various sizes bitallocation weights figure 6 shows statistically evaluated average sum squares ffl defined eqs 49 24 due finite precision computation single step forward retrieving evaluations weight bit number say kbits weight always contains one sign bit 3 leftofdecimal bits k04 rightofdecimal bits lower solid curve shows statistical evaluation finite precision error introduced neurons first hidden layer upper solid curve shows second hidden layer output layer 2layer perceptron note statistical evaluation errors shows divein around 8bit weights fact suggests implementation finite precision hardware forward retrieving purpose train network using high precision computation constraint sign plus 3 bits left decimal download well trained finite precision portion 8 bits total 4 bits right decimal weights hardware performance degradations due finite precision conversion almost negligible weight bits average squared figure statistically evaluated simulation evaluated values effl 2 introduced neurons first second hidden layers simulations also conducted verify statistical evaluations 2layer perceptron 100 inputs 100 hidden neurons 100 output neurons simulated 100 sets randomly generated 100dimensional input data tested averaged weights network also randomly generated average sum squares differences finite precision computation full precision 64 bits contributing components computation thus obtained lower dashed curve shows simulated evaluation finite precision error introduced neurons first hidden layer upper dashed curve shows output layer curves match quite consistently produced statistical evaluations 4 finite precision analysis iterative learning discussed section 31 backpropagation learning involves four consecutive steps computations forward retrieving output delta computation hidden delta computation weight updating therefore weight updating iteration presentation training pattern finite precision errors ffl 1w introduced f1w lij g given eq 48 fact propagated result error generated previous three steps therefore final mathematical expression finite precision error single weight updating iteration formulated straightforward manner based existing derivations given eqs 38 44 46 48 statistical evaluation value average sum squares weight updating error ffl 1w due finite precision computation single learning iteration thus computed backpropagation learning discussed simply nonlinear optimization problem based simple gradient descent search elegant way computing gradients using chain rule layers network gradient descent search updates weights based first derivative approximation error surface updating individual weight independent others 6 therefore even approach computationally efficient behave unwisely converge slowly complex error surfaces due strong influence introduced gradient descent search approximation real effect learning convergence accuracy due finite precision computation difficult measure therefore statistically evaluated average sum squares ffl 1w determine networks propensity learn 41 ratio finite precision errors meaningful measure ae potentially indicates effect finite precision computation weight updating defined ratio statistical average sum squares finite precision weight updating error ffl 1w full precision weight updating magnitude 1w 2 ratio serves useful indicator additional impact caused finite precision computation top caused gradient descent approximation backpropagation learning ratio depends number bits assigned finite precision computation current stage learning progress specified distribution difference desired actual output g specifically assume ffl x lj 0 since ability learn depend ability learn finite precision values based practical choices finite precision bit size given section 36 vs number bits say k bits assigned weights fw ij g weight updates f1w ij g statistically evaluate ratio several different stages learning figure 7 shows statistical evaluation values finite precision ratio weights connecting neurons hidden output layers 2layer mlp 4 different values used represent four different stages learning early stage middle stage convergence stage convergence stage figure 8 shows statistical evaluation values finite precision ratio weights connecting neurons hidden input layers four different values fi note finite precision ratio curves gradually along various stages learning dive around region number bits weights 1214 bits soft convergence stage 1416 hard convergence stage learning get almost steady number bits increased dive point indicates potential strong disturbance convergence accuracy backpropagation leaning long run therefore gives good guideline number bits required weights learning similar learning convergence accuracy attained using high precision computation also interesting note later stage learning impact finite precision error getting larger due smaller values fi network finetuning weights weight bits finite precision figure 7 statistical evaluation values finite precision ratio toplayer weights 2layer mlp four different stages learning evaluated 42 simulation results iterative learning verify theoretical evaluation impact caused finite precision computation simple regression problem designed maps 2dimensional inputs fx 1 1dimensional outputs fyg mlp containing 2 input neurons 8 hidden neurons 1 output neuron adopted 256 pairs randomly selected data training finite precision learning simu weight bits finite precision figure 8 statistical evaluation values finite precision ratio hiddenlayer weights 2layer mlp four different stages learning evaluated lations performed based choices bit sizes component given section 36 vs different number bits assigned fw ij g f1w ij g figure 9 shows average 256 training data squared difference desired actual outputs 2d regression problem network converges hard convergence usually required kind nonlinear regression problem note predicted point around 1516 bits weights squared difference curve dives implies inability converge desired mapping number bits weights less 16 bits similar supporting results also observed xor classification problem using mlp 2 inputs 3 hiddens 1 output see figure 10 due classification nature xor problem soft convergence good enough termination training therefore predicted point 1213 bits weights squared difference curve dives another interesting observation worthwhile mention total finite precision error single iteration weight updating mainly generated final jamming operators computation output delta hidden delta weight update therefore even though required least 13 16 bits assigned computation weight update stored total weight value number weight bits computation forward retrieving hidden delta steps learning low 8 bits without excessive degradation learning convergence accuracy002006010140184 weight bits average squared figure 9 average squared differences desired actual outputs 2d regression problem network converges 010203 weight bits average squared figure 10 average squared differences desired actual outputs problem network converges concluding remarks paper devoted derivation finite precision error analysis techniques neural network implementations especially analysis backpropagation learning mlps analysis technique proposed versatile prepare ground wider variety neural network algorithms recurrent neural networks competitive learning networks etc networks share similar computational mechanisms used backpropagation learning forward retrieving operations shown 8bit weights sufficient maintain performance using high precision computation hand network learning least 1416 bits precision must used weights avoid training process divert much trajectory high precision computation r vlsi architecture highperformance implementation limits artificial neural networks nonlinear optimization neural network learning parallel architectures artificial neural nets unified architecture artificial neural networks recursive least squares learning algorithms neural net works pizer victor l learning internal representations error propagation beyond regression new tools prediction analysis behavior science tr unified systolic architecture artificial neural networks learning internal representations error propagation ctr ming zhang stamatis vassiliadis jos g delgadofrias sigmoid generators neural computing using piecewise approximations ieee transactions computers v45 n9 p10451049 september 1996 cesare alippi luciano briozzo accuracy vs precision digital vlsi architectures signal processing ieee transactions computers v47 n4 p472477 april 1998 yongsoon lee seokbum ko fpgabased face detector using neural network scalable floating point unit proceedings 5th wseas international conference circuits systems electronics control signal processing p315320 november 0103 2006 dallas texas cesare alippi randomized algorithms systemlevel polytime analysis robust computation ieee transactions computers v51 n7 p740749 july 2002