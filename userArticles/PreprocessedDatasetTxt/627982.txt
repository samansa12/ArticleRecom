generalization generalizability measures abstractin paper define generalization problem summarize various approaches generalization identify credit assignment problem present problem solutions measuring generalizability discuss anomalies ordering hypotheses subdomain performance normalized averaged show conditions anomalies eliminated generalize performance across subdomains present measure called probability win measures probability whether one hypothesis better another finally discuss limitations using probabilities win illustrate application finding new parameter values timberwolf package vlsi cell placement routing b introduction generalization psychology tendency respond way different similar stimuli 6 transfer tendency may based temporal stimuli spatial cues physical characteristics learning hand may considered balance generalization discrimination ability respond differences among stimuli imbalance may lead negative results machine learning area artificial intelligence extends knowledge concepts understanding one observations instances research supported national science foundation grant mip 9632316 national aeronautics space administration grant nag 1613 appear ieee transactions knowledge data engineering vol 10 1 feb performance normalization statistical methods evaluate generalizability dataintensive methods knowledge intensive methods concept learning generalization figure 1 relationship concept learning generalization generalizability concept 1 number instances involved amount information carry determine learning method used learning methods classified dataintensive knowledgeintensive see figure 1 dataintensive methods symbolic concepts learned using dataintensive similaritybased methods learner shown large number related examples required identify similarities generalize concept embedded using approach mitchell 25 defines generalization process takes account large number specific observations inductive bias extracts retains important features characterize classes observations casts generalization search problem alternative generalization methods different search strategies example dataintensive learning method learning heuristics represented collection production rules 42 approach learning modifies rules based decisions made rules positive negative examples found process apportioning feedback signal individual decisions carried past well decision elements applied decision order refine heuristic method called credit assignment former credit assignment called temporal latter structural credit assignment usually difficult learning incrementally single concepts examples especially learning multiple disjunctive concepts learning data noisy case teacher may needed tell learner proper amount credit assign decision second class dataintensive learning methods decisiontheoretic methods use statistical decision theory discriminate probabilistic patterns exhibited learning examples 28 major component decisiontheoretic approach loss function measures loss learner categorizes learning example incorrectly represents statistical approach credit signment minimizing total loss using statistical methods sometimes possible show asymptotic convergence concept learned examples decisiontheoretic methods include evolutionary programming 20 genetic algorithms 13 classifier systems 5 artificial neural networks anns 22 contrast using extensive training examples dataintensive methods knowledgeintensive methods rely domainspecific knowledge learn generalize explanationbased learning learner analyzes single training example using domain knowledge concept study produce generalization example deductive justification generalization 8 26 knowledgeintensive methods work well concept generalized deduced domain knowledge evaluate quality learning generalization method measure degree learning generalization achieved generalizability measures developed simplest case measure number positive negative examples learning general cases degree example satisfies learned concept must considered statistical techniques employed determine whether learned concept generalized example learning feedforward anns effectiveness ann computes discrete f01gvalued mappings evaluated networks ability solve dichotomization problems using measures discrimination capacity vcdimension named vapnik chervonenkis 36 efficiency decision functions 23 ann performs function approximation computing either discrete multiplevalued continuous mappings measure quality using concepts combinatorial dimension approximation error estimation error finally concept pac prob ably approximately correct learning 18 useful characterizing time complexity algorithms learning discrete continuous mappings related problem generalizability normalization learned results relative baseline quality learned concept measured numerically depends attributes example may necessary normalize measure respect baseline statistical evaluations made instance quality measure learned concept may depend size learning example needs normalized results multiple learning examples aggregated statistically case generalizability learned concept may depend baseline statistical method used aggregate performance measures anomalies ordering hypotheses may happen different normalization aggregation methods used discussed detail section 33 next section summarize previous approaches generalization credit assignment present section 3 general concept gen eralizability generalizability measures anomalies generalization performance measures normalized aggregated illustrate section 4 application method section 3 find new parameter values timberwolf instance space rule space experiment planning instance selection result generalization figure 2 process inductive learning generalization generalization using induction section summarize various strategies generalization early work inductive learning generalization done simon lea 31 used training instances selected space possible instances guide search general rules process inductive learning entails mapping instance space rule space involves experiment planning instance selection result interpretation generalization see figure 2 21 generalization problem generalization involves extraction information useful guide search rule space 1 simplify search process good representation rule space must chosen generalization carried inexpensive syntactic operations turning constants variables dropping conditions adding options curve fitting zeroing coefficient specific operators used may depend representation rule space instance production rule z z 0 used represent either backward form z 0 value state vector plus associated predicate forward form z 0 computational rule evaluation execution rule constitutes credit assignment whereas creation new rules involves generalization latter entails identification subvector variables relevant creation proper decision situation reason making decision waterman 42 proposed set generalization operators modify defined symbolic values rule eliminate one variables rule change action rules errorcausing rules mitchell 25 defines generalization context language describes instances generalizations based set positive negative examples predicates matched generalizations instances hence generalizations defined within provided language consistent presented training examples general generalization also requires function evaluate positive negative examples obtained order provide feedback credit assignment simplest case function counts number positive negative ex amples decisiontheoretic approaches loss function used measure loss learner categorizes learning example incorrectly approach taken classifiersystem geneticsbased learning uses fitness function reinforcement learning evaluation function may learned independently form supervised learning order pro generalization strategies knowledge intensive dataintensive explanationbased depthfirst breadthfirst version space decisiontheoretic credit assignment structural temporal figure 3 classification generalization strategies vide proper temporal credit assignment 33 27 34 reinforcement function particularly difficult design examples drawn problem space statistically related happens evaluation data depends size examples examples drawn belong different problem subdomains possible solutions issues discussed section 33 22 generalization strategies defined mitchell 25 26 generalization strategies broadly classified data driven knowledgedriven see figure 3 paradigms use generateandtest generates alternative concepts tests test cases constructs feedbacks credit assignment aid refinement concepts generated difference lies amount tests performed datadriven methods rely domain knowledge often require extensive tests concepts consideration reliable feedbacks gen erated contrast knowledgedriven methods rely domain knowledge one tests deduce new concepts datadriven generalization strategies classified depthfirst search breadthfirst search versionspace decisiontheoretic techniques 25 depthfirst strategy starts single generalization current best hypothesis tests training example modifies hypothesis order make consistent training example advantage keeps global picture mind modifying hypothesis however usually expensive backtrack negative training example found case new hypothesis generated must tested previous training examples make sure consistent inconsistencies incur backtracking breadthfirst strategy hand generalizes specific hypotheses general ones initially starts set specific hypotheses positive training examples allow search progress breadthfirst tree generating general hypotheses whereas negative training examples prune corresponding hypothesis search tree boundary search tree therefore represents general hypotheses generated far consistent positive training examples result new general hypothesis generated needs tested positive training examples make sure consistent current hypothesis main advantage breadthfirst search depthfirst search hybrid depthfirst breadthfirst strategies versionspace strat egy version space represents set hypothesis consistent training examples defines two boundaries first boundary obtained depthfirst search bounds acceptable level specialization hypotheses consistent positive examples second boundary obtained breadthfirst search bounds acceptable level generality hypotheses inconsistent negative examples fourth class datadriven generalization strategies decisiontheoretic techniques always use single type search method may use hybrid search methods depthfirst breadthfirst searches depending evaluation results rely loss function measures expected loss learner categorizes learning example incorrectly although loss function may designed either based formal statistical methods heuristically generalization strategy generally shown converge asymptotically desired concept instance genetic algorithms hollands schema theorem 13 shows number structures knowledge base share given subset components expected increase decrease time rate proportional observed performance subset eventually converging asymptotically optimal configuration contrast datadriven techniques explanationbased generalization strategy uses domain knowledge generalize example defining concept contains example 26 analyzes single example terms domain knowledge goal concept produces proof explanation shows example instance goal concept goal concept found satisfies operationality criteria predicate concept definitions specifies form concept must learned proof tree process generalization constructed replacing instantiated rule associated general rule explanationbased strategy start general concepts derive specific ones vice versa consists two phases explanation general ization explanation phase relevant features training example isolated order create explanation structure terminates expression satisfying operationality criterion generalization phase set sufficient conditions found satisfy explanation done regressing goal concept explanation structure composing terms different parts explanation form valid generalization one problems explanationbased learning learning multiple examples may result multiple rules cannot combined single rule leads gradual degradation efficiency generalized rules another problem explanationbased generalization create new parameters hence parameters explicitly expressed proof cannot generalized context studies made generalize structure example proof fixed number rule applications proof generalized unbounded number applications 7 23 credit assignment credit assignment entails apportioning feedback signals individual decisions made past well rulesentities leading decision application credit assignment requires world model captures relationship among states decisions feedback signals generated learning system measured environment world model explicitly defined knowledgerich applications may inferred learning generalization domain knowledge available credit assignment complicated may delays getting feedback signals due decision case multiple subsequent decisions may made times decision made feedback signal received two types credit assignment structural temporal 33 structural credit assignment entails ways using feedback signals refine individual components rules hypothesis process systematic explanationbased learning involves rewriting one rule another proof structure learning approaches credit assignment may possible domain knowledge missing case one may employ populationbased learning 37 maintains population competing hypotheses delays choosing best hypothesis evaluate new ones create tests performed alternatives temporal credit assignment hand entails apportioning temporal global feedback signals learning system past decisions affect signals decision applied temporal scope interval time direct effect observed application environment temporal scope infinite state changes markovian effects due feedback signal attributed recent decision made past effects decisions felt indirectly intervening decisions states temporal scope finite state changes dependent nonmarkovian approximate temporal model needed temporal credit assignment temporal credit assignment used extensively reinforcement learning 33 credit assignment either implicit explicit example implicit credit assignment done smiths ls1 system 32 rules physically close together list representing knowledge structure stand good chance inherited group hand explicit credit assignment explicit rules defined credit assignment examples explicit temporal creditassignment mechanisms profit sharing plan bucket brigade algorithm classifier systems 14 hybrid implicit explicit credit assignment also defined 11 3 generalizability measures evaluate whether goal learning generalization achieved generalizability measures used evaluate quality generalization measures limited field machine learning used performance evaluation many areas instance evaluating speed computer one generally defines reference computer vax 11780 computes speedup computer respect reference collection benchmarks based evaluation results one generalizes speedup benchmarks tested evaluation since different regions problem space application domain may different characteristics may possible evaluate generalization across examples problem space end problem space decomposed smaller partitions generalization evaluated instance evaluating computer one defines speedups different collections benchmarks order reflect performance different applications partitioning problem space define problem subspace userdefined partition problem space conceptshypotheses one subspace evaluated independent conceptshypotheses subspaces partitioning generally guided commonsense knowledge user experience solving similar application problems identify problem sub space need know one attributes classify test cases set decision rules identify subspace test case belongs instance evaluating speedup computer partitioning class applications guided user experience class scientific applications class business applications given subspace test cases define problem subdomain partitioning subspace smaller partitions evaluation con cepthypothesis done quantitatively test cases subdomain partitioning necessary statistical performance metrics computed average maximum meaningful performance values different ranges distributions continue previous example class scientific benchmarks partitioned subdomains according computational behavior whether program cpubound iobound way test cases partitioned subspaces need know attributes classify test cases set decision rules identify subdomain test case belongs may difficult applications available attributes may well defined may large useful instance attribute classify whether benchmark program cpubound iobound imprecise may depend many underlying characteristics program evaluating performance hypothesis subdomain need compare performance across subdomains instance one would interested know whether computer high speedups across cpubound iobound applications comparison may difficult test cases different subdomains subspace may different performance distributions cannot compared statistically address issue section 33 next subsection examine formal results generalizability classification problems one subdomain 31 formal results learnability generalizability formal methods deal generalizability learning one performance measure studied extensively computational learning theory center notion paclearnability 35 concept class c learning algorithm l concept defined subset instance space learner tries learn target concept c finding points x drawn randomly whether belong target concept goal learner produce high probability hypothesis close within ffl target concept assuming learner know underlying distribution sample points following definitions survey paper kearns et al 17 concept c produced learning algorithm l input vector approximately correct error rate p c phi ffl concept class c probability distribution p accuracy parameter ffl confidence parameter ffi probability output c approximately correct least 1 gamma ffi learning algorithm probably approximately correct l said paclearn c learning algorithm l polynomial paclearning algorithm class c l paclearns c time complexity sample complexity polynomial 1 ffl 1 understand bounds estimation learning algorithm need estimate largest number inputspace points almost every possible dichotomy achieved concept class c vcdimension named vapnik chervonenkis 36 addresses issue vc dimension v concept class c size largest set inputspace points every subset u exists concept c 2 c function realized concept c set functions realizable concept sauer 29 notes whenever vc dimension function class finite number dichotomies grows subexponentially actually polynomially number points probability concept learned large estimation error producing correct outputs given set points goes rapidly zero size set increases learning algorithm whose outputs always consistent examples seen far called consistent paclearning algorithm vc dimension concept class finite consistent learning algorithm trained sufficiently large set examples likely learn correct concept blumer et al 4 derived bounds number mffl ffi examples needed consistent algorithm paclearn concept class c vc dimension improved ehrenfeucht et al 10 baum haussler 3 used results relate size neural network accuracy learned concept number examples needed order guarantee particular degree accuracy analysis suggests generalization improved pruning unnecessary hidden units learning reduced architecture vc dimension significantly larger vc dimension optimal number hidden units baum haussler establish following necessary sufficient conditions valid generalization learning neural networks thresholded binary units ffl network n nodes w weights trained least ffl log n examples classifies least correctly almost certainly classify fraction 1 gamma ffl future examples correctly ffl fully connected feedforward network one hidden layer trained fewer examples dichotomy realizable network fail find requisite set weights fraction 1 gamma ffl future examples haussler 12 shows likely feedforward networks sigmoidal units obtain low estimation error number examples must grow linearly number modifiable weights number hidden layers either following desiderata demands larger training sample lowering estimation error b increasing confidence c learning sigmoids higher slope barron 2 shows feedforward network n sigmoidal units input units trained n examples total mean squared error approximation plus estimation true function estimated function bounded nd logn summary theory learnability provides conditions bounds generalization useful certain restricted assumptions met assumptions may difficult ascertain practice difficult characterize set test cases hypotheses precisely condi tions heuristic methods measure generalizability need developed next two subsections present results area 32 anomalies performance normalization general learning problems raw performance results obtained evaluating hypotheses examples may depend size characteristics examples may directly comparable instance table 1 shows cpu times four computers evaluating three benchmarks obviously performance values cannot aggregated directly belong different ranges different distributions aggregate statistically must normalize first following show five different normalization methods average improvement ratio using performance values one hypothesis baseline normalize performance value another hypothesis computing ratio respect baseline tested table 1 summary raw cpu times four computers evaluating three benchmarks computer benchmark table 2 anomalous orderings computers decreasing average normalized speedups using three different normalization methods average improve average symmetric harmonic baseline ment ratio improvement ratio mean example average improvement ratios used aggregate performance measure drawback approach different ordering hypotheses obtained depending baseline hypothesis used illustrate point consider performance data presented table 1 second column table 2 shows three anomalous orderings four computers based average normalized speedups using computer baseline normalization shows generalization based average improvement ratios always lead consistent conclusions b average symmetric improvement ratio normalization method developed avoid anomalies inconsistent orderings two hypotheses due choice baseline hypothesis 39 idea avoid problem improvement ratios put different weights different ranges normalized performance values note degradations original improvement ratio zero one whereas improvements one infinity consequently improvement ratios averaged degradations carry less weight improvements symmetric improvement ratio defined follows si original improvement ratio ith test case symmetric improvement ratio property improvements range 0 infinity degradations range 0 negative infinity two hypotheses reverse role baseline hypotheses symmetric improvement ratios change sign hence symmetric improvement ratios avoid anomalies performance orderings two hypotheses however anomalies performance ordering still present two hypotheses concerned illustrated table 2 shows three different orderings different computers used baseline hence generalization based average symmetric improvement ratios may lead consistent conclusions c harmonic mean performance defined follows illustrated table 2 anomalies orderings still present geometric mean performance defined follows taking logarithm sides log log log log hk 5 bk hk respectively kth performance values baseline hypothesis normalized based 5 alternative way view geometric mean arithmetic mean logarithms raw performance values effect baseline hypothesis average normalized performance reflected first constant term 5 hence baseline changed constant term changed performance ordering affected illustrated example table 1 ordering c 86 c 75 c 76 c 99 unchanged baseline changed average normalized performance respect median performance belongs general class methods normalizes performance values hypotheses test case respect test casespecific constant invariant hypotheses evaluated specific method uses median performance value hypotheses test case baseline normalization unlike using baseline hypothesis may induce different ordering baseline changed median performance invariant respect hypotheses test cases subdomain using normalization method performance distributions test cases center around zero method illustrated example table 1 ordering c 76 c 75 c 99 c 86 computing ordering made simplifying assumption median performance computer three benchmarks median performance computer across possible benchmarks potential problem approach unavailability true median performance value hypotheses test case hence sample median may used instead unfortunately estimated sample medians inaccurate learning hypotheses may tested adequately sample medians sensitive hypotheses tested solutions issue still open time summary anomalies performance normalization exist either baseline fixed case median performance effect changing baseline results changing constant term transformed normalized performance case geometric mean performance cases possible order hypotheses change baseline changed necessary sufficient conditions anomalies happen still open time 33 generalizability measures across subdomains hypotheses tested across different subdomains application performance values even normalization may different ranges different distributions result performance values cannot aggregated statistically hypotheses cannot compared directly generalized across subdomains section present heuristic method evaluate performance across subdomains rangeindependent way assume performance values testing hypothesis subdomain independent identically distributed assumption allows values subdomain aggregated statistical methods averaging following present method uses sample mean statistical estimate population mean address uncertainties using sample means studied concept called probability win 39 pwin compares two sample means computes probability one sample mean larger another similar hypothesis testing take random samples test whether property population likely true false 15 obviously may difficult test hypothesis fully testing entire population test cases testing single random sample four steps general hypothesis testing specify significance level ff b specify testing hypotheses include null hypothesis h 0 alternative hypothesis h 1 c find corresponding acceptance region using lookup tables make decision sample value sample falls acceptance region accept h 0 reject h 1 otherwise reject h 0 accept h 1 probability win measures statistically much better worse sample mean one hypothesis compared another resembles significance level general hypothesis testing two major differences first one hypothesis fh specified without alternative hypothesis contrast hypothesis testing acceptance confidence given advance evaluated based sample values one advantage pwin zero one independent actual performance difference across subdomains hence used compare hypotheses uniform way across subdomains consider performance h subdomain j convenience mulation subscript j ignored following discussion let oe true mean true standard deviation mean normalized performance respect baseline hypothesis h 0 1 n samples taken calculate sample mean sample standard deviation oe central limit theorem n normal distribution function mean standard devia tion let students tdistribution degrees freedom number samples less 30 variance unknown probability hypothesis better h 0 mean value zero 2 p rh pt tgammadistributed dt 7 acceptance region hypothesis note right bound acceptance region random variable depends sample mean sample variance example 1 table 3 illustrates pwin three hypotheses see pwin h 1 increases towards one number samples increases h 1 better h 0 contrast pwin h 2 reduces zero number samples increased h 2 worse h 0 last pwin h 3 reaches maximum value 10 means h 3 definitely better h 0 note pwin considers mean variance hence pwin hypothesis close 05 clear whether hypothesis better worse baseline one normalization methods presented section 32 used 2 average normalized performance h 0 zero appropriate shifting mean value zero performed table 3 examples illustrating pwin changes increasing number samples performance normalized respect h 0 samples given baseline hypothesis h 0 show pwin h subdomain j respect average performance h 0 assuming sample mean ij sample variance ij n ij test cases pwin defined follows pwin cumulative distribution function students tdistribution degrees freedom pwin j probability true performance population mean h subdomain j better h 0 n ij 1 pwin phi standard cumulative normal distribution function 9 important point probabilities win used evaluate whether hypothesis better baseline meant rank order hypotheses hence hypothesis ordered using probabilities win performance normalized method baseline changed anomalies performance ordering may still happen illustrated reference 41 phenomenon happens mean variance baseline important determining ordering hypotheses variance performance values places another degree freedom performance ordering change ordering variance changes consequently ordering may change baseline small variance changed one large variance vice versa true even normalization methods anomalies hypotheses ordered mean values geometric mean short anomalies performance ordering happen hypotheses ranked probabilities win baseline hypothesis fixed case median performance hypotheses used baseline ready define generalizability measure across multiple sub domains different subdomains different statistical behavior performance different subdomains must treated independently cannot combined two assumptions strategies presented ffl assume set subdomains used design process representatives subdomains application subdomains behave statistically similar fashion subdomains used learning generalization ffl assume relative importance one subdomain compared another unknown performance hypotheses subdomains may dependent assumptions cannot aggregate performance values hypotheses across subdomains strategy select hypotheses worstcase performance across subdomains better minimum level objective generalization select hypothesis better incumbent hypothesis problem domain multiple hypotheses procedure attempt maximize likelihood selecting best hypothesis among given set define baseline hypothesis h 0 apply one strategies section 32 normalize performance hypothesis subdomain respect baseline hypothesis consider h better h 0 delta note pw independent subdomain j used generalization true across subdomains even subdomains tested learning following three possible outcomes comparing pw h h 0 h hypothesis better h 0 subdomains h chosen hypothesis generalization multiple hypotheses better h 0 subdomains select one hypothesis maximizes likelihood better h 0 entire domain likelihood degree confidence adjusted increasing delta equivalent placing tighter constraint subdomain hence eliminating potential hypotheses found better h 0 looser constraint c hypothesis better h 0 subdomains since hypothesis superior h 0 h 0 generalizable alternatively possible find hypotheses pw 05 hypotheses less certainty performing better across subdomains however since pw based worstcase pwin across subdomains hypotheses selected way may still perform better baseline subdomains hypotheses considered alternatives h 0 considered far generalization based one performance measure general may multiple performance measures application generalization determines whether hypothesis behaves consistently across subdomains respect performance measures problem belongs general class multiobjective optimization problems solved special forms approach propose constrain one measures optimize unconstrained measure subject constraints 39 constraints approach defined respect performance existing baseline hypothesis similar first normalizing performance respect baseline formulating constraint normalized performance larger one care must taken normalization anomalies performance ordering may happen certain normalization methods used baseline changed probabilities mean used evaluate generalizability various geneticsbased learning generalization experiments 39 15 34 24 41 40 16 38 include learning load balancing strategies distributed systems multicomputers tuning parameters vlsi cell placement routing tuning fitness functions geneticsbased vlsi circuit testing automated design feedforward neural networks design heuristics branchandbound search range estimation stereo vision learning parameters blind equalization signal processing 4 example vlsi placement routing section illustrate use generalizability measures design heuristics timberwolf 30 software package based simulated annealing sa 19 place route various circuit components piece silicon goal package minimize chip area needed satisfying constraints number layers polysilicon routing maximum signal delay path operations divided three steps placement global routing detailed routing placement routing problem nphard hence heuristics generally used sa used timberwolf efficient method randomly search space possible placements although theory sa converges asymptotically global optimum probability one results generated finite time usually suboptimal consequently tradeoff quality result cost computational time obtaining timberwolf version 60 version experimented two parameters control running time indirectly control quality result fastn slown larger fastn shorter time sa run contrast larger slown longer time sa run course one parameters used time timberwolf six major components cost function generate function table 4 parameter set timberwolf version used learning generalization parameter meaning default generalized vertical path weight estimating cost function 10 0958 vertical wire weight estimating cost function 10 0232 p4 range limiter window change ratio 10 130 p5 high temperature finishing point 230 1004 p6 intermediate temperature finishing point 810 6370 p7 low temperature finishing point 1250 12555 final iteration temperature 1550 14799 critical ratio determines acceptance prob 044 0333 p10 temperature controller turn 006 0112 initial temperature temperature decrement equilibrium condition stopping criterion many parameters components well tuned manually last ten years however settings generally heuristic lack domain knowledge set optimally moreover search single parameter set works well across multiple circuits done ad hoc fashion goal show good generalization procedure possible find single parameter set improves cost quality across multiple circuits table 4 lists parameters focused experiments corresponding default values addition package also uses random seed results different performance different seeds used used seven benchmark circuits mostly ftpmcncorg pubbenchmark 21 s298 s420 fract primary1 struct primary2 indus trial1 show generalization works divided circuits two sets first set learning set consisting three smaller circuits s298 s420 primary1 used experiment find generalized parameter set whereas second remaining four circuits testing set used test new parameter set found experiments studied standardcell placement prob lem noting kinds placement studied similar fashion also used fastn values 1 5 10 respectively domain study set performance values possible circuits large small combinations parameters timberwolf domain found different parameter values parameter set defined table 4 lead different costs qualities across different circuits well across different temperature schedules fastn hence cannot group performance values mappings multiple circuits well multiple temperature schedules one subdomain addition cannot subdivide performance values circuit given temperature schedule multiple subdomains variable left timberwolf causing performance changes initial random seed short define subdomain application set performance values quality cost mappings one circuit one temperature schedule since quality cost mapping generated timberwolf depend random seed used need normalize quality cost mapping found using new parameter set given random seed respect mapping found using default parameter set random seed experiments used symmetric improvement ratio 2 normalization method average performance values mappings circuit due different random seeds baseline normalization default parameter set fixed anomalies ordering parameter sets due changing baselines find parameter sets improve default parameter set need method systematically generate new parameter sets method explores space parameter sets systematic fashion ade quate experiments applied teacher 39 learning package based genetic algorithms developed explore space teacher found thirty sets parameter values ten following three sub domains s298 fastn 1 s420 fastn 5 primary1 fastn 10 used fixed sequence ten random seeds subdomain find statistical performance mappings learning experiment involved 1000 applications timberwolf divided ten generations based best sets parameter values applied generalization procedure obtain one generalized parameter set generalized parameter set well default parameter set shown table 4 figure 4 plots quality higher quality yaxis means reduced chip area averaged 10 runs using defined random seeds cost average execution time timberwolf generalized parameter set default parameter set seven circuits fastn 1 5 10 respec tively note performance values figure 4 normalized using 1 respect fastn 10 positive resp negative portion xaxes shows fractional improvement resp degradation computational cost respect baseline parameter set using fastn 10 circuit arrow figure points average performance default parameter set average performance generalized parameter set among 21 subdomains 7 circuits 3 temperature schedules generalized parameter set worse quality default two subdomains worse cost 4 21 subdomains see figure 4 arrows point leftupward direction implying improved quality reduced cost note experiments meant illustrate power generalization procedure expect see improvement learn functions parameters timberwolf improvements timberwolf important system actually used industry normalized symmetric quality normalized symmetric cost default generalized figure 4 comparison normalized average performance default generalized hms plots normalized respect performance applying baseline hm circuit using final remarks paper defined generalization problem summarized various approaches generalization identified credit assignment problem presented solutions measuring generalizability existing formal results measuring generalizability address restricted cases performance measures one common possibly unknown distribution general performance applications may measured multiple metrics test cases may grouped subsets sub domains subset different performance distribution con sequently existing methods cannot used measure generalizability across subdomains test cases presented systematic methods evaluate generalizability within subdomain eliminate dependence size test case subdomain shown various normalization methods normalize performance respect baseline hypothesis methods lead anomalies orderings hypotheses rankordered average normalized measure baseline changed baseline hypothesis fixed like using median performance baseline effect baseline exists constant average normalized measure like using geometric mean anomalies eliminated finally presented methods evaluate generalizability across subdomains introduced concept called probability win measures probability sample mean hypothesis better population mean baseline given number samples tested variance samples probabilities win range zero one used evaluate generalizability across subdomains un fortunately probabilities win cannot used rankordered hypotheses even performance normalized averaged using methods like geometric mean happens probabilities win used evaluate whether hypothesis better baseline meant rank order hypotheses variance performance values places another degree freedom ordering leading different ordering baseline different variance used r handbook artificial intelligence approximation estimation bounds artificial neural networks size net gives valid generalization classifying learnable geometric concepts vapnikchervonenkis dimension classifier systems genetic algorithms encyclopaedia britannica generalizing number learning multiple examples explanation based learning probability statistics engineering sciences general lower bound number examples needed learning credit assignment rule discovery systems based genetic algorithms generalizing pac model sample size bounds metric dimensionbased uniform convergence results adaptation natural artificial systems properties bucket brigade algorithm statistical generalization performancerelated heuristics knowledgelean applications statistical generalization performancerelated heuristics knowledgelean applications recent results boolean concept learning recent results boolean concept learning optimization simulated annealing genetic programming international workshop layout synthesis parallel distributed processing explorations microstructure cognition artificial neural networks concepts theory systematic method automated learning loadbalancing strategies distributed systems generalization search truck backerupper example selflearning neural networks mathematical foundations learning machines density families sets vlsi placement global routing using simulated annealing problem solving rule induction unified view flexible learning problem solving heuristics adaptive search temporal credit assignment reinforcement learning automated learning minimal configuration feed forward neural network theory learnable uniform convergence relative frequencies events probabilities teacher geneticsbased system learning generalizing heuristics generalization heuristics learned genetics based learning generalization learning techniques automating learning heuristics tr