primaldual method largescale image reconstruction emission tomography emission tomography images reconstructed set measured projections using maximum likelihood ml criterion paper present primaldual algorithm largescale threedimensional image reconstruction primaldual method specialized ml reconstruction problem reconstruction problem extremely large several data sets hessian objective function product 14 million 63 million matrix scaled transpose consider approaches suitable largescale parallel computation apply stabilization technique system equations computing primal direction demonstrate need stabilization approximately solving system using earlyterminated conjugate gradient iterationwe demonstrate primaldual method problem converges faster logarithmic barrier method considerably faster expectation maximization algorithm use extrapolation conjunction primaldual method reduces overall computation required achieve convergence b introduction paper consider image reconstruction problem emission tomography problem encountered eld nuclear medicine concerned study organ function radioactively labeled tracer compounds quantity interest problem spatial concentration radioactive emissions within object study quality reconstructed image depend upon number factors including number emission events ie counts collected scanner method used reconstruct image studies characterized poor counting statistics counts statistical reconstruction methods model poisson nature emission process shown improve image quality traditional nonstatistical reconstruction methods 35 57 lowcount problem generated considerable interest medical imaging community low radiotracer doses short scanning durations highly desirable estimation emission density organ inherently threedimensional 3d process volume 3d acquisition improves counting statistics compared 2d acquisition axially oblique coincidences either physically electronically blocked detection increases problem size considerably since 3d problem may involve image measurement vectors millions elements amount computation required perform 3d statistical reconstructions quite substantial computational studies example larger reconstructions consist 14 million image variables reconstructed measurement vector 63 million elements important use reconstruction methods converge rapidly statistical image reconstruction ariela sofer partly supported national science foundation grants dmi9414355 dmi 9800544 center information technology national institutes health bethesda maryland 20892 5624 johnsonmailnihgov z department systems engineering operations research george mason university fairfax virginia 220304444 asofergmuedu ca johnson sofer problem posed constrained nonlinear optimization problem paper present primaldual method performing statistical 3d reconstructions emission tomography specialized intricacies application demonstrate rapid convergence primaldual method computational studies lowcount 3d positron emission tomography pet data paper organized follows section 2 present statistical model develop objective function section 3 reviews em method ml recon struction section 4 develop primaldual method ml reconstruction discuss initialization stabilization extrapolation enhancements computational tests comparing primaldual results logarithmic barrier approach em method small animal data presented section 5 concluding remarks made section 6 2 statistical model objective function begin discussion forming nite parameter space image estimates customary 20 consider situation depicted figure 21 grid boxes voxels imposed emitting object simplicity figure depicted 2d concept readily extended 3d given set measurements along lines coin cidence seek estimate x expected number counts emitted voxel let x number radioactive events emitted voxel assumed independent poissondistributed random variables mean system matrix c 2 nn used model number physical eects including spatially dependent resolution attenuation elements c ij system matrix represent probability event emitted voxel detected detector pair coincidence line number events emitted voxel detected coincidence line j therefore also independent poisson variables measurements j thus realizations sums independent poisson variables considerably simplied model actual measurement process discussion validity present situation see 24 given simplied poisson model likelihood may written e ml objective function formed taking log likelihood log ignoring constant term dene objective function fml x vector 1s q sum columns c need necessarily 1 dening fig 21 relationship estimate x measurement j shown case pet emissioncount measurements taken along coincidence lines pairs detectors nite parameter space formed imposing grid voxels emitting region estimate expected emission intensity within voxel x forward transformation write gradient hessian objective function respectively hessian negative semidenite since objective function 21 concave thus local maximum also global maximum equation 22 sheds insight computational costs associated maximizing objective function given current solution estimate x k computing gradient requires rst computing forward transformation computing backward transformation k forward transformation costs performing forward transformation backward transformation similar together dominate computation associated iterative reconstruction methods especially large scale shall revisit computational structure common iterative reconstruction methods since underlying activity distribution nonnegative ml reconstruction problem constrained optimization problem lowerbound constraints maximize fml x subject x 0 ml objective function nite maximum compact level sets x 0 36 21 maximum posteriori reconstruction without regularity conditions x estimating spatial emission distribution statistically illposed problem 7 33 fully converged ml reconstruction dominated noise edge artifact generally biomedical interest 55 regularization included 4 ca johnson sofer objective function introducing bayesian formulation 20 37 given prior probabilities p fxg p fyg image measurements respectively dene posterior probability estimate x obtained maximizing posterior probability p fxjyg common choice image prior gibbs distribution p although priors eg gaussian gamma investigated 37 39 popularity gibbs priors stems part ability capture local correlation property images 19 energy function r dened sum potential functions designed discourage nonsmoothness neighborhood denotes neighborhood voxel order maintain concavity twice continuous dierentiability objective function potential function v il chosen convex continuous rst second derivatives studies used potential function v il z log z shaping constant typically set 1 38 maximum posteriori map reconstructions objective function logposterior likelihood log p fxjyg ignoring constant objective function become map reconstruction problem also posed constrained optimization problem maximize fmap x subject x 0 note future reference following although function r potential function 25 concave strictly concave vectors v scalar multiple unit vector e n since e negative denite fmap strictly concave 38 addition fmap nite maximum bounded level sets x 0 37 22 optimization problem convenience notation let us pose reconstruction problem constrained minimization problem subject x 0 2 0 case corresponds unregularized ml general shall interested fully converged map karushkuhntucker kkt rstorder necessary conditions optimality 28 point x existence lagrange multipliers x lagrangian function due strict convexity f secondorder suciency conditions satised x unique minimizer f 3 em algorithm expectation maximization em method presented dempster laird rubin 8 ml estimation iterative algorithm computing ml estimates measurements viewed incomplete data shepp vardi 53 lange carson 36 applied em method emission transmission tomography problems respectively em algorithm proven converge optimal solution 24 36 56 em algorithm emission tomography derived 56 27 optimality conditions reconstruction problem unregularized problem written diag x premultiplication x utilizing complementary slackness condition yields since applying xedpoint algorithm x equation yields em update equation x k current image estimate diag given positive initial solution x algorithm maintains nonnegativity every iteration converges xed point x optimal solution 24 asymptotic rate convergence governed spectral radius rm typically close unity one example using reasonable assumptions scanner geometry lower bound spectral radius calculated 99938 17 indeed em observed converge slowly especially close optimal solution slow convergence em algorithm limited clinical applicability cost one em iteration equivalent cost one gradient calculation mapem presence regularizing term 26 precludes closedform update equation 31 mlem mention two algorithms commonly used mapem reconstructions one step late osl algorithm 6 ca johnson sofer depierros algorithm greens osl algorithm approximates r x constant thereby permitting closedform approximated update 16 17 osl converges map solution provided upper threshold prior strength depierros algorithm true mapem implementation substitutes convex function r x separable convex twice continuously dierentiable function r x x k r x separable maximizations performed variables 9 10 regularization improves convergence rate em larger prior strengths resulting lower spectral radii however reasonable prior strengths mild moderate smoothing convergence rates osl depierros algorithm still quite close unity em update formula righthand side 31 follows kaufman 27 rst pose em algorithm optimization algorithm namely scaled steepestascent method representation allows inclusion line search 27 28 accelerate methods performance likewise map update 32 enhanced line search several approaches solving maximum likelihood estimation problem proposed include preconditioned conjugate gradient techniques 27 28 34 42 truncatednewton methods 27 28 nonnegativity constraints maintained either limiting step length using bending line search paper 44 explores active set methods 43 enforces nonnegativity via quadratic penalty objective work 29 30 31 penalized leastsquares objective used instead maximum likelihood problems solved preconditioned conjugate gradient use specialized techniques drive complementary slackness zero considerable debate within pet community regarding appropriate model reconstruction long observed unregularized maximum likelihood estimator gives grainy images however em algorithm stopped early resulting solution often produces images acceptable quality reason researchers argue early termination form smoothing regularization needed proponents map argue approach allows user control amount regularization parameter regularized objective function better conditioned either case observed emtype algorithms may lead nonuniform convergence particular algorithms may converge slowly cold spots regions low activity within regions activity areas isolated activity within cold spots use interiorpoint algorithm oers hope uniform convergence 4 primaldual approach drawbacks em algorithm motivate investigation interiorpoint approaches ml map reconstruction problems clear 21 objective function undened outside feasible region x 0 thus ml map reconstruction problems would appear natural candidates interiorpoint algorithms reconstruction problem especially suited interiorpoint approach output grayscale image whether particular value exactly zero close zero immaterial slight inaccuracies gray scale threshold inconsequential obtaining image rapidly neccessity primaldual methods enjoyed considerable success linear programming 18 32 40 recently proposed nonlinear programming 5 13 41 although closely related logarithmic barrier method primaldual methods may pose advantages logarithmic barrier method lagrange multiplier estimates may inaccurate primal variables close barrier trajectory 11 primaldual methods oer potential improved center ing barrier methods given size current problem developments presented must suitable largescale parallel computation manner similar classical barrier methods primaldual methods attempt follow barrier trajectory smooth trajectory characterized barrier parameter 12 points along trajectory satisfy perturbed version kkt conditions dening ng method maintains 43 attempting solve 41 42 xen en 0 given point x barrier parameter search direction prescribed newtons method satises unsymmetric primaldual equations 41 elimination 12 block matrix 45 yields reduced system condensed primaldual matrix given implemented algorithm primal dual variables permitted take separate steplengths primal steplength x chosen ensure sucient decrease merit function log observe f x simply logarithmic barrier function 8 ca johnson sofer identical righthand side 46 unconstrained minimizer x f x satises perturbed kkt conditions 4143 corresponding multiplier furthermore solution condensed primaldual newton equation 46 guaranteed descent direction merit function 0 since positive denite shall discuss detail computation primal search direction step length formula dual step length follows suggestion conn gould toint cgt 5 lies componentwise interval constant parameter set 100 otherwise nd 0 1 subject k1 interval 49 conditions dual step might appear rst glance overly restrictive actually designed give maxi mum exibility choice k1 cgt use bounds nonsingularity prove xed parameter value minimization f x must successful eventually solution found satises perturbed kkt conditions 4143 general neither necessary desirable reach full subproblem conver gence rather implemented shortstep algorithm one primaldual step usually needed adjusting setting barrier parameter important consideration primaldual algorithms strong uence convergence rate reduction k performed whenever criticality conditions 5 54 satised k1 constant parameters conditions satised barrier parameter reduced according constant parameter consequence 414 k cannot increase furthermore since minimization f x must successful critical solution weaker requirement must eventually found thus impossible k nondecreasing using argument cgt prove algorithm must converge kkt solution 5 practice nd primal dual direction vectors well scaled x typically close 1 far costly operations computing primal direction p x updating gradient rf x shall explore contrast costs line search primal steplength computation dual search direction 47 dual line search 49 relatively insignicant empirical evidence computational studies found shortstep algorithm gradual reduction achieves fastest convergence kkt conditions specically dene parameter values enable critical conditions met one primaldual step subproblems 41 computing primal direction large problems factoring condensed primaldual matrix even forming hessian r 2 f x would prohibitive due size matrix 376000376000 even smaller reconstructions considered paper enormous amount computation would required thus must consider methods approximating newton direction 46 approach successfully applied problem motivated truncatednewton 6 method unconstrained optimization search direction approximate truncated solution newton equations 47 49 earlyterminated conjugate gradient cg iteration 21 used obtain approximate solution 415 equivalent statement 415 seek nd direction p x approximately minimizes quadratic q p x x reasonable eective truncation point 415 based monotonicity q p x proposed 48 cg terminated subiteration l x x cg termination rule 416 important component reconstruction software consistently yields wellscaled primal direction vector long threshold value stabilization required shall discuss case section 42 cg method require storage hessian condensed primaldual matrix rather application matrixvector products 23 write rst term matrixvector product arbitrary vector v 2 n computationally 417 consists forward transformation c v followed diagonal scaling already available computation rf x followed backward transformation premultiplication c explicit recalling 48 ca johnson sofer r 2 r x v computed exactly without incurring signicant computational expense forwardandbacktransformation operation 417 dominates computational cost cg iteration operation computationally similar computing gradient one em iteration authors advocate solving simultaneously p x p using full unsymmetric primaldual equations 45 equivalent symmetrized system 13 14 52 61 unsymmetric primaldual matrix particular remains nonsingular condition number remains bounded 0 12 41 standard conditions constraint qualication strict complementarity secondorder sucient conditions satised solution application due size problem must use iterative method believe solving symmetric system via symmetric solver cg would ecient solving full unsymmetric system via unsymmetric iterative solver gmres even though symmetric system illconditioned since amount work storage required per iteration gmres increases linearly iteration count advantage using condensed system 4647 although primal search direction computed inexactly equation maintaining complementarity 47 maintained practice nd resulting primal dual direction vectors well scaled x typically close 1 411 preconditioning use preconditioner cg essential competitive algorithm since every cg subiteration costly gradient evaluation em iteration highly desirable obtain quality direction vector cg iterations per subproblem possible investigated number preconditioners including fftbased preconditioners model approximately toeplitzblocktoeplitz nature cc circulantblockcirculant approximation 2 3 highpass lter approximations fftbased preconditioner 4 em preconditioner xq 1 34 exact diagonal diagonal hessian approximations 46 preconditioners far bestperforming exact diagonal computed reasonable cost note rst righthand side term 418 similar form backward transformation although bit expensive due squaring operations found preconditioned cg method using exact diagonal preconditioner form 418 almost always requires using fewer 10 iterations achieve 416 regardless size problem many cases 3 4 cg iterations required moreover directions produced using exact diagonal preconditioner well scaled usually resulting primal step sizes near 1 lead rapid descent contrast preconditioners perform well already initial subproblems tended yield poorlyscaled search direction turn resulted small steplengths subsequent calls cg suered problem algorithm made little progress behavior particularly surprising blockcirculant fftbased preconditioners preconditioners perform well reconstruction methods especially leastsquares methods blockcirculant approximation well matched hessian structure motivated try problem ml hessian almost block circulant strong diagonal component spatially variant dependence x shiftinvariant toeplitz models yield poor approximation method 412 line search ml map reconstructions knowledge structure objective function lead substantial reduction cost implementing line search naive approach specically search direction p x found forward transformation computed possible compute objective function rst second directional derivative values trial points x k nearly negligible cost see note therefore 27 28 similar expressions exist directional rst second derivatives 24 initial forward transformation compute forward backtransformation operations required line search trial points forward transformation w reused one backward transformation subsequently required update gradient observations well behaved convex nature objective function permitted us implement highly accurate lowcost newton line search due low cost step chosen relatively strict tolerance 005 wolfe condition termination line search nd line search technique highly eective small part responsible positive results report 42 stabilization well known property hessian primal barrier function increasingly illconditioned nature 0 45 analogous results hold condensed primaldual matrix solution approached matrix becomes increasingly illconditioned detailed analysis see paper wright 60 50 nash sofer developed approximation newton direction logarithmic barrier avoids structural illconditioning barrier hessian suitable largescale problems direction sum two vectors one null space jacobian active constraints orthogonal associated decoupling based prediction binding set solution recently adapted approximation condensed newton equations arising primaldual methods although derivation valid general nonlinear constraints present special case bound constraints context 46 assume following strict complementarity holds solution 0g index set binding constraints solution n number binding constraints solution assume 0 always case reconstructions practical interest dene 0g set nonbinding constraints let x subvector variables positive optimal solution x j subvector variables zero optimal solution assume also 12 ca johnson sofer variables ordered positive variables rst ie x x j hessian objective function similarly partitioned condensed primaldual matrix ij mj j h ij x xj j diagonal matrices associated components x assume sequence iterates x generated primaldual satises following properties sucently small dene exist constants 0 l u l kk u suciently small 0 say vector matrix norm also dene exists positive constant u kk u suciently small 0 also assume near solution hessian reasonably well condi tioned diagonal terms mj j o1 become unbounded 0 contrast diagonal terms ii dier reduced hessian h ii condition ii thus ects constrained problem condensed primaldual matrix shown large eigenvalues magnitude 1 small eigenvalues dier h ii magnitude 1 condensed primaldual matrix thus suers structured illconditioning barrier hessian small values propose approximating primal newton direction p x direction whose null rangespace components computed follows system computing component x involves well conditioned matrix ii solved exactly inexactly via conjugate gradient method computation x straightforward thus illconditioning condensed primaldual avoided show assumptions accuracy approximation increases solution approached potential harm illconditioning increases using well known formula inverse partioned matrix see eg 51 61 follows ii rf ij xj 1 ii rf xj 1 ii denition g note whereas follows 50 nash sofer prove case newton direction arising logarithmic barrier objective function suciently small vector computed using approximation similar 419 420 yields descent direction respect logarithmic barrier objective function proof readily extended present primaldual case thus p x descent direction merit function f x found present problem approximation newton direction useful values order 10 4 less recently wright 60 showed errors generated backwardstable numerical methods various cholesky factorizations gaussian elimination partial pivoting solving 46 magnied structured illconditioning methods inappropriate large problems involve potentially millions variables instead nd approximate solution using cg iteration working inexact arithmetic large numbers variables convergence rate cg method depends condition 15 thus structural illconditioning lead cg iteration spend unnecessary amount work computing observed criterion terminating cg may overly optimistic illconditioned system resulting direction poorly scaled potential eect illconditioning illustrated example table 41 example encountered development motivated incorporation stabilization algorithm starting subproblem primal steplength dual steplength ncg number cg erations listed nonstabilized stabilized cases test terminated xn 75 10 5 note nonstabilized case number cg iterations rst subproblem test termination signicantly lower stabilized test nonstabilized test note also many nonstabilized subproblems either primal dual steplength small indicating poorly scaled direction loss accuracy 14 ca johnson sofer table example eect stabilization number cg iterations ncg counted beginning subproblem termination condition example nonstabilized stabilized 708e5 0392 1000 46 325e5 583e5 1000 0166 51 477e5 1000 62there much recent interest stabilization methods require prediction active set 13 14 59 approaches based factorization methods unsuitable problem large present one argument stabilization methods require prediction set active set unknown interiorpoint methods argue close solution emission tomography reconstruction problem accurate prediction active set made problem constraints simple interpretation positive variables correspond voxels containing least radioactive tracer zerovalued variables correspond voxels lack tracer activity close solution becomes suciently small stabilization appropriate set binding constraints obvious conservatively identied dependent threshold 43 extrapolation fiacco mccormick showed solutions x perturbed kkt solutions form unique dierentiable trajectory 12 perturbed kkt conditions 4143 dene central path 0 thus successful algorithm may able move along toward path discussed 12 subproblem solutions fx l trajectory approximated polynomial lk r c l l r degree approximating polynomial c k r r vectors coecients using approximation 421 nd direction x lk r c l l x set prediction next subproblems primal solution x k computed approximate subproblem solution primal feasibility maintained steplength maximum steplength violate nonnegativity x manner 47 compute dual direction vector according dual vector moved according 4x requires another dual line search minimize 410 resulting point serves starting point 1st subproblem prediction solution k1 extrapolated primaldual method viewed predictorcorrector algorithm extrapolation 422 424 serving predictor step subproblem minimization serving centering cor rector step 23 degree r approximating polynomial 1 predicting 3rd subproblem 2 4th 3 5th beyond experimented line searches conjunction 422 often 1 hence line search yields reason found 424 yields eective dual direction equivalent 47 context extrapolation although extrapolated search direction x often poorly scaled ie 1 observed directions produced always descent directions merit function lead signicant decrease objective function f number reconstructions performed computed extrapolating dual solution vector rather computing via 424 discouraging nature results led us abandon direct extrapolation dual vector favor 424 highly eective comparison following extrapolation gradient evaluation required update vector primaldual algorithm requires 12 25 subproblems perform 3d map reconstruction extrapolation adds many gradient evaluation operations computational cost extrapolation economical reduces computational burden least much adds experience data sets cost extrapolation well worthwhile data sets benets marginal extrapolation thus appears serve somewhat safeguard dicult problems extrapolated primaldual reconstruction convergence measure decrease monotonically primal dual reconstruction without extrapolation certain extrapolated steps seem cause algorithm get ahead eect transient studies weve performed algorithm ultimately converge accurate solution extrapolation 44 initialization choice initial barrier parameter may substantial eect algorithm parameter small rst subproblem may extreme diculty due ill conditioning parameter large many unnecessary subproblems required solve problem proper initialization barrier parameter involves nding suitable point barrier trajectory based initial solution x measurement data recalling perturbed necessary conditions 41 initial solution central path would satisfy ca johnson sofer premultiplying arrive r suggests following rule initialization nd quite eective r another similar initialization rule motivated goal nding initial value 0 426 cannot solved exactly try nd 0 results point close barrier trajectory according say 2norm motivation leads alternative initialization rule 51 course development initialization rules tried certain data sets although initialization rules performed well reconstructions initialized 425 usually reached optimal solution slightly less overall work initialized 427 initial estimate used frequently case positive uniform eld discussion rationale using uniform eld criteria choosing constant value primal initial solution may found 24 alternative choices initial dual vector may preferable investigation question may worthwhile 45 termination given subproblem termination based criti cality conditions 411 412 closeness subproblem solution measured subproblems solved exactly jf 12 criticality conditions however designed shortstep algorithm one truncatednewton step satisfy subproblem suciently small ensure accuracy nal solution nal termination based following two requirements found reasonably accurate solutions ensured traditional view tomographic reconstruction highly accurate solution unnecessary view stems part illposedness problem computational cost taking reconstruction full convergence empirical evidence studies ability perform certain imaging tasks cold spot detectability improves accuracy solution although termination criteria propose may appear particularly strict tomographic reconstruction perspective table properties aecting computation memory storage costs two dierentsized reconstruction problems gradient evaluation costs based 25mcount study 10 120mhz ibm risc6000 sp processors size class n n elements density storage cost c c cost c gradient thickslice 376882 536 thinslice 5 computational studies test algorithm performed number reconstructions data acquired small animal scanner data generated monte carlo simulations animal scanner 51 size problem studies involved two dierentsized problems raw coincidence data scanner binned either thickslice thinslice measurement spaces thickslice reconstructions minutes gradient evaluation using processors 120 mhz 25mcount study thinslice reconstruction data processors gradient evaluation requires 675 minutes properties summarized table 51 cost storing full n n system matrix prohibitive even thickslice reconstructions extensive exploitation sparsity symmetries inherent system matrix makes storage retrieval possible 24 25 dominant computational operations reconstruction problems forward backtransformation operations underlie em iterations gradient evaluations hessianvector products diagonal hessian calculations operations implemented parallel via data decomposition strategy partitions measurementspace vectors across processors image space vectors x replicated processors data decomposition justiable observation n n data set 25m counts 47 elements nonzero thickslice case 4 thinslice case thinslice conguration 10 times many lines response thickslice dominant computational operations implemented way exploit sparsity conserve computation 24 52 cost metrics devised metrics measure cost interior point reconstruction dene number subproblems npr number truncatednewton iterations nit number conjugate gradient subiterations ncg cost one cg iteration dominated hessianvector product equivalent cost one gradient calculation em iteration one truncatednewton iteration requires addition ncg operations one diagonal hessian evaluation plus one forward transformation one backward transformation exact cost operations varies depending size problem number counts shall approximate cost one truncatednewton iteration equivalent two gradient calculations beyond cost conjugate gradients using approximation total cost unextrapolated interiorpoint reconstructions measured units equivalent number gradient calculations ca johnson sofer table summary thickslice primaldual results comparison mapem lsem extrapolation used cases 2 study f npr nit ncg ngr mapem lsem 2465770 19 19 110 148 1000 344 g 3660344 24 24 127 175 1000 724 average ngr 183 table summary thickslice extrapolated primaldual results comparison mapem lsem cases 2 study f npr nit ncg ngr mapem lsem 2465772 17 17 94 145 960 332 f 3296029 g 3660384 20 20 100 160 1000 430 average ngr 156 em iterations extrapolation requires additional gradient calculation following extrapolation order update gradient vector extrapolation modify formula 53 computational results performed number 3d reconstructions data acquired small animal scanner data generated monte carlo simulation small animal scanner reconstructions seven datasets taken full convergence dened termination criteria 428 429 various datasets used computational studies represent fairly diverse sample types scans might encountered practice number counts datasets used studies ranged 850k 51m number binding constraints optimal solution ranged approximately 20 80 main results summarized tables 52 53 nonextrapolated extrapolated primaldual cases respectively studies reconstructions data acquired small animal pet scanner studies e g reconstructions monte carlo simulated data reconstructions performed thickslice mode 376832 variables regularization parameter set tables column mapem indicates number depierro mapem iterations required achieve value f row column lsem indicates number iterations required em algorithm search direction last term 32 enhanced table summary thickslice logarithmic barrier results comparison mapem lsem extrapolation used data sets cases study f npr nit ncg ngr mapem lsem 2465832 5 28 159 218 880 194 average ngr 265 line search avoid excessive computation function values calculated every iterations nal count rounded favor method since cost one gradient evaluation equivalent cost one em iteration numbers columns ngr mapem lsem compared directly nd primaldual method consistently reaches convergence much rapidly either mapem lsem another interesting observation made comparison tables 52 53 consider number em iterations required reach f study c table 52 lsem algorithm reached iterations table 53 data set lsem algorithm reached tions thus algorithm took 64 iterations reduce function value 10 units near solution mapem even worse requiring 180 iterations reduce function value 10 fact typical example slow limit behavior em algorithm studies em method achieve convergence results obtained primaldual method termination lagrangian gradient norm complementary slackness values terminated mapem lsem iterates consistently much higher terminated primaldual solution also performed reconstructions using stabilized logarithmic barrier algorithm based method presented 50 specialized present reconstruction problem many computational features logarithmic barrier implementation identical primaldual implementation eg truncated newton line search computation gradient hessianvector product etc detailed discussion see 24 logarithmic barrier results summarized compared mapem table 54 termination logarithmic barrier dened 429 termination criteria logarithmic barrier correspond roughly accuracy 428 429 primaldual method longstep method logarithmic barrier gives user less control exact stopping point shortstep primaldual logarithmic barrier reconstructions table 54 used extrapolation logarithmic barrier reconstructions reduced factor 10 subproblems eect extrapolation illustrated figures 51 52 figure 51 equivalent number gradient evaluations ngr reach termination plot ca johnson sofer 100 150 200 250 300 350 pd barrier ff ngr fig 51 distance optimal solution termination measured dierence objective function f f f dened lowest objective function obtained per study versus work required reach termination measured ngr equivalent number gradient evaluations studies included listed table 52 pd stands nonextrapolated primaldual pdx extrapolated primaldual ngr pd barrier fig 52 average value subproblem termination versus average ngr equivalent number gradient evaluations seven studies listed table 52 pd stands nonextrapolated prmaldual pdx extrapolated primaldual ted objective function distance f f dierence function value terminated solution lowest function value obtained recon struction seven test cases listed tables 5254 unextrapolated primaldual method achieved lowest objective function value thus f f zero unextrapolated primaldual pd results greater zero extrapolated primaldual pdx barrier results pdx results clustered region lower ngr pd results indicates extrapolation lowers computational expense solution slight deterioration nal objective compared barrier method either extrapolated unextrapolated primaldual produces equivalent better accuracy less computation required figure 52 average number equivalent gradient evaluations subproblem termination plotted average value subproblem averages ngr taken seven test cases tables 5254 compared either unextrapolated primaldual pd extrapolated primaldual study c mapem lsem pd ff ngr fig 53 improvement objective function function gradient evaluations study f pdx denotes extrapolated primaldual pd denotes unextrapolated primaldual using study f mapem lsem pd ff ngr fig 54 improvement objective function function gradient evaluations study c pdx denotes extrapolated primaldual pd denotes unextrapolated primaldual using pdx logarithmic barrier clearly slower trajectory pd pdx trajectories quite similar approximately 001 point pd curve swings pdx curve continues descent loglinearly result conrms prediction extrapolation step becomes accurate near lution resulting rapid convergence however comparison objective functions indicates value pdx perhaps one step ahead compared unextrapolated case progress reconstruction study rat skull study c compared various algorithms figure 53 measure used kf f k plotted logarithmic scale initial iterations depierro map em lsem progress rapidly ahead primal dual method however interiorpoint methods rapidly reach depierro lsem objective values hence surpass primaldual methods depicted value 2 methods achieve faster initial progress using however overall computational eort full convergence parameter setting greater progress reconstruction another example study f compared figure 54 also reconstructed number largescale thinslice reconstructions involving variables table 55 summarizes number properties 22 ca johnson sofer table summary thinslice extrapolated results including convergence measures computational costs optimal solution study f krk f 1e5 7721001 129e9 287e11 14 14 71 113 average ngr 115 extrapolated primaldual reconstructions converged solution smaller group datasets visually interesting studies selected thin slice work certain reconstructions repeated dierent values prior strength thinslice reconstructions seem require lower prior strength corresponding thickslice reconstructions visually pleasing results reconstructions using 130 prior strength generally found satisfactory thickslice reconstructions total amount work measured ngr required reach termination table 55 also quite pleasing number variables thinslice reconstruction approximately 37 times number thickslice number nonzerovalued measurements thinslice mode marginally greater thickslice mode however since number counts cases thinslice reconstructions may thus better conditioned thickslice counterparts closing comment tolerance used tests stricter usually necessary indeed less accurate solutions may still give acceptable images em method applied unregularized ml ob jective usually terminated 50 100 iterations images produced often good thus emml remains practical method sometimes reach solution desirable image quality faster interiorpoint method difculty emml convergence objectdependent 1 convergence areas high activity amidst low activity vice versa notoriously slow xed termination rule based say 50 100 iterations cannot guarantee acceptable image quality observed number reconstructions including high biomedical interest contrast mlem primaldual algorithm objectindependent convergence characteristics furthermore exible adapted solve problem eciently strict tolerance studies setting modest rate decrease barrier parameter say 2 looser tolerance setting aggressive reduction rate 6 conclusion results previous section clear primaldual method converge signicantly faster em algorithm regularized ml reconstructions emission tomography results also indicate primaldual method converges faster logarithmic barrier method use extrapolation conjunction primaldual method reduces amount computation required achieve convergence given negative regularized ml objective function minimize convex approximately solving reduced unsymmetric primaldual newton equa tions appropriate symmetrizing unsymmetric system potentially useful nonconvex problems would case require solving 2n variables without avoiding potential illconditioning stabilization technique avoids structural illconditioning condensed primaldual matrix therefore solving reduced system poses asymptotic diculty barrier parameter approaches zero computational eciency relative simplicity formation reduced system equations pose strong advantage choice primaldual method almost seems obvious problem since newtons method converges quadratically near solution wellconditioned system limit 0 one truncatednewton step per subproblem yield increasingly accurate well scaled direction subproblem solution k decreased subproblem solutions become close convex problem 14 yet example table 41 illustrates direction produced earlyterminated cg fact become less accurate smaller due structured illconditioning practice require accuracy test example table 41 termination conditions dened near point trajectory stabilization approximation becomes accurate enough guarantee descent termination criteria quite accurate standards tomography community thus although reconstruction problems unlikely severely aected illconditioning potential slow convergence near solution due illconditioning exist experience stabilization eective safeguard poor performance small values barrier parameter 7 acknowledgments study utilized highperformance computational capabilities ibm risc6000 sp system division computer research technology national institutes health bethesda md grateful jurgen seidel department nuclear medicine national institutes health kindly providing us small animal data monte carlo simulation data thanks go two anonymous referees associate editor careful reading helpful comments r noise properties em algorithm conjugate gradient methods toeplitz systems general class preconditioners statistical iterative reconstruction emission computed tomography preconditioning methods improved convergence rates iterative reconstructions primaldual algorithm minimizing nonconvex function subject bound linear equality constraints image reconstruction restoration overview common estimation structures problems maximum likelihood incomplete data via em algorithm convergence emtype algorithm penalized likelihood estimation emission tomography numerical stability eciency penalty algorithms sequential unconstrained minimization techniques stability symmetric illconditioned systems arising interior methods constrained optimization matrix computations use em algorithm penalized likelihood estimation generalized em algorithm 3d bayesian reconstruction poisson data using gibbs priors image reconstruction projections fundamentals computerized tomography methods conjugate gradients solving linear systems accelerated image reconstruction using ordered subsets projection data practical interiorpoint method convex programming nonlinear optimization volume system 3d reconstruction retractedsepta pet data using em algorithm evaluation 3d reconstruction algorithms small animal pet camera implementing accelerating em algorithm positron emission tomog raphy pet regularization envelope guided conjugate gradients constrained reconstruction conjugate gradient method primaldual interior point algorithm linear programming probability measure estimation using importance preconditioners fast poissonbased iterative reconstruction algorithms spect practical tradeo em reconstruction algorithms emission transmission tomography theoretical study maximum likelihood algorithms emission transmission tomography convergence em image reconstruction algorithms gibbs smoothing maximum posteriori probability expectation maximization algorithm image reconstruction emission tomography superlinear convergence nonlinear primaldual algorithm statistical modeling fast bayesian reconstruction positron tomog raphy fast gradientbased methods bayesian reconstruction transmission emission pet images bayesian reconstruction pet images methodology performance analysis analytic expressions eigenvalues eigenvectors hessian matrices barrier penalty functions preconditioning truncatednewton methods block truncatednewton methods parallel optimization barrier methods largescale quadratic programming maximum likelihood reconstruction emission tomography infeasible interiorpoint method linear complementarity problems statistical model positron emission tomography noise properties interior methods constrained optimization stability linear equation solvers interior point methods tr