deepest regression method deepest regression dr method linear regression introduced p j rousseeuw hubert 1999 j amer statis assoc 94 388402 dr method defined fit largest regression depth relative data paper show dr robust method breakdown value converges almost surely 13 dimension construct approximate algorithm fast computation dr two dimensions distribution regression depth derive tests true unknown parameters linear regression model moreover construct simultaneous confidence regions based bootstrapped estimates also use maximal regression depth construct test linearity versus convexityconcavity extend regression depth deepest regression general models apply dr polynomial regression show deepest polynomial regression breakdown value 13 finally dr applied michaelismenten model enzyme kinetics resolves longstanding ambiguity b introduction consider dataset z ng ae ir p linear regression want fit hyperplane form denote xpart data point z x residuals z n relative fit denoted r measure quality fit rousseeuw hubert 16 introduced notion regression depth research assistant fwo belgium postdoctoral fellow fwo belgium definition 1 regression depth candidate fit 2 ir p relative dataset z n ae ir p given rdepth z n minimum unit vectors regression depth fit ae ir p relative dataset z n ae ir p thus smallest number observations need passed tilting becomes vertical therefore always 0 rdepth z n n special case xvalues z n univariate dataset 2 ir rdepth z n rank rank outside inwards p 1 regression depth measures balanced dataset z n linear fit determined easily verified regression depth scale invariant regression invariant affine invariant according definitions rousseeuw leroy 17 page 116 based notion regression depth rousseeuw hubert 16 introduced deepest regression estimator dr robust linear regression section 2 give definition dr basic properties show dr robust method breakdown value converges almost surely 13 dimension good data come large semiparametric model section 3 proposes fast approximate algorithm medsweep compute dr higher dimensions p 3 based distribution regression depth function inference parameters derived section 4 tests confidence regions true unknown parameters constructed also propose test linearity versus convexity dataset z n based maximal depth z n applications deepest regression specific models given section 5 first consider polynomial regression update definition regression depth compute deepest regression accordingly show deepest polynomial regression always breakdown value least 13 also apply deepest regression michaelismenten model provides solution problem ambiguous results obtained two commonly used parametrizations 2 definition properties deepest regression definition 2 p dimensions deepest regression estimator drz n defined fit maximal rdepth z n rdepth z n see rousseeuw hubert 16 since regression depth fit increase slightly tilt fit passes p observations passing observations suffices consider fits p data points definition 2 several fits maximal regression depth take average note distributional assumptions made define deepest regression estimator dataset dr regression scale affine equivariant estimator univariate dataset deepest regression median dr thus generalizes univariate median linear regression population case let random pdimensional variable distribution h ir p rdepth h defined smallest amount probability mass needs passed tilting way vertical deepest regression drh fit maximal depth natural setting deepest regression large semiparametric model h functional form parametric error distribution nonparametric formally h consists distributions h ir p satisfy following conditions h strictly positive density exists med h j h note model allows skewed error distributions heteroscedasticity van aelst rousseeuw 22 shown dr fisherconsistent estimator medyjx good data come natural semiparametric model h asymptotic distribution deepest regression obtained portnoy 9 simple regression bai 2 multiple regression figure 1 shows educational spending data obtained dasl library httplibstatcmuedudasl dataset lists expenditures per pupil versus average salary paid teachers regions us fits 1 regression depth 2 deepest regression drz n average salary expenditures drz n 2 figure 1 educational spending data observations dimensions lines depth 2 deepest regression drz n average fits depth 23 017 gamma051 average fits depth 23 figure 1 illustrates lines high regression depth fit data better lines low depth regression depth thus measures quality fit motivates interest deepest regression drz n define finitesample breakdown value n estimator n smallest fraction contamination added dataset z n n explodes see also donoho gasko 6 let us consider actual dataset z n denote z nm dataset formed adding observations z n breakdown value defined znm breakdown value deepest regression always positive low original data peculiar rouseeuw hubert 16 fortunately turns original data drawn model breakdown value converges almost surely 13 dimension p theorem 1 let z sample distribution h ir p p gammagammagamma proofs given appendix theorem 1 says deepest regression break least 67 data generated semiparametric model h remaining data ie 33 points may anything result holds dimension dr thus robust leverage points well vertical outliers moreover theorem 1 illustrates deepest regression different l 1 regression defined l 1 z n j note l 1 another generalization univariate median regression zero breakdown value due vulnerability leverage points simple regression van aelst rousseeuw 22 derived influence function dr elliptical distributions computed corresponding sensitivity functions influence functions dr slope intercept piecewise smooth bounded meaning outlier cannot affect dr much corresponding sensitivity functions show already holds small sample sizes deepest regression also inherits monotone equivariance property univariate median hold l 1 estimators least squares least trimmed squares rousseeuw 14 sestimators rousseeuw yohai 20 definition regression depth depends x signs residuals allows monotone transformations response assume functional model g strictly monotone link function typical examples g include logarithmic exponential square root square reciprocal transformation regression depth nonlinear fit 4 defined 1 r due monotone equivariance deepest regression fit obtained follows first put determine deepest linear regression transformed data backtransform deepest linear regression yielding deepest nonlinear regression fit original data computation dimensions regression depth computed log n time algorithm described rousseeuw hubert 16 compute regression depth fit constructed exact algorithms time complexity pgamma1 log n datasets large n andor p also give approximate algorithm computes regression depth fit omp 3 mn log n time number p gamma 1subsets xspace used algorithm algorithm exact delta subsets considered naive exact algorithm deepest regression computes regression depth observations keeps ones maximal depth yields total time complexity log n slow large n andor high p even use approximate algorithm rousseeuw struyf 18 compute depth fit time complexity remains high simple regresssion collaborative work several specialists computational geometry yielded exact algorithm complexity log 2 n ie little linear time van kreveld et al 24 speed computation higher dimensions construct fast algorithm medsweep approximate deepest regression medsweep algorithm based regression origin regression origin rousseeuw hubert 16 defined regression depth denoted rdepth 0 requiring definition 1 therefore rdepth 0 fit ae ir p relative dataset z n ae ir p1 smallest number observations needs passed tilting way becomes vertical rousseeuw hubert 16 shown special case regression line origin p 1 deepest regression dr dataset z given slope med observations x used estimator minimax bias martin yohai zamar 11 computed time propose sweeping method based estimator 5 approximate deepest regression higher dimensions suppose dataset z ng arrange n observations rows n theta p matrix x ndimensional column vectors step 1 first step construct sweeping variables x start obtain x j 1 successively sweep x original variable x j general sweep x k x l k l compute med med x il med ik j collection indices denominator different zero replace x l sweep next variable x new x l thus obtain sweeping variables step 2 second step successively sweep x put med med med ik j replace original k thus obtain proces 78 iterated convergence reached iteration step coefficients fi updated maximal number iterations set 100 experiments shown even convergence slow 100 iteration steps already close optimum iteration proces take median intercept step 3 backtransforming obtain regression coefficients corresponding original variables obtained fit slightly adjusted passes p observations know improve depth fit start making smallest absolute residual zero directions tilt fit direction passes observation changing sign residual yields fit step 4 last step approximate depth final fit let u directions corresponding variables x compute minimum instead unit vectors right hand side expression 1 since computing median takes time first step algorithm needs op 2 n time second step takes ohpn time h number iterations adjustments step 3 also take op 2 n time computing approximate depth last step done opn log n time time complexity medsweep algorithm thus becomes op log n low measure performance algorithm carried following simulation different values p n generated ng standard gaussian distribution samples computed deepest regression medsweep algorithm measured total time needed 10 000 estimates n p also computed bias intercept average 10 000 intercepts bias vector slopes measure ave ave also give mean squared error vector slopes given true values mean squared error intercept given 1 table 1 lists bias mean squared error vector slopes bias mean squared error intercept given table 2 note bias mean squared error slope vector intercept low decrease increasing n tables 1 2 also see mean squared error seem increase p table 1 bias 9 mean squared error 10 dr slope vector obtained generating standard gaussian samples n p dr fits obtained medsweep algorithm results bias multiplied 10 gamma4 results mse 10 gamma3 50 bias 1801 1827 2853 2247 2142 mse 5455 5232 5223 5254 5865 100 bias 756 841 1116 898 1268 mse 2459 2532 2599 2615 2717 300 bias 811 598 629 739 1089 mse 811 827 826 841 842 500 bias 044 168 291 910 549 mse 493 489 502 493 497 1000 bias 634 358 677 354 398 mse 247 251 246 246 250 table 3 lists average time medsweep algorithm needed computation dr sun sparcstation 20514 see algorithm fast illustrate medsweep algorithm generated 50 points 5 dimensions according model e coming standard gaussian distribution dr fit obtained medsweep 21 algorithm needed 14 iterations till convergence second example generated 50 points according model iterations medsweep algorithm yielded fit approximate depth 21 note cases coefficients obtained algorithm approximate true parameters model well medsweep algorithm available website httpwinwwwuiaacbeustatis use explained table 2 bias mean squared error dr intercepts obtained generating 10 000 standard gaussian samples n p dr fits obtained medsweep algorithm results bias multiplied 10 gamma4 results mse 10 gamma3 50 bias 304 4870 1438 1323 1964 mse 100 bias 575 332 1221 992 170 mse 1578 1601 1692 1677 1814 300 bias 271 799 237 349 454 mse 525 531 522 523 547 500 bias 135 553 433 426 239 mse 309 315 321 321 318 1000 bias 376 340 510 075 001 mse 156 156 155 158 162 inference 41 tests parameters simple regression semiparametric model assumptions condition h state errors e independent p possible compute f n k n fx actual dataset z n invariance properties e iid say standard gaussian thus compute f n k simulating 11 ties among x even compute f n k explicitly formula 44 daniels 1954 yielding table 3 computation time seconds medsweep algorithm sample size n p dimensions time average 10 000 samples 1000 k n gamma 12 f n term probability binomial distribution bn 12 stems number e particular sign increasing n approximate bn 12 gaussian distribution due central limit theorem 12 easily extended large n distribution regression depth allows us test one several regression coefficients test combined null hypothesis corresponding pvalue equals f n k computed 12 consider dataset figure 2 species animals van den bergh 23 plot shows logarithm weight newborn versus logarithm weight mother deepest regression line depth 19 dataset yielding pvalue f 41 highly significant test significance slope easy compute rdepth horizontal lines passing observation animal dataset maximal rdepth0 equals 5 therefore corresponding pvalue p rdepth h 0 rejected pvalue 000002 interpreted way pvalue associated r 2 ftest ls regression analogously test considering lines origin observation yielding pvalue f 41 also highly significant generally test hypothesis h computing maximal regression depth lines pass observation logweight mother logweight newborns dr figure 2 logarithm weight newborn versus logarithm weight mother species animals dr line depth 19 example test hypothesis h animal data ie hypothesis weight newborn proportional weight mother compute corresponding pvalue f 41 significant 5 level 1 level tests generalize easily higher dimensions situations ties among x longer use exact formula 12 restricted bivariate case without ties fx g therefore cases compute f n k simulating 11 let us consider stock return dataset benderly zwick 3 shown figure 3 28 observations dimensions regressors output growth inflation percentages response real return stocks deepest regression obtained medsweep algorithm equals depth 11 test null hypothesis slopes zero would done r 2 ls regression compute maximal rdepth0 0 3 3 ie dataset computing exact rdepth 28 horizontal planes fast algorithm rousseeuw struyf 18 obtain value 6 simulation yields corresponding pvalue f 28 significant test significance intercept compute depth planes two observations ori gin example yields max corresponding pvalue f 28 11 1 significant 42 confidence regions order construct confidence region unknown true parameter vector use bootstrap method starting dataset z ng 2 ir p construct bootstrap sample randomly drawing n observations replacement bootstrap sample z j compute deepest regression note usually outlying estimates set natural since bootstrap samples contain disproportionally many outliers therefore dont construct confidence ellipsoid based classical mean covariance matrix f use robust minimum covariance determinant estimator mcd proposed rousseeuw 1415 mcd looks h n2 observations empirical covariance matrix smallest possible determinant center dataset defined average h points covariance matrix sigma dataset certain multiple covariance matrix obtain confidence ellipsoid level ff compute mcd set bootstrap estimates ffme confidence ellipsoid e 1gammaff given statistic robust distances robust distance rousseeuw leroy 17 bootstrap estimate given confidence ellipsoid e 1gammaff fit space also derive corresponding regression confidence region defined theorem 2 region r 1gammaff equals set let us consider educational spending data figure 1 figure 4a shows deepest regression estimates 1000 bootstrap samples drawn replacement original data using fast mcd algorithm rousseeuw van driessen 19 find center figure 4a 019 gamma095 corresponds well dr fit original data confidence region take 95 tolerance ellipse e 095 based mcd center scatter matrix yields corresponding confidence region r 095 shown figure 4b note intersection confidence region vertical line 95 probability interval observation x 0 interval spanned fitted values 95 confidence region example confidence region higher dimensions shown figure 3 shows 3dimensional stock return dataset deepest regression plane obtained medsweep algorithm 95 confidence region shown figure 3 based samples 43 test linearity simple regression observations bivariate dataset z n lie exactly straight line highest possible value hand z n lies exactly strictly convex strictly concave curve n3 lowest rousseeuw hubert 16 theorem 2 therefore seen measure linearity dataset z n note lower bound depend amount curvature lie exactly curve however soon noise ie nearly always relative sizes error scale curvature come play null hypothesis assumes dataset z n follows linear model independent errors e distribution zero median determine corresponding pvalue generate dr output growth inflation stock return figure 3 stock return dataset deepest regression plane upper lower surface 95 confidence region r 095 based samples table 4 windmill data maximal rdepth k corresponding pvalue pk obtained simulation ng xvalues dataset z n standard gaussian e j compute maximal regression depth dataset z j value k approximate pvalue pmaxrdepth kjh 0 example let us consider windmill dataset hand et al 8 consists 25 measures wind velocity corresponding direct current output shown figure 5 pvalues table 4 obtained 17 actual reject linearity 5 level b average salary expenditures dr r 095 figure 4 dr estimates 1 000 bootstrap samples educational spending data 95 confidence ellipse e 095 b plot data dr line 95 confidence region r 095 fitted value 5 specific models 51 polynomial regression consider dataset z ng ae ir 2 polynomial regression model wants fit data called degree polynomial residuals z n relative fit denoted could consider multiple regression problem regressors determine depth fit definition 1 know joint distribution degenerate ie density many properties deepest regression theorem 1 breakdown value would hold case fact set possible x forms socalled moment curve ir k inherently onedimensional better way define depth polynomial fit denoted rdepth k update definition regression depth simple regression x also univariate polynomial fit compute residuals r define regression depth rdepth k note definition depends x values signs residuals definition depth define deepest polynomial degree k 2 denote dr k z n theorem 3 dataset z ng 2 ir 2 distinct x deepest polynomial regression degree k breakdown value theorem 3 shows definition depth polynomial fit given 18 deepest polynomial regression positive breakdown value approximately 13 robust vertical outliers well leverage points section 43 rejected linearity windmill data therefore consider model quadratic component data ie figure 5 shows windmill data deepest quadratic fit depth 12 wind velocity direct current output figure 5 windmill data deepest quadratic fit regression depth 12 52 michaelismenten model field enzyme kinetics steadystate kinetics great majority enzyme catalyzed reactions studied adequately described hyperbolic relationship concentration substrate steadystate velocity v relationship expressed michaelismenten equation constant v max maximum velocity km michaelis constant michaelismenten equation linearized rewriting following three ways km gammakm known scatchard equation scatchard 21 double reciprocal equation lineweaver burke 10 woolf equation haldane 7 three relations 21 22 23 used estimate constants v max km general three relations yield different estimates constants v max km error terms also transformed nonlinear way cressie keightley 5 compared three linearizations michaelismenten relation context hormonereceptor assays concluded wellbehaved data woolf equation 23 works best data containing outliers double reciprocal equation 22 robust regression gives better results theorem 4 shows applying deepest regression woolf equation 23 yields estimates v max km deepest regression applied double reciprocal equation 22 resolves ambiguity theorem 4 let z ng denote dr fit double reciprocal equation drf 1 dr fit woolf equation satisfies cases obtain v example assays hormone receptors michaelismenten equation describes relationship amount b hormone bound receptor amount f hormone bound receptor assays used eg determine cancer treatment method see cressie keightley 5 equation 20 becomes parameters interest concentration b max binding sites dissociation constant kd hormonereceptor interaction figure 6a shows woolf plot data estrogen receptor assay obtained cressie keightley 4 note dataset clearly contains one outlier plot added deepest regression line figure 6b also show double reciprocal plot deepest regression line drf 1 cases obtain estimated values comparable least squares estimates obtained woolf equation based data except outlier hand least squares applied full data gives based woolf 50 100 150 200 250 300246f dr b dr figure woolf plot cressie keightly data deepest regression line least squares fit double reciprocal plot data deepest regression line least squares equation based double reciprocal equation estimates quite different least squares cases estimates kd highly influenced outlying observation kd come high may lead wrong conclusions eg determining cancer treatment method appendix proof theorem 1 prove theorem 1 need following lemmas lemma 1 x general position f rdepth z n pg bounded proof j ae ng denote fit passes p observations since x general position fit j nonvertical j therefore convf j j ae stands convex hull 2 lemma 2 dataset z n 2 ir p x general position ie x lie p gamma 2dimensional affine subspace ir pgamma1 deepest regression breakdown value proof lemma 1 know f rdepth z n pg bounded therefore break estimator must add least observations rdepthdrz nm z n holds rdepthdrz n z n dnp amenta et al 1 obtain follows ngammap 2 1 finally holds lemma 3 z n ae z nm proof since z n ae z nm holds rdepth z n rdepth z nm thus holds rdepth z n max lemma 4 conditions theorem 1 gammagammagamma proof let us consider dual space ie pdimensional space possible fits dualization transforms hyperplane h point observation z mapped set dz pass z dz hyperplane h given dual space regression depth fit corresponds minimal number hyperplanes h intersected halfline unit vector u dual space thus corresponds affine hyperplane v xspace direction tilt vertical unit vector u therefore define wedgeshaped region u primal space formed tilting around v direction u fit becomes vertical denote h n empirical distribution observed data z n define metric j z n sampled h holds gammagammagamma 0 follows generalization glivenkocantelli theorem formulated pollard 13 theorem 14 proved pollard 13 lemma 18 fact u constructed taking finite number unions intersections halfspaces define empirical version pi n clear 1 moreover jpi n hence gammagammagamma finally holds latter inequality uses theorem 7 rousseeuw hubert 16 valid since z n almost surely general position taking limits 27 using 26 finishes proof 2 proof theorem 1 first show lim inf 1almost surely lemma 1 know f rdepth z n pg bounded z n ae ir p sampled h therefore break estimator must add least observations implies first inequality due lemma 3 thus finally apply lemma 4 conclude lim inf next prove lim sup 1almost surely since regression depth affine invariant may assume none jjx jj dataset zero denote hyperplane equation two components fi ff corresponding slopes intercept fix two strictly positive real numbers fi 0 ff 0 consider point hyperplane passing data points holds 1 assumed none jjx jj equals 0 always find point x would unbounded dataset z nm obtained enlarging dataset z n points equal know deepest fit drz nm must pass least p different observations z nm denote j candidate deepest fit j passes clear rdepth j z nm hand j passes p data points z n rdepth j z nm seen follows first consider dataset z n1 consists n original observations one copy point theorem 7 rousseeuw hubert 16 follows theta n1p always exists unit vector x n number observations passed tilting j around u v definition 1 plus number observations passed tilting j around gammau gammav equals fit j passes exactly observations therefore suppose number observations passed tilting j around u v n1p replace u v gammau gammav note residual r pass first suppose residual r strictly positive data general position therefore always find value 0 holds x number observations passed tilting j around u tilting around u v finally adding replications change value therefore rdepth j z nm residual r strictly negative replace v similar way obtain result reasoning shows deepest fit must pass original data points since shown fits arbitrarily large slope intercept holds gammagammagamma thus lim sup 28 29 finally conclude 3 2 proof theorem 2 consider x 2 ir pgamma1 prove upper lower bounds expression 16 values dual plot hyperplane tangent ellipsoid first consider special case yielding unit sphere tangent hyperplane arbitrary point sphere given therefore hyperplane becomes tangent hyperplane sphere together yields giving lower bound upper bound corresponding expression 16 case consider general case ellipsoid diagonal matrix eigenvalues matrix eigenvectors sigma transform previous case transformation hyperplane transforms cx 1p 12 becomes tangent hyperplane yields lower bound upper bound expression 16 2 proof theorem 3 definition rdepth k 18 follows data set ng distinct x lemma 1 f rdepth k z n bounded bivariate linear fit corresponds polynomial fit holds deepest regression line dr 1 rdepthdr 1 rousseeuw hubert 16 follows deepest polynomial regression dr k degree k rdepth k dr must add least observations rdepth k dr k z nm z n k break estimator obtain follows n gamma 3k2 yields proof theorem 4 show holds every follows v switching xvalues 1 reverses order therefore according definition 1 depths 2 r regression depth center points asymptotic distributions maximal depth estimators regression multivariate location underlying structure direct linear plot application analysis hormonereceptor interactions analysing data hormonereceptor assays breakdown properties location estimates based halfspace depth projected outlyingness graphical methods enzyme chemistry handbook small data sets applied statistical science iii nonparametric statistics related topics determination enzyme dissociation constants depth deep points calculus convergence stochastic processes least median squares regression mathematical statistics applications vol b new york wileyinterscience computing location depth regression depth higher dimensions fast algorithm minimum covariance determinant estimator robust nonlinear time series analysis attractions proteins small molecules ions robustness deepest regression proceedings 15th symposium computational geometry tr robust regression outlier detection efficient algorithms maximum regression depth fast algorithm minimum covariance determinant estimator optimal algorithm hyperplane depth plane robustness deepest regression computing location depth regression depth higher dimensions ctr r wellmann katina ch h mller calculation simplicial depth estimators polynomial regression applications computational statistics data analysis v51 n10 p50255040 june 2007 christine h mller depth estimators tests based likelihood principle application regression journal multivariate analysis v95 n1 p153181 july 2005