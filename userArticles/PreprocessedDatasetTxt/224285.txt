message passing versus distributed shared memory networks workstations message passing programs executed parallel virtual machine pvm library shared memory programs executed using treadmarks programs water barneshut splash benchmark suite 3d fft integer sort embarrassingly parallel ep nas benchmarks ilink widely used genetic linkage analysis program successive overrelaxation sor traveling salesman tsp quicksort qsort two different input data sets used water water288 water1728 issmall islarge sor sorzero sornonzero execution environment set eight hp735 workstations connected 100mbits per second fddi network water1728 ep ilink sorzero sornonzero performance treadmarks within 10of pvm issmall water288 barneshut 3d fft tsp qsort differences order 10to 30 finally islarge pvm performs two times better treadmarks messages data sent treadmarks explaining performance differences extra communication caused 1 separation synchronization data transfer 2 extra messages request updates data invalidate protocol used treadmarks accumulation migratory data treadmarks b introduction parallel computing networks workstations gaining attention recent years workstation clusters use shelf products cheaper supercomputers furthermore high research supported part nsf nyi award ccr9457770 nsf cise postdoctoral fellowship award cda9310073 nsf grants ccr9116343 bir9408503 texas advanced technology program grant 003604012 speed generalpurpose networks powerful workstation processors narrowing performance gap workstation clusters supercomputers processors workstation clusters share physical memory interprocessor communication processors must performed sending messages network currently prevailing programming model parallel computing networks workstations message passing using libraries pvm 9 tcgmsg 11 express 18 message passing standard mpi 17 also developed message passing paradigm distributed nature memory system fully exposed application programmer programmer needs keep mind data decide communicate processors communicate communicate making hard program message passing especially applications complex data structures software distributed shared memory dsm systems eg 3 5 14 16 provide shared memory abstraction top native message passing facilities application written executing shared memory multiprocessor accessing shared data ordinary read write operations chore message passing left underlying dsm system easier program way dsm systems tend generate communication therefore tend less efficient message passing systems message passing paradigm communication handled entirely programmer complete knowledge data usage pattern contrast dsm system little knowledge application program therefore must conservative determining communicate since sending messages workstations expensive extra communication cause serious performance degradation much work done past decade improve performance dsm systems paper compare stateoftheart dsm system treadmarks commonly used message passing system pvm goals assess differences programmability performance dsm message passing systems precisely determine remaining causes lower performance systems ported nine parallel programs treadmarks pvm water barneshut splash benchmark suite 20 3d fft integer sort embarrassingly parallel ep nas benchmarks 2 ilink widely used genetic linkage analysis program 8 successive overrelaxation sor traveling salesman problem tsp quicksort qsort two different input sets used water water288 water1728 issmall islarge sor sorzero sornonzero ran programs eight hp735 workstations connected 100mbits per second fddi network terms programmability since test programs simple difficult port pvm however two programs namely 3d fft ilink message passing versions significantly harder develop dsm versions water1728 ep ilink sorzero sornonzero performance treadmarks within 10 pvm issmall water288 barneshut 3d fft tsp qsort differences order 10 30 finally islarge pvm performs two times better treadmarks messages data sent treadmarks explaining performance differences extra communication caused 1 separation synchronization data transfer 2 extra messages request updates data invalidate protocol used treadmarks accumulation migratory data treadmarks currently trying address deficiencies integration compiler support treadmarks rest paper organized follows section 2 introduce user interfaces implementations pvm treadmarks section 3 presents application programs results section 4 concludes paper 21 pvm pvm 9 standing parallel virtual machine message passing system originally developed oak ridge national laboratory although message passing systems tcgmsg 11 provide higher bandwidth pvm chose pvm popularity use pvm version 326 experiments 211 pvm interface pvm user data must packed send buffer dispatched received message first stored receive buffer must unpacked application data structure application program calls different routines pack unpack data different types routines syntax specifies beginning user data structure total number data items packed unpacked stride unpack calls match corresponding pack calls type number items pvm provides user nonblocking sends including primitives send message single destination multicast multiple destinations broadcast destinations send dispatches contents send buffer destination returns immediately blocking nonblocking receives provided pvm receive provides receive buffer incoming message blocking receive waits expected message arrived time returns pointer receive buffer nonblocking receive returns immediately expected message present returns pointer receive buffer blocking receive otherwise nonblocking receive returns null pointer nonblocking receive called multiple times check presence message performing work calls useful work blocking receive called message 212 pvm implementation pvm consists two parts daemon process host set library routines daemons connect using udp user process connects local daemon using tcp usual way two user processes different hosts communicate via local daemons however set direct tcp connection order reduce overhead use direct connection user processors experiments results better performance pvm designed work set heterogeneous machines provides conversion external data representation xdr conversion avoided machines used identical 22 treadmarks treadmarks 14 software dsm system built rice university efficient userlevel dsm system runs commonly available unix systems use treadmarks version 094 experiments 221 treadmarks interface treadmarks provides primitives similar used hardware shared memory machines application processes synchronize via two primitives barriers mutex locks routine tmk barrieri stalls calling process processes system arrived barrier barrier indices integers certain range locks used control access critical sections routine tmk lock acquirei acquires lock calling processor routine tmk lock releasei releases processor acquire lock another processor holding integer lock index assigned pro grammer shared memory must allocated dynamically calling tmk malloc tmk sbrk syntax conventional memory allocation calls treadmarks imperative use explicit synchronization data moved processor processor response synchronization calls see section 222 222 treadmarks implementation treadmarks uses lazy invalidate 14 version release consistency rc 10 multiplewriter protocol 5 reduce amount communication involved implementing shared memory abstraction virtual memory hardware used detect accesses shared memory rc relaxed memory consistency model rc ordinary shared memory accesses distinguished synchronization accesses latter category divided acquire release accesses rc requires ordinary shared memory updates processor p become visible another processor q subsequent release p becomes visible q via chain synchronization events practice model allows processor buffer multiple writes shared data local memory synchronization point reached treadmarks tmk lock acquirei modeled acquire tmk lock releasei modeled release tmk barrieri modeled release followed acquire processor performs release barrier arrival acquire barrier departure multiplewriter protocol two processors simultaneously modify copy shared page modifications merged next synchronization operation accordance definition rc thereby reducing effect false sharing merge accomplished use diffs diff runlength encoding modifications made page generated comparing page copy saved prior modifications treadmarks implements lazy invalidate version rc 13 lazy implementation delays propagation consistency information time acquire furthermore releaser notifies acquirer pages modified causing acquirer invalidate local copies pages processor incurs page fault first access invalidated page gets diffs page previous releasers implement lazy rc execution processor divided intervals new interval begins every time processor synchronizes intervals different processors partially ordered intervals single processor totally ordered program order ii interval processor p precedes interval processor q interval q begins acquire corresponding release concluded interval p iii interval precedes another interval transitive closure partial order known hb1 1 vector timestamps used represent partial order processor executes acquire sends current timestamp acquire message previous releaser piggybacks response set write notices timestamps greater timestamp acquire message write notices describe shared memory modifications precede acquire according partial order acquiring processor invalidates pages incoming write notices access fault page brought uptodate fetching missing diffs applying page increasing timestamp order write notices without corresponding diffs examined usually unnecessary send diff requests processors modified page processor modified page interval must diffs intervals precede including processors treadmarks sends diff requests subset processors recent interval preceded recent interval another processor lock statically assigned manager manager records processor recently requested lock lock acquire requests directed manager necessary forwarded processor last requested lock lock release cause communication barriers centralized manager number messages sent barrier 2 theta n gamma 1 n number processors 31 experimental testbed testbed used evaluate two systems 8node cluster hp9000735125 workstations 125mhz parisc7100 processor 96 megabytes main memory machines 4096byte page size connected 100mbps fddi ring treadmarks user processes communicate using udp pvm processes set direct tcp connections since machines identical data conversion external data representation disabled udp tcp built top ip udp connectionless tcp connection oriented tcp reliable protocol udp ensure reliable delivery treadmarks uses lightweight operationspecific userlevel protocols top udp ensure reliable delivery 32 overview ported nine parallel programs treadmarks pvm water barneshut splash benchmark suite 20 3d fft ep nas benchmarks 2 ilink widely used genetic linkage analysis program 8 sor tsp qsort ran three nine programs using two different input sets water 288 1728 molecules bucket size 2 10 2 15 sor internal elements matrix initialized either zero nonzero values execution times sequential programs without calls pvm treadmarks shown table 1 table also shows problem sizes used application figures 1 12 show speedup curves applications speedup computed relative sequential program execution times given table 1 amount data number messages sent 8processor execution shown table 2 pvm versions counted number userlevel messages amount user data sent run treadmarks counted total number udp messages total amount data communicated program problem size timesec 28 2391 sorzero 1024 theta 3072 50 iterations 79 sornonzero 1024 theta 3072 50 iterations 68 issmall iterations 38 islarge iterations 42 tsp 19 cities 95 qsort 512k integers 53 water288 288 molecules 5 iterations 12 water1728 1728 molecules 5 iterations 435 barneshut 4096 bodies 17 3d fft 64 theta 64 theta 64 6 iterations 41 table 1 sequential time applications program treadmarks pvm messages kilobytes messages kilobytes sorzero 7014 509 1414 8607 sornonzero 7014 8853 1414 8607 issmall 872 2582 140 573 islarge 9918 73591 140 18350 qsort 33742 57083 3129 16000 water288 5028 4945 620 1520 water1728 8511 21170 620 9123 barneshut 62350 18277 280 12704 table messages data 8 processors treadmarks pvm nprocs speedups2468 figure treadmarks pvm nprocs speedups2468 figure treadmarks pvm nprocs speedups2468 figure treadmarks pvm nprocs speedups2468 figure treadmarks pvm nprocs speedups2468 figure 5 islarge treadmarks pvm nprocs speedups2468 figure treadmarks pvm nprocs speedups2468 figure treadmarks pvm nprocs speedups2468 figure treadmarks pvm nprocs speedups2468 figure 9 water1728 treadmarks pvm nprocs speedups2468 figure treadmarks pvm nprocs speedups2468 figure treadmarks pvm nprocs speedups2468 figure 33 ep embarrassingly parallel program comes nas benchmark suite 2 ep generates pairs gaussian random deviates tabulates number pairs successive square annuli parallel version communication summing teninteger list end program treadmarks updates shared list protected lock pvm processor 0 receives lists processor sums test solved class problem nas benchmarks 2 28 pairs random numbers generated results shown figure 1 sequential program runs 2391 seconds treadmarks pvm achieve speedup 79 using 8 processors compared overall execution time communication overhead negligible 34 redblack sor redblack successive overrelaxation sor method solving partial differential equations parallel version program divides red black array roughly equal size bands rows assigning band different processor communication occurs across boundary rows bands treadmarks version arrays allocated shared memory processors synchronize using barriers pvm processor explicitly sends boundary rows neighbors ran redblack sor 1024 theta 3072 matrix floating point numbers 51 iterations problem size shared red black row occupies one half pages first iteration excluded measurement eliminate differences due fact data initialized distributed manner pvm version treadmarks done master process first test sorzero edge elements initialized 1 elements 0 second test sornonzero elements matrix initialized nonzero values change values iteration results sorzero shown figure 2 sequential program runs 79 seconds 8 processors treadmarks version pvm version achieve speedups 458 471 respectively treadmarks speedup 97 pvm due load imbalance neither pvm treadmarks achieves good speedup load imbalance occurs floatingpoint computations involving zeros take longer involving nonzeros causing processors working middle parts array take longer iterations results sornonzero shown figure 3 initial values nonzero single processor time drops 79 seconds 68 seconds 8 processors speedup obtained treadmarks 680 91 pvm speedup 744 compared first test improved speedup due better load balance treadmarks pvm performance relatively close low communication rate sor use lazy release consistency treadmarks although processor repeatedly writes boundary pages two barriers diffs boundary pages sent barrier response diff requests neighbors number messages 5 times higher treadmarks pvm n processors pvm sends 2 theta n gamma 1 messages end iteration treadmarks sends 2 theta n gamma 1 messages implement barrier 8 theta n gamma 1 messages page diffs boundary rows boundary row requires two diffs one page behavior exemplifies two performance drawbacks treadmarks relative pvm separation synchronization data transfer multiple diff requests due invalidate protocol result diffing treadmarks much less data sent sorzero treadmarks pvm pages remain zero integer sort 2 nas benchmarks requires ranking unsorted sequence keys using bucket sort parallel version divides keys among processors first processor counts writes result private array buckets values private buckets summed finally processors read sum rank keys treadmarks version shared array buckets processor also private array buckets counting keys processor locks shared array buckets adds values private array shared array releases lock waits barrier processors finished updates processor reads final result shared array buckets ranks keys pvm version processor bucket array private memory counting keys processors form chain processor 0 sends local array buckets processor 1 processor 1 adds values local array buckets values array buckets receives forwards result next processor etc last processor chain calculates final result broadcasts tested two sets input data first test issmall sorted 2 23 keys ranging 0 2 iterations second test islarge keys range 0 2 15 try 2 23 keys 2 20 key range suggested nas benchmarks extremely low computationcommunication ratio suitable workstation clusters speedups shown figures 4 5 sequential execution time issmall 38 seconds 8 processor speedups pvm treadmarks 760 656 respectively islarge sequential program runs 42 seconds pvm treadmarks achieve speedups 467 230 respectively issmall treadmarks version sends 4 times data 5 times messages pvm version extra messages due separate synchronization diff requests 782 messages sent treadmarks compared 140 pvm 500 synchronization messages 150 diff requests islarge treadmarks sends 70 times messages pvm shared bucket array islarge contains 2 15 integers spread pages therefore time shared bucket array accessed treadmarks sends 32 diff requests responses pvm handles transmission shared array single message exchange extra data treadmarks comes phenomenon call diff accumulation time processor acquires lock modify shared array buckets previous values array completely overwritten current treadmarks implementation however preceding diffs sent lock acquired even though completely overlap phenomenon occurs barrier every processor reads final values shared bucket time processor gets diffs made processors modified shared bucket array iteration assuming bucket size b number processors n pvm amount data sent iteration 2 theta n gamma 1 theta b amount data sent treadmarks n theta n gamma 1 theta b although diffs obtained one processor diff accumulation also results messages sum diff sizes exceeds maximum size udp message since treadmarks mtu 48 kilobytes extra messages due diff accumulation serious problem 36 tsp tsp solves traveling salesman problem using branch bound algorithm major data structures pool partially evaluated tours priority queue containing pointers tours pool stack pointers unused tour elements pool current shortest path evaluation partial tour composed mainly two procedures get tour recursive solve subroutine get tour removes promising path priority queue path contains threshold number cities get tour returns path otherwise extends path one node puts promising paths generated extension back priority queue calls recursively subroutine get tour returns either promising path longer threshold priority queue becomes empty procedure recursive solve takes path returned get tour tries permutations remaining nodes recursively updates shortest tour complete tour found shorter current best tour treadmarks version major data structures shared subroutine get tour guarded lock guarantee exclusive access tour pool priority queue tour stack updates shortest path also protected lock pvm version uses masterslave arrangement n processors n slave processes 1 master process words one processor runs master one slave process remaining processors run slave process master keeps major data structures private memory executes get tour keeps track optimal solution slaves execute recursive solve send messages master either request solvable tours update shortest path solved 19 city problem recursive solve threshold 12 speedups shown figure 6 sequential program runs 95 seconds 8 processors treadmarks obtains speedup 621 78 speedup 797 obtained pvm 8 processors treadmarks sends 11 times messages 150 times data pvm performance gap comes difference programming styles pvm version tsp tours directly solvable recursive solve minimum tour exchanged slaves master message exchanges take 2 messages contrast treadmarks major data structures migrate among processors get tour takes least 3 page faults obtain tour pool priority queue tour stack amount data diff accumulation average processor gets n gamma 1 diffs page fault n number processors system furthermore contention lock protecting get tour average 8 processors process spends 11 15 seconds waiting lock acquires 37 qsort qsort quicksort algorithm used partition unsorted list sublists sublist sufficiently small integers sorted using bubblesort qsort parallelized using work queue contains descriptions unsorted sublists worker threads continuously remove lists treadmarks version qsort list work queue shared accesses work queue protected lock unlike tsp qsort processor releases task queue without subdividing subarray removes queue subarray divided processor reacquires control task queue places newly generated subarrays back task queue pvm version uses masterslave arrangement similar tsp n slaves 1 master n number processors master maintains work queue slaves perform partitioning sorting experiments array size 512k bubblesort threshold 1024 speedups shown figure 7 sequential program runs 53 seconds 8processor speedups using treadmarks pvm 446 617 respectively several factors contribute 28 difference performance important reason diff requests treadmarks 8 processors treadmarks sends 10 times messages pvm among 33742 messages sent treadmarks 32000 messages sent due diff requests diff transmission since bubblesort threshold 1024 intermediate subarrays larger one page resulting multiple diff requests subarray addition false sharing diff accumulation also occurs case intermediate subarrays work queue migrate among processes 38 water water splash 20 benchmark suite molecular dynamics simulation main data structure water onedimensional array records record represents molecule contains molecules center mass atoms computed forces displacements first six derivatives time step intra intermolecular potentials computed avoid computing n 2 2 pairwise interactions among molecules spherical cutoff range applied parallel algorithm statically divides array molecules equal contiguous chunks assigning chunk processor bulk interprocessor communication happens force computation phase processor computes updates intermolecular force molecules n2 molecules following array wraparound fashion treadmarks version water program original splash suite tuned get better performance center mass displacements forces molecules allocated shared memory variables molecule record allocated private memory lock associated processor addition processor maintains private copy forces force computation phase changes forces accumulated locally order reduce communication shared forces updated processors finished phase processor updated private copy forces molecules belonging processor j acquires lock j adds contributions forces molecules owned processor j pvm version processors exchange displacements force computation communication occurs pairwise intermolecular forces computed time processors communicate locally accumulated modifications forces used two data set sizes 288 molecules 1728 molecules ran 5 time steps results shown figures 8 9 sequential execution time 288molecule simulation 12 seconds 8processor speedups treadmarks pvm 504 723 respectively 1728 molecules sequential program runs 435 seconds treadmarks pvm achieve speedups 725 744 8 processors respectively low computationcommunication ratio separation synchronization data communication false sharing major reasons 30 gap performance 288 molecules pvm two userlevel messages sent pair processors interact one message read displacements message write forces treadmarks extra messages sent synchronization diff requests read displacements write shared forces barrier terminates phase shared forces updated processor may fault reading final force values molecules last processor update values false sharing false sharing causes processor bring updates molecules access may result communication one processor molecules page updated two different processors 8 processors false sharing occurs 7 118 pages molecule array consequently treadmarks sends 5028 messages compared 620 messages pvm false sharing also causes treadmarks version send unnecessary data another cause additional data sent treadmarks diff accumulation assuming n processors n even molecules belonging processor modified n2 processors protected lock average processor gets n2 diffs since molecules modified processor case diffs completely overlapping adding false sharing diff accumulation 8 processors treadmarks sends 23 times data pvm increased computationcommunication ratio reduced false sharing cause treadmarks perform significantly better 1728 molecules treadmarks sends 13 times data pvm compared 23 times 288 molecules 39 barneshut barneshut splash 20 benchmark suite nbody simulation using hierarchical barneshut method treestructured hierarchical representation physical space used leaf tree represents body internal node tree represents cell collection bodies close physical proximity major data structures two arrays one representing bodies representing cells sequential algorithm loops bodies body traverses tree compute forces acting parallel code four major phases time step construct barneshut tree 2 get bodies partition bodies among processors 3 force computation compute forces bodies 4 update update positions velocities bodies phase 1 executed sequentially running parallel slows execution phase 2 dynamic load balance achieved using costzone method processor walks barneshut tree collects set logically consecutive leaves computation time spent phase 3 treadmarks version array bodies shared cells private maketree processor reads shared values bodies builds internal nodes tree private memory barriers maketree force computation update phases synchronization necessary force computation phase barrier end force computation phase ensures processors finished reading positions processors pvm version every processor broadcasts bodies end iteration processor obtains bodies creates complete tree phase 1 communication required ran barneshut 4096 bodies 6 timesteps last 5 iterations timed order exclude cold start effects figure 10 shows speedups sequential program runs 17 seconds 8 processors pvm treadmarks achieve speedups 304 270 respectively low computation communication ratio need finegrained communication 19 contribute poor speedups treadmarks pvm pvm network saturated 8 processors every processor tries broadcast time diff requests false sharing major reasons treadmarks lower performance 8 processors treadmarks sends 44 data 222 times messages pvm although set bodies owned processor adjacent barneshut tree adjacent memory false sharing maketree page fault causes processor send diff requests several processors reason force computation processor may fault accessing bodies bring unwanted data 310 3d fft 3d fft nas 2 benchmark suite numerically solves partial differential equation using three dimensional forward inverse ffts assume input array n 1 theta n 2 theta n 3 organized rowmajor order 3d fft first performs n 3 point 1d fft n 1 theta n 2 complex vectors performs n 2 point 1d fft n 1 theta n 3 vectors next resulting array transposed n 1 point 1d fft applied n 2 theta n 3 complex vectors distribute computation array elements along first dimension elements complex matrix ijk assigned single processor communication needed first two phases n 3 point ffts n 2 point ffts computed single processor processors communicate transpose processor accesses different set elements afterwards treadmarks version barrier called transpose pvm version messages sent explicitly send messages must figure part array goes part b array needs come index calculations 3dimensional array much errorprone simply swapping indices treadmarks making pvm version harder write results obtained running 64 theta 64 theta 64 array double precision complex numbers 6 iterations excluding time distributing initial values beginning program matrix size 132 specified class problem nas benchmarks scaled problem limited swap space machines available us speedup curves shown figure 11 sequential execution time 41 seconds speedup 441 obtained treadmarks 8 processors 81 speedup 547 obtained pvm release consistency treadmarks sends almost amount data pvm except 6processor execution however pagebased invalidate protocol many messages sent treadmarks pvm 8 processors 4 megabytes data communicated transpose page size 4096 bytes transpose therefore requires 1000 diff requests responses anomaly occurs 6 processors attribute false sharing matrix size multiple 6 page modified one processor read two processors although two processors read disjoint parts page diff sent result treadmarks 14 messages 17 data sent 6 processors 8 processors 311 ilink ilink 7 15 widely used genetic linkage analysis program locates specific disease genes chro mosomes input ilink consists several family trees program traverses family trees visits nuclear family main data structure ilink pool genarrays genarray contains probability genotype individual since genarray sparse index array pointers nonzero values genarray associated one bank genarrays large enough accommodate biggest nuclear family allocated beginning program bank reused nuclear family computation moves new nuclear family pool genarrays reinitialized person current family computation either updates parents genarray conditioned spouse children updates one child conditioned parents siblings use parallel algorithm described dwarkadas et al 8 updates individuals genarray parallelized master processor assigns nonzero elements parents genarray processors round robin fashion processor worked share nonzero values updated genarray accordingly master processor sums contributions processors treadmarks version bank genarrays shared among processors barriers used synchronization pvm version processor local copy genarray messages passed explicitly master slaves beginning end nuclear family update since genarray sparse nonzero elements sent diffing mechanism treadmarks automatically achieves effect since nonzero elements modified nuclear family update diffs transmitted master contain nonzero elements used clp data set 12 allele product 2 theta 4 theta 4 theta 4 results shown figure 12 sequential program runs 1473 seconds 8 processors treadmarks achieves speedup 557 93 599 obtained pvm high computationtocommunication ratio leads good speedups also explains fact pvm treadmarks close performance however able identify three reasons lower performance treadmarks first versions send nonzero elements pvm performs transmission single message treadmarks sends diff request response page genarray clp data set size genarray pages second false sharing occurs treadmarks nonzero values parents genarrays assigned processors round robin fashion pvm parents genarrays distributed processor gets part genarray treadmarks processor gets nonzero elements page including belonging processors third final reason difference performance diff accumulation bank genarrays reinitialized beginning computation nuclear family although processors need newly initialized data treadmarks also sends diffs created previous computations 312 summary experience pvm treadmarks conclude easier program using treadmarks using pvm although little difference programmability simple programs programs complicated communication patterns ilink 3d fft takes lot effort figure send send results show use release consistency multiplewriter protocol treadmarks performs comparably pvm variety problems experimental environment examined results corroborated 4 performed similar experiment comparing munin dsm system message passing v system 6 five twelve experiments treadmarks performed within 10 pvm remaining experiments barneshut lesser extent islarge exhibit poor performance pvm treadmarks data sets used applications low computationtocommunication ratio network workstations remaining five experiments performance differences 10 30 separation synchronization data transfer requestresponse nature data communication treadmarks responsible lower performance treadmarks programs pvm data communication synchronization integrated together send receive operations exchange data also regulate progress processors treadmarks synchronization locksbarriers communicate data moreover data movement triggered expensive page faults diff request sent order get modifications addition pvm benefits ability aggregate scattered data single message access pattern would result several miss messages invalidatebased treadmarks protocol although multiplewriter protocol addresses problem simultaneous writes page false sharing still affects performance treadmarks multiple processors may write disjoint parts page without interfering processor reads data written one writers synchronization point diff requests sent writers causing extra messages data sent current implementation treadmarks diff accumulation occurs result several processors modifying data common pattern migratory data diff accumulation serious problem diff sizes small several diffs sent one message conclusions paper presents two contributions first results show large variety programs performance well optimized dsm system comparable message passing system especially problems practical size ilink water simulation 1728 molecules treadmarks performs within 10 pvm terms programmability experience indicates easier program using treadmarks using pvm although little difference programmability simple programs programs complicated communication patterns ilink 3d fft lot effort required determine data send send data second observe four main causes lower performance treadmarks compared pvm separation synchronization data transfer treadmarks additional messages send diff requests invalidatebased treadmarks protocol false sharing finally diff accumulation migratory data currently integrating compiler analysis dsm runtime system alleviate problems compiler determine future data accesses prefetching reduce cost page faults diff requests furthermore cases data movement piggybacked synchronization messages overcoming separation synchronization data movement addition compiler place data way minimize false sharing overhead acknowledgments would like thank rand hoven hewlettpackard stephen poole performance group giving us access workstation clusters used experiments described would also like thank nenad nedelkovic edmar wienskoski participated early version project r weak ordering new definition nas parallel benchmarks shared memory parallel programming entry consistency distributed memory multiprocessors techniques reducing consistencyrelated information distributed shared memory systems implementation performance munin distributed v kernel performance diskless work stations memory consistency event ordering scalable sharedmemory multiprocessors portable tools applications parallel computers lazy release consistency software distributed shared memory treadmarks distributed shared memory standard workstations operating systems strategies multilocus linkage analysis humans memory coherence shared virtual memory systems message passing interface forum express users guide implications hierarchical nbody methods multiprocessor architectures splash stanford parallel applications sharedmemory tr memory coherence shared virtual memory systems implementation performance munin networkbased concurrent computing pvm system lazy release consistency software distributed shared memory implications hierarchical nbody methods multiprocessor architectures techniques reducing consistencyrelated communication distributed sharedmemory systems weak orderingmyampersandmdasha new definition memory consistency event ordering scalable sharedmemory multiprocessors distributed v kernel performance diskless workstations ctr mordechai geva yair wiseman common framework interprocess communication cluster acm sigops operating systems review v38 n4 p3344 october 2004 michael kistler lorenzo alvisi improving performance software distributed shared memory speculation ieee transactions parallel distributed systems v16 n9 p885896 september 2005 yichang zhuang tyng yue liang cekuen shieh junqi lee laurence tianruo yang groupbased load balance scheme software distributed shared memory systems journal supercomputing v28 n3 p295309 june 2004 jerry clarke emulating shared memory simplify distributedmemory programming ieee computational science engineering v4 n1 p5562 january 1997 fengguang song shirley moore jack dongarra feedbackdirected thread scheduling memory considerations proceedings 16th international symposium high performance distributed computing june 2529 2007 monterey california usa mark w macbeth keith mcguigan philip j hatcher executing java threads parallel distributedmemory environment proceedings 1998 conference centre advanced studies collaborative research p16 november 30december 03 1998 toronto ontario canada david aldabass evtim peytchev mohamed khalil manling ren scalability issues urban traffic systems proceedings 1st international conference scalable information systems p31es may 30june 01 2006 hong kong benghong lim chichao chang grzegorz czajkowski thorsten von eicken performance implications communication mechanisms allsoftware global address space systems acm sigplan notices v32 n7 p230239 july 1997 harjinder sandhu tim brecht diego moscoso multiplewriter entry consistency cluster computing nova science publishers inc commack ny 2001 honghui lu alan l cox sandhya dwarkadas ramakrishnan rajamony willy zwaenepoel compiler software distributed shared memory support irregular applications acm sigplan notices v32 n7 p4856 july 1997 n p manoj k v manjunath r govindarajan casdsm compiler assisted software distributed shared memory international journal parallel programming v32 n2 p77122 april 2004 martin schulz jie tao carsten trinitis wolfgang karl smile integrated multiparadigm software infrastructure scibased clusters future generation computer systems v19 n4 p521532 may sandhya dwarkadas alan l cox willy zwaenepoel integrated compiletimeruntime software distributed shared memory system acm sigplan notices v31 n9 p186197 sept 1996 karthik balasubramanian david k lowenthal efficient support pipelining software distributed shared memory systems realtime system security nova science publishers inc commack ny henri e bal raoul bhoedjang rutger hofman ceriel jacobs koen langendoen tim rhl frans kaashoek performance evaluation orca sharedobject system acm transactions computer systems tocs v16 n1 p140 feb 1998