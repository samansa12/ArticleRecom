replication requirements mobile environments replication extremely important mobile environments nomadic users require local copies important data however todays replication systems mobileready instead improving mobile users environment replication system actually hinders mobility complicates mobile operation designed stationary environments replication services cannot provide mobile users capabilities require replication mobile environments requires fundamentally different solutions previously proposed nomadicity presents fundamentally new different computing paradigm outline requirements mobility places replication service briefly describe roam system designed meet requirements b introduction mobile computing rapidly becoming standard types environments academic com mercial private widespread mobility impacts multiple arenas one particular importance data replication replication especially important mobile environments since disconnected poorly connected machines must rely primarily local resources monetary costs communication mobile combined lower bandwidth higher latency reduced availability eectively require important data stored locally mobile machine case shared data multiple mobile users mobile stationary chines replication often best sometimes viable approach many replication solutions 416 assume static infrastructure connections work sponsored advanced research projects agency contract dabt6394c0080 gerald popek also aliated carsdirectcom may transient connection location set possible synchronization partners always remain however mobile users denition static replication service forces adjust static infrastructure hinders mobility rather enables extraordinary actions long distance telephone calls lowbandwidth links necessary users conform underlying static model costing additional time money providing degraded service additionally mobile users diculty interoperating mobile users communication patterns topologies typically predened according underlying infrastruc ture often direct synchronization mobile users simply permitted systems 21418 simply traded communication problem another one scaling provide ability anyto synchronization model suers inherent scaling problems limiting usability real environments good scaling behavior important mobile scenario mobile users clearly require local replicas mobile machines yet replicas must also stored oce environment reliability intra oce use nonmobile personnel system administration activities like backups addition ally typical methods reducing replication fac tors local area network sharing tech niques simply feasible mobile con text mobile users require local replicas critical information cases desire local access noncritical objects well cost performance reasons inability scale well large obstacle mobile user restriction static infrastructure discussed main problem mobile users replicating data using systems designed mobility instead replication system improving state mobile com puting actually hinders mobility users nd forced adjust physical motion computing needs better match system expects paper outlines requirements replication service designed mobile context conclude description roam replication solution redesigned especially mobile computing built using ward architecture 11 enables rather hinders mobil ity provides replication environment truly suited mobile environments 2 replication requirements mobile users special requirements beyond simple replication required anyone wishing share data discuss requirements particular mobile use anytoany communication larger replication factors detailed controls replication behavior lack premotion actions omit discussion wellunderstood ideas case optimistic replication discussed 23517 21 anytoany communication denition mobile users change geographic location cannot predicted priori machines geographically collocated given time given typically cheaper faster ecient communicate local partner rather remote one mobile users want ability directly communicate synchronize whomever nearby consistency correctly maintained even two machines cannot directly synchronize demonstrated systems based clientserver model 416 local synchronization increases usability level functionality decreasing inherent synchronization cost users geographically collocated dont want updates eventually propagate longdistance suboptimal path two machines next synchronization instantaneous since users expect nearby machines synchronize quickly e ciently cannot predicted machines geographically collocated point future replication model capable supporting anytoany communication required model must allow machine communicate machinethere secondclass clients system anytoany communication also required mobile arenas appliance mobility 6 motion device device system system instance given desktop lap top palmtop unlikely one would want impose strict clientserver relationship three rather one would want able communicate others providing anytoany communication equivalent using peertopeer replication model 10 1418 anyone directly synchronize anyone else everyone must denition equals least respect updategeneration abilities however argued peer models mobile environments relative insecurity regarding physical devices themselvesfor example laptops often stolen argument since mobile computers physically less secure secondclass citizens respect highly secure servers located behind locked doors 15 classbased distinction intended provide improved security limiting potential security breach secondclass object argument based assumption security features must encapsulated within peer model therefore unauthorized access peer thwarts security barriers mecha nisms however systems truffles 13 demonstrated security policies modularized logically situated around peer replication framework still remaining independent replication system truffles extension peerbased systems ficus 2 rumor 14 incorporates encryptionbased authentication overthewire privacy integrity services increase replicas condence peers truffles supports protected denition modication security policies example part security policy could accept new le versions specic authenticated replicaswhich eectively degree security provided secondclass replicas mentioned architecture problems caused unauthorized access peer replica different unauthorized access client clientserver model thus question update exchange topologies anytoany compared stylized rigid structure clientserver models dealt independently security issue question enforce proper security controls 22 larger replication factors replication systems provide handful replicas given object ad ditionally peer algorithms never traditionally scaled well finally argued peer solutions simply nature cannot scale well 15 however mobile environments seem require peerbased solution described also seem negate assumption handful replicas enough claim need thousands writable copies seem likely environments common today envisioned near future require larger replication factors current systems allow first foremost mobile user requires local replica laptop doubling replication factors data stored users desktop laptop additionally although replication factors often minimized oce environments due lanstyle sharing remoteaccess capabilities networkbased le sharing cannot utilized mobile environments due frequency network partitions wide range available bandwidth transfer latency second consider case appliance mobility discussion assumes user one static machine one mobile machine future see use many smart devices capable storing replicated data palmtop computers becoming common even wristwatch download calendar data another machine researchers 19 built systems allow laptop palmtop machines share data dynamically opportunis tically dicult imagine devices near future capability store conceivably update replicated data devices potentially increase replication factors dramatically finally argued need larger replication factors independent mobile sce nario case air trac control 9 scenarios possibly requiring larger replication factors include stock exchanges network routing airline reservation systems military command control readonly strategies classbased techniques cannot adequately solve scaling prob slem least mobile scenario classbased solutions applicable mobility reasons described section 21 readonly strategies viable solutions force users preselect writable replicas beforehand limit number writable copies general one cannot predict replicas require writeaccess ones must provide ability replicas generate updates even though may never 23 detailed replication controls denition replication service provides users degree replication control method indicating objects want replicated many systems provide replication largegranularity basis meaning users requiring one portion container must locally replicate entire container systems perhaps adequate stationary environments users access large disk pools network resources replication control becomes vastly important mobile users nomadic users general access omachine sources therefore objects locally stored eectively inaccessible everything user requires must replicated locally becomes problematic container large replicating largegranularity container means replicated objects deemed unimportant particular user unimportant data occupies otherwise usable disk space cannot used critical objects mobile context network disconnections commonplace important data cannot stored locally causes problems ranging minor inconveniences complete stoppages work productivity described kuenning 7 kuennings studies user behavior indicate set required data fact completely stored locally underlying replication service provides appropriate exibility individually select objects replication users automated tools therefore require fairly detailed controls objects replicated without mobile users cannot adequately function 24 premotion actions one possible design point would users register nomads specic time duration becoming mobile control structures algorithms replication system could greatly simplied users would act stationary register motion unusual case instance suppose user taking threeday trip los angeles new york traveling machines los angeles new york could exchange state recongure users portable correctly interact machines new york since replication requires underlying distributed algorithms part reconguration process would require changing saving distributed state stored portable ensure correct algorithm execution however design policy drastically restricts way mobility occur match reality mobile use mobility cannot always predicted scheduled often chaos real life causes unpredicted mo bility car fails en route work freeway trafc causes unforeseeable delays child picked early school family emergency occurs weather delays travel plans users often forced become mobile earlier remain mobile longer initially intended general cannot require users know priori either become mobile long additionally design policy makes underlying assumptions connectivity accessibility machines two aected geographic areas los angeles new york example assumes mobility occurs necessary machines accessible statetransformation operation occur inaccessibility 5of participant process blocks users mobility policy seems overly strictive match reality mobile use perhaps user wants change geographic locations precisely local machine un available perhaps user needs become mobile instant connectivity multiple required sites since neither mobility connectivity predicted one cannot make assumptions combination two reasons believe solutions require premotion actions viable mobile scenario premotion actions force users adapt system rather system support desired user behavior real solution must provide type getup go functionality required people everyday use 3 roam roam system designed meet set requirements based ward model 11 currently implemented tested university california los angeles 31 ward model ward model combines classical elements traditional peertopeer clientserver models yielding solution scales well provides replication exibility allowing dynamic reconguration synchronization topology models main grouping mechanism ward wide area replication domain ward collection nearby machines possibly loosely connected denition nearby depends factors geographic location expected network connectivity bandwidth latency cost see 12 full discussion issues wards created replicas added system new replica chooses whether join existing ward form new one believe possible automate assignment ward membership issues involved complex avoided attempting current system instead decision controlled human system administrator knowledgeable user necessary decision altered later using wardchanging utilities although members ward equal peers ward designated ward master similar server clientserver model several important dierences since ward members peers two ward members directly synchronize one another typical clientserver solutions allow clienttoclient synchronization whether design accident mobile users often encounter mobile users cases direct access ward member may easier cheaper ecient access ward master since ward members peers ward member serve ward master automatic reelection wardmaster recongura tion occur ward master fail become unavailable algorithms exist resolve multiplemaster scenarios correctness aected transient ward master fail ure system maintains better consistency ward master typically available accessible since neither inaccessible ward master multiple ward masters aects overall system correctness see section 32 reelection problem considerably easier related distributed reelection problems ward master required store actual data intraward objects though must able identify ie name complete set clientserver strategies force server store superset clients data ward master wards link wards ward master aware replicas outside ward one manner ward model achieves wardsidraw figure 1 basic ward architecture overlapped members mobility feature section 34 good scalingby limiting amount knowledge stored individual replicas traditional peer models force every replica learn replicas existence ward model replicas knowledgeable replicas within ward fact replicas completely unaware existence wards ward masters belong higherlevel ward forming twolevel hierarchical model 1 ward masters act wards behalf bringing new updates ward exporting others ward gossiping known updates consistency maintained across replicas ward masters communicate directly allowing information propagate independently within ward figure 1 illustrates basic architecture well advanced features discussed later sections wards dynamically formed replicas created dynamically maintained suitable wardmember candidates change ward destruction occurs automatically last replica given ward destroyed 32 system correctness important feature ward model system correctness depend precisely one master per ward even recon guration updated les ow replicas without loss information incorrect behavior purposes ward master simply another replica whether communicating within ward wards master maintains consistency using algorithms nonmaster replicas thus propagation information within among wards follows correctness algorithms 1 rationale behind twolevel hierarchy impact scaling discussed section 35 rst described 1 ward master becomes temporarily unavail able information continue propagate ward members due peer model however information usually propagate wards master returns exception rule occur ward member temporarily permanently moves another ward described section 34 carrying data master suers permanent failure new master elected must demonstrate correctness suer transition new master correctness violated either failed master information cannot reconstructed failed masters participation required completion distributed algorithm rst case occur lost information created master yet propagated another replica case lost information cannot aect correctness situation never existed second case handled distributed failurerecovery algorithms invoked administrator declares old master unrecoverable new ward master elected possibility creating multiple masters correctness aected case master play special role algorithms purpose ward master coordinate behavior ward members rather serve conduit information ow wards multiple masters like overlapped members section 342 merely provide another communication path wards since peertopeer algorithms assume arbitrary communication patterns correctness aected multiple ward masters 33 flexibility model replication exibility important feature ward model set data stored within ward called ward set dynamically ad justable set ward members ward members change data demands alter replicated data store locally ward set changes similarly mobile machines join leave ward set ward participants changes ward set ward membership locally recorded replicated optimistic fashion additionally ward member including ward master locally store dierent subset ward set replication exibility called selective replication 10 provides improved eciency resource utilization ward members locally store objects actively require replication decisions made manually automated tools 58 since ward set varies dynamically dier ent wards might store dierent sets ward sets equivalent essence model provides selective replication wards selves reconciliation topologies algorithms 10 apply equally well within single ward ward masters brie algorithms provide machines communicate multiple partners ensure data object synchronized directly another replica addition ally data synchronization algorithms support reconciliation nonlocal data via thirdparty datastorage site allowing ward master reconcile data stored locally stored somewhere within ward 34 support mobility model supports two types mobility intraward mobility occurs machines within ward become mobile within limited geographic area machines encountered ward members since ward members peers direct communication possible encountered machine intraward mobility might occur within building traveling coworkers house local coee shop perhaps interesting interward mobility occurs users travel data another geographic region encountering machines another ward examples include businessmen traveling remote oces distant collaborators meeting common conference interward mobility raises two main issues first recall due models replication exibility two wards might identical ward sets thus mobile machine may store data objects kept new ward vice versa second consider typical patterns mobility often users travel away home location short time system would perform poorly transient mobile actions required global changes data structures across multiple wards hand mobile users occasionally spend long periods time locations either permanently semi permanently changing denition home scenarios users provided quality service terms local performance time synchronize data experienced previous home solution resolves issues den ing two types interward mobilityshortterm transient longterm semipermanentand providing ability transparently automatically upgrade former latter two operations called ward overlapping ward changing respectively collectively two called ward motion enable peerto peer communication two replicas ward model regardless ward membership 341 ward changing ward changing involves longterm perhaps permanent change ward membership moving replica physically changes notion home ward forgetting information previous ward similarly participants old new wards alter notion current membership ward membership information maintained using optimistic algorithms used replicating data problem tracking membership often disconnected environments straightforward addition new ward member may change ward set since ward master responsible interward synchronization data ward set ward set must expand properly encompass replicated data stored moving replica similarly ward set old ward may shrink size ward set dynamically optimistically recalculated ward membership changes wardset changes propagate ward masters op timistic needtoknow fashion ward masters care changes learn since ward sets potentially change changes eventually propagated ward masters ward changing heavyweight operation however users benet local data synchronized completely within local ward giving users best possible quality service reconciliation performance 342 ward overlapping contrast ward overlapping intended lightweight mechanism causes global changes within system new ward aected operation localization changes makes lightweight operation perform undo ward overlapping allows simultaneous multi ward membership enabling direct communication members ward make mechanism lightweight avoid changing ward sets making new replica lapped member instead fulledged pant ward members except ward master cannot distinguish real overlapped members dierence management ward set instead merging existing ward set data stored mobile machine ward set remains unaltered data shared mobile machine ward set reconciled locally members new ward however data outside new ward cannot reconciled locally must either temporarily remain unsynchronized else reconciled original home ward 343 ward motion summary replica enters another ward two possibilities ward set change remain former creates performanceimproving heavyweight solution latter causes moderate performance degradation synchronizing data stored new ward provides lightweight solution transient mobile situations since operationally equivalent system transparently upgrade overlapping changing motion seems permanent rst expected additionally since ward formation dy namic users easily form mobile workgroups identifying set mobile replicas new possibly temporary ward using ward lapping mobile workgroups formed without leaving old wards ward motion dynamic ward formation destruction allow easy straightforward communication set replicas entire system 35 scalability scalability ward model directly related degree replication exibility ward sets dynamically change unpredictable ways therefore method ward master identify ward set list entry individually fully hierarchical generalization ward model two levels faces scaling problems due physical problems maintaining indexing lists entries nevertheless proposed model scales well within intended environment allows several hundred readwrite replicas given ob ject meeting demands everyone single developer mediumsized committee large international company model could adapted scale better restricting degree replication freedom instance ward sets changed regular fashions could named unit instead naming members dramatically improving scalability however believe replication exibility important design consideration targeted mobile envi ronment one users absolutely require chosen impose regularity 4 performance 41 disk space overhead roam like rumor stores nonvolatile data structures lookaside databases within volume hidden user users perspective anything actual data overhead eectively shrinks size disk minimal disk overhead therefore important visible criterion user satisfaction additionally roam designed scalable system ward model support hundreds replicas minimal impact wards specically creation new replica ward x aect disk overhead replicas wards therefore measured disk overhead roam using two dierent volumes rst volumes chosen typical representative users personal subtree second chosen stress roam storing small les would exaggerate systems space overhead empirically measuring overhead dierent conditions tted equations describe overhead terms number les types les number replicas number wards equations summarized follows full results given 12 new directory costs 42kb object directory new le costs 24kb rst replica within ward even without user data costs 5736kb additional replica within ward costs object stored replica new ward costs 644kb 42 synchronization performance since roams main task synchronization data also measured synchronization per formance performed experiments two portable machines cases minimizing extraneous processes avoid nonrepeatable ef fects one machine dell latitude xp 486dx4 running 100mhz 36mb main memory second ti travelmate 133mhz pentium 64mb main memory reconciliation always performed transferring data dell machine ti machine words reconciliation process always executed ti machine course reconciliation performance depends heavily sizes les dated since roam performs wholele transfers updated le must transfered across network entirety would expect reconciliation take time data updated therefore varied amount data updated 0 100 within trial randomly selected set updated les since les selected random given gure x approximation amount data updated rather exact gure mea surements used personalsubtree volume mentioned section 41 performed least seven trials data point performed dierent experiments conditions rst two compared roam rumor synchronization performance 10mb quiet ethernet wavelan wireless cards respectively third studied effect increasing numbers replicas fourth studied eect increasing numbers wards fth looked eects selective replication 10 dierent replication patterns synchronization performance experiments showed roam 10 25 slower rumor running similar numbers replicas slowdown due roams exible structure uses processes ipc simplify code enhance scalability reconciliation 136mb volume roam takes 46 206 seconds depending transport mechanism number les modied number replicas ward also studied impact multiple wards synchronization performance varied number wards one previous ex periments six placed three replicas within one wards measured synchronization two previously described portable machines experiments showed 95 level condence adding wards impact synchronization performance two replicas 43 scalability already discussed aspects roams scalability disk space overhead section 41 however another major aspect scalability ability create many replicas still system perform well synchronization synchronization performance includes two related issues first reconciliation time given replica given ward largely unaected total number replicas wards second time distribute update replica replica presumably faster ward model standard approach like rumor else failed task 431 reconciliation time measure behavior reconciliation time total number replicas increases used hybrid simulation created 64 replicas test volume reducing hardware requirements using servers store wards replicas actively participating experiments found 95 level condence synchronization time change system conguration varied one ward total three replicas 7 wards total 64 replicas 432 update distribution another aspect scalability concerns distribution updates replicas scalable system would presumably deliver updates replicas quickly nonscalable system least large numbers replicas additionally may perform better small numbers replicas scalable system least perform worse rather measuring elapsed time depends many complicated factors con nectivity network partitions available machines reconciliation intervals considered number individual pairwise reconciliation ac tions analytically developed equations characterize distribution updates assume replicas one replica r generates update must propagate replicas following equations identify number separate reconciliation actions must occur average worst case propagate update r replica nonward system rumor since replicas 1 yet update reconciliation uses ring replicas need 1reconciliation actions average worst case requires 1 reconciliation actions analysis roam little com plicated assume replicas divided n wards ward mn mem bers propagating update r requires rst sending r rs ward master sending rs ward master ss ward mas ter nally course r members ward much expense saved however solve general problem rst discussing special case conditions need 1 reconciliation actions average distribute update replica ward master and2 actions average ward masters building blocks calculate average roam requires following number reconciliation actions2 note performance setting eliminates benet grouping however also interesting note becomes 1 two wards improve required time distribute updates although improve aspects data structure size network utilization general roam distributes updates faster rumor 2 n 3 wise roam performs rumor respect update distribution two equations calculate optimal number wards given value 2m conditions yield factor three improvement 50 replicas factor 200 replicas multilevel implementation larger degrees improvement possible analysis roam also indicates worst case roam requires 2m reconciliation actions special case r ward 1 reconciliation actions required average 1 worst case 44 ward motion recall section 34 roam supports two dierent avors ward motion overlapping changing overlapping lightweight temporary form motion easy perform undo however synchronization performance become worse overlapping moving replica stores objects part new ward must synchronized original ward else remain unsynchronized presumably short time period changing heavyweight permanent form motion costs provides optimal synchronization performance new ward experimentally investigated costs forms ward motion using 136mb volume used tests four types costs involved operations 1 initial setup costs moving replica 2 disk overhead moving replica 3 costs imposed wards ward mem bers 4 ongoing costs synchronization summarize results complete data given 12 found setting either type motion took 60 80 seconds depending number les stored local machine somewhat surprisingly ward changing required 7 elapsed time overlapping disk overhead moving replica depends size destination ward essence members destination ward must tracked members replicas original ward occupying 644kb plus 12 bytes per le see section 41 ward lapping cost must paid original wards members destination wards members ward changing desti nations members must tracked either case costs insignicant compared space required volume costs imposed replicas wards minimal ward overlapping ward changing old new ward masters must change ward sets dierences need propagated ward masters gossiping however amount information must propagate minimal 255 bytes per le changes ward membership additional network load still quite low finally wards overlapped synchronization costs increase moving replica must communicate new ward old one synchronization tempo rary new ward take amount time would original ward however synchronize les available new ward original must contacted measured additional costs various replication patterns test volume described 12 simulate fact communication original ward probably longdistance thus slower used wavelan network experiments found synchronization time depends number les found original ward number modied les les modied time essentially constant 45 seconds contrast 100 locallystored les modied synchronization took 78 169 seconds depending exact set les shared two ward masters 5 conclusion replication required mobile computing todays replication services provide features required mobile users nomadicity requires replication solution provides toany communication scalable fashion suciently detailed control replication decisions roam designed implemented meet goals paving way improved mobile computing new better avenues mobile research performance experiments shown roam indeed scalable handle mobility patterns expected displayed future users r ficus large scale reliable distributed file system little work project disconnected operation coda presentation glomo pi meeting february 4 university califor nia predictive file hoarding disconnected mobile operation automated hoarding mobile computers peer replication selective control ward model scalable replication architecture mobil ity scalable replication system mobile distributed computing uence scale distributed highly available experience disconnected operation mobile computing environment tr coda influence scale distributed file system design ficus large scale reliable distributed file system disconnected operation coda file system managing update conflicts bayou weakly connected replicated storage system automated hoarding mobile computers seer roam