galerkin projection methods solving multiple linear systems paper consider using conjugate gradient cg methods solving multiple linear systems ai coefficient matrices ai righthand sides bi different general particular focus seed projection method generates krylov subspace set direction vectors obtained solving one systems called seed system cg method projects residuals systems onto generated krylov subspace get approximate solutions whole process repeated systems solved papers literature f chan w l wan siam j sci comput peterson r mittra ieee trans antennas propagation 37 1989 pp 14901493 considered case coefficient matrices ai righthand sides different extend analyze method solve multiple linear systems varying coefficient matrices righthand sides theoretical error bound given approximation obtained projection process onto krylov subspace generated solving previous linear system finally numerical results multiple linear systems arising image restorations recursive least squares computations reported illustrate effectiveness method b introduction want solve iteratively using krylov subspace methods following linear systems x real symmetric positive definite matrices order n general 6 j b 6 b j 6 j unlike direct methods coefficient matrices righthand sides arbitrary nearly hope solve efficiently completely unrelated systems fortunately many practical applications coefficient matrices righthand sides arbitrary often information sharable among coefficient matrices righthand sides situation occurs instance recursive least squares computations 20 wave scattering problem 14 4 9 numerical methods integral equations 14 image restorations 13 paper aim propose methodology solve related multiple linear systems efficiently 24 smith et al proposed considered using seed method solving linear systems coefficient matrix different righthand sides ie seed method select one seed system solve conjugate gradient method perform galerkin projection residuals onto krylov subspace generated seed system obtain approximate solutions unsolved ones approximate solutions refined conjugate gradient method 24 effective implementation galerkin projection method developed uses direction vectors generated conjugate gradient process perform projection 6 chan wan observed seed method several nice properties instance conjugate gradient method applied successive seed system converges faster usual cg process another observation righthand sides closely related method automatically exploits fact usually takes restarts solve systems 6 theory developed explain phenomena remark seed method viewed special implementation galerkin projection method considered analyzed earlier solving linear systems multiple righthand sides see instance parlett 19 saad 21 van der vorst 26 padrakakis et al 18 simoncini gallopoulos 22 23 different approach based lanczos method multiple starting vectors recently proposed freund malhotra 9 paper extend seed method solve multiple linear systems 11 different coefficient matrices different righthand sides b j 6 b k analyze seed method extend theoretical results given 6 see theoretical error bounds approximation obtained projection process depends projection eigenvector components error onto krylov subspace generated previous seed system different system previous one unlike 6 general case coefficient matrices different possible derive precise error bounds since different eigenvectors general fortunately many applications even though indeed different may related structured way allows precise error analysis case two applications study paper namely image restorations recursive least squares rls computations precisely image restoration application eigenvectors coefficient matrices rls computations coefficient matrices differ rank1 rank2 matrices numerical examples applications given illustrate effectiveness projection method see numerical results eigenvector components righthand sides effectively reduced projection process number iterations required convergence decreases employ projected solution initial guess moreover examples involving general coefficient matrices instance eigenvectors differ low rank matrix also given test performance projection method observe similar behaviour numerical results image restoration rls computations numerical results demonstrate projection method effective paper organized follows x2 first describe analyze seed projection algorithm general multiple linear systems x3 study multiple linear systems arising image restoration rls applications numerical examples given x4 concluding remarks given x5 2 derivation algorithm conjugate gradient methods seen iterative solution methods solve linear system equations minimizing associated quadratic functional simplicity let associated quadratic functional linear system x minimizer f j solution linear system x idea projection method restart seed system k x selected unsolved ones solved conjugate gradient method approximate solution x j nonseed system j x obtained using search direction p k generated ith iteration seed system precisely given ith iterate x j nonseed system direction vector p k approximate solution x j found solving following minimization problem easy check minimizer 22 attained p k r j seed system k x solved desired accuracy new seed system selected whole procedure repeated following discussion call method projection method note 23 matrixvector multiplication j p k j required projection nonseed iteration general cost method expensive general case matrices j k different however x3 consider two specific applications matrices k j structurally related therefore matrixvector products j p k j computed cheaply using matrixvector product j generated seed iteration order reduce extra cost projection method general case propose using modified quadratic function compute approximate solution nonseed system note used k instead j definition case determine next iterate non seed system solving following minimization problem min ff approximate solution x j nonseed system j x given p k r j projection process require matrixvector product involving coefficient matrix j nonseed system therefore method increase dominant cost matrixvector multiplies conjugate gradient iteration fact extra cost one inner product two vector additions two scalarvector multiplications one division call method projection method ii course unless j close k sense expect method work well f j far current f j summarize methods table 1 lists algorithms projection methods ii remark krylov subspace methods instance conjugate gradient especially combined preconditioning known powerful methods solution linear systems 10 incorporate preconditioning strategy projection method speed convergence rate idea approach precondition seed system preconditioner c k restart meanwhile approximate solution nonseed system j x also obtained space direction vectors generated conjugate gradient iterations preconditioned seed system formulate preconditioned projection method directly produces vectors approximate desired solutions nonseed systems table 2 lists preconditioned versions projection methods ii systems solved select kth system seed iteration unsolved systems jk perform usual cg steps oe kk x kk r kk else perform galerkin projection x kj r kj j p kk end end end systems solved select kth system seed iteration unsolved systems jk perform usual cg steps oe kk x kk r kk else perform galerkin projection x kj r kj end end end table 1 projection methods left ii right kth system seed restart first second superscripts used denote kth restart jth system subscripts used denote ith step cg method systems solved select kth system seed iteration unsolved systems jk perform usual cg steps oe kk x kk oe kk r kk z kk preconditioning else perform galerkin projection x kj r kj end end end systems solved select kth system seed iteration unsolved systems jk perform usual cg steps oe kk x kk oe kk r kk z kk preconditioning else perform galerkin projection x kj r kj end end end table 2 preconditioned projection methods left ii right emphasize 6 19 21 23 24 authors considered using projection method solving linear systems coefficient matrix different righthand sides paper use projection methods ii solve linear systems different coefficient matrices righthand sides important question regarding approximation obtained process accuracy projection method easy derive error bounds since direction vectors generated seed system k x k orthogonal j orthogonal general following discussion analyze projection method ii however numerical results x4 shows projection method efficient applications generally faster convergent projection method ii 21 analysis projection method ii projection method ii following lemma exact arithmetic assume seed system k x selected using projection method ii approximate solution nonseed system j x j ith iteration given x kj x kj th iterate nonseed system v k lanczos vectors generated steps lanczos algorithm seed system k x solved lanczos algorithm proof let columns v k orthonormal vectors idimensional krylov subspace generated steps lanczos method following wellknown threeterm recurrence e ith column identity matrix fi k i1 scalar 24 see 24 approximate solution x kj nonseed system computed subspace generated direction vectors fp kk generated seed iteration however subspace generated direction vectors exactly subspace spanned columns v k therefore x kj moreover easy check 24 25 p kk follows solution x kj obtained galerkin projection onto krylov subspace k k generated seed system equivalently x kj determined solving following problem noting solution 0 result follows analyze error bound projection method ii without loss generality consider two symmetric positive definite nbyn linear systems 1 x eigenvalues normalized eigenvectors denoted k q k respectively 2 theorem gives error bounds projection method ii solving multiple linear systems different coefficient matrices righthand sides theorem 1 suppose first linear system 1 x solved desired accuracy steps let x 12 0 solution second system 2 x obtained projection onto km generated first system zero vector initial guess second system x 02 eigendecomposition x 0 expressed eigenvector components c k bounded 6 q 2 vm orthonormal vectors km p 1 1 orthogonal projection onto km 1 vm matrix representation projection 1 onto km proof 26 get x 12 x since vm orthogonal vectors km kvm follows 6 q 2 6 q 2 theorem 1 basically states size eigenvector component c k bounded e k f krylov subspace km generated seed system contains eigenvectors q 2 well projection process kill eigenvector components initial error nonseed system ie e k small hand f depends essentially different system 2 x 2 previous one 1 x particular small f also small remark 2 b 1 6 b 2 term f becomes zero q 1 6 q 1 km k wellknown krylov subspace km generated seed system contains eigenvectors q 1 k well particular chan wan 6 following result estimate bound sin 6 q 1 6 gamma 1 1 chebyshev polynomial degree j sin 6 q 1 assume eigenvalues 1 distinct tmgammak 12 k grows exponentially increases therefore magnitude sin 6 q 1 small sufficiently large implies magnitude e k small sufficiently large unfortunately cannot result general case since q 1 k except special cases discussed next section 3 applications galerkin projection methods section consider using galerkin projection method solving multiple linear systems arising two particular applications image restorations recursive least squares computations applications coefficient matrices differ parameterized identity matrix low rank matrix note theorem 1 theoretical error bound projection method depends e k f general easy refine error bound e k f however cases error bound e k f investigated 31 tikhonov regularization image restorations image restoration refers removal reduction degradations blur image using priori knowledge degradation phenomena see instance 13 quality images degraded blurring noise important information remains hidden cannot directly interpreted without numerical processing matrixvector notation linear algebraic form image restoration problem nbyn pixel image given follows b x j n 2 vectors n 2 byn 2 matrix given observed image b matrix represents degradation possibly statistics noise vector j problem compute approximation original signal x illconditioning naively solving lead extreme instability respect perturbations b see 13 method regularization used achieve stability problems 1 3 classical tikhonov regularization 12 stability attained introducing stabilizing operator called regularization operator restricts set admissible solutions since causes regularized solution biased scalar called regularization parameter introduced control degree bias specifically regularized solution computed solution min b min term kdxk 2 2 added order regularize solution choosing kth order difference operator matrix forces solution small kth order derivative rectangular matrix full column rank one find solution solving normal equations regularization parameter controls degree smoothness ie degree bias solution usually small choosing trivial problem cases priori information signal degree perturbations b used choose 1 generalized crossvalidation techniques may also used eg 3 priori information known may necessary solve 310 several values example lcurve method discussed 7 choosing parameter requires solving linear systems different values gives rise multiple linear systems solved proposed projection methods applications 13 5 regularization operator chosen identity matrix consider simplicity two linear systems 2 case employ projection method solve multiple linear systems matrixvector product 2 nonseed iteration computed cheaply adding 1 ap generated seed iteration together moreover refine error bound projection method ii theorem 1 assume steps conjugate gradient algorithm performed solve first system note case eigenvectors first second linear systems ie q 1 k therefore bound sin 6 q 1 using lemma 2 shall prove krylov subspace first linear system contains extreme eigenvectors well bound convergence rate effectively classical conjugate gradient bound reduced condition number theorem 2 let x 12 0 solution second system obtained projection onto km generated first system bound 2 norm error vector steps conjugate gradient process given 2 4kx x 12 2 x 12 ith iterate cg process 2 x projection x span fq 1 1 reduced condition number 2 2 proof first expand eigencomponents x wellknown 10 exists polynomial degree constant term 1 x 12 x 12 using properties conjugate gradient iteration given 10 2 2 2 x 12 term kx 2 bounded classical cg error estimate x 12 2 4kx x 12 2 noting using theorem 1 lemma 2 result follows substitution 27 312 see perturbation term ffi contains two parts one depends ratio 2 1 regularization parameters two linear systems depends well krylov subspace seed system contains extreme eigenvectors remark regularization parameter practice always greater 0 image restoration applications illconditioning particular 1 6 0 ratio 2 1 near 1 magnitude term near zero hand according lemma 2 galerkin projection kill extreme eigenvector components therefore quantity 311 also small k close 1 hence perturbation term ffi becomes small cg method applied solve nonseed system converges faster usual cg process 32 recursive least squares computations signal processing recursive least squares rls computations used extensively many signal processing control applications see alexander 2 standard linear least squares problem posed follows given real pbyn matrix x full column rank n x x symmetric positive definite pvector b find nvector w solves min w rls computations required recalculate w observations ie equations successively added deleted problem 313 instance many applications information arrives continuously must incorporated solution w called updating sometimes important delete old observations effect removed w called downdating associated sliding data window alternatively exponential forgetting factor fi instance 2 may incorporated updating computations exponentially decay effect old data time use fi associated exponentiallyweighted data window 321 rank1 updating downdating sliding window rls time step data matrix desired response vector given tgammap1 respectively p length sliding window one always assumes p n solve following least squares problem min wt assume row added row removed step 1 righthandside desired response vector modified corresponding fashion one seeks solve modified least squares problem min wt1 updated least squares estimate vector wt 1 time step 1 note normal equations given therefore coefficient matrices time step differ rank2 matrix 322 exponentiallyweighted rls exponentiallyweighted case data matrix xt desired response vector dt time step defined 2 recursively fi forgetting factor x rls algorithms recursively solve least squares estimator wt time n least squares estimator time found solving corresponding least squares problems normal equations given respectively remark two coefficient matrices differ rank1 matrix plus scaling 323 multiple linear systems rls computations consider multiple linear systems rls computations ie solve following least squares problem successively arbitrary block size rls computations implementation recursive least squares estimators proposed used 8 algorithms updates filter coefficients minimizing average least squares error set data samples instance least squares estimates computed modifying cholesky factor normal equations 2 operations per adaptive filter input 20 approach employ galerkin projection method solve multiple linear systems arising sliding window exponentiallyweighted rls computations sliding window rls computation rank1 updating downdating 315 multiple linear systems given 1st system xt theta exponentiallyweighted case 316 multiple linear systems given 1st system xt theta according 318 consecutive coefficient matrices differ rank2 matrix sliding data window case 319 consecutive coefficient matrices differ rank1 matrix scaled coefficient matrix exponentiallyweighted case rls computations projection method used solve multiple linear systems matrixvector product nonseed iteration computed inexpensively instance matrixvector product new system computed generated seed iteration extra cost inner products remark linear systems 318 319 need inner products coefficient matrices xt xt differ ranks rank2s matrices analyze error bound given projection method ii case coefficient matrices differ rank1 matrix ie r unit 2norm component greater zero exponentially weighted case note using eigenvalueeigenvector decomposition 1 obtain diagonal matrix containing eigenvalues 1 1 r shown 11 1 k k eigenvalues 2 k 2 computed solving secular equation q 1 1 0 moreover eigenvectors q 2 k 2 calculated formula q 2 theorem 3 suppose first linear system 1 x solved desired accuracy cg steps eigenvector components c k second system bounded jc 6 q 1 q 1 1 q 1 fq 1 g orthonormal eigenvectors 1 km krylov subspace generated first system proof note theorem 1 jc k j jp using 320 theorem 1 lemma 2 analyze term jp 6 q 1 since jfl ik j j sin 6 q 1 less 1 small large 6 q 1 remaining lemma 2 close 1 n 6 q 1 sufficiently small large moreover note ae 0 see 10 therefore values q 1 magnitude eigenvector q 1 maximum value jfl ik j attained either may expect second term inequality 321 small k close 1 n combining facts deduce e k also small k close 1 n hand scalar ae small ie 2norm rank1 matrix small f also small illustrate result apply projection method ii solve 1 x b 1 b 2 random vectors unit 2norm figures 1 2 show extreme eigenvector components b 2 killed projection especially jaej small property suggests projection method useful solve multiple linear systems arising recursive lease squares computations numerical examples given next section illustrate efficiency method section provide experimental results using projection methods ii solve multiple linear systems 11 experiments performed matlab machine stopping criterion kr kj tol tolerance used first second examples tikhonov regularization image restoration recursive least squares estimation exactly discussed x3 coefficient matrices eigenvectors example 1 example 2 coefficient matrices differ component number log component rhs projection component number log component rhs projection b component number log component rhs projection component number log component rhs projection c figure 1 size distribution components original right hand side b 2 b b 2 galerkin projection ae 1 size distribution components c original right hand side b 2 b 2 galerkin projection component number log component rhs projection component number log component rhs projection b component number log component rhs projection component number log component rhs projection c figure 2 size distribution components original right hand side b 2 b b 2 galerkin projection ae gamma1 size distribution components c original right hand side b 2 b 2 galerkin projection linear systems 1 2 3 4 total starting projection method 36 37 43 starting projection method ii 36 48 55 76 205 starting previous solution 36 54 66 87 243 starting random initial guess 38 starting projection method 9 9 9 11 38 using preconditioner starting projection method ii 9 9 11 using preconditioner starting previous solution 9 13 16 23 61 using preconditioner table 3 example 1 number matrixvector multiplies required convergence systems regularization parameter rank1 rank2 matrices see extremal eigenvector components righthand sides effectively reduced projection process moreover number iterations required convergence employ projected solution initial guess less required usual cg process example consider 2dimensional deconvolution problem arising groundbased atmospheric imaging try remove blurring image see figure 3a resulting effects atmospheric turbulence problem consists 256by 256 image ocean reconnaissance satellite observed simulated groundbased imaging system together 256by256 image guide star figure 3b observed similar circumstances data provided phillips air force laboratory kirkland afb nm prof bob plemmons wake forest university restore image using identity matrix regularization operator suggested 5 therefore solve linear systems 310 different regularization parameters also test effectiveness preconditioned projection method preconditioner employed blockcirculant circulantblock matrix proposed 5 table 3 shows number matrixvector multiplies required convergence systems using projection method save number matrixvector multiplies iterative process without preconditioning table 3 also see performance projection method better projection method ii comparison present restorations images regularization parameters 0072 0036 0009 figure 3 see value large restored image smooth value small noise amplified restored image solving multiple linear systems successively projection method select figure 3e presents restored image better others b c figure 3 example 1 observed image guide star image b restored images using regularization parameter linear systems 1 2 3 4 5 total starting projection method 45 31 28 25 24 153 starting projection method ii 45 37 starting previous solution 45 43 44 42 40 214 linear systems 1 2 3 4 5 total starting projection method 68 51 45 36 starting projection method ii 68 55 starting previous solution 68 61 59 56 54 308 b table 4 example 2 number matrixvector multiplies required convergence systems exponentiallyweighted rls computations b sliding window rls computation example example test performance projection methods ii block sliding window exponentiallyweighted rls computations illustrate convergence rate method using adaptive finite impulse response system identification model see 15 second order autoregressive process white noise process variance 1 used construct data matrix xt x32 reference unknown system wt nth order fir filter gaussian white noise measurement error variance 0025 added desired response dt x32 tests forgetting factor fi 099 order n filter 100 case exponentiallyweighted rls computations consecutive systems differ rank1 positive definite matrix whereas case sliding window computations consecutive systems differ sum rank1 positive definite matrix rank1 negative definite matrix table 4 lists number matrixvector multiplies required convergence systems arising exponentiallyweighted sliding window rls computations observe performance projection method better projection method ii projection method requires less matrixvector multiplies using previous solution initial guess note figures 4 5 eigenvector components b 2 effectively reduced projection cases exponentiallyweighted sliding window rls computations see decreases eigenvector components using projection method indeed greater using projection method ii next three examples consider general coefficient matrices ie consecutive linear systems differ scaled identity matrix rank1 rank2 matrices examples matrixvector products nonseed iteration may computed cheaply component number log component rhs projection component number log component rhs using projection method b component number log component rhs using projection method ii component number log component rhs using previous solution initial guess c figure 4 example 2 exponentiallyweighted rls computations size distribution components original right hand side b 2 b b 2 using projection method c b 2 using projection method ii b 2 gamma 2 x 1 using previous solution initial component number log component rhs projection component number log component rhs using projection method b component number log component rhs using projection method ii component number log component rhs using previous solution initial guess c figure 5 example 2 sliding window rls computations size distribution components original right hand side b 2 b b 2 using projection method c b 2 using projection therefore apply projection method ii solve multiple linear systems however phenomena examples 1 2 observed three examples well example 3 example consider discrete illposed problem discretization fredholm integral equation first kind particular integral equation shall use one dimensional model problem image reconstruction 7 image blurred known pointspread function desired solution f given kernel k point spread function infinitely long slit given ae sinsin use collocation n 64 equidistantly spaced points gamma2 2 derive matrix exact solution x compute exact righthand sides perturb uncorrelated errors white noise normally distributed zero mean standard derivation 10 gamma4 choose matrix equal second derivative operator different regularization parameters used compute lcurve see figure test performance projection method ii solving multiple linear systems emphasize consecutive systems differ scaled identity matrix table 5 shows number iterations required convergence 10 systems using projection method ii using previous solution initial guess residual norm see projection method requires 288 matrixvector multiplies solve systems one using previous solution initial guess requires 365 matrixvector multiplies particular tenth system solved without restarting conjugate gradient process projection example 4 consider integral equation z 2ftdt corresponding dirichlet problem laplace equation interior ellipse semiaxis c 0 solve case unique solution righthand side given linear systems 1 2 3 4 5 6 7 8 starting projection 79 38 33 25 27 26 23 21 15 1 288 method ii starting previous 79 44 37 34 solution table 5 example 3 number matrixvector multiplies required convergence systems 64 65 66 67 68 69 7 71 72 least squares residual norm solution seminorm figure example 3 tikhonov lcurve regularization parameters used table 4 matrixvector multiply log residual norm figure 7 example 4 convergence behaviour systems 23822 dcd coefficient matrices k right hand sides b obtained discretization integral equation 422 size systems 100 values c arbitrary chosen intervals 2 5 0 1 respectively emphasize example consecutive discretized systems differ low rank small norm matrices convergence behaviour systems shown figure 7 plot steepest declining line denotes convergence seed also nonseed last restart note plot residual norm cost number matrixvector multiply place iteration number may compare efficiency methods remark shape plot obtained similar numerical results given 6 galerkin projection method solving linear systems multiple right hand sides use solution second system initial guess third system number iteration required 13 however number iteration required 8 projection method ii residual norm previous solution method see figure 8 figure 9 shows components corresponding righthand side third system galerkin projection projection using previous solution initial guess figure clearly reveals eigenvector components b 3 effectively reduced projection example 5 matrices final set experiments corresponding threepoint centered discretization operator gamma dx ax du dx 0 1 function ax given two parameters discretization performed using grid size h 165 yielding matrices size 64 different values c right hand sides systems generated randomly 2norm 1 remark consecutive linear systems differ low rank small norm matrices cg iteration log residual norm b c figure 8 example 4 convergence behaviour third system projected solution initial guesses b previous solution vector initial guess c random vector initial guess linear systems 1 2 3 4 5 6 7 8 starting projection 83 method ii starting previous solution table example 5 number matrixvector multiplies required convergence systems c example table 6 shows number iterations required convergence systems using projection method ii using previous solution initial guess residual norm observe results one using projected solution initial guess converges faster using previous solution initial guess figure 10 shows components corresponding righthand side seveth system galerkin projection projection illustrates projection reduce eigenvector components effectively e components rhs projection component number log component rhs using projection method ii component number log component rhs using previous solution initial guess b c figure 9 example distribution components original right hand side b 3 b b 3 galerkin projection c b 3 gamma 3 x 2 using previous solution initial rhs projection component number log component rhs using projection method ii component number log component rhs using previous solution initial guess b c figure 10 example 5 size distribution components original right hand side b 7 b b 7 galerkin projection c b using previous solution concluding remarks paper developed galerkin projection methods solving multiple linear systems experimental results show method efficient method end concluding remarks extensions galerkin projection method 1 block generalization galerkin projection method employed many appli cations method select one system seed krylov subspace generated seed larger initial guess obtained galerkin projection onto subspace expected better one drawback block method may break singularity matrices occurs arising conjugate gradient process details block galerkin projection methods refer chan wan 6 2 literature nonsymmetric systems multiple righthand sides vast two methods proposed block generalizations solvers nonsymmetric systems block biconjugate gradient algorithm 17 16 block gmres 25 block qmr 4 9 recently simoncini gallopoulos 23 proposed hybrid method combining galerkin projection process rishardson acceleration technique speed convergence rate conjugate gradient process spirit modify galerkin projection algorithms solve nonsymmetric systems multiple coefficient matrices righthand sides r regularization super resolution springer verlag block qmr method computing multiple simultaneous solutions complex symmetric systems generalization strangs preconditioner applications toeplitz least squares problems analysis projection methods solving linear systems multiple righthand sides analysis discrete illposed problems means lcurve block implementation adaptive digital filters blockqmr algorithm nonhermitian linear systems multiple righthand sides matrix computations modified matrix eigenvalue problems theory tikhonov regularization fredholm equations first kind fundamentals digital image processing fast rls adaptive filtering fftbased conjugate gradient iterations variable block cg algorithms solving large sparse symmetric positive definite linear systems parallel computers block conjugate gradient algorithm realted methods new implementation lanczos method linear problems new look lanczos algorithm solving symmetric systems linear equations lanczos method solving symmetric linear systems several righthand sides memoryconserving hybrid method solving linear systems multiple righthand sides iterative method nonsymmetric systems multiple righthand sides conjugate gradient algorithm treatment multiple incident electromagnetic fields etude de quelques methodes de resolution de problemes lineaires de grande taille sur multiprocesseur iteration solution method solving fa tr