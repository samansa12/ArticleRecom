compiling parallel code sparse matrix applications developed framework based relational algebra compiling efficient sparse matrix code dense doany loops specification representation sparse matrix paper show framework used generate parallel code present experimental data demonstrates code generated bernoulli compiler achieves performance competitive handwritten codes important computational kernels b introduction sparse matrix computations ubiquitous computational science however development highperformance software sparse matrix computations tedious errorprone task two reasons first standard way storing sparse matri ces since variety formats used avoid storing zeros best choice format dependent problem architecture second algorithms takes lot code reorganization produce efficient sparse program tuned particular format illustrate points describing two formats classical format called compressed column storage ccs 10 modern one used blocksolve library 11 serve running examples abstract ccs format illustrated fig 1 matrix compressed along columns stored using three arrays colp vals rowind values nonzero elements column j stored array section 1 row indices nonzero elements column j stored 1 illustrated fig 1b matrix many zero columns zero columns stored results called compressed compressed column storage format cccs case another level indirection added colind array compress column dimension well fig 1c colp vals rowind colp vals rowind 3 6 8a example matrix b ccs format c cccs format figure 1 illustration compressed column storage format general simple format however exploit application specific structure matrix format used blocksolve library exploits structure present sparse matrices arise solution pdes multiple degrees freedom figure 2a adapted 11 illustrates grid would arise 2d linear multicomponent finiteelement model three degrees freedom discretization point degrees freedom illustrated three dots discretization point stiffness matrix model would groups rows identical column structure called inodes identical nodes nonzero values inode gathered dense matrix shown fig 2c matrices also rich cliques partition cliques shown fig 2a using dashed rectangles library colors contracted graph induced cliques reorders matrix shown fig 2b symmetric matrices lower half stored together diagonal black triangles along diagonal correspond dense matrices induced cliques gray offdiagonal blocks correspond sparse blocks matrix stored using inodes notice matrix stored collection smaller dense matrices fact helps reduce sparse storage overhead improve performance matrixvector products parallel execution color divided among processors therefore processor receives several blocks contiguous rows processor offdiagonal blocks actually stored column column inodes performing matrixvector product storage organization makes processing messages containing nonlocal values vector efficient addition allows overlap computation communication separating matrixvector product portion accesses local data one deals nonlocal data incoming messages main algorithm consider paper matrixvector product core computation iterative solvers linear systems consider performance mflops sparse matrixvector product single processor ibm sp2 variety matrices storage formats shown table 1 descriptions matrices formats found appendix boxed numbers indicate highest performance given matrix clear set experiments single format appropriate kinds problems demonstrates difficulty developing sparse blas sparse matrix computations even limit formats table 1 one still provide least 6 versions sparse matrixmatrix product color 1 color 0 color 2 subgraph generated 2d linear finite element model b colorclique reordering block solve library b f e c e f c inode storage figure 2 illustration blocksolve format assuming result stored single format lack extensibility sparse blas approach addressed objectoriented solver libraries like petsc library argonne 4 libraries provide templates certain class solvers example krylov space iterative solvers allow user add new formats providing hooks implementations algebraic operations matrixvector product however many cases implementations matrixvector products quite tedious case blocksolve library also libraries useful developing new algorithms radically different solution generate sparse matrix programs using restructuring compiler technology compiler given dense matrix program declarations matrices actually sparse responsible choosing appropriate storage formats generating sparse matrix programs idea explored bik 6 7 approach limited simple sparse matrix formats representative used highperformance codes intuitively trade ability handle variety formats ability compile arbitrary loop nests taken different approach previously shown efficient sparse sequential code generated variety storage formats doall loops loops reductions 13 14 approach based viewing arrays relations execution loop nests evaluation relational queries demonstrated method describing storage formats access methods general enough specify name diagonal coordinate crs itpack jdiag bs95 small 21972 8595 16000 7446 21818 2921 685 bus 1133 5379 20421 4869 31406 2475 gr memplus 0268 4648 15299 0250 12111 4138 sherman1 table 1 performance sparse matrixvector product variety formats yet specific enough allow important optimizations since class doany loops covers matrixvector matrixmatrix products also important kernels within highperformance implementations direct solvers incomplete preconditioners allows us address needs number important applications one think sparse code generator providing extensible set sparse blas codes used implement variety applications like dense blas routines parallel execution one need specify data computation partitioned information call distribution relation come variety formats case sparse matrix formats distribution relation formats also application dependent case regular blockcyclic distributions distribution relations specified closedform formula allows ownership information computed compiletime however regular distributions might provide adequate loadbalance many irregularly structured applications hpf2 standard 9 provides two kinds irregular distributions generalized block indirect generalized block distribution processor receives single block continuous rows suggested standard processor hold block sizes processors distribution relation replicated permits ownership determined without communication indirect distributions general user provides array map element mapi gives processor ith row assigned map array distributed variety ways however require communication determine ownership nonlocal data chaos library 15 allows user specify partitioning information providing list row indices assigned processor list indices transferred distributed translation table equivalent map array partitioned block wise scheme general indirect scheme used hpf2 also requires communication determine ownership build translation table already discussed partitioning scheme used blocksolve library somewhat different general generalized block distribution provided hpf2 yet structure indirect distribution furthermore distribution relation blocksolve library replicated since processor usually receives small number contiguous rows goal provide parallel code generation strategy following properties ffl strategy depend fixed set sparse matrix formats ffl depend fixed set distributions ffl system extensible possible add new formats without changing overall code generation mechanism ffl time generality come expense performance compiler must exploit structure available sparse matrix partitioning formats solve problem extend relational approach generation parallel sparse code starting dense code specification sparse matrix formats data partitioning information view arrays distributed relations parallel loop execution distributed query evaluation addition different ways representing partitioning information regular irregular unified viewing distribution maps relations outline rest paper section 2 outline relational approach sequential sparse code generation section 3 describe sparse parallel code generation algorithm section 4 present experimental evidence advantages approach section 5 presents comparison previous work section 6 presents conclusions ongoing work relational model sparse code generation consider matrixvector product suppose matrix vector x sparse vector dense execute code efficiently necessary perform iterations ij xj zero set iterations described following set constraints 1 first row represents loop bounds constraints second row associate values array indices example predicate ai j constraints value j finally constraints third row specify iterations update nonzero values problem compute efficient enumeration set iterations specified constraints 1 iterations need efficient access corresponding entries matrices vectors since constraints linear sets computed convex cannot use methods based polyhedral algebra fouriermotzkin elimination 2 enumerate sets approach based relational algebra models x relations tables hold tuples array indices values conceptually relation corresponding sparse matrix contains zero nonzero values view iteration space loop relation hi ji tuples write first two rows constraints 1 following relational query relational algebra notation summarized appendix test elements sparse arrays x nonzero use predicates nzai j nzxj notice dense nzy evaluates true array therefore constraints third row 1 rewritten predicate p called sparsity predicate use algorithm bik wijshoff 6 7 compute sparsity predicate general using definition sparsity predicate finally write query defines indices values sparse computation oe relational algebra selection operator reduced problem efficiently enumerating iteration points satisfy system constraints 1 problem efficiently computing relational query involving selections joins problem turn solved determining efficient order joins 4 performed determining joins implemented decisions depend storage formats used sparse arrays 21 describing storage formats following ideas relational database literature 16 20 sparse storage format described terms access methods properties unlike database relations usually stored flat collections tuples sparse storage formats hierarchical structure must exploited efficiency example ccs format provide way enumerating row indices without first accessing particular column use following notation describe hierarchical structure array indices means given column index j access set hi vi tuples row indices values matrix operator used denote hierarchy array indices term hierarchy j v example programmer must provide methods search enumerate indices level must specify properties methods cost search whether enumeration produces sorted output methods properties used determine good join orders join implementations relational query extracted program described 14 way describing storage formats compiler access methods properties solves extensibility problem variety storage formats described compiler compilation strategy depend fixed set formats details formats specified compiler see 13 indices permutations kinds index translations easily incorporated framework suppose permutation p stored using two integer arrays perm iperm represent permutation inverse view p relation tuples hi 0 original index 0 permuted index suppose rows matrix example permuted using p view relation hi tuples query sparse matrixvector product sparsity predicate 23 summary highlights ffl arrays sparse dense relations ffl access methods define relation view data structures implement particular format ffl view loop execution relational query evaluation ffl query optimization algorithm needs know highlevel structure relations provided access methods actual implementation eg role colp rowind arrays ccs storage ffl permutations also handled compiler ffl compilation algorithms independent particular set storage formats new storage formats added compiler generating parallel code ancourt et al 1 described problem generating spmd code dense hpf programs reduced computation expressions polyhedral algebra describe problem generating sparse spmd code loop nest reduced problem evaluating relational algebra queries distributed relations section 31 describes distributed arrays represented section 32 describes distributed query translated sequence local queries communication statements section 33 discuss code generation algorithm used context blocksolve data structures 31 representing distributed arrays uniprocessor case relations highlevel views underlying data structures parallel case relation view partitions fragments stored processor formats fragments defined using access methods outlined sec 21 problem must address describing distributed relations fragments lets start following simple example ffl matrix partitioned row processor p gets fragment matrix p ffl let j row column indices array element original matrix let 0 j 0 corresponding indices fragment p partition row column indices j global row index whereas 0 thought local row offset translate 0 processor keeps integer array ind p ind p 0 processor keeps list global row indices assigned represent partition notice processor p array ind p viewed relation ind p 0 local fragment matrix also viewed relation p define global matrix follows projection operator defined appendix b case processor p carries information translates fragment p contribution global relation situations processors p might translation information fragment stored p good example distributed translation table used chaos library 15 suppose global indices fall range number processors let e given global index index owner processor p local offset 0 stored processor processor q holds array hp indexed need general way representing index translation schemes key view index translation relation distributed relation first case global relation defined example chaos library relation defined ind q h view mentioned array hp 0 tuples relation blocki q h shorthand constraints 8 9 defined index translation relation indi ind defined example 10 11 similarly define global relations x vectors matrixvector product assuming distributed way rows general distributed relations described inda r distributed relation r p fragment processor p ind globaltolocal index translation relation index translation relation different different arrays assume always specifies 11 mapping global index pair hp 0 notice example partitioning ind relation 10 11 satisfy definition 15 call 15 fragmentation equation specify distribution computation recall iteration set loop also represented relation ii j matrixvector product example p access methods fragmentation colp global relations vals lowlevel data structures rowind p bernoulli compiler hpf distributed arrays alignmentdistribution compiler figure 3 flow information hpf bernoulli compiler could require user supply full fragmentation equation would burdensome user would provide local iteration set p set really determined compiler using policy ownercomputes rule addition relation stored need allow multiple storage formats mechanisms independent policy used determine distribution relation iterations given distribution relation ind define local iteration set p simple definition allows us treat iteration set relation uniformly together relations question notice fragmentation equation 15 explicit alignmentdistri bution scheme used hpf bernoulli compiler global relations described hierarchy views first local fragments defined access methods views lowlevel data structures global relations defined views local fragments fragmentation equation hpf alignment distribution provide mapping global indices proces sors full globaltolocal index translation local storage layout full index translation derived compiler removes user responsibility flexibility defining local storage formats difference flow information hpf bernoulli compiler illustrated fig 3 mistake user may specify inconsistent distribution relations ind incon sistencies general detected runtime example verified runtime user specified distribution relation ind fact provides 11 onto map problem unique framework hpf valuebased distributions 21 similar problem basically function specified values runtime properties checked runtime possible generate debugging version code check consistency distributions beyond scope paper 32 translating distributed queries let us return query sparse matrixvector product relations x defined 12 13 14 translate distributed query 17 sequence local queries communication statements expanding definitions distributed relations algebraic simplification follows 321 general strategy distributed query literature optimization problem find sites evaluate parts query 17 context say banking database spread across branches bank partitioning relations fixed may optimal query submitted system choice sites might nontrivial ap plications see 20 detailed discussion general distributed query optimization problem case expect placement relations correlated query given us user particular placement iteration space relation tells us query processed query evaluated processor p p p set iterations assigned processor p resolve references global relations x first exploiting fact join case require communication directly translated join local fragments resolve remaining references computing communication sets performing actual communication relations x example outline major steps 322 exploiting collocation order expose fact join done without communi cation expand join using definitions relations assumed index translation relation ind provides 11 mapping global index processor numbers deduce q nothing statement fact aligned 3 5 join translated p notice join global index translated join local offsets sparsity predicate p originally refers distributed relations translated query replace references global relations references local relations 323 generating communication query used p p computes set global indices j x referenced processor join set index translation relation tell us get element recvind p used p tells us elements x must communicated processor p processor q ind relation distributed case chaos library evaluation query 22 might require communication communication also expressed computed framework applying parallel code generation algorithm recursively 324 summary summary represent distributed arrays distributed relations represent globaltolocal index translation relations distributed relations represent parallel doany loop execution distributed query evaluation ffl compiling dense hpf programs ancourt et al 1 describe computation sets communication sets etc described expressions polyhedral algebra derive similar results sparse programs using relational algebra 33 compiling blocksolve formats discussed introduction blocksolve library splits matrix two disjoint data structures collection dense matrices along diagonal shown using black triangles figure 2b offdiagonal sparse portions matrix stored using inode format figure 2c computation matrixvector product x dense matrices along diagonal refer local portions vector x also offdiagonal sparse blocks stored way makes easy enumerate separately elements matrix refer local elements x require communication altogether view matrix stored blocksolve library format sum ad sl snl ffl ad represents dense blocks along diagonal ffl sl represents portions sparse blocks refer local elements x ffl snl represents portions sparse blocks refer nonlocal elements x ad sl snl partitioned row distribution library assigns small number continuous rows processor distribution relation also replicated thus reducing cost computing ownership information handwritten library code compute communication sets index translations products involving ad sl portions matrix access directly local elements x use code generation technology produce code competitive handwritten code straightforward approach start sequential dense matrix dataparallel program matrixvector product since matrix represented three fragments ad sl snl approach essentially computes three matrix vector products performance code discussed next section careful comparison code handwritten code reveals performance code suffers fact even though products involving ad sl require communication still require globaltolocal index translation elements x used computation view ad sl global relations stored global row column indices hide fact local indices x determined directly data structures ad sl redundant index translation introduces extra level indirection accesses x degrades node program performance point automatic approach handling problem however circumvent problem cost increasing complexity input program specifying code products ad sl node program level code product snl still specified global dataparallel level local local p etc local portions arrays snl x global views compiler generates necessary communication index translations product snl mixed specification dataparallel node level programs unique approach example hpf allows programmer escape node program level using extrinsics 9 general sophisticated composite sparse formats one used blocksolve library might require algorithm specification different level dense loop currently exploring ways specifying storage formats get good sequential performance without drop node level programs parts application 4 experiments section present preliminary performance measurements ibm sp2 algorithm studied parallel conjugate gradient 18 solver diagonal preconditioning cg solves large sparse systems linear equations iteratively following terminology chaos project parallel implementation algorithm divided inspector phase executor phase 15 inspector determines set values communicated performs preprocessing executor performs actual computation communication iterative applications cost inspector usually amortized several iterations executor order verify quality compilergenerated code demonstrate benefit using mixed localglobal specification 24 algorithm application measured performance inspector executor following implementations cg algorithm ffl blocksolve handwritten code blocksolve library ffl bernoullimixed code generated compiler starting mixed lo calglobal specification 24 ffl bernoulli naive code generated compiler starting fully dataparallel specification 23 ran different implementations solver set synthetic threedimensional grid problems connectivity resulting sparse matrix corresponds 7point stencil 5 degrees freedom discretization point ran solver 2 4 8 16 32 64 processors ibm sp2 cornell theory center run kept problem size per processor constant theta 30 places 135 theta 10 3 rows 45 theta 10 6 nonzeroes total processor limited number solver iterations 10 tab 2 shows times seconds numerical solution phase executor tab 3 shows overhead inspector phase ratio time taken inspector time taken single iteration executor comparative performance bernoullimixed blocksolve versions verifies quality compiler generated code 24 difference due aggressive blocksolve bernoullimixed bernoulli sec sec diff sec diff table 2 numerical computation times 10 iterations blocksolve bernoullimixed bernoulli indirectmixed indirect table 3 inspector overhead overlapping communication computation done handwritten code currently bernoulli compiler generates simpler code first exchanges nonlocal values x computation inspector bernoullimixed code twice expensive blocksolve code cost still quite negligible 27 executor 10 iterations comparison bernoulli bernoullimixed code illustrates importance using mixed localglobal specification 24 bernoulli code perform redundant work order discover references x fact local require communication amount work proportional problem size number unknowns much larger number elements x actually communicated result inspector bernoulli code order magnitude expensive one blocksolve bernoullimixed implementations performance executor also suffers redundant globaltolocal translation introduces extra level indirection final code even local references x result executor bernoulli code 10 slower bernoullimixed code demonstrate benefit exposing structure distribution relations measured inspector overhead using indirect distribution format hpf2 standard 9 implemented two versions inspectors using support indirect distribution chaos library 15 ffl indirectmixed inspector mixed localglobal specification 24 ffl indirect inspector fully data parallel specification tab 3 shows ratio time taken indirect inspectors time taken single iteration bernoulli executors executor code exactly cases measured executors bernoulli implementations order magnitude difference performance indirectmixed bernoullimixed inspectors due fact indirectmixed inspector perform asymptotically work requires expensive communication setting distributed translation table indirectmixed inspector necessary resolve nonlocal references requires round alltoall communication volume proportional problem size ie number unknowns additionally querying translation table order determine ownership information requires alltoall communication global index j processor queried ownership information even though communication pattern problems limited nearestneighbor connectivity difference indirect bernoulli inspectors pronounced number references translated proportional problem size still indirect inspector perform alltoall communication determine ownership nonlocal data relative effect inspector performance overall solver performance de pends course number iterations taken solver turn depends condition number input matrix get better idea relative performance bernoullimixed indirectmixed implementation range problems plotted fig 4 ratios time indirectmixed implementation would take time bernoullimixed implementation would take 8 64 processors range iteration counts 5 k 100 lines fig 4 plot values ratio inspector overhead bernoullimixed version r inspector overhead indirectmixed version k iteration count simple calculation shows would take 77 iterations indirectmixed solver 64 processors get within 10 performance bernoullimixed 8 processors number 43 iterations get within 20 would take 21 39 iteration 8 64 processors respectively data demonstrate inspector cost somewhat amortized iterative solver still important exploit structure distribution relations lead order magnitude savings inspector cost improves overall performance solver also noted indirectmixed version slower two bernoulli versions also requires programming effort compiler starts specification level dense loops 23 24 whereas hpf compiler needs sequential sparse code input target class problems sparse doany loops approach results better quality parallel code reducing programming effort indirectmixed bernoullimixed number iterations figure 4 effect problem conditioning relative performance 5 previous work closest alternative work combination biks sparse compiler 6 7 work specifying compiling sparse codes hpf fortran 19 21 22 one could use sparse compiler translate dense sequential loops sparse loops fortran vienna fortran compiler used compile sparse loops however biks work work done ujaldon et al reducing inspector overheads sparse codes limits user fixed set sparse matrix storage distribution formats reducing possibilities exploiting problemspecific structure 6 conclusions presented approach compiling parallel sparse codes userdefined data struc tures starting doany loops approach based viewing parallel doany loop execution relational query evaluation sparse matrices distribution information distributed relations relational approach general enough represent variety storage formats however generality come expense performance able exploit properties distribution relation order produce inexpensive inspectors well produce quality numerical code executors experimental evidence shows important achieving performance competitive handwritten library codes far focused efforts versions iterative solvers conjugate gradient algorithm use incomplete factorization preconditioners core operation solvers sparse matrixvector product product sparse matrix skinny dense matrix currently investigating techniques used automatic generation highperformance codes operations matrix factorizations full incomplete triangular linear system solution r linear algebra framework static hpf code distribution scanning polyhedra loops global optimizations parallelism locality scalable parallel machines solving alignment using elementary linear algebra advanced compiler optimizations sparse computations automatic data structure selection transformation sparse matrix computations quality numerical software assessment enhancement high performance fortran forum computer solution large sparse positive definite systems users manual scalable library software parallel solution sparse linear systems algorithm 586 itpack 2c fortran package solving large sparse linear systems adaptive accelerated iterative methods compiling parallel sparse code userdefined data structures relational approach sparse matrix compilation database management systems solving elliptic problems using ellpack kyrlov subspace methods supercomputers new dataparallel language features sparse matrix computations principles database knowledgebase systems distributed memory compiler design sparse problems tr solving elliptic problems using ellpack principles database knowledgebase systems vol krylov subspace methods supercomputers scanning polyhedra loops global optimizations parallelism locality scalable parallel machines runtime compilation techniques data partitioning communication schedule reuse advanced compiler optimizations sparse computations automatic data structure selection transformation sparse matrix computations database management systems matrix market algorithm 586 itpack 2c fortran package solving large sparse linear systems adaptive accelerated iterative methods computer solution large sparse positive definite distributed memory compiler design sparse problems solving alignment using elementary linear algebra ctr chunyuan lin yehching chung jenshiuh liu efficient data compression methods multidimensional sparse array operations based ekmr scheme ieee transactions computers v52 n12 p16401646 december yuan lin david padua automatic parallelization sparse irregular fortran programs1this work supported part army contract dabt6395c0097 army contract n6600197c8532 nsf contract mip9619351 partnership award ibm work necessarily representative positions policies army government scientific programming v7 n34 p231246 august 1999 roxane adle marc aiguier franck delaplace toward automatic parallelization sparse matrix computations journal parallel distributed computing v65 n3 p313330 march 2005 chunyuan lin yehching chung data distribution schemes sparse arrays distributed memory multicomputers journal supercomputing v41 n1 p6387 july 2007 eunjin im katherine yelick richard vuduc sparsity optimization framework sparse matrix kernels international journal high performance computing applications v18 n1 p135158 february 2004 chunyuan lin jenshiuh liu yehching chung efficient representation scheme multidimensional array operations ieee transactions computers v51 n3 p327345 march 2002 chunyuan lin yehching chung jenshiuh liu efficient data parallel algorithms multidimensional array operations based ekmr scheme distributed memory multicomputers ieee transactions parallel distributed systems v14 n7 p625639 july