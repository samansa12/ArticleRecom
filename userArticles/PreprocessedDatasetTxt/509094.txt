tuning strassens matrix multiplication memory efficiency strassens algorithm matrix multiplication gains lower arithmetic complexity expense reduced locality reference makes challenging implement algorithm efficiently modern machine hierarchical memory system report implementation algorithm uses several unconventional techniques make algorithm memoryfriendly first algorithm internally uses nonstandard array layout known morton order based quadtree decomposition matrix second dynamically select recursion truncation point minimize padding without affecting performance algorithm virtue cache behavior morton ordering technique critical performance combination done code multiplies effectivenessperformance comparisons implementation competing implementations show implementation often outperforms alternative techniques 25 however also observe wide variability across platforms across matrix sizes indicating time single implementation clear choice platforms matrix sizes also note time required convert matrices tofrom morton order noticeable amount execution time 5 15 eliminating overhead reduces execution time b introduction central role matrix multiplication building block numerical codes generated significant amount research techniques improving performance basic operation several efforts 3 6 12 13 14 19 focus algorithms whose arithmetic complexity work supported part darpa grant dabt639810001 nsf grants cda972637 cda95 12356 duke university equipment donation intel corporations technology education 2000 program chatterjee partially supported nsf career award ccr9501979 lebeck partially supported nsf career award mip9702547 log 2 7 instead conventional 3 algorithm strassens algorithm 23 matrix multiplication variants practical algorithms classic examples theoretically highperformance algorithms challenging implement efficiently modern highend computers deep memory hierarchies strassens algorithm achieves lower complexity using divideandconquer approach unfor tunately technique two potential drawbacks first division proceeds level single matrix elements recursion overhead measured instance recursion depth additional temporary storage becomes significant reduces performance overhead generally limited stopping recursion early performing conventional matrix multiplication submatrices recursion truncation point 13 second division step must efficiently handle oddsized matrices solved one several schemes embedding matrix inside larger one called static padding decomposing submatrices overlap single row column called dynamic overlap performing special case computation boundary cases called dynamic peeling previous implementations addressed two drawbacks independently present novel solution simultaneously addresses issues specifically dynamic peeling introduced method avoid large amounts static padding large amount static padding artifact using fixed recursion truncation point minimize padding dynamically selecting recursion truncation point range sizes however scheme induce significant variations performance using canonical storage scheme columnmajor matrices using hierarchical matrix storage scheme dynamically select recursion truncation point within range sizes ensures high stable performance computations leaf recursion tree limits amount static padding measured execution times implementation modgemm two alternative im plementations dgefmm uses dynamic peeling 13 dgemmw uses dynamic overlap 6 dec alpha sun ultrasparc ii results show wide variability performance three implementations alpha implementation modgemm ranges 30 slower 20 faster dgefmm matrix sizes 150 1024 ultra mod gemm generally faster 25 dgefmm large matrices 500 larger dgefmm generally faster small matrices 25 also determine time convert matrices tofrom morton order ranges 5 15 total execution time eliminating conversion time assuming matrices already morton order modgemm outperforms dgefmm nearly matrix sizes alpha ultra greater benefits ultra remainder paper organized follows section 2 reviews conventional description strassens algorithm section 3 discusses implementation issues affect memory efficiency solutions issues section 4 presents performance results code section 5 cites related work compares techniques section 6 presents conclusions future work background strassens original algorithm 23 usually described following divideandconquer form let b two n theta n matrices n even integer partition two input matrices b result matrix c quadrants follows 11 12 21 22 1 symbol ffl equation 1 represents matrix multiplication compute four quadrants result matrix follows 22 ffl b 21 paper discuss implement winograds variant 7 strassens algorithm uses seven matrix multiplications 15 matrix additions wellknown minimum number multiplications additions possible recursive matrix multiplication algorithm based division quadrants division matrices quadrants follows equation 1 computation proceeds follows c 22 u 6 compared original algorithm noteworthy feature winograds variant identification reuse common subexpressions shared computations responsible reducing number additions also contribute worse locality reference unless special attention given aspect computation discuss paper numerical issues concerning fast matrix multiplication algorithms covered elsewhere 10 21 interface order stay consistent previous work area permit meaningful comparisons implementation winograds variant follows calling conventions dgemm subroutine level 3 blas library 5 thus implementation computes c ff scalars opa theta k real matrix opb k theta n real matrix c theta n real matrix opx either x x matrices b c stored columnmajor order leading dimensions lda ldb ldc respectively 3 memory efficiency issues techniques useful practice implementation strassenwinograd algorithm must answer three questions truncate recursion handle arbitrary rectangular matrices lay array data memory promote better use cache memory questions independent although past implementations treated thus review implementation issues identify possible solution strategies justify specific choices 31 recursion truncation point seven products computed recursively invoking strassens algorithm smaller sub problems switching conventional algorithm matrix size called recursion truncation point 13 strassens construction longer advantageous one estimate running time counting arithmetic operations recursion truncation point would around 16 however empirically observed value parameter least order magnitude higher discrepancy direct result poor algorithmic locality reference strassens algorithm implementations strassens algorithm encountered use empirically chosen cutoff criterion determining matrix size terminate recursion 32 handling arbitrary matrices divideandconquer techniques effective matrices evenly partitioned recursive invocation algorithm first note rectangular matrices present particular problem partitioning matrix dimensions even trouble arises encounter matrices one dimensions odd size several possible solutions problem ffl simplest solution static padding pad n theta n matrices additional rows columns containing zeros padded n 0 theta n 0 matrix satisfies evendimensions condition level recursion ie n recursion depth strassens original solution solution often quoted algorithms textbooks however statically predetermined value overhead static padding become quite severe adding almost three times number original matrix elements worst case furthermore naive implementation idea arithmetic done additional zero elements pure overhead finally based relation n 0 interference phenomena reduce performance interference effects mitigated using nonstandard data layouts discuss section 33 ffl second solution dynamic peeling peels extra row column level separately adds contributions overall solution later fixup computation 13 eliminates need extra padding reduces portion matrix strassens algorithm applies thus reducing potential benefits recursive strategy fixup computations matrixvector operations rather matrixmatrix operations limits amount reuse reduces performance ffl third solution dynamic overlap finesses problem subdividing matrix submatrices conceptually overlap one row column computing results shared row column subproblems ignoring one copies interesting solution complicates control structure performs extra computations ideally would like avoid implementation complexity dynamic peeling dynamic overlap possibility excessive static padding figure 1 mortonordered matrix layout small square theta tile laid contiguously columnmajor order number tile gives relative position sequence tiles 33 data layout significant fraction computation strassenwinograd algorithm occurs routine multiplies submatrices recursion truncates performance matrix product largely determined cache behavior issue explicitly considered previous work implementing fast recursive matrix multiplication algorithms default columnmajor layout array data assumed primary condition performance choose tile size tiles fit firstlevel cache thus avoiding capacity misses 11 easily achieved using tile sizes range shown figure 2 second achieve performance stability varies also important tile contiguous memory thus avoiding selfinterference misses 17 given hierarchical nature algorithm decomposition quadrants within quadrants within hierarchical layouts morton ordering 8 naturally suggest storing matrices operational definition morton ordering follows divide original matrix four quadrants lay quadrants memory order nw ne sw se submatrix larger side laid recursively using morton ordering theta tile laid using columnmajor ordering see figure 1 secondary benefit keeping tiles contiguous memory matrix addition operations performed single loop rather two nested loops thus reducing loop overheads 34 connections among issues begin observing recursion truncation point determines amount padding since matrix must evenly divide recursive invocations algorithm also note recursion truncation point multiplying theta submatrices using conventional algorithm carefully selecting truncation point minimize amount padding required figure 2 shows effects tile size selection padding four lines correspond size size padding tilesize best case padding tilesize best case padding matrix size figure 2 effect tile size padding minimizing padding tiles chosen range 64 original matrix size n padded matrix size n 0 tile size chosen minimize padding padded matrix size n 0 fixed tile size tile size achieves minimum padding figure demonstrates ability select range tile sizes dramatically reduce amount extra storage making independent matrix size n contrast fixed tile size require significant padding proportional matrix size worst case consider square matrix size 513 fixed tile size 32 static padding requires padded matrix size 1024 nearly twice original matrix dimension contrast flexibility choosing tile size allows us select tile size 33 requires padding 15 worst case amount extra elements dimension padded matrix size 528 recursively divided four times achieve 33 theta 33 tiles however exploit flexibility tile size selection ensure performance matrix multiplication algorithm tiles sensitive choice tile size important consideration since significant portion algorithms computation occurs routine multiplies tiles figure 3a figure 3b show performance matrix multiplication c theta b varies function relation tile size leading dimension line graph corresponds different submatrix size 24 28 32 submatrices operate chosen base matrix follows noncontiguous submatrices created setting leading dimension submatrix leading dimension base matrix corresponds xaxis contiguous submatrices formed setting leading dimension submatrix tile size corresponds line graph figure 3 see contiguous submatrices exhibit much stable performance noncontiguous submatrices expected noncontiguous submatrices exhibit dramatic drop performance base matrix power two 256 case due selfinterference alpha contiguous submatrices clearly outperform noncontiguous performance differential pronounced ultra however instability noncontiguous matrix size500700900mflops contiguous submatrices noncontiguous submatrices matrix size500700900mflops contiguous submatrices noncontiguous submatrices dec miata b sun ultra figure 3 effect data layout matrix multiply performance submatrices still exists results justify use morton ordering within implementation implementation details envision three different alternatives storage layout input output matrices assumed laid morton order interface level input output matrices copied columnmajor storage morton order interface level conversion morton order done incrementally across levels recursion first alternative feasible library implementation among two options converting matrices morton order top level easier implement performed relatively fast practice 5 15 total execution time see section 41 incorporate necessary matrix transposition operations conversion columnmajor morton order handy requires single core routine strassenwinograd algorithm alternative solution requires multiple code versions indirection using pointers handle cases correctly choosing tile sizes range described general induce small constant amount padding implementation explicitly padded matrix zeros performed redundant computation pad could afford pad guaranteed small alternative scheme avoiding overhead would created tiles uneven sizes required control overhead keep track tile start offsets similar pieces information handle rectangular cases treating two dimensions separately tile dimension chosen minimize padding dimension method choosing tile dimension independently works ratio columns rows rows columns within certain limits highly rectangular matrices pose problem two recommended tile dimensions may require different levels recursion following example illustrates difficulties method matrices consider highly rectangular matrix dimensions 1024x256 choose tile dimensions independently first consider matrix 1024 rows choose number rows x x x lean wide b b wide lean b figure 4 handling highly rectangular matrices tile minimizes row padding ie number additional rows case 32 chosen recursion required unfold depth 5 next consider matrix 256 columns number columns tile chosen minimize number columns padded choose 32 recursion must unfold depth 3 clearly naively choosing two tile dimensions independently work highly rectangular matrices since unfold recursion depth 5 depth 3 overcome limitation matrix divided submatrices submatrices require depth recursion unfolding dimensions matrix product reconstructed terms submatrix products given matrix ffl wide meaning columnstorows ratio exceeds desired ratio ffl lean meaning rowstocolumns ratio exceeds desired ratio ffl wellbehaved meaning columnstorows ratio rowstocolumns ratio within desired ratio since two input matrices b one forms total nine possible combinations figure 4a figure 4b show two examples input matrices b divided result c reconstructed results submatrix multiplications finally note 1 0 common values ff fi parameters order avoid performing extra arithmetic parameter values core routine strassenwinograd algorithm computes ffl b set c temporary otherwise postprocess compute c ff postprocessing necessary section compares performance implementation modgemm strassens algorithm previous implementation uses dynamic peeling dgefmm 13 use au thors original code previous implementation uses dynamic overlap dgemmw 6 measure execution time various implementations 500 mhz dec alpha miata 300 mhz sun ultra 60 alpha machine 21164 processor 8kb directmapped level 1 cache 96kb 3way associative level 2 cache 2mb directmapped level 3 cache 512mb main memory ultra two ultrasparc ii processors level 1 cache 2mb level 2 cache 512mb main memory use one processor ultra 60 matrix size080100120140normalized execution time modgemm matrix size080100120140normalized execution time modgemm vs dgefmm b dgemmw vs dgefmm figure 5 performance strassen winograd implementations dec miata timed execution implementation using unix system call getrusage matrix sizes ranging 150 1024 dgefmm use empirically determined recursion truncation point 64 matrices less 500 compute average invocations algorithm overcome limits clock resolution execution times larger matrices large enough overcome limitations reduce experimental error execute experiments three times matrix size use minimum value comparison programs compiled vendor compilers cc f77 fast option sun compilers workshop compilers 42 dec compilers dec c v56071 digital fortran 77 v501383678f figure 5 figure 6 show results alpha ultrasparc respectively report results execution time normalized dynamic peeling implementation dgefmm alpha see dgefmm generally outperforms dynamic overlap dgemmw see figure 5b contrast implementation modgemm varies 30 slower 20 faster dgefmm also observe modgemm outperforms dgefmm mostly range matrix sizes 500 800 whereas dgefmm faster smaller larger matrices finally comparing figure 5a figure 5b see modgemm generally outperforms dgemmw results quite different ultra see figure 6 striking difference performance dgemmw see figure 6b outperforms modgemm dgefmm matrix sizes ultra another significant difference modgemm generally faster dgefmm large matrices 500 larger dgefmm generally faster small matrices important observation results variability performance across platforms across matrix sizes ongoing research efforts targeted understanding variations section 42 reports preliminary findings following section analyzes penalty converting morton order matrix size080100120140normalized execution time modgemm matrix size080100120140normalized execution time modgemm vs dgefmm b dgemmw vs dgefmm figure performance strassen winograd implementations sun ultra matrix size50150 conversion cost age execution matrix size50150 conversion cost age execution dec miata b sun ultra figure 7 morton conversion time percentage total execution time matrix size080100120140normalized execution time modgemm matrix size080100120140normalized execution time modgemm dec miata b sun ultra figure 8 performance modgemm without matrix conversion 41 morton conversion time important aspect implementation recursive data layout provides stable performance dynamic tile size selection previous performance results include time convert two input matrices columnmajor morton order convert output matrix morton order back columnmajor figure 7a figure 7b show cost conversion percentage entire matrix multiply platforms graphs see morton conversion accounts 15 overall execution time small matrices approximately 5 large matrices results show morton conversion noticeable fraction execution time eliminating conversion cost ie assuming matrices already morton order produces commensurate improvements performance implementation figure 8a figure 8b shows without conversion costs modgemm indeed execute faster increases number matrix sizes outperforms dgefmm specifically alpha figure 8a modgemm superior matrix sizes larger 500 ultra figure 8b see matrix sizes dgefmm outperforms modgemm furthermore without morton conversion modgemm competitive dgemmw outperforms many matrix sizes 42 cache effects initial efforts gain insight performance variability implementation begins analysis cache behavior present preliminary results used atom 22 perform cache simulations 16kb directmapped cache blocks dgefmm modgemm implementations figure 9 shows miss ratios implementation matrix sizes ranging 500 523 first observation graph modgemm miss ratios 6 2 lower dgefmm 8 matches expec tations second observation unexpected dramatic drop modgemms miss ratio matrix size 513 preliminary investigations using cprof 18 reveal drop due reduction conflict misses matrix size00400080miss figure 9 cache miss ratios 16kb directmapped understand phenomenon consider matrix sizes 505 512 padded matrix size 512 recursion truncation point tile size 32 conventional algorithm applied submatrices 8kb size 32x32x8 correspond four quadrants 64x64 submatrix morton ordering quadrants allocated contiguously memory quadrants separated multiple cache size 16kb conflict cache example since nw sw quadrants separated ne quadrant map locations cache ie first elements nw sw quadrants separated 16kb therefore operations involving two quadrants incur significant number cache misses currently examining ways eliminate conflict misses 5 related work discuss three separate areas related work implementations strassentype algorithms hierarchical schemes matrix storage compiler technology improving cache behavior loop nests 51 implementations previous implementations strassens algorithm include baileys implementation cray 2 3 dgemmw implementation douglas et al 6 dgefmm implementation husslederman et al13 bailey coding fortran used static twolevel unfolding recursion code duplication douglas et al introduced dynamic overlap method handling oddsized dimensions husslederman et al introduced dynamic peeling method implementations careful limit amount temporary storage specifically consider performance memory hierarchy code cases crays issue arise memory system cachebased section 4 gives extensive performance comparisons implementation vs dgefmm dgemmw kreczmar 16 proposes elegant memoryefficient version strassens algorithm based overwriting one input arguments scheme space savings directly applicable two reasons cannot assume input matrices overwritten scheme requires several copying operations reduce performance 52 hierarchical schemes matrix storage wise coauthors 1 24 investigated algorithmic advantages quadtree representations matrices morton ordering also appeared parallel computing literature used load balancing irregular problems 20 recently frens wise 8 discuss implementation recursive 3 matrix multiplication algorithm using hierarchical matrix layout sequence recursive calls unusual manner get better reuse cache carry recursion level single matrix elements truncate recursion reach tile sizes fit upper levels memory hierarchy 53 cache behavior several authors 17 15 4 21 discuss loop transformations tiling attempt reduce number cache misses incurred loop nest thus improve performance loop transformations specific matrix multiplication conventional threeloop algorithm matrix multiplication falls category codes handle lam rothberg wolf 17 investigated modeled influence cache interference performance tiled programs emphasized importance tiles contiguous memory avoid selfinterference misses proposed data copying satisfy condition toplevel conversion columnmajor layout interface level morton ordering used internally viewed logical extension proposal ghosh et al 9 present analytical representation cache misses perfect loop nests use guide selected code optimization problems work like work cited relies linear row columnmajor storage arrays therefore immediately apply code 6 conclusions future work matrix multiplication important computational kernel performance dictate overall performance many applications strassens algorithm matrix multiplication achieves lower arithmetic complexity log 2 7 conventional algorithm 3 cost worse locality reference furthermore since strassens algorithm based divideandconquer implementation must handle oddsize matrices reduce recursion overhead terminating recursion reaches individual matrix elements issues make difficult obtain efficient implementations strassens algorithm paper presented practical implementation strassens algorithm winograd vari ant exploits ability dynamically select recursion truncation point based matrix size efficiently handles oddsized matrices achieve using nonstandard array layout called morton order converting standard layouts eg columnmajor internal morton layout interface level exploiting dynamic selection recursion truncation point minimize padding compare implementation two alternative implementations use dynamic peeling dgefmm 13 dynamic overlap dgemmw 6 execution time measurements dec alpha sun ultrasparc ii reveal wide variability performance three imple mentations alpha implementation modgemm ranges 30 slower 20 faster dgefmm matrix sizes 150 1024 ultra modgemm generally faster dgefmm large matrices 500 larger dgefmm generally faster small matrices eliminating time convert matrices tofrom morton order 5 15 total execution time modgemm outperforms dgefmm nearly matrix sizes ultra matrices alpha future work includes investigating techniques improve performance stability strassens algorithm minimizing code complexity also plan examine effects rectangular input matrices implementation supports interface level 3 blas dgemm routine 2 plan examine performance variety input parameters r experiments quadtree representation matrices extra high speed matrix multiplication cray2 tile size selection using cache organization data layout set level 3 basic linear algebra subprograms gemmw portable level 3 blas winograd variant strassens matrixmatrix multiply algorithm efficient procedures using matrix algorithms cache miss equations analytical representation cache misses accuracy stability numerical algorithms evaluating associativity cpu caches tensor product formulation strassens matrix multiplication algorithm implementation strassens algorithm matrix multiplication ibm engineering scientific subroutine library guide reference memory requirements strassens algorithms cache performance optimizations blocked algorithms cache profiling spec benchmarks case study dynamic partitioning nonuniform structured workloads spacefilling curves data transformations eliminating conflict misses atom system building customized program analysis tools gaussian elimination optimal costs quadtree representation nondense matrices tr extra high speed matrix multiplication cray2 evaluating associativity cpu caches set level 3 basic linear algebra subprograms cache performance optimizations blocked algorithms lapacks users guide atom gemmw tile size selection using cache organization data layout dynamic partitioning nonuniform structured workloads spacefilling curves multilevel blocking cache miss equations autoblocking matrixmultiplication tracking blas3 performance source code data transformations eliminating conflict misses implementation strassens algorithm matrix multiplication accuracy stability numerical algorithms cache profiling spec benchmarks efficient procedures using matrix algorithms experiments quadtree representation matrices ctr siddhartha chatterjee alvin r lebeck praveen k patnala mithuna thottethodi recursive array layouts fast matrix multiplication ieee transactions parallel distributed systems v13 n11 p11051123 november 2002 hossam elgindy george ferizis improving memory access patterns execution strassens matrix multiplication algorithm proceedings 27th australasian conference computer science p109115 january 01 2004 dunedin new zealand k fatahalian j sugerman p hanrahan understanding efficiency gpu algorithms matrixmatrix multiplication proceedings acm siggrapheurographics conference graphics hardware august 2930 2004 grenoble france kang su gatlin larry carter architecturecognizant divide conquer algorithms proceedings 1999 acmieee conference supercomputing cdrom p25es november 1419 1999 portland oregon united states john mellorcrummey david whalley ken kennedy improving memory hierarchy performance irregular applications proceedings 13th international conference supercomputing p425433 june 2025 1999 rhodes greece john mellorcrummey david whalley ken kennedy improving memory hierarchy performance irregular applications using data computation reorderings international journal parallel programming v29 n3 p217247 june 2001 igor kaporin aggregation cancellation techniques practical tool faster matrix multiplication theoretical computer science v315 n23 p469510 6 may 2004 sandeep sen siddhartha chatterjee towards theory cacheefficient algorithms proceedings eleventh annual acmsiam symposium discrete algorithms p829838 january 0911 2000 san francisco california united states lillian lee fast contextfree grammar parsing requires fast boolean matrix multiplication journal acm jacm v49 n1 p115 january 2002 sandeep sen siddhartha chatterjee neeraj dumir towards theory cacheefficient algorithms journal acm jacm v49 n6 p828858 november 2002 paolo dalberto alexandru nicolau adaptive strassens matrix multiplication proceedings 21st annual international conference supercomputing june 1721 2007 seattle washington nico galoppo naga k govindaraju michael henson dinesh manocha lugpu efficient algorithms solving dense linear systems graphics hardware proceedings 2005 acmieee conference supercomputing p3 november 1218 2005 siddhartha chatterjee alvin r lebeck praveen k patnala mithuna thottethodi recursive array layouts fast parallel matrix multiplication proceedings eleventh annual acm symposium parallel algorithms architectures p222231 june 2730 1999 saint malo france mohamed f mokbel walid g aref ibrahim kamel analysis multidimensional spacefilling curves geoinformatica v7 n3 p179209 september chunyuan lin yehching chung jenshiuh liu efficient data parallel algorithms multidimensional array operations based ekmr scheme distributed memory multicomputers ieee transactions parallel distributed systems v14 n7 p625639 july mohamed f mokbel walid g aref irregularity multidimensional spacefilling curves applications multimedia databases proceedings tenth international conference information knowledge management october 0510 2001 atlanta georgia usa siddhartha chatterjee vibhor v jain alvin r lebeck shyam mundhra mithuna thottethodi nonlinear array layouts hierarchical memory systems proceedings 13th international conference supercomputing p444453 june 2025 1999 rhodes greece richard vuduc james w demmel jeff bilmes statistical models empirical searchbased performance tuning international journal high performance computing applications v18 n1 p6594 february 2004