unsupervised learning multiple motifs biopolymers using expectation maximization meme algorithm extends expectation maximization em algorithm identifying motifs unaligned biopolymer sequences aim meme discover new motifs set biopolymer sequences little nothing known advance motifs may present meme innovations expand range problems solved using em increase chance finding good solutions first subsequences actually occur biopolymer sequences used starting points em algorithm increase probability finding globally optimal motifs second assumption sequence contains exactly one occurrence shared motif removed allows multiple appearances motif occur sequence permits algorithm ignore sequences appearance shared motif increasing resistance noisy data third method probabilistically erasing shared motifs found incorporated several distinct motifs found set sequences different motifs appear different sequences single sequence may contain multiple motifs experiments show meme discover crp lexa binding sites set sequences contain one sites meme discover 10 35 promoter regions set e coli sequences b introduction problem addressed work identifying characterizing shared motifs set unaligned genetic protein sequences motif defined pattern common set nucleic amino acid subsequences share biological property interest dna binding sites regulatory protein computer science terminology problem given set strings find set nonoverlapping approximately matching substrings report concerned contiguous motifs biological terms means appearances motif may differ point mutations insertions deletions allowed computer science terms means approximately matching substrings must length simpler version problem given dataset biopolymer sequences believed contain single shared motif locate starting position sequence appearance shared motif describe shared motif report addresses general problem finding describing multiple distinct shared motifs set biopolymer 52 timothy l bailey charles elkan sequences assumed anything known advance width position letter frequencies motifs even many common motifs may exist set sequences several methods presented literature work problems related discovering multiple distinct shared motifs set biological sequences purpose research extend range problems attacked hertz et al 11 presented greedy algorithm discovering single shared motif present set sequences lawrence reilly 14 extended work developing expectation maximization em algorithm solving problem lawrence et al 13 solve related problem discovering multiple distinct motifs number occurrences motif sequence known using gibbs sampling strategy report describes meme new tool intended help discover motifs neither number motifs number occurrences motif sequence known 1 meme incorporates three novel ideas discovering motifs ffl first subsequences actually occur input dna protein sequences used starting points em converges iteratively locally optimal motifs increases likelihood finding globally optimal motifs ffl second heuristic modification em algorithm allows assumption sequence contains exactly one occurrence shared motif removed allows multiple appearances motif occur sequence permits algorithm ignore sequences appearance shared motif increases resistance noisy data ffl third motifs probabilistically erased found allows several distinct motifs found set sequences different motifs appear different sequences single sequence may contain multiple motifs 11 searching tools versus learning tools section explains place meme spectrum sequence analysis tools experts biological sequence analysis may wish skip directly next section searching tools sequence analysis tools may divided two broad cate gories searching tools learning tools grail blastx fasta etc searching tools whereas meme learning tool searching tool also called patternmatching tool takes input one sequences pattern decides pattern matches input sequence pattern may another sequence blastx fasta ii consensus subsequence regular expression defining motif prosearch 12 iii highlevel combination features grail 20 unsupervised learning multiple motifs biopolymers using em 53 learning tools supervised learning tool also called supervised patternrecognition tool takes input set sequences discovers pattern sequences share supervised learning often done humans rather software openended problem harder searching example prosite profiles created amos bairoch personally examining families proteins 2 unsupervised learning tool takes input set sequences discovers pattern sequences share unsupervised learning harder supervised learning space possible patterns much larger pattern discovered required given input sequence unsupervised learning algorithm must simultaneously look cluster input sequences pattern members cluster common performs unsupervised learning output learning tool namely pattern often given search tool order find new sequences exhibit pattern however even members family sequences already known applying learning tool family still useful examining patterns subsets family common give insight structure function evolution 12 expectation maximization em algorithm lawrence reilly 14 introduced expectation maximization method means solving supervised motif learning problem algorithm takes input set unaligned sequences motif length w returns probabilistic model shared motif idea behind method sequence dataset contains single example motif shall refer model data oneoccurrencepersequence model oneper model assumed motif appears starting offset example unknown known subsequences length w sequence starting known offset could aligned since insertions deletions allowed observed frequencies letters column alignment could used model motif fact example motif assumed generated sequence independent discrete random variables observed frequencies letters columns maximum likelihood estimates distributions random variables course since original sequences dataset unaligned offsets known must also estimated em algorithm estimates probability shared motif starts position j sequence dataset given data initial guess description motif probability estimates used reestimate probability letter l column c motif ae lc letter alphabet reestimations done described appendix em algorithm alternately reestimates z ae ae changes little 54 timothy l bailey charles elkan iteration iteration notation z used refer matrix offset refers matrix letter probabilities ae ij pseudocode description basic em algorithm given em starts estimate model parameters ae provided user generated random 1 em dataset w 2 choose starting point ae 3 f 4 reestimate z ae 5 reestimate ae z 6 g change ae ffl 7 return 8 g em algorithm simultaneously discovers model motif sequence independent discrete random variables parameters ae estimates probability possible starting point examples motif sequences dataset z definition 8 likelihood model given training data probability data given model em algorithm finds values model parameters maximize expected likelihood data given model ae missing data z oneoccurrencepersequence model data used lawrence reilly 14 logarithm likelihood n number sequences dataset l length sequences w length shared motif l alphabet sequences ae lj unknown probability letter l position j motif ae l0 unknown probability letter l nonmotif positions f lj observed frequency letter l position j motif f l0 observed l nonmotif positions sequences shown expectation maximization algorithms find values model parameters likelihood function assumes local maximum 7 reasonable assume correct solution problem characterizing shared motif occurs global maximum likelihood function reason else equal parameter values model give higher values likelihood function considered better solutions problem 2 unsupervised learning multiple motifs biopolymers using em 55 13 limitations em oneoccurrencepersequence model em oneper model suffer several limitations first clear choose starting point initial value ae quit trying different starting points makes difficult satisfied correct shared motif found second oneper model assumes sequence dataset contains exactly one appearance shared motif means sequences multiple appearances undercontribute sequences appearances overcontribute characterization motif many sequences appearances motif dataset may make impossible em oneper model find shared motif finally em oneper model assumes one shared motif sequences keep looking motifs characterizing one makes em oneper model incapable finding motifs insertions variable length incapable discovering multiple motifs may occur different sequences given dataset eliminating reducing limitations em oneper model would make method less susceptible noise dataset able find complex patterns data last least useful exploring datasets may contain instances several different motifs algorithm described report meme extends em algorithm overcome limitations described meme chooses starting points system atically based subsequences sequences training dataset allows use either oneper model different model eliminates assumption one sequenceone occurrence allows sequence contain zero one several appearances shared motif call new model n occurrencesperdataset model nper model assumes dataset contains exactly n occurrences motif n specified user finally meme probabilistically erases appearances motif found continues searching shared motifs dataset meme algorithm nper model tested two datasets first dataset combining sequences containing crp binding sites 14 sequences containing lexa binding sites 11 meme discovered lexa binding site first pass crp binding site second pass second dataset contained 231 e coli promoter sequences 9 3 meme discovered tataat ttgaca consensus sequences 4 first second passes respectively demonstrates ability meme avoid local optima tolerate large number sequences contain motif find multiple motifs single dataset 2 meme algorithm meme algorithm core modified version em algorithm 14 pseudocode algorithm given inner loop algorithm based em algorithm run repeatedly different starting points chosen model either oneper model nper model shall refer particular application em algorithm simply em follows starting points derived actual subsequences occur input dataset em run one iteration convergence starting point save time run em produces probabilistic model possible shared motif starting point yields model highest likelihood chosen em run convergence starting point model shared motif thus discovered printed finally appearances shared motif dataset erased outer loop repeats whole process discover shared motifs following sections describe steps algorithm detail 1 meme dataset w nsites passes f 2 3 subsequence dataset f 4 run em 1 iteration starting point 5 derived subsequence 6 choose model shared motif highest likelihood 7 run em convergence starting point 8 generated model 9 print converged model shared motif 10 erase appearances shared motif dataset 11 g 12 g 13 g output meme includes specificity logodds matrix spec logodds matrix l rows w columns calculated spec information content score subsequence calculated summing entries matrix corresponding letters subsequence 5 score gives measure likelihood subsequence instance motif versus instance background together suitable threshold information content score used classify subsequences new sequences part training set 21 using subsequences starting points em given different starting points ie initial letter probability matrices ae em algorithm may converge different final models models local maxima likelihood function described earlier correct model shared motif expected model globally maximizes likelihood function em guaranteed find global maximum local maximum previous unsupervised learning multiple motifs biopolymers using em 57 authors 14 5 recommended using several starting points em choosing model highest likelihood choose starting points discussed detail one might try using randomly chosen letter frequency matrices starting points sequences dataset provide way choose intelligent ones since models motifs allow insertions deletions optimal model must agree well contiguous subsequences sequences datasetthe instances motif sequences good way search space possible starting points em thus convert subsequence length w letter probability matrix use matrix starting point approach used meme since starting point letter frequency matrices obtained subsequences corresponding actual occurrences shared motif close correct letter probability matrix ie model em tend converge global optimum run starting points 6 example suppose unknown optimal value ae shared motif trying discover using meme actually letter position motif consensus sequence tataat presumably sequence something close ie mutations occurs least one sequences dataset reasonable postulate choose starting point em letter probability matrix derived simple manner consensus sequence subsequence similar em tend converge optimal model try subsequences length six example sequences dataset reasonable assume least one close tataat cause em converge optimal model note meme use possible subsequences given length ones actually occur dataset question remains convert subsequence letter probability matrix one cannot simply convert matrix probability 10 letter subsequence 00 others ie convert tataat letter position motif g 58 timothy l bailey charles elkan em algorithm cannot move starting point starting point offset probabilities estimated 00 except subsequences match starting point subsequence exactly cause reestimation letter frequencies yield starting point effective somewhat arbitrary solution fix frequency letter subsequence value fix frequencies letters length alphabet ensures frequencies column sum 10 x close 10 starting point close subsequence results reported paper 05 values x 04 08 worked approximately equally well experimental data shown value x starting point em generated subsequence tataat letter position motif 017 05 017 05 05 017 would highly expensive computationally run em convergence every possible starting point corresponding subsequence length w input dataset turns necessary em converges quickly subsequences similar shared motif best starting point often detected running one iteration em described meme able find shared motifs run one iteration possible subsequence starting point run convergence starting point highest likelihood words meme runs em specified number iterations one iteration results reported subsequence starting point chooses starting point yields highest likelihood runs em convergence starting point since iteration em algorithm takes computation time roughly linear size dataset number subsequences linear size dataset meme takes time 2 n size dataset characters 22 dealing multiple appearances shared motif meme allows user specify either oneper model nper model used oneper model meme uses em algorithm lawrence reilly 14 fit model dataset fit nper model heuristic modification em algorithm used oneper model assumes sequence dataset contains exactly one appearance shared motif characterized assumption determines way offset probabilities reestimated reestimation procedure ensures offset probabilities sequence sum 10 means unsupervised learning multiple motifs biopolymers using em 59 given sequence one appearance shared motif cannot contribute reestimation letter frequencies sequence one appearance additionally sequence appearances shared motifa common event exploring new shared motifsit contributes erroneously reestimation letter frequencies meme modifies em algorithm fitting nper model dataset instead normalizing reestimated offset probabilities sum 10 sequence offset probabilities normalized sum usersupplied value nsites subject constraint single offset probability may exceed 10 normalization done sequences simultaneously sequence sequence intent nsites expected number appearances shared motif dataset nsites set equal number sequences dataset possible nper model get approximately results oneper model dataset one appearance shared motif sequence datasets appearances motif distributed one per sequence meme nper model able choose models assign offset probabilities fashion satisfies two constraints mentioned relaxation one motif appearance per sequence constraint nper model allows meme benefit sequences multiple appearances shared motif also help alleviate problem sequences contain motif blurring characterization nsites lower number sequences dataset meme assign low offset probabilities positions sequence contain motif contrast oneper model must assign offset probabilities summing 10 sequence dataset effect various settings nsites discussed section 43 summary exact value chosen nsites critical necessary know advance exactly many times motif present dataset one side effect allowing single sequence offset probabilities sum 10 long repeated sequences seen meme using n per model though multiple appearances shorter sequence example w 6 sequence aaaaaaaa treated nper model roughly though three appearances sequence aaaaaa nper model might allow offsets 1 2 3 sequence maximum probability 10 oneper model would allow since total offset probability single sequence must sum 10 problematic far surprising find 3 nonoverlapping occurrences sequence aaaaaa find one occurrence sequence aaaaaaaa would like meme search nsites nonoverlapping occurrences motif overcome difficulty meme enforces additional constraint calculating offset probabilities nper model renormalizes offset probabilities w adjacent offsets probabilities sum greater 10 essentially makes nper model treat sequences like aaaaaaaa way oneper model assigning probability 13 three offsets identical subsequences aaaaaa start 23 finding several shared motifs single dataset sequences contains one distinct shared motif em oneper model cannot directly find one motifs similarity em may always converge conserved motif 7 another possibility em may converge model describes part conserved motifits left right side instance meme algorithm solves problem probabilistically erasing shared motif found em repeating em find next shared motif effectively removing motif found meme able find next motif without interference conserved motifs found first manner meme erases motif designed continuous possible new variables w ij defined associate weight position j sequence weights represent probability given position given sequence part motif previously discovered meme weights set initially 10 meme discovers shared motif offset probability appearance motif starts position j sequence assuming independence probability position k sequence part newly discovered motif product 1 j k gamma w k old value w ij updated multiplying probability potential motif overlaps example newly discovered shared motif w ij used reestimating letter frequencies instead summing offset probabilities z ij weighted offset probabilities summed understand weighting scheme effectively erases previously discovered motifs suppose meme discovered one motif looking second suppose position j sequence start appearance first motif found new weights w ij w ijw gamma1 less hence cannot contribute much reestimation ae effectively erased notice position matches discovered motif poorly z ij low weight position remain fairly high degree position erased proportional certainty z ij part previously discovered motif makes meme less sensitive chance similarities match threshold set positions z ij value threshold completely erased 3 experimental results section describes experiments using meme conducted two datasets cases model used meme nper model first dataset unsupervised learning multiple motifs biopolymers using em 61 referred crplexa dataset comprises dna fragments contain binding sites crp lexa regulatory proteins crplexa dataset consists samples crp dataset plus samples lexa dataset described second dataset referred promoter dataset contains samples prokaryotic promoter regions also described detail overview contents datasets given table 1 crp dataset taken stormo hartzell 19 turn derived berg von hippel 3 de crombrugghe et al 6 contains fragments e coli believed contain one crp binding sites dataset contains binding sites verified dnase protection experiments dataset compiled fragments contain putative crp binding sites determined sequence similarity known binding sites fragment dataset contains 105 bases fragments aligned particular way lexa dataset taken table 11 contains 16 dna fragments believed contain one lexa binding sites dataset contains binding sites verified dnase protection experiments dataset compiled additional 11 putative lexa binding sites determined sequence similarity known binding sites also present dataset fragments contain 100 bases preceding 99 bases following transcription start position gene three fragments shorter 200 bases flanking start position gene available one samples lexa dataset overlaps sample crp dataset overlap includes known crp site promoter dataset taken cardon stormo 5 contains 231 fragments believed contain promoter regions dataset originally compiled harley reynolds 9 contained 288 fragments cardon stormo omitted number fragments highly redundant sequences known mutant promoters fragments roughly comprise positions gamma50 10 respect start transcription 8 previous work harley reynolds 9 shown promoter motif seems consist two highly conserved submotifs width 6 separated variablelength spacer spacer usually 15 16 17 bases long although cannot directly model variablelength motif indirectly discovering two highly conserved ends motifs 31 meme discover two different binding site motifs run 5 passes crplexa dataset value w chosen based prior knowledge literature approximate size crp lexa binding sites dna basepairs 9 value nsites chosen arbitrarily half number sequences dataset roughly many footprinted sites type table 1 overview contents datasets dataset samples average length samples proven crp sites proven lexa sites crp lexa crplexa 34 150 19 11 promoter 231 58 na na dataset mentioned previously exact value nsites critical meme discover motifs first pass meme yielded excellent model lexa binding site second pass produced model crp binding site subsequent passes produced models unknown significance results meme crplexa summarized table 2 table 2 models found pass meme crplexa dataset visually summarized consensus sequence derived ae matrix choosing letter highest probability values information content loglikelihood give qualitative idea statistical significance model higher values imply model significant models found lexa crp passes 1 2 meme considerably higher loglikelihood information content models found later passes note pass starting subsequence final consensus imodel loglikelihood model produced first pass meme crplexa identified characterized lexa binding site extremely well quality model judged partly degree correctly identifies known lexa binding sites dataset one way using model produced meme examine values z ij see positions samples dataset given high probabilities start motif meme prints four highest values z ij sample dataset pass table 3 shows values z ij pass 1 meme known lexa binding sites easily seen model found first pass characterizes lexa binding site furthermore values z ij 017 model appears specific lexa binding site consensus sequence model discovered pass 1 meme crplexa dataset also agrees exceedingly well lexa binding site meme prints consensus ie probable letter position motif determined ae pass consensus pass 1 unsupervised learning multiple motifs biopolymers using em 63 matches consensus reported 11 perfect dna palindrome table 3 values z ij model found meme pass 1 crplexa dataset positions known lexa sites virtually known sites high values z ij compared rest positions samples table shows positions known sites site 1 site 2 site 3 values z ij model positions positions values z ij 017 although site position 112 colicin e1 sequence z ij value 005 one four highest z ij values sequence proven sites known hima uvrc z ij positions samples low less 00001 sample site cloacin df13 97 0998684 colicin e1 97 0948441 112 0051543 colicin ia 99 0998709 colicin ib 99 0990472 reca 71 0999987 recn 71 0999988 93 0865704 111 0134281 sula 85 0999990 umudc 91 0999931 uvra uvrb 71 0999972 uvrd 102 0998539 colicin 34 0683563 48 0314723 mucab hima uvrc indicates site known sequence similarity known sites another way seeing well model learned pass 1 meme characterizes lexa binding sites plot information content score subsequence input data figure 1 shows information content scores crp lexa samples first pass model scores zero set zero figure make easier interpret easily seen model gives known binding sites high scores subsequences receive low scores next pass meme discovers crp motif consensus sequence reports pass 2 ttttttgatcggtttcacac agrees well consensus found oneper model reported 14 significantly model characterizes crp motif well judging values z ij various positions samples dataset table 4 shows values z ij found pass 2 crplexa dataset according 14 crp dataset contains 24 known crp binding sites 18 verified protection experiments value z ij eight 099 model eleven z ij values 01 turns three samples lexa dataset also contain score base number information content scores input subsequences known lexa sites figure 1 information content score subsequence crplexa dataset using specificity matrix found pass 1 meme crp samples short curves top lexa samples long curves bottom vertical scale highest peak 243 bits values zero set zero unsupervised learning multiple motifs biopolymers using em crp binding sites sample labeled colicin e1 lexa dataset actually sequence overlaps sample labeled cole 1 crp dataset overlap contains crp motif lexa samples colicin ia colicin ib also appear contain crp sites virtually identical colicin e1cole 1 crp site sites z ij 0999 extremely high overrepresentation particular version crp binding site model learned pass 2 seems biased towards representing version binding site present colicin genes may explain model fit crp sites equally well table 4 values z ij model found meme pass 2 crplexa dataset positions known crp sites 24 known crp sites eight high values z ij twelve stated bound values z ij among top four z ij values given sequence three last three sites labeled b c actually lexa dataset crp dataset sequence named colicin e1 actually gene cole 1 overlaps crp site region site colicin ia may reported previously colicin ib site previously reported lexa site sample site ecocya 50 0006001 ecodeop 7 0999845 ecogale 42 0497545 ecoilvbpr ecomale 14 0997871 ecomalk 29 000129 61 0035443 ecoompa 48 0177722 ecotnaa 71 0999222 pbrp4 53 0004511 tdc 78 0506702 colicin e1 27 b 0999186 colicin ia 13 c 0999692 colicin ib 13 0999333 indicates site known sequence similarity known sites b lexa dataset sample overlaps crp sample cole 1 c site may reported previously apparent crp site may confused lexa site varley boulnois 21 hertz et al 11 figure 2 shows information content scores crplexa dataset computed specificity matrix learned pass 2 meme although score base number information content scores input subsequences known crp sites figure 2 information content score subsequence crplexa dataset using specificity matrix found pass 2 meme crp samples short curves top strong match model three colicin samples lexa dataset seen second third fourth long curves vertical scale highest peak 1892 bits values zero set zero model well defined pass 1 clearly matches known crp sites large degree 32 meme discover two parts single binding site run 5 passes promoter dataset 231 value w chosen based prior knowledge derived literature approximate size gamma10 gamma35 regions promoters value nsites chosen based assumption sample dataset contains promoter first pass meme yielded model whose consensus tataat known gamma10 region consensus second pass produced model whose consensus tttaca unsupervised learning multiple motifs biopolymers using em 67 close conventional gamma35 region consensus ttgaca passes produced models unknown significance results meme promoter dataset summarized table 5 table 5 models found pass meme promoter dataset summarized consensus sequences gamma10 gamma35 region models found first two passes meme much higher loglikelihood information content models found pass starting subsequence final consensus model loglikelihood models learned first two passes meme promoter dataset applied first thirty samples dataset information content score subsequence dataset plotted figures 3 4 base corresponding start transcription sample position 50 horizontal axis plot column peaks position 37 figure 3 shows model identifies gamma10 consensus region promoters column peaks position 15 figure 4 confirms second model identifies gamma35 region promoters even though consensus sequence slightly different generally accepted one 4 robustness meme algorithm crplexa dataset promoter dataset also used test usefulness various separate ideas entering design meme algorithm evaluate sensitivity algorithm particular values chosen several parameters overall algorithm appears gratifyingly robust except noted meme run using nper model 41 subsequencederived starting points work well em idea running em one iteration starting points derived possible subsequence input dataset tested following experiments demonstrate method appears work well predicting good starting points run em convergence experiments consisted running em one iteration possible subsequencederived starting point two datasets likelihood models thus obtained plotted starting position subsequence starting point derived thus one point plotted position sample dataset 68 timothy l bailey charles elkan unsupervised learning multiple motifs biopolymers using em 69 loglikelihood residue number loglikelihood models one em iteration known crplexa sites figure 5 loglikelihood one iteration em starting points derived possible subsequence crplexa dataset em appears converge quickly starting points derived subsequences near lexa binding sites short curves top crp samples longer curves lexa samples vertical axis curve scaled highest peaks 4816 lowest valleys 6425 hoped starting points would yield models significantly higher likelihood even one iteration em could run convergence starting points likely model thus obtained could selected output meme first experiment combined crplexa dataset used meme algorithm run one iteration em possible starting point loglikelihood values derived models plotted position sequence starting point derived seen figure 5 large peaks likelihood function occurring lexa samples information content scores plotted graph would similar appearance since em maximizes likelihood model information content log likelihood chosen criterion choosing starting points information content could also used similar results unsupervised learning multiple motifs biopolymers using em 71 loglikelihood base number performance starting points sample recn pass 1 known lexa sites figure 6 em finds models high likelihood run one iteration crplexa dataset starting points derived subsequences sample recn starting points correspond well known lexa binding sites whose left ends indicated horizontal axis investigation showed peaks tended occur positions known lexa binding sites figure 6 shows expanded view curve sample recn recn sample contains three lexa binding sites whose left ends marked horizontal axis figure peaks curve occur near positions phenomenon observed lexa samples except hima uvrc previous researchers 11 noted match lexa consensus 42 erasing one motif necessary find another closer inspection plots peaks could also seen curves crp samples positions corresponding known crp binding sites figure 7 shows expanded view crp sample tnaa seen figure difficult distinguish peaks generated starting points derived 72 timothy l bailey charles elkan loglikelihood base number performance starting points sample ecotnaa pass 1 known crp sites figure 7 loglikelihood model 1 iteration em meme varies strongly starting point plot shows loglikelihood model one iteration em dataset crplexa run starting points generated subsequences sample labeled tnaa subsequences crp binding sites peaks correspond known sites appears peaks due em starting converge model related lexa motif even bad model highly conserved lexa motifs may loglikelihood similar best model crp binding sites due fact lexa binding sites much highly conserved crp binding sites highest peaks produced subsequences crp samples much lower highest peaks produced lexa samples also crp sample produced peak position corresponding crp binding site clearly higher peaks produced subsequences crp samples shows necessity somehow eliminating lexa binding sites data order able discover best starting points run em learn model crp binding sites unsupervised learning multiple motifs biopolymers using em 73 43 expected number motif appearances critical choice nsites critical ability meme using nper model find one distinct motifs parts motifs dataset would necessary know advance many appearances motif dataset would restrict usefulness meme discovering completely new motifs sequence data alone fortunately meme discovers models motifs nsites set wide range values running meme values nsites probably suffice find motifs represented dataset run crplexa dataset various values nsites parameters fixed models found meme pass examined see fit known consensus sequences lexa crp table 6 shows passes meme models lexa crp motifs discovered information content loglikelihood models meme always finds model lexa motif first pass low nsites finds lexa due presumably fact lexa binding sites get completely erased meme effectively erases nsites occurrences motif pass 5 fifteen lexa binding sites still enough left pass 2 find another model lexa motif meme found model crp motif within four passes values nsites tried except 5 usually crp second model found values information content loglikelihood lexa models always much higher models found meme always true crp models nsites close actual number known binding sites dataset information content loglikelihood crp model much higher models unknown biological significance found meme 44 nper model less sensitive noise oneper model removal onemotifappearancepersequence assumption intended among things make nper model less sensitive noise oneper model example suspected one sequences dataset noise ie contain appearance motif nsites set value less number sequences dataset correctly locates appearances motif model found higher loglikelihood found using oneper model forced choose appearance every sequence dataset test assumption meme run oneper model nper model datasets contained varying numbers randomly generated sequences nsites set fixed value time random sequences letter frequencies dataset whole length datasets used crp lexa various numbers random sequences 74 timothy l bailey charles elkan table 6 meme finds models lexa crp binding sites nsites values 10 35 nsites 10 lexa crp usually found first two passes fail find crp first five passes nsites pass consensus imodel loglikelihood motif added cases meme nper model learned correct concept unsupervised learning multiple motifs biopolymers using em 75 first pass datasets random sequences meme using oneper model could tolerate meme nper model learned model crp binding site sequences added sequences crp dataset learned model even 50 random sequences although learned second pass meme oneper model able learn lexa binding site model 60 random samples added dataset learned offcenter model 20 random samples dataset meme nper model however learned correct lexa model even 80 random samples added dataset figure 8 shows information content crp lexa models learned meme nper model oneper model first pass datasets various numbers random sequences added crp models learned nper model also consistently higher information content learned oneper model true even model learned random sequences added dataset presumably indicative fact nper model taking advantage sequences multiple appearances crp site models learned nper model lexa extremely robust number random samples added dataset almost decrease information content matter many random samples present oneper model hand found models lower information content random samples dataset clear figure 8 meme using nper model find set highly conserved binding sites even datasets vast majority sequences contain oneper model suffers fact must always average one supposed motif appearance sample meme n per model thus able deal particular type noisesamples containing motif appearancesif good estimate true number motif appearances nsites available 5 discussion meme algorithm demonstrates power several new ideas subsequence derived starting points shown powerful way selecting starting points em may useful methods well since em tends converge quickly good starting points meme saves great deal time running em one iteration starting point greedily selecting best starting point based likelihood learned model modifications em algorithm allow meme drop assumption sequence contains exactly one appearance motif fit nper model dataset shown give meme ability discover motifs datasets contain many sequences contain motif finally probabilistic weighting scheme used meme erase appearances motif found pass demonstrated work well finding multiple different motifs well motifs multiple parts 76 timothy l bailey charles information content model found first pass number random sequences added dataset sensitivity oneper nper models irrelevant sequences crp model found oneper model crp model found nper model lexa model found oneper model lexa model found nper model figure 8 information content lexa crp models found first pass meme nper model oneper model run separately crp lexa datasets different numbers random examples added comparative advantage nper model clear especially motifs whose occurrences highly conserved nper model finds good models even many sequences containing motif present meme run set 15 nper model unsupervised learning multiple motifs biopolymers using em 77 meme algorithm prove useful analyzing biological sequence data robust tool discovering new motifs sequence data alone little prior knowledge available meme used discover motifs sequence data alone performing unsupervised learning effectively meme finds clusters similar subsequences set sequences measure unlikeliness cluster information content model example used decide methods ie wetlab experimentation applied verify sites match model actually biologically related plots information content scores various positions sequences dataset figure 1 figure 2 also helpful biologist discovering clusters significant may statistical artifacts meme used dataset sequences known contain motif promoter dataset performing supervised learning models meme learns allow motif variable length ie insertions deletions allowed meme limited learning restricted class motifs may possible use multiple models learned meme passes dataset features another learning algorithm example decision tree learner id3 15 cart 4 could use models learned meme promoter dataset features learn classification rule e coli promoters since first two passes meme found models gamma10 gamma35 regions promoter approach high chance success another promising idea use short motifs learned meme construct starting points hidden markov models innovations added em algorithm meme also used hidden markov models hmms 10 idea using subsequencederived starting points may adaptable use hmms method used meme probabilistically erasing sites pass would certainly easy add standard forwardbackward hmm learning algorithm also possible design hmm like nper model eliminates assumption one motif per sequence may also possible adapt meme innovations learning concepts 16 discovered crp sites colicin ia colicin ib samples site colicin ib mentioned 21 either lexa site possibly crp site 11 appear classified lexa site results reported indicate site probably crp binding site lexa binding site information content score site crp model around 16 whereas less 1 lexa model mention crp site found colicin ia found literature acknowledgments work supported part national science foundation award iri9110813 timothy bailey supported nih genome analysis predoctoral training grant hg00005 authors grateful michael 78 timothy l bailey charles elkan gribskov many useful conversations course work reported douglas w smith extensive suggestions writing article several colleagues advice encouragement appendix reestimating ae z oneper nper models iteration em values letter probabilities motif model ae offset probabilities z must reestimated oneper model z values reestimated using bayes rule current estimate ae models given values z ae estimated expected values letter frequencies done described describe em algorithm two model types formally following definitions useful let n number sequences w length motif l length sequence assume length estimate q iterations em probability site begins position j sequence given model data let ae q lk estimate q iterations em probability letter l appearing position k motif let ith sequence dataset ij letter appearing position j sequence define indicator variable ij site starts position j sequence 0 otherwise ignore probability letters outside motif consider probability letters motif model types em must calculate probability sequence given motif start model written ae q l k k sequence letter l k position j forms basis calculating z q oneper model bayes rule used estimate z q p ae q bayes rule states z q prior probability motif begins position j sequence p 0 estimated assumed uniform unsupervised learning multiple motifs biopolymers using em 79 simplifies z q probability estimated sites completely within sequence j assumed within range calculations z q notice formula z q ensures sums 10 sequence enforces implicit assumption oneper model sequence contains exactly one appearance shared motif nper model modified em algorithm normalizes z q sum positions sequences nsites written formally z q z calculated nper model undergoes two normalizations enforce constraints z q ij less equal 10 sum z q ij window length w less equal 10 constraints written formally z q z q many different ways constraints could enforced particular manner chosen reduces computational effort claim made best choice two constraints enforced separately applying following two algorithms order figure presents first algo rithm makes one passes offset probabilities normalizing sum nsites squashing setting 10 would exceed 10 normalization pass offset probabilities get squashed another pass made raise value offset probabilities never squashed nsites total enforced practice usually passes needed second algorithm given figure run next enforce constraint window w positions offset probabilities sum 10 achieved dividing sequence adjacent windows length w normalizing within window separately windows shifted one right process repeated done w possible shifts win dows guarantees window width w offset probabilities summing greater 10 may reduce total sum nsites squashing algorithm could repeated correct done interest saving computation time 1 squash z unnormalized z q 2 total total z q sequences positions 3 nsites number appearances motif expected 4 l length sequences 5 n number sequences f 7 renormalize f 8 9 11 12 14 p 1 f 19 21 g 24 g 25 g 26 g 27 return 28 g figure a1 squash normalize z ij sum nsites constraining 0 1 unsupervised learning multiple motifs biopolymers using em 81 1 smooth z q normalized offset probabilities 2 l length sequences 3 n number sequences 4 5 6 offset 7 8 9 11 localp 12 13 z q 14 g 15 g 17 g 19 return figure a2 smooth constrain sum offset probabilities window width w sum 10 notes 1 name meme several explanations first acronym multiple em motif elicitation second english word meme means theme motif whose propagation cultural evolution similar propagation gene biological evolution third meme greedy algorithma algorithm 2 related measure used occasionally paper model information content model 17 sum information content position motif j positions motif information content position motif defined ae lj log ae lj l overall frequency letter l dataset information content model thus defined relationship model loglikelihood discussed stormo 18 bailey 1 3 promoter sequences dna sequences precede genes necessary transcription dna messenger rna 4 consensus sequence motif sequence consisting commonly occurring letter position appearances motif ties resolved arbitrarily 5 see 17 discussion matrixbased scoring sequences 6 using possible subsequences first dataset sequence suggested 19 meme approach using subsequences sequences preferable since makes order sequences given unimportant using first sample also eliminates problem first sample happening contain motif occurrence 7 idea conserved motif comes biological idea occurrences motifs often related evolution well conserved motif one whose appearances almost identical little mutation occurred since separated common ancestor 8 biologists often number bases ie letters dna sequence base 1 base transcription dna messenger rna begins bases preceding start transcription given negative numbers starting 1 0 used 9 best value w known advance meme run repeatedly different values 14 addresses question choosing best value w run meme uses single value w motifs found r likelihood vs information aligning biopolymer sequences prosite dictionary sites patterns proteins selection dna binding sites regulatory proteins expectation maximization algorithm identifying proteinbinding sites variable lengths unaligned dna fragments cyclic amp receptor protein role transcription activation maximum likelihood incomplete data via em algorithm pattern classification scene analysis analysis e protein modeling using hidden markov models analysis globins fast searching protein sequences regular expression patterns related protein structure function detecting subtle sequence signals gibbs sampling strategy multiple alignment expectation maximizationem algorithm identification characterization common sites unaligned biopolymer sequences induction decision trees stochastic contextfree grammars modeling rna computer methods analyzing sequence recognition nucleic acids consensus patterns dna locating proteincoding regions human dna sequences multiple sensorneural network approach analysis cloned colicin ib gene complete nucleotide sequence implications regulation expression tr ctr uri keich pavel pevzner finding motifs twilight zone proceedings sixth annual international conference computational biology p195204 april 1821 2002 washington dc usa rhonda harrison charles delisi hypothesis driven approach condition specific transcription factor binding site characterization sc proceedings 2002 acm symposium applied computing march 1114 2002 madrid spain gert thijs kathleen marchal magali lescot stephane rombauts bart de moor pierre rouz yves moreau gibbs sampling method detect overrepresented motifs upstream regions coexpressed genes proceedings fifth annual international conference computational biology p305312 april 2225 2001 montreal quebec canada topon kumar paul hitoshi iba identification weak motifs multiple biological sequences using genetic algorithm proceedings 8th annual conference genetic evolutionary computation july 0812 2006 seattle washington usa mathieu blanchette comparative analysis method detecting binding sites coding regions proceedings seventh annual international conference research computational molecular biology p5766 april 1014 2003 berlin germany benjamin raphael lungtien liu george varghese uniform projection method motif discovery dna sequences ieeeacm transactions computational biology bioinformatics tcbb v1 n2 p9194 april 2004 anshul kundaje manuel middendorf feng gao chris wiggins christina leslie combining sequence time series expression data learn transcriptional modules ieeeacm transactions computational biology bioinformatics tcbb v2 n3 p194202 july 2005 darya chudova padhraic smyth pattern discovery sequences markov assumption proceedings eighth acm sigkdd international conference knowledge discovery data mining july 2326 2002 edmonton alberta canada qicheng jason l wang james r gattiker mining biomolecular data using background knowledge artificial neural networks handbook massive data sets kluwer academic publishers norwell 2002 eleazar eskin profiles patterns back branch bound algorithm finding near optimal motif profiles proceedings eighth annual international conference resaerch computational molecular biology p115124 march 2731 2004 san diego california usa derong liu xiaoxu xiong zengguang hou bhaskar dasgupta identification motifs insertions deletions protein sequences using selforganizing neural networks neural networks v18 n56 p835842 june 2005 saurabh sinha discriminative motifs proceedings sixth annual international conference computational biology p291298 april 1821 2002 washington dc usa jeremy buhler martin tompa finding motifs using random projections proceedings fifth annual international conference computational biology p6976 april 2225 2001 montreal quebec canada francis l chin henry c leung yiu w lam roni rosenfeld w w tsang david k smith jiang finding motifs insufficient number sequences strong binding transcription facto proceedings eighth annual international conference resaerch computational molecular biology p125132 march 2731 2004 san diego california usa xiaoming wu bo wang changxin song jingzhi cheng combined model varied gibbs sampling algorithm used motif discovery proceedings second conference asiapacific bioinformatics p99104 january 01 2004 dunedin new zealand bill c chang asanga ratnaweera saman k halgamuge harry c watson particle swarm optimisation protein motif discovery genetic programming evolvable machines v5 n2 p203214 june 2004 bill chiu eamonn keogh stefano lonardi probabilistic discovery time series motifs proceedings ninth acm sigkdd international conference knowledge discovery data mining august 2427 2003 washington dc yi lu shiyong lu farshad fotouhi yan sun zijiang yang lily r liang pdc pattern discovery confidence dna sequences proceedings 2nd iasted international conference advances computer science technology p345350 january 2325 2006 puerto vallarta mexico mathieu blanchette algorithms phylogenetic footprinting proceedings fifth annual international conference computational biology p4958 april 2225 2001 montreal quebec canada alberto apostolico cinzia pizzi motif discovery monotone scores discrete applied mathematics v155 n67 p695706 april 2007 alberto apostolico fangcheng gong stefano lonardi verbumculus discovery unusual words journal computer science technology v19 n1 p2241 january 2004