robust space transformations distancebased operations many kdd operations nearest neighbor search distancebased clustering outlier detection underlying kgrd data space tupleobject represented point space presence differing scales variability correlation andor outliers may get unintuitive results inappropriate space usedthe fundamental question paper addresses appropriate space propose using robust space transformation called donohostahel estimator first half paper show key properties estimator particular importance kdd applications involving databases stability property says spite frequent updates estimator change much b lose usefulness c require recomputation second half focus computation estimator highdimensional databases develop randomized algorithms evaluate well perform empirically novel algorithm develop called hybridrandom algorithm cases least order magnitude faster fixedangle subsampling algorithms b introduction many kdd operations nearest neighbor search distancebased clustering outlier detection underlying kd data space tupleobject represented point space often times tuple represented simply point kd space formally transformation tuple point p identity matrix begin arguing identity transformation appropriate many distancebased operations permission make digital hard copies part work personal classroom use granted without fee provided copies made distributed profit commercial advantage copies bear notice full citation first page copy otherwise republish post servers redistribute lists requires prior specific permission andor fee sigkdd 2001 san francisco california usa particularly presence variability correlation liers andor diering scales consider dataset following attributes systolic blood pressure typical range 100160 mm mercury mean 120 body temperature degrees celsius small standard deviation eg 12 degrees sick pa age range 2050 years age example note dierent attributes dierent scales units eg mm hg vs degree celsius dierent variability eg high variability blood pressure vs low variability body temperature also attributes may correlated eg age blood pressure may outliers example operation 1 nearest neighbor search consider nearest neighbor search using euclidean distance function original data space ie identity transformation results likely dominated blood pressure readings variability much higher attributes consider query point blood pressure using euclidean distance point 120 40 35 nearer query point 130 37 35 terms similaritydissimilarity nding meaningful intuitively body temperature 40 degrees far away body temperature 37 degrees fact person body temperature 40 degrees needs medical attention immediately simple x problem somehow weight various attributes one common approach apply normalization transformation normalizing attribute range 01 usually satisfactory solution single outlier eg blood pressure could cause virtually values contained small subrange making nearest neighbor search produce less meaningful results another common x apply standardization trans formation subtracting mean attribute dividing standard deviation transformation superior normalization transforma tion outliers may still uential skewing mean standard deviation equally importantly transformation take account possible correlation attributes example older people tend higher blood pressure younger people means could double counting determining distances example operation 2 data mining operations data clustering one studied operations data mining input clustering algorithm distance function specied although algorithms deal nonmetric distance functions eg clarans 20 algorithms require metric ones among subclass algorithms received lot attention recently class densitybased algorithms eg dbscan 11 denclue 14 density region computed based number points contained xed size neighborhood thus density calculation viewed xed radius search hence concerns raised nearest neighbor search apply densitybased clustering outlier detection another important operation data mining particularly surveillance applications many outlier detection algorithms distance densitybased 16 21 7 issues diering scale variability corre lation outliers could seriously aect eectiveness algorithms rst glance statement outliers could impact eectiveness outlier detection algorithms may seem odd attention paid outliers possible outliers may aect quantities used scale data eectively masking hiding 3 contributions paper fundamental question addressed paper appropriate space presence diering scale variability correla tion outliers far seen spaces associated identity normalization standardization transformations inadequate paper focus robust space transformations robust estimators distance computation points space treated fairly specically among many robust space estimators studied statistics propose using donoho stahel estimator dse section 3 show two important properties dse rst euclidean property says inappropriate original space euclidean distance function becomes reasonable dse transformed space second arguably important property stability property says transformed space robust updates spite frequent updates transformed space lose usefulness requires recomputation stability particularly meaningful property kdd applications amount eort x spent set index transformed space certainly would like spend another amount x every single update database section 3 give experimental results showing dse transformed space stable easily withstand adding many tuples database eg 50 database size shown key properties second half paper focus computation dse highdimensional eg 10 attributes databases original dse algorithm dened independently donoho stahel 25 refer fixedangle algorithm section 4 show original algorithm scale well dimension ality stahel also proposed version algorithm uses subsampling ie taking samples sam ples 25 however number subsamples used order obtain good results well known follow work rousseeuw least median squares 22 come heuristic seems work well shown section 6 comparison purposes implemented algorithm applied heuristics eg number subsamples evaluated eectiveness eciency last least section 5 develop new algorithm refer hybridrandom algorithm computing dse experimental results show hybridrandom algorithm least order magnitude ecient fixedangle subsampling algorithms fur thermore support broader claim dse transformation used kdd operations hybridrandom algorithm run eciently eg compute estimator 100000 5d tuples tens seconds total time related work space transformations studied database kdd literature however class distancepreserving transformations eg 12 objective reduce dimensionality space far space transformations go focus much preserving distances providing robustness stability principal component analysis pca useful data duction wellstudied statistics literature 17 15 10 idea nd linear combinations tributes either maximizing minimizing variabil ity unfortunately pca robust since outliers radically aect results outliers also masked hidden points moreover pca lacks stability requirements desire cf section 3 svd robust either may fail detect outliers due masking many clustering algorithms proposed recent years distancebased densitybased 11 26 1 14 results presented paper improve eectiveness algorithms producing meaningful clusters outlier detection received considerable attention recent years designed large highdimensional datasets notion dboutliers introduced 16 distance based variation notion considered 21 notion outliers studied 7 densitybased notions detection algorithms benet results presented paper developing eective multidimensional indexing structures subject numerous studies 13 4 6 however paper indexing structures instead focus determining appropriate space within index created 24 nearest neighbor search based quadratic form distance functions considered distances computed using matrix study assumes prior knowledge applications may dataindependent wellknown example distance two color histograms entry represents degree perceptual similarity two colors 12 however applications far clear suitable could focus paper propose meaningful way picking datadependent fashion 2 background donohostahel two similar attributes compared attributes independent scale vari ability points within distance point p lie within circle radius centered p presence diering scales variability correlation points within distance point p lie within ellipse correlation major minor axes ellipse lie standard coordinate axes correlation ellipse rotated angle see fig 4 later generalizes higher dimen sions 3d ellipsoid resembles football covariance determining footballs size correlation determining orientation estimator also called scatter matrix k k square matrix k dimensionality original data space estimator related ellipsoid follows suppose x kdimensional column vectors euclidean distance x expressed denotes transpose operator quadratic form distance function expressed dax x 6 0 x ax 0 called positive denite matrix x ax yields ellipsoid donohostahel estimator fixedangle algorithm dse robust multivariate estimator location scatter essentially outlyingnessweighted mean covariance downweights point many robust standard deviations away sample univariate projection 19 22 estimator also possesses desirable statistical properties ane equivariance transformations eg principal component analysis possess dse estimator choice although many estimators choose 22 example chose dse minimum volume ellipsoid estimator dse easier compute scales better much less bias especially dimensions 2 18 extended work consider robust estimators one seems perform best based numerous simulations analyses dse interest space deal dse paper although application focus outlier detection add dse general transformation useful many applications described section 1 fig 1 gives skeleton initial algorithm proposed stahel 25 computing estimator 2d let us step algorithm understand estimator dened input dataset containing n 2d points form step 1 iterate unit circle consider large number possible glesdirections project iterate degrees rather 360 degrees since 180360 degree range redundant hereafter call algorithm fixedangle donoho stahel algorithm 1 ie 0 using small increment eg 1 degree unit vector b compute c compute mad dened 14826 medianjx mj 2 3 compute robust multivariate centre weighting function wt dened follows 25 4 compute robust covariance matrix 5 return donohostahel estimator location scatter figure 1 dse fixedangle algorithm 2d fixedangle algorithm point projected onto line corresponding rotating xaxis giving value x mathematically given dot product u unit vector call u projection vector step 1b compute median x values mad acronym median absolute deviation median better estimator scatter standard deviation presence liers finally step 1d yields measures outlying projection respect note analogous classical standardization value x standardized x mean standard deviation x respectively replacing mean median standard deviation mad robust inuence outliers value obtained classical standardization robustness achieved rst identifying outlying points downweighting uence step 1 computes point angle degree outlyingness point respect measure outlying point possible angles step 2 computes point maximum degree outlyingness possible step 3 maximum degree point high threshold 25 uence point weakened decreasing weight function finally points weighted accordingly location center r covariance matrix r computed 3 key properties dse section examine whether estimator useful distancebased operations kdd applications section 6 provide experimental results showing difference estimator make rst section conduct detailed examination properties estimator show estimator possesses euclidean property stability property essential database applications euclidean property section show dse transformation applied euclidean distance function becomes readily applicable call euclidean property lemma 1 donohostahel estimator scatter r positive denite matrix proof omitted brevity according standard matrix algebra 2 key implication lemma matrix r decomposed diagonal matrix whose entries eigen values q matrix containing eigenvectors r decomposition critical following lemma says quadratic form distance wrt r two vectors x euclidean distance transformed vectors transformed space lemma 2 let x two vectors original space suppose transformed space described r quadratic form distance wrt r equal euclidean distance xr yr proof r proof rather standard include provide context comments vector x original space tuple relation vector transformed ie future operations require extra transformations example indexing tuples transformed stored indexing structure query point z given z similarly transformed zr point euclidean distance function used transformed vectors eg xr zr furthermore many existing distancebased structures ecient eective dealing euclideanbased calculations examples include rtrees variants indexing 13 4 outlier detection algorithm studied 16 key message space transformation wrt r expensive compute bring eciencyeectiveness subsequent processing stability property second property analyze dse concerns stability transformation stable transformed space lose usefulness even presence frequent updates important issue database applications amount eort x spent setting index transformed space certainly would like spend another amount x every single update index statistics notion breakdown point estimator quanties proportion dataset contaminated without causing estimator become arbitrarily absurd 27 pursue formal approach regarding breakdown points instead resort experimental evaluation experiments used real dataset computed r inserted deleted tuples thereby changing dnew measure stabil ity compared matrix r rdnew numerical computation domain heuristics measuring dierence matrices universally agreedupon metric 9 make comparison intuitive instead picked distancebased operationoutlier detectionand compared results section 6 gives details experiments brief proceeded follows used old estimator transform space dnew found outliers dnew b used updated estimator dnew transform space dnew found outliers dnew measure dierence two sets detected outliers use standard precision recall 23 dene answer set set outliers found given algorithm ii target set ocial set outliers found using suciently exhaustive search ie using fixedangle algorithm relatively small angular increment precision percentage answer set actually found target set recall percentage target set answer set ideally want 100 precision 100 recall fig 2 shows results 25 50 75 100 new tuples added 25 50 75 tuples deleted new tuples randomly chosen followed distribution tuples originally deleted tuples randomly chosen second third columns show number outliers found third column giving real answer second column giving approximated answer using old transformation fourth fth columns show precision recall clearly show dse transformation stable even 50 change database invalidate old transformation recomputation appears unnecessary results shown fig 2 newly added tuples followed distribution tuples originally results shown fig 3 tried drastic sce nario newly added tuples called junk tuples followed totally dierent distribution ected relatively higher numbers second third columns fig 3 nevertheless despite presence tuples two distributions precision recall gures still close 100 shows stability dse change outliers dnew using precision recall dnew 25 inserts 17 15 882 100 50 inserts 17 75 inserts 100 inserts 37 29 784 100 25 deletes 13 13 100 100 50 deletes 75 deletes 15 19 100 789 figure 2 precision recall distribution outliers inserted dnew using precision recall dnew 25 53 52 943 962 375 74 70 919 971 50 95 90 926 978 625 108 100 907 980 figure 3 precision recall drastically dierent distribution 4 kd subsampling algorithm previous section showed dse possesses desirable euclidean stability properties kdd applications remaining question whether associated cost considerable let us consider compute r eciently k 2 dimensions complexity fixedangle algorithm recall fig 1 gives 2d fixedangle algorithm proposed donoho stahel extension algorithm 3d beyond straightforward instead using unit circle use unit sphere 3d thus two angles1 2through iterate similarly kd deal unit hypersphere k 1 angles iterate understand performance fixedangle algo rithm conduct complexity analysis step 1 fig 1 angle requires nding median n values n size dataset finding median takes time time selection algorithm partition array nd median entry note sorting needed thus 2d increments iterate complexity rst step oan kd k 1 angles iterate increments angles total complexity rst step oa k 1 kn step 2 kd case k 1 projection vectors evaluate thus complexity step nds robust center done okn time step 4 sets k k robust covariance matrix takes ok 2 n time hence total complexity fixedangle algorithm oa k 1 kn suce say running basic kd algorithm impractical larger values k intuition behind subsampling algorithm 2d rst two steps fixedangle algorithm compute point degree outlyingness value obtained taking maximum value measures outlying projection wrt fixedangle algorithm exhaustive enumeration high dimensions approach infeasible let us see better way determine good projection vectors consider points b c fig 4a shows 2d scenario involving correlated attributes fig 4b shows projection points onto line orthogonal major axis ellipse points projected gure note bs projection appears belong bulk points projected ellipse appear outlying projection also although outlying projection algorithm subsampling number iterations chosen select k 1 random points dataset together origin points form hyperplane origin subspace v compute basis orthogonal complement v ii choose unit vector u orthogonal complement use vector u fixed angle algorithm b c continue step 1b beyond fixed angle algorithm shown fig 1 takes role figure 5 dse subsampling algorithm kd line c fig 4c c outlying b clearly fig 4d shows yet another projection seen projection vectors chosen greatly uence values donohostahel al gorithm applying subsampling goal use lines orthogonal axes ellipse ellipsoid kd improve odds obtaining good projection line may better projection vectors identify outliers good choices increased chance detecting outliers using orthogonal lines many outliers likely stand orthogonal projection see fig 4 nonoutlying points especially within ellipsoids unlikely stand project common relatively short interval line knew axes ellipse would need subsampling however since know parameters ellipsoid b general many points many dimensions involved calculating parameters ellipsoid use following approach called subsampling 2d idea rst pick random point p set n input points compute line orthogonal line joining p origin note reasonable proba bility likely pick point p ellipse resulting orthogonal line may approximate one axes ellipse essence subsampling details subsampling algorithm kd kd rst nd random sample k 1 points together origin form subspace v next need nd subspace orthogonal v called orthogonal complement v 2 point everything else proceeds fixedangle algorithm fig 5 outlines subsampling algorithm kd dse computation one key detail fig 5 deserves elaboration compute determine begin analyzing probability getting bad subsample subsample randomly chosen subsample likely good k 1 points within ellipsoid let userchosen parameter fraction points outside ellipsoid typically varies 001 05 bigger value conservative demanding user quality subsamples b c figure 4 bivariate plots showing eect dierent projection lines data points b projection onto line orthogonal major axis ellipse c projection onto line orthogonal minor axis projection onto another line let us say smallest number subsamples least 95 probability get least one good subsample subsamples given probability getting good subsample probability picking k 1 random points within ellipsoid 1 conversely probability getting bad subsample 1 1 thus probability subsamples bad 1 1 hence determine base value solving following inequality 1 1 1 095 example section 6 show quality estimator varies complexity subsampling algorithm kd determine basis orthogonal complement hyperplane origin k 1 nonzero points ok 3 time using gaussjordan elimination 2 9 using basis simply pick unit vector u projection vector continue basic fixedangle algorithm recall section 4 basic algorithm runs oa k 1 kn time step 1 ok 2 n time remaining steps subsampling algorithm however perform total iterations iteration consists k 1 randomly selected points thus step 1 subsampling algorithm runs omk 3 time thus following analysis section 4 entire algorithm runs omk 3 5 kd randomized algorithms subsampling algorithm scalable respect k fixedangle algorithm mk 3 complexity factor still costly number subsamples large ie high quality estimator thus section explore kd dse estimator computed eciently first implement simple alternative fixedangle algorithm called purerandom evaluating strengths weaknesses develop new algorithm called hybridrandom combines part purerandom algorithm part subsampling algorithm section 6 provide experimental results showing eectiveness eciency purerandom algorithm recall fig 1 fixedangle algorithm high complexity due k 1 factor k 1 denotes number projection unit vectors examined however given projection unit vector complexity step 1 reduces drastically okn certainly possible algorithm well randomly selects r projections examine projections happen good uential pro jections skeleton algorithm called purerandom algorithm purerandom r number projection vectors chosen select kd projection unit vector u randomly ie pick k 1 random angles b c continue step 1b beyond fixed angle algorithm takes role figure presented fig 6 following analysis shown section 4 easy see complexity pure random algorithm orkn randomization also used subsampling algorithm random draw subspace v formed 1 points dataset orthogonal complement v computed purerandom case however random draw projection vector order purerandom algorithm produce results comparable subsampling algorithm likely r hybridrandom algorithm conceptually pure random algorithm probes kd space blindly reason value r may need high acceptable quality question whether random draws projection vectors done intelligently specically areas kd space randomization skip equivalently areas randomization focus new algorithm develop called hybridrandom rst apply subsampling algorithm small number subsamples consider orthogonal complement v passes origin imagine rotating line small angle anchored origin thus creating cone rotation yields patch surface kd unit hypersphere fixedangle al gorithm know projection vectors close give markedly dierent results second phase hybridrandom algorithm restrict random draws projection vectors stay clear previously examined conespatches using euclidean inner product law cosines collision two vectors b occurs dist 2 radius patch surface kd unit hypersphere determine used following heuristic say vectors b close cos 095 angle vectors thus 2 hence upper bound use 01 two observations order first patches large counterproductive many promising projection vectors may excluded second although increasing number patches improves accuracy favourable results obtained relatively patches eg 100 shown section 6 fig 7 gives skeleton hybridrandom algorithm steps 1 3 use subsampling algorithm nd initial projection vectors including eigenvectors scatter matrix keep iteration step 4 new random projection vector generated way stays clear existing projection vectors algorithm hybridrandom 1 run subsampling algorithm small number iterations eg 2 compute k eigenvectors resulting scatter matrix gives us approximation axes ellipsoid 3 initialize set previously examined projection vectors consist projection vectors step 1 k eigenvectors step 2 4 r number extra random patches desired randomly select 2 unique vectors b least 2 radians apart b compute new vector u linear combination b particular u b randomly chosen 1 c u within radians existing vector redo previous step new two vectors still close second attempt go back step 4a normalize u unit vector add f continue step 1b beyond fixed angle algorithm takes role figure 7 dse hybridrandom algorithm kd recall earlier discussion complexity subsampling algorithm om1k 3 m1 number subsamples taken pure random algorithm complexity or1kn r1 number random projections probed easy see hybridrandom algorithm requires complexity expect m2 m1 r2 r1 experimental results follow 6 experimental evaluation experimental setup evaluate donohostahel transformation picked distancebased outlier detection operation described 16 explained section 3 use precision recall 23 compare results base dataset 855record dataset consisting 199596 national hockey league nhl player performance statistics publicly available statistics downloaded sites professional hockey server httpmaxwelluhhhawaiieduhockey since reallife dataset quite small created number synthetic datasets mirroring distribution statistics within nhl dataset specically determined distribution attribute original dataset using 10partition histogram generated datasets containing 100000 tupleswhose distribution mirrored base dataset optional preprocessing step applied box cox transformation normality 8 nd appropriate parameters p distancebased outliers implementation unless otherwise stated used 5d case 100000 tuples default attributes goals assists penalty minutes shots goal games played tests run sun microsystems ultra1 proces sor running sunos 57 256 mb main mem ory four dse algorithms presented fixed angle algorithm deterministic three involve randomization used median results several runs precision almost always 100 recall often varied usefulness donohostahel transformation introduction motivated usefulness donoho stahel transformation arguing identity transformation ie raw data well normalization standardization transformations may give good results experiment reported show concrete situation based outlier detection based 199596 nhl statistics conducted experiment using two attributes penaltyminutes goalsscored note range penaltyminutes 0335 range goalsscored 069 fig 8 compares top outliers found using identity standardization donohostahel transformations also shown actual penaltyminutes goalsscored identied players identity transformation ie transformation players highest penalty minutes dominate classical standardization dominance shifts players highest goalsscored matthew barnaby appearing lists ever cases identied outliers trivial sense merely extreme points tribute barnaby may simon top5 penaltyminutes lemieux jagr top2 goalsscored donohostahel transformation identied outliers lot interesting surprising donald transform top outliers penaltymins goalsscored ation found raw data raw data matthew barnaby 335 15 chris simon 250 matthew barnaby 335 15 standard jaromir jagr 96 62 ization mario lemieux 54 69 matthew barnaby 335 15 donoho donald brashear 223 0 stahel jan caloun 0 8 joe mullen 0 8 figure 8 identied outliers usefulness donoho stahel transformation brashear even top15 far penalty minutes goes goalsscored performance unim pressive penaltyminutes 223 goalsscored yet unique combination amass high number penalty minutes player needs play lot plays lot likely score least goals incidentally 0 goals extreme univariate point however well 100 players share value similar comments apply jan caloun joe mullen 0 penaltyminutes 8 goalsscored raw gures look unimpressive players exceptional ways 1 point without appropriate space transformation outliers would likely missed internal parameters algorithms every algorithm presented key internal parameters fixedangle case parameter number angles tested per dimension randomization algorithms number subsamples r number random projection vectors let us examine choices parameters aect quality estimator computed precision recall used evaluate quality however results presented precision always 100 thus report recall values four graphs fig 9 contrast cpu times ii recall values iii number iterations patches used one four algorithms left hand yaxis denes cpu times minutes top two graphs seconds bottom two graphs right hand yaxis conjunction recall curve see gures legend denes recall values note however recall range varies one graph another fig 9a measures cpu time minutes shows fixedangle algorithm take long time nish especially number random angles tested increases horizontal axis tens thousands iterations recall small decrease angle increment dimension cause large number additional iterations occur many datasets necessary use increments small degrees eg 75 hours cpu time 100000 tuples 5d determining number outliers present omit long runs graphs allow us clearly contrast cpu times recall values compared fixedangle algorithm purerandom algorithm achieves given level recall quickly al though fig 9b shows still take long time achieve high levels recall recall subsampling algorithm key issue many subsamples use based heuristic presented section 4 base value determined 47 multiples 47 subsamples used recall curve fig 9c clear 47 sub samples recall value poor even 3 141 subsamples recall value becomes rather acceptable strength subsampling algorithm 1 actually even hear jan caloun experiment 199596 caloun played total 11 games scored 8 goalsalmost goal per game rarity nhl search world wide web reveals caloun played grand total 13 games nhl11 games 199596 2 games 1996 97before disappearing nhl scene also learned scored rst four nhl shots tie nhl record give acceptable results short time recall curve diminishing rate return may take long time subsampling reach high level recall conrmed fig 10 since hybridrandom algorithm uses subsampling algorithm rst phase 47 expected hybridrandom algorithm behaves well subsampling algorithm beginning mediocre levels recall 7075 cf fig 10 shown fig 9d hybridrandom algorithm allowed execute longer steadily quickly improves quality computation thus terms cpu time start subsampling curve quickly switch purerandom curve reap benets fast algorithm pruned randomization achieving given rate recall experiment shows algorithm trades eciency quality picked reasonable set parameter values algorithm let us compare algorithms head tohead specically xed recall rates compare time taken algorithm deliver recall rate run time fixedangle algorithm typically several orders magnitude others comparable quality omit fixedangle algorithm results fig 10 compares hybridrandom algorithm purerandom subsampling algorithms higher rates recall general subsampling algorithm eective quick consistent results however improve quality take long time contrast hybridrandom algorithm allowed run bit longer deliver steady improvement quality case point achieve 90 recall current example takes subsampling algorithm almost 14 hours achieve level recall produced hybridrandom algorithm two minutes never theless must give subsampling algorithm credit giving hybridrandom algorithm excellent base start computation fig 10 purerandom algorithm signicantly outperforms subsampling algorithm always case expect recall rate purerandom volatile cases purerandom algorithm returns substantially dierent outliers large numbers iterations hybridrandom algorithm tends focused consistent scalability dimensionality dataset size fig 11a shows scalability dimensionality subsampling hybridrandom algorithms used moderate levels recall eg 75 60000 tuples anal ysis high levels recall would favor hybridrandom algorithm results shown 282 iterations subsampling algorithm 90 patches hybrid random algorithm experience shown numbers iterations patches satisfactory assuming satised conservative levels recall fig 11a shows algorithms scale well conrms complexity analysis section 4 fig 11b shows subsampling hybridrandom algorithms scale dataset size 5d conservative levels recall algorithms seem scale well hybridrandom algorithm outperforms run time recall fixedangle algorithm 5d 100000 tuples number angles tested iterations cpu time minutes cpu time recall recall run time recall purerandom algorithm 5d 100000 tuples number random angles iterations cpu time minutes cpu time recall recall 5002060100run time recall subsampling algorithm 5d 100000 tuples number subsamples iterations cpu time seconds cpu time recall recall 700100run time recall hybridrandom algorithm 5d 100000 tuples delta00800 number patches cpu time seconds cpu time recall recall figure 9 plots run time recall top left fixedangle b top right purerandom c bottom left subsampling bottom right hybridrandom 9050150250350run times achieve given level recall 3 algorithms 5d 100000 tuples percent recall cpu time seconds purerandom subsampling hybridrandom figure 10 run time vs recall subsampling purerandom hybridrandom algorithms subsampling algorithm high levels recall would favor hybridrandom algorithm even shown 7 conclusion results returned many types distancebased kdd operationsqueries tend less meaningful attention paid scale variability correlation outliers underlying data paper presented case robust space transformations support operations nearest neighbor search distancebased clustering outlier detection appropriate space one preserves euclidean property ecient euclidean distance operations performed without sacricing quality meaningfulness results b stable presence nontrivial number updates saw distance operations ordinarily would inappropriate operating raw data even normalized standardized data actually appropriate transformed space thus end user sees results tend intuitive meaningful given application presented data mining case study detection outliers support claims considering issues eectiveness measured precision recall especially latter efciency measured scalability dimensionality dataset size believe hybridrandom algorithm developed paper excellent choice among donohostahel algorithms tens seconds cpu time robust estimator computed accounts scale variability correlation outliers also able withstand signicant number database updates eg 50 tuples without losing eectiveness requiring recomputation many cases involving high levels recall randomized algo rithms particular hybridrandom algorithm least order magnitude faster sometimes several orders magnitude faster alternatives conclusion believe results shown robust estimation place kdd community nd value many kdd applications 8 r automatic subspace clustering high dimensional data data mining applications elementary linear algebra applications version outliers statistical data lof identifying densitybased local outliers box numerical analysis fast algorithm robust principal components based projection pursuit densitybased algorithm discovering clusters large spatial databases noise rtrees dynamic index structure spatial searching algorithms mining distancebased outliers large datasets bias robust estimation scale behaviour staheldonoho robust multivariate estimator robust regression outlier detection introduction modern information retrieval breakdown covariance estimators sting statistical information grid approach spatial data mining high breakdown point estimates regression means minimization ecient scale tr robust regression outlier detection rtree efficient robust access method points rectangles efficient effective querying image content distancebased indexing highdimensional metric spaces automatic subspace clustering high dimensional data data mining applications densitybased indexing approximate nearestneighbor queries efficient algorithms mining outliers large data sets introduction modern information retrieval rtrees algorithms mining distancebased outliers large datasets efficient effective clustering methods spatial data mining efficient useradaptable similarity search large multimedia databases ctr peng sun robert freund computation minimumvolume covering ellipsoids operations research v52 n5 p690706 sep oct 2004 cateni v colla vannucci fuzzy logicbased method outliers detection proceedings 25th conference proceedings 25th iasted international multiconference artificial intelligence applications p561566 february 1214 2007 innsbruck austria leejay wu christos faloutsos making every bit count fast nonlinear axis scaling proceedings eighth acm sigkdd international conference knowledge discovery data mining july 2326 2002 edmonton alberta canada jaideep vaidya chris clifton privacypreserving kmeans clustering vertically partitioned data proceedings ninth acm sigkdd international conference knowledge discovery data mining august 2427 2003 washington dc