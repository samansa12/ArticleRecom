approximating bayesian belief networks arc removal abstracti propose general framework approximating bayesian belief networks model simplification arc removal given upper bound absolute error allowed prior posterior probability distributions approximated network subset arcs removed thereby speeding probabilistic inference b introduction today applications based bayesian belief network 1 formalism emerging reasoning decision making problem domains inherent uncertainty current applications range medical diagnosis prognosis 1 computer vision 10 information retrieval 2 applications grow larger belief networks involved increase size topology network becomes dense runtime complexity probabilistic inference increases dramatically reaching state realtime decision making eventually becomes prohibitive exact inference general bayesian belief networks proven nphard 3 many applications computing exact probabilities belief network liable unrealistic due inaccuracies probabilistic assessments network therefore general approximate methods suffice furthermore employment approximate methods alleviates probabilistic inference network least extend approximate methods provide probability estimates either employing simulation methods approximate first introduced henrion 7 methods based model simplification examples annihilating small probabilities 8 removal weak dependencies 13 former approach stochastic simulation methods 4 provide approximate inference based generating multisets configurations variables belief network multiset conditional probabilities interest estimated occurrence frequencies probability estimates tend approximate true probabilities part work done utrecht university dept computer science netherlands 1 paper adopt term bayesian belief network belief network short belief networks also known probabilistic networks causal networks recursive models generated multiset sufficiently large unfortunately computational complexity approximate methods still known nphard 5 certain accuracy probability estimates demanded hence like exact methods simulation methods exponential worstcase computational complexity demonstrated kjaerulff 13 forcing additional conditional independence assumptions portrayed belief network provides promising direction towards belief network approximation view model simplification however kjaerulffs method specifically tailored bayesian belief universe approach probabilistic inference 9 model simplification applied network directly belief universes obtained belief network method identifies weak dependencies belief universe network removes removing specific links network thereby enforcing additional conditional independencies portrayed network result speedup probabilistic inference obtained cost bounded error inference paper propose general framework belief network approximation arc removal proposed approximation method adopts similar approach kjaerulffs method 13 respect means quantifying strength arcs network terms kullbackleibler information divergence statistic general kullbackleibler information divergence statistic 14 provides means measuring divergence probability distribution approximation distribution see eg 22 however important differences noted approaches firstly type independence statements enforced approach renders direct dependence relationship portrayed arc superfluous contrast kjaerulffs method links may rendered superfluous well consequence apply localized changes network allows large set arcs removed simultaneously secondly mentioned kjaerulffs method operates bayesian belief universe approach probabilistic inference using cliquetree propagation algorithm lauritzen spiegelhalter 16 contrast framework propose operates network directly therefore applies type method probabilistic inference finally given upper bound posterior error probabilistic inference allowed possibly large set arcs removed simultaneously belief network requiring one preevaluation network contrast kjaerulffs method conditional independence assumptions added network one time rest paper organized follows section 2 provides preliminaries bayesian belief network formalism introduces notions information theory section 3 present method removing arcs belief network analyze consequences removals represented joint probability distribution section 4 practical approximation schemes discussed aimed reducing computational complexity inference belief network conclude section 5 advantages disadvantages presented method compared existing methods approximating networks preliminaries section briefly review basic concepts bayesian belief network formalism notions information theory sequel assume reader well acquainted probability theory basic notions graph theory 21 bayesian belief networks bayesian belief networks allow explicit representation dependencies well independencies using graphical representation joint probability distribution general undirected directed graphs powerful means representing independency models see eg 21 22 associated belief networks algorithms probabilistic inference network propagating evidence providing means reasoning uncertain knowledge represented network belief network consists qualitative quantitative representation joint probability distribution qualitative part takes form acyclic digraph g vertex represents discrete statistical variable stating truth proposition within problem domain sequel notions vertex variable used interchangeably arc digraph denote called tail arc vertex called head arc represents direct causal influence vertices discerned vertex v called immediate predecessor vertex v j vertex v j called immediate descendant vertex v furthermore associated digraph belief network numerical assessment strengths causal influences constituting quantitative part network sequel ease exposition assume binary statistical variables taking values domain ftrue falseg however generalization variables taking values finite domain straightforward variable v represents proposition denoted v false denoted v set variables v conjunction called configuration scheme v configuration c v v conjunction value assignments variables v sequel use concept configuration scheme denote specific property holds possible configurations set variables definition 21 bayesian belief network tuple acyclic digraph gg set realvalued functions theta fc g called assessment functions configuration set g v immediate predecessors vertex v probabilistic meaning assigned topology digraph belief network means dseparation criterion 18 criterion allows detection dependency relationships vertices networks digraph traversing undirected paths called chains comprised directed links digraph chains blocked set vertices stated formally following definition definition 22 let acyclic digraph let chain g blocked set vertices contains three consecutive vertices one following three conditions fulfilled chain chain chain oe set vertices composed x 2 descendants note chain blocked contains case vertex x 2 called headtohead vertex respect 6 definition 23 let acyclic digraph let x z v g disjoint subsets vertices g set said dseparate sets x z g denoted g every chain v v j g blocked dseparation criterion provides detection probabilistic independence relations digraph belief network stated formally following definition definition 24 let acyclic digraph let pr joint probability distribution v g digraph g imap pr hx j z j g implies x pr j z disjoint subsets x z v g ie x conditionally independent z given pr chainrule representation joint probability distribution probability theory initial probability assessment functions belief network provide information necessary uniquely defining joint probability distribution set variables discerned respects independence relations portrayed digraph 11 18 theorem 25 let g gamma belief network defined definition 21 defines joint probability distribution pr v g g imap pr belief network therefore uniquely represents joint probability distribution computing conditional probabilities network several efficient algorithms developed pearls polytree algorithm cutset conditioning 18 19 method cliquetree propagation lauritzen spiegelhalter 16 combinations 20 widely used algorithms exact probabilistic inference simulation methods provide approximate probabilistic inference see 4 overview 22 information theory kullbackleibler information divergence 14 several important applications sta tistics one measuring well one joint probability distribution approximated another simpler dependence structure see eg 22 sequel make extensive use kullbackleibler information divergence defining kullbackleibler information divergence formally concept continuity introduced 14 definition 26 let v set statistical variables let pr pr 0 joint probability distributions v pr absolutely continuous respect pr 0 subset variables denoted pr pr 0 k x prc x configurations c x x write pr pr 0 pr pr 0 k v short note continuity relation reflexive transitive relation probability distributions furthermore continuity relation satisfies ffl pr pr 0 k x pr pr 0 k subsets variables x v x subsets variables x v configuration c joint probability distribution pr absolutely continuous respect distribution pr 0 set variables x pr also absolutely continuous respect pr 0 subset x addition posterior distribution configuration c also absolutely continuous respect posterior distribution definition 27 let v set statistical variables let x v let pr pr 0 joint probability distributions v kullbackleibler information divergence cross entropy pr respect pr 0 x denoted ipr pr 0 x defined sequel write short note information divergence symmetric pr pr 0 finite pr absolutely continuous respect pr 0 furthermore information divergence satisfies subsets variables x v especially prcx subsets variables subsets variables x v independent pr pr 0 principle base logarithm kullbackleibler information divergence immaterial providing unit measure sequel use natural logarithm assumption following property holds proposition 28 let v set statistical variables let pr pr 0 joint probability distributions v furthermore let kullbackleibler information divergence defined definition 27 x v hence kullbackleibler information divergence provides upper bound absolute divergence jprc x configurations c x x property kullbackleibler information divergence known information inequality 15 r cc appoximation ctp appoximation figure 1 reducing complexity cutset conditioning cc cliquetree propagation ctp removing arc 3 approximating belief network removing arcs section propose method removing arcs belief network investigate consequences removal computational resources error introduced ease exposition method removing single arc belief network introduced first based method observations made method multiple simultaneous arc removals presented 31 reducing complexity belief network removing arcs computational complexity exact probabilistic inference belief network depends large extend connectivity digraph network removing arc digraph network may substantially reduce complexity probabilistic inference network pearls polytree algorithm method cutset conditioning 18 19 undirected cycles called loops 18 broken resulting smaller loop cutsets used size cutset determines computational complexity inference network large extend method cliquetree propagation 16 belief network first transformed decomposable graph computational complexity inference depends large extend size largest clique decomposable graph removal appropriate arc edge results splitting cliques several smaller cliques see eg method kjaerulff 13 yielding reduction computational complexity inference decomposable graph figure 1 depicted effect removing arc digraph belief network method cutset conditioning method cliquetree propagation cutset conditioning vertex cutset eg vertex drawn shading required break loop since removal arc breaks loop smaller cutset may necessary cliquetree propagation decomposable graph obtained example belief network three cliques 4 vertices removal arc results decomposable graph four smaller cliques one 2 three 3 vertices approximate methods computational complexity example forward simulation 4 depends extend distance root vertex leaf vertex therefore removal arcs may also yield reduction complexity approximate inference however difficult analyze measure amount reduction complexity general comparison exact methods sequel discuss arc removal view exact methods probabilistic inference 32 removing arc belief network although several methods removing arc belief network devised method removal arc defined following definition natural choice made clear analyze effects removal definition 31 g gamma belief network let pr joint probability distribution defined b let v r arc g define tuple b acyclic digraph v g vr6vs gg set functions note network b resulting removal arc v r digraph g belief network b constitutes belief network network assessment functions head vertex arc changed sequel refer b vr6vs approximated belief network removal arc operation computing b vr6vs referred approximating network removal arc belief network may result change represented joint probability distribution however represented dependency structure distribution portrayed graphical part network may retained introducing virtual arc two vertices physical arc removed virtual arc may serve detection dependencies independencies original probability distribution using dseparation criterion virtual arc however used probabilistic inference still allowing faster approximate computation prior posterior probabilities simplified network 33 error introduced removing arc removing arc belief network yields slightly simplified network faster inference exhibits errors marginal conditional probability distributions section analyze errors introduced prior posterior distributions upon belief network approximation removal arc effects summarized introducing change qualitative ignoring virtual arcs well change quantitative representation joint probability distribution qualitative error prior posterior distributions change qualitative belief network representation probabilistic dependency structure removing arc belief network described following lemma lemma 32 let g acyclic digraph let v r arc g let digraph g arc removed g hfv r g j g vr 6vs proof prove hfv r g vr 6vs holds show every chain vertex v r vertex v g vr6vs blocked set g vr 6vs v chain v r v two cases distinguished ffl comprises arc chain blocked g vr 6vs v ffl comprises arc acyclic must contain headtohead vertex v k ie vertex two converging arcs since oe g vr 6vs blocked g vr 6vs v property states removing arc digraph g belief network simplified graphical representation yields variable v r conditionally independent variable v given g vr 6vs v set immediate predecessors v digraph g arc quantitative error prior distribution change qualitative dependency structure portrayed network quantitative counterpart two inherently linked together belief network formalism analyze error approximated prior probability distribution similar 13 22 use kullbackleibler information divergence quantitative comparison terms divergence joint probability distribution defined belief network approximated joint probability distribution obtained removing arc network facilitate investigation give expression approximated joint probability distribution terms original distribution first introduce additional notions related arcs digraph useful describing properties notions build observation set immediate predecessors g vr 6vs v dseparates tail vertex v r head vertex v digraph g arc removed definition 33 let acyclic digraph let v r arc g define arc block v r denoted fi g set vertices g furthermore define arc environment v r g denoted j g set vertices joint probability distribution defined approximated belief network factorized terms joint probability distribution defined original network lemma 34 let g gamma belief network let pr joint probability distribution defined b let v r arc g let b approximated belief network removal v r defined definition 31 joint probability distribution pr vr6vs defined b vr6vs satisfies pr vr6vs c v g arc environment v r defined definition 33 proof theorem 25 joint probability distribution pr vr6vs defined network pr vr6vs c v g exploiting definition 31 leads since fl vs v j c g vs clearly property links graphical implications removing arc belief network numerical probabilistic consequences removal variable v r rendered conditionally independent variable v given g vr 6vs v removal arc v r one important consequences investigated amount absolute divergence prior probability distribution approximated distribu tion information inequality subsets x v pr pr vr6vs joint probability distributions set variables v defined belief network network arc removed respectively however recall bound finite pr absolutely continuous respect pr vr6vs prove property following lemma lemma g gamma belief network let v r arc g let approximated belief network removal v r defined definition 31 joint probability distribution pr defined b absolutely continuous respect joint probability distribution pr vr6vs defined b vr6vs proof prove pr absolutely continuous respect pr vr6vs v g prove prc v g implies pr vr6vs configurations c v g g first observe chain rule probability theory arc environment arc g defined definition 33 consider configuration c v g v g configuration prc j g vr vs furthermore prc vr c vs j c g vs nfvr implies observations lead hence prc conclude pr pr vr6vs 2 property absolute continuity kullbackleibler information divergence provides proper upper bound error introduced joint probability distribution removal arc network however bound rather coarse expected removing arc may always affect prior probabilities specific marginal distributions defined network observation formalized following lemma states divergence prior marginal distributions always zero sets vertices descendants head vertex arc removed fact property direct result chainrule representation joint probability distribution belief network lemma 36 let g gamma belief network let pr joint probability distribution defined b let v r arc g let b approximated belief network removal v r defined definition 31 joint probability distribution pr vr6vs defined b vr6vs satisfies pr vr6vs c v g n oe denotes set comprised v descendants proof first prove g v applying theorem 25 marginalizing pr obtain configurations c x x assumption configurations occur within sum adhere c v since g v k find rearranging terms configurations c x x hence similar exposition network b vr6vs pr vr6vs cx observe definition 31 obtain pr vr6vs cx principle marginalization conclude pr vr6vs c property provides key observation applicability multiple arc removals described section 34 quantitative error posterior distributions belief networks generally used reasoning uncertainty processing evidence probability hypothesis computed network given evidence belief network framework amounts computing revised probabilities posterior probability distribution given evidence investigate implications posterior distributions removal arc begin investigation exploring general properties kullbackleibler information divergence lemma 37 let v set statistical variables let x v subsets v let pr pr 0 joint probability distributions v kullbackleibler information divergence satisfies proof distinguish two cases case pr pr case ffl assume pr pr 0 k xy assumption implies information divergence 27 therefore c xy used fact configuration c 0 set variables probability distribution undefined prc 0 configuration c 0 x probability prc 0 logprc 0 definition therefore let first sum last equality range configurations c prc 0 rearranging terms find log note ipr pr ffl assume pr 6 pr implies ipr pr show ipr pr observe assumption exists configuration c 0 x prc 0 two cases distinguished case pr 0 c 0 case pr 0 c 0 assume pr 0 c 0 implies prc 0 yields pr 6 pr 0 k definition 27 ipr pr using fact divergence nonnegative assume pr 0 c 0 configurations c 0 x hence definition 27 implies nonnegative conclude ipr pr property kullbackleibler information divergence leads following lemma stating upper bound absolute divergence posterior probability distribution defined belief network given evidence approximated posterior probability distribution defined another approximated network lemma 38 let v set statistical variables let pr pr 0 joint probability distributions v pr pr 0 let kullbackleibler information divergence subsets variables x v configurations c furthermore upper bound absolute divergence finite proof consider two subsets x v configuration c configuration pr pr 0 implies pr 0 hence posterior distributions welldefined furthermore since pr pr 0 also implies follows proposition 28 finite upper bound furthermore lemma 37 yields 0 consider divergence isolation since configuration c 0 prc 0 divergence finite nonnegative observations finally find finite upper bound property information divergence absolute divergence posterior distribution given evidence c subset variables belief network b approximated network b vr6vs removal arc v r v bounded pr joint probability distribution defined b pr vr6vs joint probability distribution defined b vr6vs bound finite since pr absolutely continuous respect pr vr6vs furthermore bound find worst case ie error probabilistic inference approximated belief network inversely proportional square root probability evidence unlikely evidence larger error may 34 multiple arc removals section generalize method single arc removal belief networks method multiple simultaneous arc removals thereby still guaranteeing finite upper bound error introduced prior posterior distributions recall definition 31 removing arc yields appropriate change assessment functions head vertex arc removed therefore operation applied parallel arcs sharing head vertex formalize requirement introduce notion linear subset arcs digraph acyclic digraph set vertices indexed ascending topological order relation oe g ag theta ag set arcs g defined v r pairs arcs g furthermore let ag subset arcs g say linear respect g order oe g total order either pair distinct arcs note linear subset arcs digraph contains pair arcs head vertex common formally define simultaneous removal linear set arcs belief network g gamma belief network let ag linear subset arcs g define multiply approximated belief network denoted network resulting simultaneous removal arcs b definition 31 obtain network digraph v ga gg set functions analyze error introduced prior well posterior distribution removal linear set arcs belief network exploit information inequality obtaining proper upper bound essential requirement joint probability distribution defined original network absolutely continuous respect distribution defined multiply approximated network prove exploit ordering relation arcs digraph defined ordering relation induces total order arcs linear subset arcs digraph show consecutive removal arcs belief network arc linear order yields multiply approximated network transitivity continuity relation directly implies joint probability distribution defined original network absolutely continuous respect distribution defined multiply approximated network lemma 311 let g gamma belief network let pr joint probability distribution defined b let 1 linear subset arcs g ordered respect oe g defined definition 39 ie pairs arcs v r j multiply approximated belief network removal arcs defined definition 310 vrn 6vsn approximated network righthand side approximated removal defined definition 31 proof proof induction cardinality base case holds hypothesis induction consider arc principle duction prove prove obviously digraphs obtained removal arc identical ie leaves us proof probability assessment functions first observe simultaneous removal arcs network b yields network ba probability assessment functions fl 0 observe removal arc v rnvsn network b anfvrn vsn g yields probability assessment functions find fl 00 remains prove fl 0 vsn equivalently prv sn j c g vsn nfvrn observe ordering relation oe g find arcs n fv rn v sn g removed b arc v rn v sn digraph g b ie assuming ascending topological order vertices implies n g hence g v sn fv sn g oe induction hypothesis apply lemma 36 arc find prv sn c g vsn nfvrn thermore yields prv sn j c g vsn nfvrn vsn conclude result property multiple arc removals kullbackleibler information divergence joint probability distribution defined belief network respect distribution defined multiply approximated network finite furthermore arc linearity implies following additive property kullbackleibler information divergence lemma 312 let g gamma belief network let pr joint probability distribution defined b let ag linear subset arcs g let multiply approximated belief network removal arcs defined definition 310 let pra joint probability distribution defined ba kullbackleibler information divergence satisfies proof first prove pr pra assume arcs linear set ordered according relation oe g defined definition 39 ie pairs arcs v r j lemma 35 find pr pr vr 1 pr vr 1 transitive conclude pr pra application lemma 311 observation find linear arc new probability assessment function fl 0 leads log vs linearity set arcs removed sufficient condition property stated yet necessary one observations information inequality provides finite upper bound error introduced prior posterior distributions approximated belief network simultaneous removal linear set arcs bound obtained summing information divergences joint probability distribution defined network approximated distribution removal arc individually set arcs example 1 consider belief network g gamma g digraph depicted figure 2 figure 2 information divergence arc digraph example belief network table 1 information inequality absolute divergence approximated example belief network set gamma consists probability assessment functions fl arc v r digraph g information divergence ipr pr vr6vs joint probability distribution pr defined b joint probability distribution pr vr6vs defined approximated network b vr6vs removal v r computed depicted next arc figure 2 note despite presence arc conditionally independent given variable v 7 fact fl v9 v 9 graphically portrayed dependence rendered redundant arc removed without introducing error probability distribution since ipr pr v86v9 shown figure 2 table 1 gives upper bound provided information inequality absolute divergence approximated joint probability distributions removal various linear subsets arcs networks digraph table compressed leaving linear sets containing arc set fv 8 second third column unchanged leaving arc note subset arcs containing arcs 7 linear example concluded upper bound provided information inequality exceeds absolute divergence factor 2 3 furthermore note arcs weight value absolute divergence example absolute divergence sets containing arc approximation schemes section present static dynamic approximation schemes belief networks schemes based observations made previous section 41 static approximation scheme clearly arcs significantly reduce computational complexity inference belief network upon removal desirable remove however error introduced upon removal may large arc error introduced upon removal arc expressed terms kullbackleibler information divergence efficiently computating information divergence arc unfortunately straightforward computation kullbackleibler information divergence computationally far expensive requires summing configurations entire set variables operation order o2 jv gj however following property kullbackleibler information divergence exploited compute information divergence locally lemma 41 let v set statistical variables let x z v mutually disjoint subsets v let pr pr 0 joint probability distributions v pr 0 c kullbackleibler information divergence satisfies proof exploiting factorization pr 0 terms pr find pr pr 0 using definition 27 derive log prc xy z since yields c xyz c xyz efficiently computing kullbackleibler information divergence ipr pr vr6vs linear subset arcs digraph belief network suffices sum configurations arc block fi g amounts computing quantity c g vs fvsg log derived application chain rule probability theory hence computation information divergence ipr pr vr6vs requires probabilities prc g vs computed original belief net work fact latter two sets probabilities simply computed former set probabilities using marginalization conditional probabilities used compute furthermore probabilities prc g vs known divergence ipr pr vr6vs share head vertex v computed simultaneously since computations require probabilities prc g vs selecting set arcs removal selecting optimal set linear arcs removal one carefully weight advantage reduction computational complexity inference belief network disadvantage error introduced represented joint probability distribution removal arcs given linear subset arcs digraph belief network b define function expressing exact reduction computational complexity inference network b k cost function expressing computational complexity inference net work furthermore define exact divergence function given arcs probability distribution pr defined network b absolute divergence note function k depends algorithms used probabilistic inference example cliquetree propagation algorithm lauritzen spiegelhalter employed kb expresses sum number configurations sets variables cliques decomposable graph rendered upon moralization subsequent triangulation digraph kba expresses complexity terms approximated network b removal arcs assume optimal triangulation moral graphs b ba since bad triangulation moral graph ba may even yield negative value cb pearls polytree algorithm cutset conditioning employed kb equals number configurations set variables loop cutset digraph optimal selection method weights advantage expressed cb disadvantage expressed removal set arcs network b unfortunately optimal selection scheme first depend heavily algorithms used probabilistic inference secondly depend purpose network within specific application furthermore rather expensive computational point view evaluate exact measures c possible linear subsets arcs general employment heuristic measures selection near optimal set arcs removal suffice avoid costly evaluations possible subsets arcs heuristic measures based combining local advantages disadvantages removing arc individually heuristic functions c respectively c expressing impact computational complexity error introduced removing arc may defined various degrees sophistication fact kullbackleibler information divergence measures well one joint probability distribution approximated another exhibiting simpler dependence structure 22 13 hence instead computing absolute divergence information inequality used information divergence associated arc described previous section note combines divergence removing arc separately independently defining heuristic function c valuing reduction computational complexity inference exact methods probabilistic inference upon removal set arcs belief network following scheme employed complexity methods exacts inference depends large extend connectivity digraph belief network arc digraph g set loops undirected cycles denoted loopset loopset arc consists loops digraph containing arc loopset arc provides local information role arc connectivity digraph set found depthfirst search chains graph backtracking possibilities storing set vertices found along chain form bitvector define heuristic function c cb loopset ie c expresses number distinct loops broken removal set arcs digraph plus fraction ff 2 0 1 total number arcs rendered superfluous optimal value ff depends algorithm used exact probabilistic inference combined measure reflecting tradeoff advantage c disadvantage arc removal may form suggested kjaerulff 13 chosen cb comparable function w expresses desirability removing set arcs belief network suppose maximum absolute error 0 allowed probabilistic inference multiply approximated belief network suppose probability evidence processed never smaller constant observe lemma 38 set arcs safely removed network 1ipr pra 2 hence optimal set arcs found removal solve following optimization problem maximize wba ag subject linear note constraint ensures error prior posterior probability distribution never exceeds optimization problem solved employing simulated annealing technique 12 using evolutionary algorithm 17 find linear set arcs removal nearly optimal real optimal solution appropriate search since heuristic functions involved search process example 2 consider belief network example 1 suppose probability evidence processed approximated belief network exceed suppose maximum absolute error allowed conditional probabilities inferred approximated network first three loops g identified loop 1 constitutes vertices g thus loopset arc loopset arc c following table obtained cb linear set desirable set arcs removal wb 49547 note removal graph ga singly connected therefore network least twice fast probabilistic inference compared original network using either pearls polytree algorithm cutset conditioning method cliquetree propagation probability probability evidence observed upper bound figure 3 posterior error probabilities inferred approximated example belief network actually probability evidence processed approximated network error inferred probabilities bounded requires prc figure 3 show observed maximum absolute error upper bound obtained evidence c v g prc 0205 3 efficiently computing approximation belief network removal linear set arcs belief network requires computation new set probability assessment functions reflect introduced qualitative conditional independence quantitative conditional independence recall definition 31 new probability assessment functions removal arc v r selected removal kullbackleibler information divergence ipr pr vr6vs sufficiently small order error introduced approximating network removal bounded probabilities prv fact already computed computation information divergence ipr pr vr6vs arcs digraph belief network probabilities stored temporarily suffices assign probabilities new probability assessment functions head vertex arc selected removal 42 dynamic approximation scheme section consider belief networks singly connected digraphs special case approximation singly connected digraph exhibits loops one chain exists two vertices digraph networks arcs removed dynamically evidence processed contrast static removal arcs preprocessing phase inference described previous section therefore computational complexity processing evidence reduced depending evidence estimate lower bound probability evidence provided advance detailed description analysis method beyond scope paper however practical outline scheme presented based pearls polytree algorithm first show variables network retain prior probabilities upon removal arc lemma 42 let g gamma belief network singly connected digraph g let pr joint probability distribution defined b furthermore let arc g let b approximated belief network removal defined definition 31 let pr vr6vs joint probability distribution defined b vr6vs pr vr6vs proof assume vertices singly connected digraph indexed ascending topological order ie pair vertices directed path v g j proof induction index variable v base case lemma 36 pr vr6vs apply chain rule principle marginalization obtain pr vr6vs c g vr 6vs c g vr 6vs singly connected variables mutually independent dseparation criterion hence pr vr6vs c g vr 6vs assumption vertices g ordered ascending topological order induction hypothesis assume applying principle induction find pr vr6vs c g vr 6vs c g vr 6vs consider singly connected digraph singly connected digraph chain exists v r v except chain constituting arc v r therefore g vr 6vs holds singly connected digraph g vr6vs subset variables v g observation independence relationship variables v r v given g vr 6vs v remain unchanged evidence given subset variables informally speaking means evidence processed belief network compute kullbackleibler information divergence posterior probability distribution defined belief network posterior distribution approximated network removal arc locally similar exposition properties kullbackleibler information divergence applied general belief networks multiple arc removals presented previous sections shown belief network consisting singly connected digraph pr joint probability distribution defined network pra joint probability distribution defined multiple approximated network removal arcs note computation divergence expensive computational resources computation causal diagnostic messages vertex v pearls polytree algorithm assuming logarithms require one time unit furthermore fact using pearls polytree algorithm arcs physically removed blocking causal diagnostic messages updating probability distribution suffice observation envisage approximate wavefront version polytree algorithm sending messages blocked two connected vertices graph probabilistic dependency relationship vertices weak block messages information divergence per blocked arc small total sum information divergences blocked arcs exceed predetermined constant maximum absolute error allowed probabilistic 5 discussion related work presented scheme approximating bayesian belief networks based model simplification arc removal section compare proposed method methods belief network approximation existing belief network approximation methods annihilating small probabilities belief universes 8 removal weak dependencies belief universes 13 methods proven successful reducing complexity inference belief network reallife applications using bayesian belief universe approach 9 method annihilating small probabilities jensen andersen reduces computational effort probabilistic inference method cliquetree propagation used probabilistic inference basic idea method eliminate configurations small probabilities belief universes accepting small error probabilities inferred network end k smallest probability configurations selected belief universe k chosen sum probabilities selected configurations universe less predetermined constant constant determines maximum error approximated prior probabilities belief universes compressed take advantage zeros introduced jensen andersen point range probabilities evidence known advance method applied approximate belief network error approximated posterior probabilities computed network bounded predetermined constant similar method annihilating small probabilities method removal weak dependencies kjaerulff reduces computational effort probabilistic inference method cliquetree propagation used kjaerulffs approximation method method annihilation complementary techniques 13 basic idea method remove edges chordal graph constructed digraph belief network model weak dependencies weaker dependencies smaller error introduced represented joint probability distribution approximated upon removal edge method operates junction tree belief network given constant set edges removed sequentially error introduced prior distribution smaller removal edge results decomposition clique containing edge two smaller cliques results simplification junction tree thereby reducing computational complexity inference network comparing methods approximating belief networks first find method annihilating small probabilities belief universes introduces error inversely proportional probability evidence 8 methods based removing arcs introduces error inversely proportional square root probability evidence furthermore since original joint probability distribution absolutely continuous respect approximated probability distribution processing evidence approximated belief network method safe sense undefined conditional probabilities arise evidence nonzero probability original distribution evidence processed approximated belief network superset evidence processed original network contrast method annihilating small probabilities belief universes hand however advantage annihilating small probabilities method operates quantitative part belief network whereas arc removal methods change qualitative representation well remedied introducing virtual arcs replace removed arcs virtual arcs used probabilistic inference method presented paper similarities kjaerulffs method removal weak dependencies belief universes 13 methods aim reducing inference belief network removing arcs edges however independency statements enforce contrast v r v j c n fv r kjaerulffs method c v g denotes clique containing edge removed kjaerulffs method furthermore kjaerulffs method removal based cliquetree propagation algorithm restricts removal one edge clique time order error introduced bounded predetermined constant contrast method allows larger set arcs edges removed parallel still guaranteeing introduced error bounded predetermined constant regardless algorithms probabilistic inference used summarize conclusions scheme propose approximating belief networks operates directly digraph belief network relatively low computational com plexity provides bound posterior error presence evidence independent algorithms used probabilistic inference acknowledgements author would like acknowledge valuable discussions linda van der gaag utrecht university netherlands r index expression belief networks information disclosure computational complexity probabilistic inference using bayesian belief networks tutorial stochastic simulation algorithms belief networks approximating probabilistic inference bayesian belief networks nphard propagating uncertainty bayesian networks probabilistic logic sam pling approximations bayesian belief universes knowledgebased systems bayesian updating causal probabilistic networks local computations use causal probabilistic networks high level models computer vision journal australian mathematical society reduction computational complexity bayesian networks removal weak dependencies information theory statistics lower bound discriminating information terms variation local computations probabilities graphical structures application expert systems genetic algorithms probabilistic reasoning intelligent systems networks plausible inference probabilistic inference multiply connected belief networks using loop cutsets combination exact algorithms inference bayesian belief networks substantive research hypothesis graphical models applied multivariate statistics tr ctr helge langseth olav bangs parameter learning objectoriented bayesian networks annals mathematics artificial intelligence v32 n14 p221243 august 2001 marek j druzdzel linda c van der gaag building probabilistic networks numbers come guest editors introduction ieee transactions knowledge data engineering v12 n4 p481486 july 2000 helge langseth thomas nielsen fusion domain knowledge data structural learning object oriented domains journal machine learning research 4 1212003 magnus ekdahl timo koski bounds loss probability correct classification model based approximation journal machine learning research 7 p24492480 1212006 rina dechter irina rish minibuckets general scheme bounded inference journal acm jacm v50 n2 p107153 march russell greiner christian darken n iwan santoso efficient reasoning acm computing surveys csur v33 n1 p130 march 2001