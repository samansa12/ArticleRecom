approximate nonlinear filtering twodimensional diffusion onedimensional observations low noise channel asymptotic behavior nonlinear continuous time filtering problem studied variance observation noise tends 0 suppose signal twodimensional process one components noisy onedimensional function signal depending unnoisy component observed low noise channel approximate filter considered order solve problem detectability assumptions prove filtering error converges 0 upper bound convergence rate given efficiency approximate filter compared efficiency optimal filter order magnitude error two filters observation noise vanishes obtained b introduction due vaste application engineering problem filtering random signal x noisy observations function hx signal considered several authors particular case small observation noise widely studied several articles devoted research approximate filters asymptotically efficient observation noise vanishes among one notices first group onedimensional system observed injective observation function h see 4 5 1 case filtering error small observation noise small one find efficient suboptimal finitedimensional filters multidimensional case appears later 6 7 assumption injectivity h required particular extended kalman filter studied h injective process fx g cannot always restored observation filtering error always small case studied 3 however classes problems fx g restored fhx g cases filtering error small one looks efficient suboptimal filters instance fx g sometimes obtained fhx g quadratic variation see 2 8 9 11 interested another case hx differentiable respect time fx g obtained fhx g derivative precisely consider framework 10 describe consider twodimensional process x given ito equation dx 1 dx 2 1 initial condition x 0 concerned problem estimating signal observation process modelled equation dy f standard independent realvalued wiener processes small nonnegative parameter particular f 1 x 1 position moving body r x 2 speed body submitted dynamical force described f 2 random force described oe one noisy observation position functions h x 2 injective signal x least theoretically exactly restored observation interested asymptotic case 0 look good approximation optimal filter approximation finite dimensional solution finite dimensional equation driven problem dealt 10 oe constant means formal asymptotic expansion optimal filter stationary situation aim work rigorous mathematical study filter proposed 10 namely solution f 12 initial condition 0 filter fact correspond extended kalman filter stationary gain one neglects contribution derivatives f f 1 stability filter evident requires assumptions stable prove work x 1 x 2 also verify 6 improved oe constant h linear f 2 linear respect x 1 case referred almost linear case proofs follow method 7 contents organized follows section 2 introduce assumptions needed sequel study filtering error converges zero precisely obtain rate 5 section 3 error approximate filter optimal filter studied prove 6 section 4 devoted almost linear case notations following notations used oe f 22 jacobian matrices f sigma either 2 theta 2 matrix phi r 2 valued linevector phi realvalued see section 3 symbol used transposition matrices describing behaviour approximate filters write asymptotic expressions meaning given following definition definition 11 consider real vectorvalued stochastic process f g fi real p 1 write q 0 ff 0 positive constants c 1 c 2 c 3 e gammac 3 ff 0 small situation process f g usually said converge zero rate order fi time scale order ff 2 estimation x following assumptions used throughout article last one depends parameter ffi 1 random variable moments finite standard independent wiener processes independent x 0 h3 function h c 3 bounded derivatives h 0 positive h4 function f c 3 bounded partial derivatives f 12 positive h5 function oe c 2 bounded partial derivatives h6ffi one hasffi oe f x positive oe h f consider system 12 filter 3 let f filtration generated filtration generated assumption h6ffi says system contain much nonlinearity satisfied may small positive probability filter loose signal see 8 similar problem rather restrictive condition discuss end section general case hold theorem 21 assume h1 h5 exists universal ffi 1 h6ffi holds one x 1 l p p 1 entering proof result rescale system order reduce notation h6ffi replace processes x 1 x 1 oe oe f functions f 1 oe h respectively replaced oe oe oeoe f replaced oe f h apply filter 3 new system obtain f oe shows problem reduced case consider change basis defined matrix inverse gamma1 2 2 consider process going check z solution linear stochastic differential equation study exponential stability equation enable estimation components z theorem immediately follow equation z equations 1 3 dt equation introduce taylor expansions functions f h hx 1 r 2 r valued processes depending fx g fm g obtain linear equation x applying transformation 7 deduce z equation type precise computation shows 11 12 11 21 22 e 2 theta 2 matrix valued process uniformly bounded converges 0 similarly matrixvalued process u also uniformly bounded stability constant matrix general case ffi 1 coefficients controlled matrix uniformly close gamma2i ffi close 1 particular 2 one choose ffi therefore small end proof theorem 21 goal deduce estimate z l 2p p integer itos formula 8 process kz solution deduce moment order p kz k 2 finite dt 9 one z consequence cauchyschwarz inequality one ku thus obtain inequality dt gammap ff moreover exists c 0 dt solving differential inequality one obtains c 00 thus z 14 order magnitude components x follows 7 form gamma1 pi remark 10 time scale estimation order one compare time scale obtained observation function injective see instance 5 means takes time estimate signal surprising since second component signal well observed also systems time scale different components signal see 8 theorem 21 need assumption h6ffi restriction non linearity system otherwise difficult ensure filter loose signal problem also occurs 8 actually chosen filter 3 gives good approximation next section stable one 4 replace processes constant numbers oe f h obtain filter constant gain work previous estimations prove result theorem 21 holds filter without h6ffi soon f thus two filters filter stable tracks signal rather weak assump tions filter 3 seems fragile gives good stability assumptions better approximation optimal filter 3 estimation main result contained section theorem 31 states rate convergence approximate filter considered paper towards optimal filter order give proof theorem sequence steps needed change probability measure differentiation respect initial condition integration parts formula similar method proof adopted 7 theorem 21 may problem stability general non linear case theorem 31 assume h1 h6ffi h7 law x 0 c 1 positive density p 0 respect lebesgue measure rp 0 l 2 ffi h6ffi close enough 1 filter given equation 3 satisfies x 1 l 2 rest section devoted proof theorem suppose theorem 21 1 consider matrix depends notice p solution stationary riccati equation e f sigma e process r 4 also need inverse p namely change probability measure random variables viewed functions initial condition x 0 wiener processes w w going make change variables view girsanov theorem viewed change probability measure however estimations made original probability p thus consider new probability measure given f dp ae gamma z thx 1 z th 2 x 1 ds oe probability p socalled reference probability one checks easily girsanov theorem w standard independent wiener processes let us define probability measure e p f ds oe processes e z tsigma ds standard independent wiener processes e p hand one logl z thx 1 z th 2 x 1 ds gamma z tsigma differentiation respect initial condition estimation random variables involved computation viewed functions fy g let us denote r 0 differentiation respect initial condition x 0 puted l p particular see 13 14 processes x logl differentiable obtain respectively matrix vectorvalued processes aim estimate process logl u integration parts enable conclude applying operator r 0 14 one gets logl z th 0 x 1 x 1 dy z th 0 x 1 x 1 also differentiate 13 sigma 0 jacobian matrix sigma obtain matrix r 0 x invertible itos calculus shows equation 16 one write logl r 0 logl r 0 logl r 0 since one h 0 x 1 x 1 hand equations x equations 1 3 respectively one writing differential p form obtain dt j 2 one write taylor expansions f h hx 1 using expansions together consequence 12 equation 19 obtain dt j 2 dt adding equation 18 obtain process v 15 satisfies j 2 dt 20 matrix given consider also matrixvalued process sigma equation 20 written form u gammar j 2 u u apply 12 last line deduce ekv 0 order gamma3 dt estimate terms righthand side computing matrix obtain 11 21 12 22 e uniformly bounded proof theorem 21 see matrix simply satisfies thus 2 ffi close enough 1 small enough ff also notice ffp u u bounded thus 24 implies small dt let us first estimate j 3 deduce riccati equation 11 satisfied p process defined 21 satisfies computing matrix applying theorem 21 check spaces l p thus term sigma u easily shown order magnitude hand looking equation applying itos formula prove c 2 function ae bounded derivatives one applying result functions involved p gamma1 appears j 1 deduce terms j 3 involving j 1 j 2 also order gamma1 finally oe respectively order 12 32 last term order gamma1 deduce also estimate j 4 j 5 check order gamma34 thus 26 enables conclude l 2 take conditional expectation respect estimation conditional expectation contraction l 2 thus ev o1 l 2 therefore obtain definition 15 logl r 0 application integration parts formula estimation righthand side 27 completed means integration parts formula proved lemma 342 7 w functional defined probability space differentiable respect initial condition spaces iis differentiation respect ith component x 0 0 28 write equation 17 form sigma dt e r 0 equation differentiated respect x 0 apply integration parts formula 28 coefficients matrix r 0 ith line p gamma1p 0 0 summing multiplying u logl r ir 0 u 0 30 first term 30 exactly term want estimate 27 second term 30 17 22 proceed study 23 stability matrix obtained 25 boundedness u implies r 0 exponentially small l 2 second term negligible let us study third term 30 differentiating 29 transforming back e w w get u gammasigma p gamma1 summing using ae jth line ae obtain phi solution gammasigma oe 0 jacobian oe computation shows multiplication right u yields process also gamma1 term involving second derivative oe gamma12 proceeding study 23 deduce phi order gamma12 thus 27 30 estimation psi phi yield multiply right matrix u coefficients order 32 first column second column deduce order claimed theorem pi 4 almost linear case interesting consider particular case oe h 0 f 12 constant system 12 8 dx 1 x 2 dx 2 dy particular h6ffi holds possible improve upper bounds given theorem 31 result stated following proposition proposition 41 assuming h1 h7 hold 32 filter given equation verifies x 1 l 2 proof proof follows closely sequence steps adopted theorem 31 matrices r constant processes j 1 j 2 j 5 zero order improved order gamma34 thus v gamma14 obtain gamma14 27 end proof see bounded multiplication u yields process order gamma12 process phi 31 bounded small conclude deduce proposition pi r approximation techniques non linear filtering theory piecewise monotone filtering small observation noise approximate filter conditional law partially observed process nonlinear filtering asymptotic analysis optimal filtering problem onedimensional diffusions measured low noise channel nonlinear filtering onedimensional diffusions case high signalto noise ratio nonlinear filtering smoothing high signaltonoise ratio efficiency extended kalman filter non linear systems small noise estimation quadratic variation nearly observed semimartingales application filtering filtrage lineaire par morceaux avec petit bruit dobservation asymptotic analysis optimal filtering problem two dimensional diffusions measured low noise channel nonlinear filtering control switching diffusion small observation noise tr