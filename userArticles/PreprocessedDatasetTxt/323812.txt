active management data caches exploiting reuse information abstractas microprocessor speeds continue outpace memory subsystems speed minimizing average data access time grows importance multilateral caches afford opportunity reduce average data access time active management block allocation replacement decisions evaluate compare performance traditional caches multilateral caches three active block allocation schemes mat nts pcs also compare performance nts pcs multilateral caches nearoptimal nonimplementable policy pseudoopt employs future knowledge achieve active allocation active replacement nts pcs evaluated relative pseudoopt respect miss ratio accuracy predicting reference locality actual usage accuracy tour lengths blocks cache results show multilateral schemes outperform traditional cache management schemes fall short pseudoopt increasing prediction accuracy incorporating active replacement decisions would allow closely approach pseudoopt performance b introduction minimizing average data access time paramount importance designing highperformance machines unfortunately access time ochip memory measured processor clock cycles increased dramatically disparity main memory access work done university michigan times processor clock speeds widen eect disparity compounded multipleissue processors continue increase number instructions issued cycle many approaches minimizing average data access time common solution incorporate multiple levels cache memory onchip still allocate replace blocks manner essentially caches rst appeared three decades ago recent studies 231610818 explored better ways congure manage resource precious rstlevel l1 cache active cache management active block allocation replacement improve performance given size cache maintaining useful blocks cache active management retains reuse information previous tours blocks uses manage block allocations andor replacements subsequent tours 1 order partition allocation blocks within cache structure several proposed schemes 1610818 incorporate additional data store within l1 cache structure intelligently manage state resulting multilateral 2 17 cache exploiting reuse pattern information structures perform active block allocation still relegate block replacement decisions simple hardware replacement algorithms processor designers typically design largest possible caches still ever growing processor die multilateral designs shown perform well better larger single structure caches requiring less die area 1722 given die size reducing die requirements attain given rate data supply free space resources example dedicate space branch prediction data forwarding instruction supply instruction reorder buer paper evaluate performance three proposed cache schemes perform active block allocation compare performance one another traditional singlestructure caches implement mat 10 nts 16 pcs 18 schemes using 1 tour cache block time interval allocation block cache subsequent eviction given memory block many tours cache use term multilateral refer level cache contains two data stores disjoint contents operate parallel hardware similar possible order fair comparison block allocation algorithms uses experiments show making placement decisions based eective addressbased block reuse nts scheme outperforms macroblock based pcbased approaches mat pcs respectively three schemes perform comparably larger directmapped caches better associative caches similar size examine performance optimal nearoptimal multilateral caches determine performance potential multilateral schemes optimal nearoptimal schemes excel block replacement decisions block allocation decisions direct consequence replacement decision compare performance two implemented multilateral schemes nearoptimal scheme determine reason performance implemented schemes perform better improvements need made block allocation replacement choices rest paper organized follows section 2 discusses techniques aid reducing average data access time section 3 discusses active cache management detail presents past eorts perform active block allocation section 4 presents simulation methodology section 5 evaluates performance three multilateral schemes section 6 present performance optimal nearoptimal multilateral schemes perform nearoptimal replacement blocks compare decisions made nearoptimal scheme made two implementable schemes conclusions given section 7 background many techniques reducing tolerating average memory access time prominent among 1 store buers used delay writes bus idle cycles order reduce bus contention 2 nonblocking caches overlap multiple load misses fullling requests hit cache 1912 hardware software prefetching methodologies attempt preload data memory cache needed 514615 caching 11 improves performance direct mapped caches addition small fully associative cache l1 cache next level hierarchy schemes contribute reducing average data access time paper approaches problem premise average data access time reduced exploiting reuse pattern information actively manage state l1 cache approach used techniques reduce average data access time 3 active cache management active cache management used improve performance given size cache structure controlling data placement management cache keep active working set resident even presence transient references active management caches consists two parts allocation blocks within cache structure demand miss 3 replacement blocks currently resident cache structure 4 block allocation todays caches passive straightforward blocks demand fetched placed corresponding set within cache structure however decision take consideration blocks usefulness usage characteristics examining past history given block one method aiding future allocations block decisions range simply caching target block bypassing placing block particular portion cache structure hope making best use target cache block blocks remain cache simple block replacement policies used choose block eviction todays caches choice often suboptimal multilateral cache allocation replacement problems coupled particular blocks available replacement direct consequence allocation demandmissed block recently several approaches ecient management l1 data cache via block 3 allocation decisions blocks loaded demand miss eg prefetched blocks streaming buer scheme proposed 11 bypassing schemes considered however schemes make proper bypass decisions allocation decisions prefetched data improve upon performance schemes evaluated herein 4 consider writeallocate caches paper write noallocate caches follow subset rules writes subject allocation decisions allocation decisions emerged literature nts 16 mat 10 dualselective 8 pcs 18 however none approaches makes sophisticated block replacement decisions instead relegates decisions respective cache substructures 31 nts model nts nontemporal streaming cache 16 locationsensitive cache management scheme uses hardware dynamically partition cache blocks two groups temporal nontemporal nt based reuse behavior past tour block considered nt tour l1 word block reused blocks classied nt subsequently allocated separate small cache placed parallel main l1 cache blocks marked prior information available handled main cache data placement decided using reuse information associated eective address requested block eectiveness nts reducing miss ratio memory trac average access penalty demonstrated primarily mostly numeric programs 32 mat model mat memory address table cache 10 another scheme based use eective addresses however dynamically partitions cache data blocks two groups based frequency reuse blocks become tagged either frequently infrequently accessed memory address table used keep track reuse information granularity grouping macroblock dened contiguous group memory blocks considered usage pattern characteristics blocks determined infrequently accessed allocated separate small cache scheme shown signicant speedups generic caches due improved miss ratios reduced bus trac resulting reduction average data access latency 33 dual cacheselective cache model dual cache 8 two independent cache structures spatial cache temporal cache cache blocks dynamically tagged either temporal spatial locality prediction table used maintain information recently executed loadstore instruction blocks tagged neither spatial temporal nd place cache bypass cache method useful handling vector operations random access patterns large strides introduce self interference however two caches necessarily maintain disjoint contents temporal cache designed smaller line size compared spatial cache required data found caches read temporal cache written parallel order overcome replication coherence problem authors proposed simplied version dual cache called selective cache selective cache one memory unit like conventional cache incurs hardware cost due locality prediction table dual cache data exhibiting spatial locality temporal locality selfinterfering cached benchmarks study scheme shown perform better conventional cache size selective cache improvement bypass cache 7 relies compiler hints decide whether block cached bypassed 34 pcs model pcs program counter selective cache 18 multilateral cache design evolved cna cache scheme 23 pcs cache decides data placement block based program counter value memory instruction causing current miss rather eective address block nts cache thus pcs tour performance blocks recently brought cache memory accessing instruction rather recent tour performance current block brought cache used determine placement block performance pcs best programs reference behavior given datum wellcorrelated memory referencing instruction brings block cache 35 multilateral cache schemes several cache schemes considered multilateral caches assist cache 13 used hp pa7200 victim cache 11 however neither schemes actively manage cache structures using reuse information obtained dynamically program execution assist cache uses small data store staging area data entering l1 potentially prevents data entering l1 indicated compiler hint victim cache excels performance majority cache misses con ict misses result limited associativity main cache buer victim scheme serves dynamically increase associativity hot spots typically directmapped main cache schemes shown perform well 22 require costly data path two data stores perform data migrations require without intercache data path present schemes cannot operate use previous tour information actively deciding data store allocate block victim cache shown perform well relative activelymanaged schemes main cache directmapped 18 paper evaluate multilateral schemes use dynamic information allocate data among two data stores direct data path 4 simulation methodology simulator set benchmark programs used compare performance multilateral cache strategies section describes dynamic superscalar processor memory simulators used evaluate cache memory structures system conguration used methods metrics benchmarks constitute simulation environment 41 processor memory subsystem processor modeled study modication simoutorder simulator simplescalar 3 toolset simulator performs outoforder ooo issue execution completion derivative mips instruction set architecture schematic diagram targeted processor memory subsystem shown figure 1 summary chosen parameters architectural assumptions memory subsystem modeled mlcache tool discussed consists separate instruction data cache perfect secondary data cache main memory instruction cache perfect responds single cycle data cache modeled fetch mechanism fetches 16 instructions program order per cycle branch predictor perfect branch prediction issue mechanism outoforder issue 16 operations per cycle 256 entry instruction reorder buffer ruu 128 entry loadstore queue loads may execute prior store addresses known functional units multdiv 8 fp multdiv 8 ls units f u latency int alu11 int mult31 int fp div1212 ls11 instruction cache perfect cache 1 cycle latency data cache multilateral l1 writeallocate latency latency nonblock ing 8 memory ports processor cache data cache secondary cache main memory figure 1 processor memory subsystem characteristics conventional data cache split two subcaches disjoint contents placed parallel within l1 multilateral cache subcache unique conguration size setassociativity replacement policy etc b caches probed parallel equidistant cpu b nonblocking 32byte lines single cycle access times standard singlestructured data cache model would simply congure cache desired parameters set b cache size zero l2 cache access latency bus l1 l2 bytescycle data bandwidth l1 l2 access fully pipelined miss request sent l1l2 bus every cycle 100 pending requests l2 cache modeled perfect cache order focus study management strategies l1 42 mlcache simulation tool mlcache 22 eventdriven timingsensitive cache simulator based latency eects le cache timing model discussed depth 21 easily congured model various single multilateral cache structures using library cache state data movement routines interactions modeled library routines users write support routine description checkforcachehit check see accessed block present cache update place accessed block cache moveover move accessed block one cache another doswap move accessed block cache1 cache2 move evicted block cache1 doswapwithinclusion place accessed block cache1 cache2 move evicted block cache2 cache1 dosaveevicted move block evicted cache1 cache2 findandremove remove block cache checkforreuse determine block exhibits temporal behavior word reuse table 1 basic support routines provided mlcache simulator user call routines conguration le control cache state interactions management routines call simulator tool easily joined wide range eventdriven processor simulators described processor model work based simplescalar toolset together combined processorand cache simulator simplescalarmlcache provide detailed evaluations multiple cache designs running target workloads proposed processorcache congurations mlcache easily retargetable due provision library routines user choose perform actions take place cache situation routines accessed single c le named congc user simply modies congc describe desired interactions caches processor memory user also controls actions occur via delayed update mechanism built cache simulator delayed update used allow behavioral cache simulator dineroiii 9 account latency latency latencyadding eects use delayed update causes eects access ie access placement cache removal replaced block etc occur calculated latency access passed table 1 shows routines provided brief description interactions needed additional library routines added however brief examples easy see modular librarybased simulator already allows signicant range cache congurations examined evaluate performance three multilateral schemes mat nts pcs compare performance three traditional singlestructure caches 16k directmapped cache 16k 2way associative cache 32k directmapped cache cong urations evaluated caches shown table 2 43 simulated cache schemes performing realistic comparison among program counter eective address schemes requires detailed memory simulators mat nts pcs cache management schemes described chose omit selective cache block allocation decisions similar made pcs pcs hardware implementation simpler ensure fair comparison evaluation placed management schemes platform within uniform multilateral environment using mlcache tool congurations includes 32entry structure stores reuse information described scheme following subsections describe implementations mat nts pcs cache management schemes main cache labeled cache auxiliary buer labeled cache b caches placed equidistant cpu three schemes congured similar possible one another performance dierences attributed primarily dierences among block allocation decisions make 431 structure operation nts cache nts cache using model 18 adapted scheme proposed 16 actively allocates data within l1 based blocks usage characteristics particular blocks known exhibited nontemporal reuse placed b others presumably temporal blocks sent done hope allowing temporal data remain larger cache longer periods time shorter lifetime nontemporal data short quickly accessed small associative b cache memory access desired data found either b data returned processor 0 added latency block remains cache single mat nts pcs cache b b b size 16k16k32k 16k 2k 16k 2k 16k 2k associativity 121 1 full 1 full 1 full replacement policy lru lru lru lru latency next level table 2 characteristics four congurations studied timeslatencies cycles found miss block entering l1 checked see entry detection unit du du contains temporality information blocks recently evicted l1 managed follows entry du describes one block contains block address matching tnt bit indicate temporality recent tour eviction block checked see exhibited temporal reuse ie word block referenced least twice justcompleted tour l1 cache structure tnt bit set accordingly du corresponding du entry found evicted block new du entry created made mru du structure miss new missed block address matches entry du tnt bit entry checked block placed indicates temporal b du entry made mru du better chance remaining du future allocation predictions thus creation access entry du treated use du 32 entries simulations maintained lru replacement matching du entry found missed block assumed temporal placed 432 structure operation pcs cache pcs cache 18 decides data placement based program counter value memory instruction causing current miss rather eective address block nts cache thus performance blocks missed individual memory accessing instructions rather individual data blocks determines placement data pcs scheme pcs cache structure modeled similar nts cache du indexed memory accessing instructions program counter updated manner similar nts scheme block replaced temporality bit entry associated pc memory accessing instruction brought block cache beginning tour set according blocks reuse characteristics justcompleted tour cache du entry matches pc value one created replaces lru entry du instruction subsequently misses loaded block placed b instructions pc hits du prediction bit indicates nt otherwise block placed instruction misses du data placed 433 structure operation mat cache mat cache 10 structure memory address table mat keeping track reuse information guiding data block placement b cache l1 structure implementation mat 32entry fully associative structure like du nts pcs note however original implementation mat reported 10 used 1k entry direct mapped table mat entry consists macroblock address nbit saturating counter 8bit counter 1kb macroblock size used original study memory access caches b checked parallel requested data time counter corresponding mat entry accessed block incremented corresponding entry one created counter set 0 lru entry mat replaced counter serves indicator usefulness given macroblock used decide whether block macroblock placed b cache next tour cache miss macroblock address incoming block used index mat entry exists counter value incremented compared decremented counter macroblock corresponding block would replaced incoming block placed cache counter decremented ensure data eventually replaced counter resident data continue decrease reaccessed often enough continues con ict recently accessed blocks counter value incoming block higher con icting block currently cache incoming block replaces block cache situation indicates incoming block macroblock shown usefulness earlier tours macroblock con icting block resides thus given higher priority residing larger main cache counter value incoming block less current resident block incoming block placed smaller b cache finally entry corresponds incoming block block placed cache default new entry created mat counter initialized zero entry corresponds con icting block currently cache counter value assumed 0 permitting new block replace easily entry found mat resident block cache another macroblock maps set mat must accessed recently current block therefore less likely used near future nts pcs schemes direct data path b caches unlike schemes however mat structure updated every access cache instead replacements 44 benchmarks table 3 shows 5 integer 3 oating point programs spec95 benchmark suite used study programs varying memory requirements simulations done using training data sets program run completion exception perl terminated rst 15 billion instructions 45 relative cache eects ratio important metric evaluating cache management scheme cache hitmiss ratio however ooo processors multiported nonblocking caches eective memory latencies seen processor vary according number outstanding miss requests since main focus study evaluate eectiveness l1 cache instruction count memory references millions perfect memory performance program millions loads stores cycle count millions ipc spec95 integer benchmarks compress 3568 737 599 535 66644 gcc 26385 6115 3624 4350 60648 go 54813 11579 4140 9133 60049 perl 150000 39682 26983 23289 64408 floating point benchmarks hydro2d 97450 19611 6090 12763 76353 su2cor 105409 26220 8474 15234 69192 table 3 eight benchmarks memory characteristics structure using special management techniques relative cache eects ratio rcr developed 17 rcr given processor running cache conguration x relative cache conguration base given cyclecount base cyclecount p erfectcache 1 cyclecount p erfectcache total number cycles needed execute program processor perfect cache conguration rcr normalized metric base cache perfect cache 1 base cache conguration 0 perfect cache conguration cache congurations perform better base rcr 0 1 lower rcr better cache conguration performs worse base rcr 1 rcr gives indication nite cache penalty reduction obtained using given cache conguration rcr mirrors performance indicated speedup numbers isolates cache penalty cycles total run time rescales fraction penalty traditional base cache thus gives direct indication well memory subsystem performs relative ideal perfect cache addition overall performance speedup metric used measure relative performance gains cache management approach section 5 46 block tour reuse concept eectiveness cache management scheme also measured ability minimize cumulative number block tours program run individual cache block tours monitored classied based reuse patterns exhibit tour sees reuse word block considered dynamic temporal tour sees word reuse dynamic nontemporal dynamic temporal dynamic nontemporal tours classied either spatial one word used nonspatial one word used allows us classify tour one four data reuse groups 1 nontemporal nonspatial ntns 2 nontemporal spatial nts nonspatial tns management schemes result fewer longer tours consequently higher percentage data references blocks making ts tours ntns nts tours problematic frequent references data likely cause cache pollution minimize impact bad tours good multilateral cache management scheme utilize accurate block behavior prediction mechanism data allocation decisions 5 experimental results miss ratio often used rank performance benets particular cache schemes however miss ratio weakly correlated performance latencymasking processors nonblocking caches furthermore fails capture latencyadding eect delayed hits overall performance delayed hits discussed 2221 accesses data currently returning cache behalf earlier miss cache block delayed hits incur latencies larger cache hits generally less full cache miss requested data already transit next level memory two programs exhibiting similar miss ratios may thus quite dierent overall execution times due diering numbers delayed hits extent latency masking shown 22 avoid oversimplifying cache schemes impact overall performance instead concentrate two metrics timingsensitive experiments overall speedup relative compress gcc go hydro2d li perl su2cor swim table 4 miss ratios 6 cache schemes running 8 benchmarks base cache relative cache eects ratio presented 51 miss ratio table 4 shows miss ratios six cache congurations running eight benchmarks figure 2 shows corresponding speedup relative 16k directmapped cache naively might assume comparing two congurations particular application higher miss ratio would imply lower speedup since cache stalls account portion run time relative speedup would less relative miss ratio however comparison table 4 figure 2 shows assumption valid compress example miss ratio pcs 101x nts run time 104x longer gcc 32k directmapped cache actually higher miss ratio less run time 16k 2way associative cache swim nts higher miss ratio less run time mat 16k 32k directmapped 16k 2way caches thus relative miss ratio alone inadequate indicator relative performance latency masking miss latency overlap delayed hits must incorporated timing model get accurate performance assessment therefore concentrate performance analysis latencysensitive metrics speedup rcr 52 speedup speedup achieved scheme program shown figure 2 single directmapped 16k1w cache taken base overall speedup obtained using multilateral cache schemes ranges virtually none hydro2d 16 go nts clearly benchmarks tested benet improvements oered cache schemes evaluated ie better management l1 200 400 600 800 1000 1200 1400 1600 compress gcc go hydro2d li perl su2cor swim nts pcs figure 2 overall execution time speedup evaluated cache schemes relative single directmapped data store multilateral schemes increased associativity single cache 16k2w larger cache 32k1w benchmarks appreciable performance gain base cache multilateral schemes often perform well better either higherassociative single cache larger directmapped cache compress gcc benchmarks larger working sets benet larger overall cache space provided directmapped structure although even benchmarks multilateral schemes able obtain signicant part performance boost via better management cache despite smaller size multilateral caches generally perform well compared larger directmapped cache generally faster 2way associative cache multilateral schemes larger directmapped cache oers fast access smaller associative b cache still accessed quickly due small size experiments show using 8way associative b cache instead fully associative b cache would reduce performance less 1 among multilateral schemes see nts scheme provides greatest speedup benchmarks except li mat performs best su2cor swim pcs performs best best multilateral cache speedups order 1 three benchmarks mat pcs schemes perform well groups blocks exhibit similar reuse behavior consecutive tours cache nts scheme may however fail detect reuse patterns correlates reuse information individual cache blocks opposed macroblock memory regions mat memory accessing instructions pcs thus mat pcs schemes perform well programs exhibit simd singleinstruction multipledata behavior reference behavior nearby memory blocks blocks referenced memory accessing instruction may better indicator reuse behavior usage individual block last tour however nts scheme still competitive three benchmarks thus gives best overall performance schemes full suite benchmarks 53 rcr performance figure 3 shows rcr performance mat nts pcs two singlestructure caches 16k directmapped cache serves base comparison see nts pcs eliminate 50 nite cache penalty experienced go perl compress gcc li 32k singlestructure directmapped cache performs best however dierence rcr bestperforming multilateral scheme large except li reduces nite cache penalty twice much mat best multilateral scheme benchmark none caches show signicant improvement rcr remaining three benchmarks hydro2d su2cor swim instances multilateral scheme experience poor performance eg mat perl benchmark relative multilateral schemes block allocation scheme mat well matched characteristics perl benchmark many blocks shortlived frequently accessed blocks placed smaller fullyassociative cache behavior continues many blocks contend space smaller 2k fully associative cache larger 16k cache badly underutilized phenomenon occur multilateral cache schemes extreme multilateral schemes performance may degrade b cache 010030050070090compress gcc go hydro2d li perl su2cor swim rcr nts pcs figure 3 rcr performance evaluated congurations running eight benchmarks rcrs near 10 performance similar base 16k directmapped cache rcrs closer 0 approach performance perfect cache performance degradation addressed improved block allocation mechanisms discussed section 6 54 performance dierences causes though mat nts make block allocation decisions based eective address block accessed performance diers mat may make poor allocation decisions either missed block block would replace cache markedly dierent desirability perceived desirability macroblock resides disparity desirability occurs blockbased desirability mechanism used nts perform better table 5 presents tour analysis go su2cor performance nts relative mat well demonstrated go nts appear manage tours program better actually reduces number tours mat experiences 32 case su2cor performance dierence mat nts small terms rcr clear application much nontemporal spatial data 17 neither scheme reduces tours seen 16kb directmapped cache 6 tour analysis two singlestructure caches also shown two widely disparate benchmarks analyses used compare mat nts also used compare multilateral caches singlestructure caches go 32k directmapped cache management scheme total tours reduction tours total percentage references tour groups cache management scheme total tours reduction tours total percentage references tour groups table 5 tour analysis go top su2cor bottom cache best singlestructure cache performance though slightly lower percentage ts tours nts cache substantially overall tours thus worse overall performance mat nts su2cor 2way associative directmapped caches fail signicantly improve performance base cache due high percentage nts data accessed nts performs better pcs overall speedup rcr dierent basis decision making used pcs nts pc eective address respectively results dierent performance pcs scheme may place block suboptimally cache since placement uenced blocks previously referenced requesting pc example set blocks may brought cache one instruction beginning large routine blocks may reused dierent ways dierent parts program execution eg temporal initialization phase nontemporal main programs execution blocks usage characteristics may attributed single entry du tied pc instruction brought blocks cache tours end instructions entry du updated particular tours behavior directly aecting placement next block requested instruction eect allocation decisions pcs uenced recently replaced block associated load instruction question characteristics recently replaced blocks persistent discussed section 634 allocation decisions made pcs load instruction vary often potentially degrading performance however program counter management schemes may good given instruction loads data whose usage strongly biased 14 one direction ie tours almost temporal almost nontemporal case accurate behavior predictions future tours result good block placement instruction however data blocks loaded instruction diering usage characteristics ie weakly biased 14 placement decisions blocks poor block usage history tnt kept single bit nts pcs macroblock access frequency kept nbit counter mat reducing counter size mat generally leads decreased performance 10 however keeping tour history using 2 3bit counter nts pcs showed virtually performance benet 1bit scheme 6 using reuse information data cache management multilateral schemes operates assumption reuse information useful actively managing cache section assess value reuse information making placement decisions multilateral l1 cache structures rst examine optimal cache structures determine exploit reuse information cache management outline experiments performed validate use reuse information compare performance multilateral schemes performance nearoptimally managed caches size 61 optimally nearoptimally managed caches beladys min 2 optimal replacement algorithm singlestructure cache ie results fewest misses 5 interesting see reuse information used 5 note consider timing models section min optimal algorithm manage single cache interested determining optimal replacement algorithm multilateral cache makes replacement decisions reuse information might best exploited however direct extension min multilateral caches known exception b caches multilateral conguration fully associative blocks free move two caches necessary order retain useful blocks cache structure multilateral cache however degenerate reduces single fully associative cache size equal total cache plus cache b case min used optimally manage hardwarepartitioned fully associative single cache refer beladys min algorithm applied dualfully associative caches opt opt gives upper bound performance multilateral cache given size associativity comparing opt implementable schemes yield direct comparison replacement decisions based reuse information since multilateral caches typically caches b diering associativity performance dierence implementable schemes opt may due replacement decisions also mapping restrictions placed implementable schemes limited associativity would instead like compare performance implementable schemes optimally managed multilateral conguration b caches diering associativity order better attribute dierences placement replacement decisions management policy rather associativity conguration pseudoopt 20 multilateral cache management scheme congurations associativity greater b conguration opt free movement blocks b allowed scheme provided contents b disjoint management adapted beladys min algorithm follows miss incoming block lls empty slot corresponding set cache cache b one exists empty slot exists set cache extended set blocks dened consisting blocks cache cache b map set cache reference b c f b e f c hitmiss figure 4 example showing pseudoopt suboptimal extended set includes block currently resident cache b ie sets whose extended set larger associativity cache block extended set whose next reference farthest future found block currently reside cache b swapped one cache b blocks extended set incoming block placed cache block set next referenced farthest future moved cache b overlling one block cache b block next referenced farthest future replaced choice optimal cases illustrated following example reference pattern figure 4 consider design directmapped cache size 2 blocks 2 sets one block cache b references shown gure block addresses upper case letters map set 0 lower case letters map set 1 cache gure shows contents set 0 set 1 cache cache b memory access using pseudoopt incur 7 misses rst 3 compulsory misses empty blocks replaces c time 4 since c next referenced future f b time replace b replaces f rather finally f c miss however minimum possible number misses 6 achieved replacing f set 0 cache instead c cache b time 4 e replaces b time 6 swapping b c nally f misses c hits cases b caches fully associative pseudoopt reduces opt although pseudoopt also implementable policy performance seen table 6 close opt furthermore pseudoopts performance much better implementable multilateral schemes performance performance dierence implementable schemes opt thus due nonoptimal allocation replacement decisions small portion performance dierence dierence opt pseudoopt due restricted associativity implementable schemes therefore use pseudoopt comparison implementable schemes order eliminate associativity dierence evaluations give better idea realizable performance limitedassociativity multilateral cache dierences placement replacement decisions seen implementable schemes provide insights performance relative nearoptimal scheme 62 simulation environment evaluate opt pseudoopt schemes collected memory reference traces generated simplescalar processor environment 3 trace entry contains eective address accessed type access load store program counter instruction responsible access skipped rst 100 million instructions avoid initialization eects analyzed subsequent 25 million memory references limited number memory references evaluated due space processing time required perform opt pseudoopt cache evaluations experiments use spec95 integer benchmarks compress gcc go li perl since sampling small portion oating point program likely generate references form part regular loop resulting low miss rates however sampled traces integer programs reasonably mirror actual memory reference behavior complete program execution shown section 5 traces annotated include information necessary perform opt pseudoopt replacement decisions counters many useful statistics included outcome access hit miss would occurred optpseudoopt conguration usage information block tour seen scheme number blocks reuse category cache instant time ie number ntns nts tns ts blocks resident cache statistics gathered information regarding performance opt pseudoopt management schemes compared performance implementable schemes optimal schemes accessbyaccess basis due smaller size input sets section compared full program executions performed section 4 chose directmapped 8kb cache fully sociative 1kb b cache 32b blocksize pseudoopt conguration opt conguration simply 9k fully associative cache using larger 162k caches section 4 evaluations would useful determining schemes performance spec benchmarks already small moderately sized working sets 5 relatively short traces would show little performance benet active management large cache whereas benets active management highlighted using smaller caches mlcache simulator used section deals cache memory processor without processor eects timing little signicance mlcache used behaviorallevel simulator 63 results 631 analysis opt vs pseudoopt analyzed annotated traces produced opt pseudoopt runs determine relative performance based miss ratio measurement block usage information particular counted number tours show reuse pattern number type block resident cache given instant often block prior reuse characteristic changes usage pattern subsequent tour 632 miss ratio miss ratios table 6 show performance pseudoopt relatively close opt except go note miss ratio experiments straightforward performance metric simulations behavioral include access latencies processor latencymasking eects go performance disparity opt pseudoopt due limitations associativity cache possibly also compress gcc go li perl pseudoopt table miss ratios 81kb 16kb cache congurations trace inputs pons pseudoopt noswap scheme suboptimal replacement policy pseudoopt replacement number swaps done opt rearrange cache contents least desired cache block replaced however pseudoopt movement choices limited mapping requirements directmapped cache one block mapping set b associated incoming block need swapped replacement despite limited choice blocks replace performance pseudoopt still close opt except go dierence performance pseudoopt opt always much smaller dierence implementable congurations pseudoopt actual performance implementable schemes discussed section 64 addition advantage future knowledge pseudoopt diers implementable schemes freely allowing blocks move b caches order obtain best block replacement however movement actually accounts little performance dierence seen pseudoopt implementable schemes verify claim created version pseudoopt called pons pseudoopt swap disallows data movement caches management scheme used pons pseudoopt except blocks swapped cache b time incoming block replaces block b set next referenced farthest future pons thus possible replaced block cache b referenced sooner future block set extends set cache b could replaced swaps allowed however see table 6 miss ratios pseudoopt pons actually compressopt compresspo gccopt gccpo goopt gopo liopt lipo perlopt perlpo benchmarkscheme blocks cache ts tns nts ntns figure 5 dynamic cache block occupation opt pseudoopt denoted po grouped ntns nts tns ts usage patterns close indicating omission intercache data movement small eect nearoptimum performance particular performance dierence pseudoopt pons much smaller dierence pseudoopt implementable schemes showing major advantage pseudoopt comes management using future knowledge opposed ability move blocks caches since multilateral schemes allow data movement b caches use pseudoopt basis comparison implementable multilateral cache schemes 633 cache block locality analysis examined locality cache blocks resident cache structure given time counting number blocks category time miss taking average duration program locality cache blocks opt pseudoopt congurations shown figure 5 opt pseudoopt managed caches expected data exhibits temporal reuse occupies large portion cache space nontemporal data occupies 23 cache compress pseudoopt furthermore vast majority blocks l1 temporal spatial though blocks relatively small size 32 bytes nd spatial reuse exploited well cache managed properly expected pseudoopt conguration vs opt conguration generally holds fewer ts blocks compress gcc go li perl compress gcc go li perl tour opt pseudoopt persistence persistence figure usage persistence within opt pseudoopt cache due pseudoopt congurations limited placement options suboptimal replacement decisions however perl exception observation opt pseudoopt perls data cache spatial ts nts seen figure 5 keeping nts block cache long enough obtains ts status done pseudo opt slightly increases miss ratio perl benchmark opt indicating maximizing number ts blocks cache always best policy 634 block usage persistence presence certain types blocks cache shows potential benet managing cache reuse information management based information straightforward blocks dierent usage characteristics dierent portions program execution making block usage hard predict prediction particular blocks usage pattern similar branch prediction problem however branch outcomes easier predict optimal block usage characteristics aiding placement decisions assess value reuse information examined persistence cache block behavior opt pseudoopt schemes ie block exhibits given usage characteristic current tour likely maintain characteristic next tour high correlation past future use ie blocks usage characteristic persistent prediction future usage behavior easier block persistence terms notsame therefore analogous samedirection terminology used branch prediction studies 14 predict path specic branch take given behavior history instead determining block persistence terms four usage patterns ntns nts tns ts decided examine persistence nt patterns coarser granularity grouping directly relevant placement decisions 2unit cache structures figure 6 presents data block usage persistence successive tours general blocks exhibit nt usage behavior prior tours strong likelihood exhibiting nt behavior future tours opt scheme likelihood ranges 63 perl 95 compress evaluated benchmarks however pseudoopt scheme shows somewhat less persistence ranging 57 go 87 compress nt blocks persistence nt blocks opt pseudoopt schemes well 50 blocks exhibit usage behavior less persistent thus less predictable block persistence opt scheme ranges 40 go 97 li similar pseudoopt 45 compress 92 li li skews numbers somewhat regardless type usage characteristics block exhibited tour next tour highly likely exhibit temporal reuse temporal blocks persistent li much less persistent three benchmarks compress gcc go perl represent wider range program execution li alone see future tours temporal blocks harder predict nontemporal blocks ie temporally tagged blocks weakly biased toward exhibiting usage patterns next tour 64 multilateral scheme performance given performance reuse information opt pseudoopt congurations determine implementable schemes perform result restrict evaluation implementable schemes nts pcs two multilateral congurations actively place data within l1 based individual block reuse information 641 miss ratio performance nts pcs compare pseudoopt conguration used congured nts pcs structures 8kb directmapped cache 1kb fully associative lrumanaged cache 32b block size 32entry detection unit du miss ratio performance two congurations shown table 6 along performance opt pseudoopt pons directmapped 16kb single structure cache concurrence results earlier analyses multilateral congurations 161722 nts pcs caches perform well directmapped cache nearly twice size however performance pseudoopt indicates still room improvement 642 prediction accuracy nts makes placement decisions based given blocks usage recent past block exhibited nontemporal reuse placed smaller b cache next tour otherwise placed cache pcs makes placement decisions based reuse blocks loaded memory accessing instruction recently replaced block loaded particular pc exhibited nontemporal reuse next block loaded pc predicted placed smaller b cache otherwise placed larger cache schemes accessed blocks matching entry du placed cache default accuracy predictions determined based actual usage blocks pseudoopt scheme instance prediction behavior block classied correct actual usage block pseudoopt scheme noted section 62 annotated traces fed tracedriven cache simulator modied version mlcache contain actual block usage information tours seen pseudoopt scheme simulator provides information selected schemes block prediction actual usage accuracy given information easy determine well congurations predict blocks usage consequently whether properly placed within tour prediction accuracy actual usage accuracy nts pcs nts pcs compress 0240 0210 0402 0412 total total 0527 0595 0696 0675 go 0588 0621 0646 0652 total 0561 0596 0624 0609 total 0786 0723 0787 0758 perl 0326 0382 0727 0738 total 0327 0510 0861 0875 table 7 tour prediction actual usage accuracy nts pcs broken nt overall total accuracy accuracies relative actual block usage pseudoopt l1 cache structure table 7 shows prediction accuracy nts pcs benchmarks examined general ignoring li due excessively high temporal reuse prediction accuracy relatively low ranging 25 compress nts 60 go pcs despite larger granularity block typing two categories nt rather complete four category breakdown nts pcs schemes show poor prediction accuracies directly impacting block allocation decisions resulting overall performance improved block usage prediction schemes may result better block placement higher performance 643 actual usage accuracy regardless prediction accuracy given block exhibit reuse characteristics based duration time tour cache examined actual usage block evicted caches implementable schemes see usage compared blocks usage pseudoopt scheme comparison sheds light eect eliminating movement blocks caches l1 tour compress gcc go li perl pseudoopt 12933 26695 17312 25946 11778 222973 44004 342215 957 54749 nts 41 441 20 520 14 261 33 663 77 1304 pcs 37 489 21 498 13 241 32 661 79 1340 8k1w 36 354 20 357 12 140 26 480 37 359 table 8 average tour lengths pseudoopt nts pcs directmapped single cache tour lengths measure number accesses handled block cache tours broken nt see table 7 relative low prediction accuracy saw section 642 actual usage blocks cache closer actual usage blocks pseudo opt thus despite placement decisions blocks still exhibited reuse behavior akin seen pseudoopt conguration furthermore although nts exhibited lower tour prediction accuracy pcs benchmarks except li exhibited higher actual usage accuracy benchmarks except perl 644 tour lengths accuracies interesting block tour lengths implementable schemes directly related tour lengths pseudoopt scheme pseudoopt elaborate replacement policy disparate tour lengths aect comparisons two ways first tours shorter pseudoopt conguration implementable schemes implementable schemes may keep block l1 longer necessary permit exhibit seemingly benecial usage patterns ie tns ts makes particular block seem useful longer presence block l1 implementable schemes may unduly shorten tours signicant number blocks leading misclassication precluding optimal placement conversely tours longer pseudoopt implementable schemes corresponding blocks implementable schemes may replaced exhibit optimal usage characteristics causing poor usage accuracy poor prediction accuracy poor placement small b cache hence shorter tour lengths table 8 shows average tour length blocks showing nt usage characteristics expected nt tours much shorter tours tours pseudoopt average much longer tours either nts pcs due future knowledge greater exibility management data entered l1 cache structure nts pcs block placed l1 remains cache replaced thus subject replacement policy inherent corresponding cache instance block deemed either nts pcs placed directmapped cache subsequent block maps set also marked earlier evicted possibly exhibit optimum usage characteristics pseudoopt deemed desirable time block would simply moved b cache result tour length block would tend much longer pseudoopt nts pcs see clear gap average tour length blocks pseudoopt vs implementable schemes dierence tour lengths however much larger dierence miss ratio implementable vs pseudoopt schemes performance dierence much smaller average tour length pseudoopt increased greatly tour lengths con icting blocks may helped much better management scheme blocks may chosen remain cache nearly entire programs execution accessed regularly though necessarily frequently would thus long tour lengths longlived blocks greatly increase average tour lengths seen benchmark though may reduce overall number misses small amount study see improving upon prediction usage characteristics block management blocks placed l1 cache structure might improve performance nts pcs schemes reduce performance gap exists schemes pseudoopt see table 8 multilateral schemes increase tour lengths relative single directmapped cache structure nearly size indicating schemes making decisions improve data usage performance relative conventional cache conclusions paper evaluated three dierent implementable methodologies mat nts pcs managing onchip data cache based active block allocation via capturing exploiting reuse information general actively managed cache structure signi cantly improves upon performance traditional passivelymanaged cache structure similar size competes one nearly twice size individual eec tive address reuse history scheme used nts generally gives better performance macroblock eective addressbased mat pcbased pcs approaches compared performance pcs nts schemes performance nearoptimally managed cache structure pseudoopt dierence performance block usage prediction actual block usage tour lengths implementable schemes pseudoopt shows much room improvement activelymanaged caches thus multilateral cache structures actively place data within cache show promise improving cache space usage however prediction strategies used current schemes simple improving prediction algorithms well actively managing blocks placed l1 cache structure active replacement help improve performance implementable schemes may enable approach optimal cache space usage acknowledgments research supported national science foundation grant mip 9734023 gift ibm simulation facility provided intel technology education 2000 grant r study replacement algorithms virtual storage computer evaluating future microprocessors simplescalar tool set prefetching memory system behavior spec95 benchmark suite reducing memory latency via nonblocking prefetching caches improving cache performance selective cache bypass data cache multiple caching strategies tuned di documentation improving directmapped cache performance addition small fully associative cache prefetch buers reducing con icts directmapped caches temporalitybased design utilizing reuse information data cache management improving performance l1 cache associated bu early design cycle timing simulation caches mlcache exible multilateral cache simulator tr ctr prabhat jain srinivas devadas daniel engels larry rudolph softwareassisted cache replacement mechanisms embedded systems proceedings 2001 ieeeacm international conference computeraided design november 0408 2001 san jose california salvador petit julio sahuquillo jose david kaeli exploiting temporal locality drowsy cache policies proceedings 2nd conference computing frontiers may 0406 2005 ischia italy mark brehob richard enbody eric torng stephen wagner online restricted caching journal scheduling v6 n2 p149166 marchapril mark brehob stephen wagner eric torng richard enbody optimal replacement nphardfor nonstandard caches ieee transactions computers v53 n1 p7376 january 2004 mark brehob richard enbody eric torng stephen wagner online restricted caching proceedings twelfth annual acmsiam symposium discrete algorithms p374383 january 0709 2001 washington dc united states wang nelson passos improving cache hit ratio extended referencing cache lines journal computing sciences colleges v18 n4 p118123 april martin kampe per stenstrom michel dubois selfcorrecting lru replacement policies proceedings 1st conference computing frontiers april 1416 2004 ischia italy j sahuquillo petit pont v milutinovi exploring performance split data cache schemes superscalar processors symmetric multiprocessors journal systems architecture euromicro journal v51 n8 p451469 august 2005 zhigang hu stefanos kaxiras margaret martonosi timekeeping memory system predicting optimizing memory behavior acm sigarch computer architecture news v30 n2 may 2002 youfeng wu ryan rakvic liling chen chyichang miao george chrysos jesse fang compiler managed microcache bypassing high performance epic processors proceedings 35th annual acmieee international symposium microarchitecture november 1822 2002 istanbul turkey