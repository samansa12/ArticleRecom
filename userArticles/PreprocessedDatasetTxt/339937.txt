parallel algorithm reduction tridiagonal form eigendecomposition onesided orthogonal transformations orthogonalize columns matrix related methods finding singular values advantages lending parallel vector implementations simplifying access data requiring access rows columns used find eigenvalues matrix given factored form finite sequence transformations leading partial orthogonalization columns described permits tridiagonal matrix whose eigenvalues squared singular values derived implementation fujitsu vpp series discussed timing results presented b introduction symmetric eigenvalue problems appear many applications ranging computational chemistry structural engineering algorithms symmetric eigenvalue problems extensively discussed literature 11 9 implemented various software packages eg lapack 1 broader introduction parallel computers scientific computing new parallel algorithms suggested 7 2 following another new parallel algorithm suggested particularly well adapted vector parallel computers low operation counts eigenvalue problems solved iterative algorithms general algebraic sense equivalent finding n zeros polynomial however two main classes methods solve symmetric eigenvalue problem first class requires matrix vector products inspect alter matrix elements matrix class includes lanczos method 9 date november 1995 1991 mathematics subject classification 65y0565f30 key words phrases parallel computing reduction algorithms onesided reductions computer sciences laboratory australian national university canberra act 0200 australia anu supercomputer facility australian national university centre mathematics applications australian national university hegland h kahn r osborne particular advantages sparse matrices however general lanczos method difficulties finding eigenvalues eigenvectors second class methods iteratively applies similarity transforms matrix get sequence orthogonally similar matrices converge diagonal matrix second class methods consists mainly two subclasses first subclass uses givens matrices similarity transforms jacobis method successfully implemented parallel 2 12 disadvantage method high operation count second subclass methods first reduces matrix orthogonal similarity transform tridiagonal form uses special methods symmetric tridiagonal matrices parts algorithms pose major challenges parallel implementation second stage tridiagonal eigenvalue problem solvers popular methods include divide conquer 7 multisectioning 9 reduction tridiagonal form discussed earlier algorithms use block matrix algorithms see 6 5 3 however methods achieved optimal performance one problem similarity transforms require multiplications sides seen 12 jacobi method based onesided transformations allows better vectorization requires less communication original jacobi algorithms assuming positive semidefinite intermediate matrices b defined factors seen following onesided idea also used reduction algorithm algorithm form part subroutine library distributed memory computer fujitsu vpp 500 often application subroutines libraries allow user little freedom choice distribution data local memories processors onesided algorithms allow large range distributions perform equally well next section one twosided reduction tridiagonal form de scribed section 3 reinterprets reduction orthogonalisation procedure similar gramschmidt procedure reinterpretation used introduce new onesided reduction algorithm section 4 computation eigenvectors discussed section 5 contains timings comparisons algorithms reduction tridiagonal form 3 2 reduction tridiagonal form class methods solve eigenvalue problem symmetric matrices 2 r nthetan first reduces tridiagonal form second step solves eigenvalue problem tridiagonal matrix problem finding eigendecomposition tridiagonal matrix discussed accumulating transformations used finding eigenvectors symmetric matrix reduction tridiagonal form produces factorization q orthogonal symmetric tridiagonal offdiagonal elements nonzero positive factorization uniquely determined first column q 9 proof fact leads directly lanczos algo rithm lanczos method advantages especially sparse matrices methods based sequences simple orthogonal similarity transforms 9 p118 preferable dense matrices 21 householders reduction method attributed householder uses householder transformations reflections following let ff ij denote elements matrix hwahw zeros rows 3 n first column columns 3 n first row computation hw equivalently w fl fi 1 requires multiplications n additions o1 using matrix vector product v application hw rank two update 4 hegland h kahn r osborne takes n 2 n multiplications additions computation p 2n2 multiplications additions computation q n 2 multiplications 2n 2 additions rank two update gives total 2n 2 multiplications 3n 2 second step v found hvhwahwhv additional zeros columns 4 n second row rows 4 n second column procedure repeated remaining matrix tridiagonal sizes remaining submatrices decrease step n gamma k matrix size k processed requiring multiplications 3k 2 additions gives total multiplications additions tridiagonal matrix uniquely determined problem example different starting vectors lanczos procedure lead different tridiagonal matrices also different matrices obtained different arithmetic precision used 9 p123124 however despite apparent lack definition eigenvalues eigenvectors original problem still determined error proportional machine precision summary sequential householder tridiagonalization algorithm follows calculate fl w ak end vector parallel processors householder algorithm disad vantages firstly iterations progress length vectors used calculations decreases efficient use vector processor prefer long vector lengths secondly parallel environment input matrix partitioned banded manner across onedimensional array processors algorithm reduction tridiagonal form 5 severely load imbalanced avoid various authors suggested using cyclic 5 toruswrap mappings data 10 3 also parallel imple mentation rank two update requires copies vectors w q processors leads heavy communication load 22 onesided reduction onesided algorithm developed overcome difficulties inherent parallel version sequential householder algorithm addition expected onesided algorithm generates less fillin sparse matrices twosided algorithms matrix given finite element form real symmetric matrices either given factored represented factored form b cholesky factorization used spectrum might shifted gamma positive definite parameter chosen using gershgorin shift exact arithmetic used reduction eigenvectors gamma equal eigenvectors eigenvalues shifted however precision computations affected introduction shift householder similarity transform done applying hw b rank one modification must computed flbw computation p requires multiplications additions rank one modification n 2 multiplications n 2 additions giving together 2n 2 n multiplications 2n additions contrast twosided algorithm number additions approximately number multiplications algorithm advantageous architectures addition multiplication parallel means better load balancing computation w costly method original householder method householder vector w computed first column 1 column reconstructed first factored representation requires n 2 n multiplications additions thus computation householder matrix hv requires o1 terms multiplications n 2 additions 6 hegland h kahn r osborne adding terms one reduction step needs multiplications additions overall costs algorithm multiplications additions total number operations increased compared original algorithm time used computer additions multiplications parallel speed matrix already factorized however time would taken account well summary sequential version onesided algorithm follows calculate fl w k end 3 onesided algorithm orthogonalization procedure order develop parallel version algorithm onesided reduction interpreted orthogonalization procedure like gramschmidt process let b denote ith column b matrix interpreted gramian b follows onesided tridiagonalization procedure constructs gramian c tridiagonality condition orthogonality certain c 2 34 reduction tridiagonal form 7 first step orthogonality c established setting ffi kj denotes kronecker delta subsequent step linear combinations formed c 4 orthogonal c 2 c orthogonal c 1 linear combinations well subsequent steps destroy earlier orthogonality relations basic observation used proof method algorithm reduction tridiagonal form follows c 1 k end fl k ij new c k orthogonal c kgamma1 matrix ijkn orthogonal iteration k find orthogonal transformation c kgamma1 c k j orthogonal c kgamma1 equivalent making offdiagonal elements ff jkgamma1 ff kgamma1j zero proposition 31 let c k j computed previous algorithm c 1 8 hegland h kahn r osborne c n tridiagonal orthogonal matrix q proof proof based fact mentioned earlier orthogonality relations invariant uses induction proof similar one given next section corresponding parallel algorithm remark original householder algorithm formulated similar way treating b inner product coordinate transformations change matrix tridiagonal 31 parallel algorithm following single program multiple data model spmd used basic assumption processors programmed way although actions might slightly different thus spmd algorithm described pseudocode denoting one processor data matrix b distributed processors columns cyclic fashion means processor 1 contains columns contains p total number processors formally processor q contains b pg order simplify notation let n ng processor number furthermore mod denotes mod function mapping positive negative integers c 1 broadcast c 1 1 else receive c 1for k gather c k end coefficients fl h ij ij modified columns orthogonal last unmodified one thus ng n q reduction tridiagonal form 9 coefficients also form orthogonal matrices follows c k overwritten c k note last calculations c k duplicated p processors leaves c k k processors ready next step communication required iteration gathering p vectors c k proposition 32 let c k j computed previous algorithm c 1 c n tridiagonal orthogonal matrix q proof let c 1 first show 1 orthogonal matrix q k 2 linear hull c k n orthogonal linear hull c 1 3 proposition 32 consequence obtained setting 1 proof uses induction k statement easily seen true case n dimensional identity matrix case orthogonality conditions empty matrix 1 two two matrix thus tridiagonal remainder proof consists proving induction step assume three properties valid shown also valid construction follows c orthogonal g thus q one retrieves first property particular existence g follows existence constructed householder matrices linear hull c m1 n subspace linear hull c n thus orthogonal c 1 furthermore constructed orthogonal c second property follows hegland h kahn r osborne finally tridiagonal remains show 2nd column zeros first rows first elements column c m1 zero second property proof used sequential parallel algorithm practical implementations orthogonalisation c k j achieved forming products c kgamma1 correspond individual elements updated version symmetric matrix householder transformations used zero relevant offdiagonal elements first stage iteration transformations carried locally gathering step transformation p formed elements replicated processors although householder transformations used would also possible use givens transformations 32 analysis using onesided reduction tridiagonal one step complete eigensolver several possible components error first cholesky factorization however result wilkinson 11 equation 4443 shows quantity extremely small thus error incurred working cholesky factor b subsequent calculations minimal second step tridiagonal matrix c ngamma2 produced successive reduction orthogonal similarity transforms c k transformed matrix calculated kth stage error eigenvalues c ngamma2 bounded numerical error transform 11 kth step given p exact orthogonal matrix corresponding actual computed data stage bound difference follows wilkinson 11 equation 3453 fact ij word length error introduced reduction tridiagonal small final stage calculation eigenvalues tridiagonal matrix determined accuracy high relative largest element tridiagonal matrix applies example eigenvalues computed using reduction tridiagonal form 11 sturm procedure result guarantee high relative accuracy determination small eigenvalues important jacobi method becomes method choice 4 4 calculating eigenvectors order calculate eigenvectors symmetric matrix orthogonal matrix q defining reduction accumulated achieved starting identity matrix distributed cyclically processors updating multiplying householder transformations used update c forming q explicitly preferable storing details transformations applying eigenvectors tridiagonal matrix usual procedure sequential implementations reason matrix q distributed way renders multiplication left inefficient discussed fully following 41 calculation eigenvectors oneprocessor version n theta n symmetric matrix reduced tridiagonal form sequence householder transformations represented orthogonal n theta n matrix q nlambdan tridiagonal onesided algorithm actually calculated eigendecomposition given diagonal matrix containing eigenvalues v 2 r nlambdan matrix eigenvectors combining two equations gives eigenvectors columns matrix q obtained rediagonalization procedure represented householder matrices h eigenvector matrix v tridiagonal matrix represented matrix elements order get matrix element representation u form product hegland h kahn r osborne product done two different ways first method multiplies v h k formally define sequence u u method often used example 8 called backward transformation second method computes elementwise representation q first recursion specifically computes matrixvector product qv called forward accumulation 8 difference two methods first one applies householder transforms h k left second applies right addition second method requires computation product two matrices element form sequential case using twosided householder duction first method preferred avoids matrix multiplication however multiprocessor version multiplication left householder transformation requires extra communication columns matrix eigenvectors v distributed processors fact one purpose onesided reduction avoid communication reduction tridiagonal form certainly communication required matrix multiplication qv fewer large blocks matrix less demanding 42 multiprocessor version multiprocessor case added complication b assumed cyclically distributed although explicitly laid use p permutation transforms b cyclic layout matrix q product 2 theta n gamma 2 matrices formed householder transformations h 1 j refers transformations carried locally step reduction h 2 j refers transformation using one column b processor reduction tridiagonal form 13 carried redundantly processors find q start identity matrix distributed cyclically across processors match implicit layout b matrix updated transforming householder transformations used update b obtain tridiagonal matrix matrix eigenvectors v tridiagonal matrix obtained block layout whilst matrix q cyclic layout find eigenvectors original symmetric matrix must taken account instead need qpv cyclic ordering must reversed finally eigenvectors given premultiplication permutation p involves reordering coefficients eigenvectors carried locally processor involve communication 5 timings parallel onesided reduction tridiagonal implemented tested fujitsu vpp500 vpp500 parallel supercomputer consisting vector processors connected full crossbar network theoretical peak performance pe 16 gflops maximum size memory processor gbyte vpp500 scalable 4 222 processing elements access available 16 processor machine processor perform send receive operations simultaneously crossbar network peak data transfer rate 400 mbytess onesided algorithm particularly suited architecture vpp500 calculation elements updated matrix current version c vectorisable loops length n size input matrix conventional twosided householder reduction vector length decreases iteration table shows timings speeds obtained 16 processor vpp500 two times speed given first reduction without accumulating transformations later eigenvectors calculations second reduction accumulation speed given gflops code written parallel language vppfortran basically fortran77 added compiler directives achieve parallel constructs data layout interprocessor communication performance figures appears algorithm scalable far performance maintained size problem increased along 14 hegland h kahn r osborne 1024 38 846 53 1018 2048 26 1000 37 1164 3072 28 3135 43 3371 table 1 timings speed matrices increasing size number processors onesided reduction tridiagonal matrix size 2048 using 2 processors achieves nearly 70 peak performance approximately 60 matrix size 4096 using 4 processors formal analysis communication required onesided algorithm gives communication volume proportional number processors computational load proportional n 3 isoefficiency obtained pp gamma 1n 2 n 3 constant scalability algorithm evident speeds matrices size n p processors compared matrices size 4n using 2p processors example compare speeds doubling gflop rate interesting compare performance figures published results alternative parallel twosided householder reductions tridiagonal comparisons general algorithms machine architectures different straightforward comparison time taken reduce matrix fixed size measured machines similar peak gflop rate practice first step cholesky factorization adds overhead one tenth time taken onesided reduction tridiagonal accessible published results refer algorithms implemented intel machines reduction tridiagonal form 15 dongarra van de geijn 5 give times parallel reduction tridiagonal using panel wrapped storage 128 nodes 520 node intel touchstone delta equating peak mflop rates suggests comparable 6 processor vpp500 matrix size 4000 implementation intel took twice long onesided reduction size matrix 4 processor vpp500 scalapack implementation parallel reduction tridiagonal given choi dongarra walker 3 extrapolating graphs gflop rates seen times taken various sized problems two three times taken onesided algorithm size problems vpp500 comparable peak performance smith hendrickson jessup 10 use square toruswrap mapping matrix elements processors tested code intel machine corresponding 12 processor vpp500 twosided algorithm inferred taken time slightly larger problem 16 processor vpp500 using onesided algorithm smith et al algorithm sophisticated twosided algorithms uses toruswrap mapping level 3 blas 6 conclusion new algorithm reduction symmetric matrix tridiagonal first step finding eigendecomposition developed starting cholesky factorization symmetric matrix orthogonal transformations based householder reductions applied factor matrix tridiagonal form reached referred onesided reduction leads updating factor matrix iteration rank one rather rank two conventional householder reduction tridiagonal transformations also accumulated allow calculation eigenvectors algorithm suited parallelvector architectures fujitsu vpp500 shown perform well complete calculation eigenvalues eigenvectors symmetric matrix extra time cholesky factorization observed one tenth required reduction tridiagonal significant overhead 7 acknowledgement authors would like thank prof r brent helpful comments mr makato nakanishi fujitsu japan help access vpp500 hegland h kahn r osborne r solution singularvalue symmetric eigenvalue problems multiprocessor arrays design parallel jacobis method accurate qr reduction condensed form eigenvalue problem distributed memory architectures block reduction matrices condensed forms eigenvalue computations fully parallel algorithm symmetric eigenvalue problem matrix computations symmetric eigenvalue problem parallel algorithm householder tridiagonal ization algebraic eigenvalue problem parallel ordering algorithm efficient onesided jacobi svd computations tr