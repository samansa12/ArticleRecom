hidden markov models text categorization multipage documents traditional setting text categorization formulated concept learning problem instance single isolated document however perspective appropriate case many digital libraries offer contents scanned optically read books magazines paper propose general formulation text categorization allowing documents organized sequences pages introduce novel hybrid system specifically designed multipage text documents architecture relies hidden markov models whose emissions bagofwords resulting multinomial word event model generative portion naive bayes classifier rationale behind proposal taking account contextual information provided whole page sequence help disambiguation improves single page classification accuracy results two datasets scanned journals making america collection confirm importance using whole page sequences empirical evaluation indicates error rate obtained running naive bayes classifier isolated pages significantly reduced contextual information incorporated b figure 1 bayesian network naive bayes classifier 22 hidden markov models hmms introduced several years ago tool probabilistic sequence modeling interest area developed particularly seventies within speech recognition research community rabiner 1989 last years large number variants improvements standard hmm proposed applied undoubt edly markovian models regarded one significant stateoftheart approaches sequence learning besides several applications pattern recognition molecular biology hmms also applied text related tasks including natural language modeling charniak 1993 recently information retrieval extraction freitag mccallum 2000 mccallum et al 2000 recent view hmm particular case bayesian networks bengio frasconi 1995 lucke 1995 smyth et al 1997 helped theoretical understanding ability conceive extensions standard model sound formally elegant framework hmm describes two related discretetime stochastic processes first process pertains hidden discrete state variables denoted xt forming firstorder markov chain taking realizations finite set x1xn second process pertains observed variables emissions denoted dt starting given state time 0 given initial state distribution px0 model probabilistically transitions new state x1 correspondingly emits observation d1 process repeated recursively end state reached note form computation may suggest hmms closely related stochastic regular grammars charniak 1993 markov property prescribes xt1 conditionally independent x1xt1 given xt furthermore assumed dt independent rest given xt two conditional independence assumptions graphically depicted using bayesian network figure 2 result hmm fully specified following conditional probability pxt xt1 transition distribution 2 pdt xt emission distribution since process stationary transition distribution represented square stochastic matrix whose entries transition probabilities abbreviated pxi x j following classic literature emissions restricted hidden markov models 199 figure 2 bayesian networks standard hmms symbols finite alphabet multivariate continuous variables rabiner 1989 explained next section model allows emissions bagofwords 3 multipage classifier turn description classifier multipage documents case categorization task consists learning examples function maps whole document sequence d1dt corresponding sequence page categories c1ct section presents architecture asociated algorithms grammar extraction training classification 31 architecture system based hmm whose emissions associated entire pages document thus realizations observation dt bagofwords representing text tth page document hmm states related pages categories deterministic function maps state realizations page categories assume surjection bijection ie state realizations categories enriches expressive power model allowing different transition behaviors pages class depending page actually encountered within sequence however page contents depends category context category within sequence5 multiple states may introduce many parameters may convenient assume pdt xi constrains emission parameters given page category form parameters sharing may help reduce overfitting emission distribution modeled assuming conditional word independence given class like eq 1 dt pdt ck 4 therefore architecture graphically described merging bayesian networks hmms naive bayes shown figure 3 remark state 200 frasconi et al figure 3 bayesian network describing architecture sequential classifier hence category page depends contents page also contents pages document summarized hmm states probabilistic dependency implements mechanism taking contextual information account algorithms used paper derived literature markov models rabiner 1989 inference learning bayesian networks pearl 1988 heckerman 1997 jensen 1996 classification naive bayes lewis gale 1994 kalt 1996 following give details integration methods 32 induction hmm topology structure topology hmm representation allowable transitions hidden states precisely topology described directed graph whose vertices state realizations x1xn whose edges pairs hmm said ergodic transition graph fullyconnected however almost interesting application domains less connected structures better suited capturing observed properties sequences modeled since convey domain prior knowledge thus starting right structure important problem practical hidden markov modeling example consider figure 4 showing simplified graph describes transitions parts hypothetical set books possible state realizations start title dedication preface toc regular index end note simplified example onetoone mapping figure 4 example hmm transition graph hidden markov models 201 structure kind could handcrafted domain expert may advantageous learn automatically data briefly describe solution adopted automatically infer hmm transition graphs sample multipage documents let us assume pages available training documents labeled class belong one imagine take advantage observed labels search effective structure space hmms topologies approach based application algorithm datadriven model induction adapted previous works construction hmms text phrases information extraction mccallum et al 2000 algorithm starts building structure explain available training sequences maximally specific model initial structure many paths initial final state training sequences every path associated one sequence pages ie distinct state created every page training set state x labeled x category corresponding page document note unlike example shown figure 4 several states generated category algorithm iteratively applies merging heuristics collapse states augment generalization capabilities unseen sequences first heuristic called neighbormerging collapses two states x x neighbors graph x second heuristic called vmerging collapses two states x x share transition common state thus reducing branching factor structure 33 inference learning given hmm topology extracted algorithm described learning problem consists determining transition emission parameters one important distinction needs made training bayesian networks whether variables observed assuming complete data variables observed maximum likelihood estimation parameters could solved using onestep algorithm collects sufficient statistics parameter heckerman 1997 case data complete following two conditions met 1 onetoone mapping hmm states page categories ie 2 category known page training documents ie dataset consists sequences pairs d1 c1dt ct ct known category page number pages document assumptions estimation transition parameters straightforward accomplished follows nci cj number times page class ci follows page class cj trainingsetsimilarlyestimationofemissionparametersinthiscasewouldbeaccomplished exactly like case naive bayes classifier see eg mitchell 1997 pw nw ck number occurrences word w pages class ck v vocabulary size 1v corresponds dirichlet prior parameters heckerman 1997 plays regularization role whose words rare within class conditions 1 2 however normally satisfied first order model accurately different contexts category may occur may convenient multiple distinct hmm states page category implies page labels determine unique state path second labeling pages training set time consuming process needs performed hand may important use also unlabeled documents training joachims 1999 nigam et al 2000 means label c may available assumption 2 satisfied assumption 1 derive following approximated estimation formula transition parameters nxi x j counts many times state xi follows x j state merge procedure described section 32 however general presence hidden variables requires iterative maximum likelihood estimation algorithm gradient ascent expectationmaximization em implementation uses em algorithm originally formulated dempster et al 1977 usable bayesian network local conditional probability distributions belonging exponential family heckerman 1997 em algorithm essentially reduces baumwelch form rabiner 1989 modification evidence entered state variables since multiple states associated category even labeled documents page category known state evidence takes form findings jensen 1996 state evidence taken account estep changing forward propagation follows ct forward variable baumwelch algorithm emission probability pdt obtained eq 4 using ck themstepisperformedinthestandardwayfortransitionparametersbyreplacingcounts eq 5 expectations given observed variables emission probabilities hidden markov models 203 also estimated using expected word counts parameters shared indicated eq 3 counts summed states label thus case incomplete data eq 6 replaced pw ck number training sequences nw ck number occurrences word w pages class ck pxt xi d1dt probability state xi page given observed sequence pages d1 dt readers familiar hmms recognize latter quantity computed baumwelch procedure estep sum p extends training sequences sum extends pages pth document training set e msteps iterated local maximum incomplete data likelihood reached note page categories observed convenient use estimates computed eq 7 starting point rather using random initial parameters similarly initial estimate emission parameters obtained eq 6 interesting point related application em algorithm learning labeled unlabeled documents nigam et al 2000 paper concern allow learner take advantage unlabeled documents training set major difference method nigam et al 2000 assumes flat singlepage documents applied multipage documents would equivalent zeroorder markov model cannot take contextual information account 34 page classification given document pages classification performed first computing sequence states x1xt likely generated observed sequence pages mapping state corresponding category xt likely state sequence obtained running adapted version viterbis algorithm whose general form maxpropagation algorithm bayesian networks described jensen 1996 briefly following quantity computed using following recursion 204 frasconi et al optimal state sequence retrieved backtracking finally categories obtained ct xt contrast note naive bayes clas sifier would compute likely categories ct comparing eqs 1115 eq 16 see classifiers rely emission model pdt cj naive bayes employs prior class probability compute final prediction hmm classifier takes advantage dynamic term square brackets eq 12 incorporates grammatical constraints 4 experimental results section describe set experiments give empirical evidence effectiveness proposed model main purpose experiments make comparison multipage classification approach traditional isolated page classification system like well known naive bayes text classifier evaluation conducted realworld documents naturally organized form page sequences used two different datasets associated two journals making america moa collection moa joined project university michigan cornell university see httpmoaumdlumicheduabouthtml shaw blumson 1997 collecting making available digitized information history evolution processes american society xix xx century 41 datasets first dataset subset journal american missionary sociological magazine strong christian guidelines task consists correctly classifying pages previously unseen documents one ten categories described table 1 categories related topic articles related parts journal ie contents receipts advertisements dataset selected contains 95 issues 1884 1893 total 3222 ocr text pages special issues final report issues typically november december issues removed dataset contain categories found rest ten categories temporally stable second dataset subset scribners monthly recreational cultural magazine printed second half xix century table 2 describes categories selected classification task filtered dataset contains total 6035 ocr text pages organized issues ranging year 1870 1875 although spanning shorter temporal interval number pages second dataset larger first one issues 34 times longer hidden markov models 205 table 1 categories american missionary domain name description cover index surveys editorial articles afroamericans surveys american indians surveys reports china missions articles female condition education childhood magazine information lists founders contents mostly graphic little text description table 2 categories scribners monthly domain name description 1 article 2 books authors home abroad generic articles book reviews broad cultural news poems tales articles home living scientific articles articles fine arts news reports category labels two datasets obtained semiautomatically starting moaxmlfilessuppliedwiththedocumentscollectionstheassignedcategorieswerethen manually checked case page containing end beginning two articles belonging different categories page assigned category ending article page within document represented bagofwords counting number word occurrences within page worth remarking datasets instances text documents output ocr system imperfections recognition algorithm presence images pages yields noisy text containing misspelled nonexistent words trash characters see bicknese 1998 report ocr accuracy moa digital library although errors may negatively affect learning process subsequent results evaluation phase made attempts correct filter misspelledwordsexceptforthefeatureselectionprocessdescribedinsection43however since ocr extracted documents preserve text layout found original image necessary rejoin word fragments hyphenated due line breaking 206 frasconi et al 42 grammar induction case completely labeled documents possible run structure learning algorithm presented section 32 figure 5 show example induced hmm topology journal american missionary structure extracted using 10 issues year 1884 training set vertex transition graph associated one hmm state labeled corresponding category index see table 1 edges labeled transition probability source target state estimated case counting state transitions state merging procedure see eq 7 values also used initial estimates pxi x j subsequently refined em algorithm associated stochastic grammar implies valid sequences must start index page class 1 followed page general communications class 8 next state associated page editorial article 2 self transition value 091 meaning high probability next page belong editorial lower probability 007 next page one south survey 3 probability 0008 indians 4 bureau womens work 6 figure 6 show one example induced hmm topology journal scribners monthlyobtainedfrom12trainingissuesyear1871althoughissuesofscribnersmonthly longer number categories comparable american missionary extracted transition diagram figure 6 simpler one figure 5 reflects less variability sequential organization articles scribners monthly note category 7 home society rare never occurs 1871 43 feature selection text pages first preprocessed common filtering algorithms including stemming stop words removal still bagofwords representation pages leads highdimensional feature space responsible overfitting conjunction algorithms based generative probabilistic models feature selection technique limiting overfitting removing noninformative words documents experi ments performed feature selection using information gain yang pedersen 1997 criterion often employed different machine learning contexts measures average number bits information category gained including word document dictionary term w gain defined k k pck wlog pck w w denotes absence word w feature selection performed retaining words highest average mutual information class variable ocr errors however produce noisy features may responsible poor performance hidden markov models 207 figure 5 data induced hmm topology american missionary year 1884 numbers node correspond 208 frasconi et al figure 6 data induced hmm topology scribners monthly year 1871 numbers node correspond hidden markov models 209 even feature selection performed reason may convenient prune dictionary applying information gain criterion words occurring less given threshold h training set preliminary experiments showed best performances achieved pruning words less occurrences 44 accuracy comparisons following compare isolated page classification using standard naive bayes sequential classification using proposed hmm architecture although classification accuracy could estimated fixing split available data training test set suggest method attempts incorporate peculiarities digital libraries domain particular handlabeling documents purpose training expensive activity working large training sets likely unrealistic practical applications reason experiments deliberately used small fractions available data training moreover problem temporal stability journal organization may change time test attempted address aspect assuming training data available given year decided test generalization journal issues published different years splitting according publication year advantage training algorithm since increases likelihood different issue organizations represented training set resulting method related kfold crossvalidation common approach accuracy estimation partitions dataset k subsets iteratively use one subset testing k 1 training experiments reversed proportions data training test sets using journal issues one year training remaining issues testing believe setting realistic case digital libraries following experiments hmm classifiers trained first extracting transition structure initializing parameters using eqs 6 7 finally tuning parameters using em algorithm found initial parameter estimates close final solution found em algorithm typically 2 3 iterations sufficient em converge 441 american missionary dataset results ten resulting experiments shown figure 7 hybrid hmm classifier performing sequential classification consistently outperforms plain naive bayes classifier working isolated pages graph top summarizes results obtained without feature selection averaging results ten experiments nb achieves 619 accuracy hmm achieves 804 corresponds 484 error rate reduction graph bottom refers results obtained selecting best 300 words according information gain criterion average accuracy case 698 nb 806 hmm 357 error rate reduction cases words occurring less 10 times training sets pruned using feature selection nb improves hmm performance essentially moreover standard deviation accuracy smaller nb 28 compared 42 hmm larger variability case hmm due figure 7 isolated vs sequential page classification american missionary dataset column classifiers trained documents corresponding year tested remaining issues structure induction algorithm facts sequential organization journal issues temporally less stable article contents 442 scribners monthly dataset similar experiments carried scrib ners monthly journal results using feature selection shown top figure 8 average accuracy 810 isolated page classification 896 sequential classi fication error reduction 425 feature selection average accuracy drops 753 isolated page classifier remains similar sequential classifier hidden markov models 211 figure 8 isolated vs sequential page classification scribners monthly dataset noticeably feature selection different effects two datasets coupled naive bayes classifier tends improve accuracy american missionary tends worsen scribners monthly hand hmm almost insensitive feature selection datasets apparently counterintuitive since emission model almost two classifiers except em tuning emission parameters case hmm however remarked naive bayes final prediction biased class prior eq 16 hmms prediction biased extracted grammar eqs 1115 latter provides robust information effectively compensates crude approximation emission model prescribing 212 frasconi et al conditional word independence robustness also affects positively performance suboptimal set features selected representing document pages 45 learning using ergodic hmms following experiments provide basis evaluating effects structure learning algorithm presented section 32 present setting trained ergodic hmm ten states state mapped exactly one class emission parameters initialized using eq 6 transition probabilities initialized random values case em algorithm takes full responsibility extracting sequential structure data training arcs associated probability less 0001 pruned away evaluation performed using american missionary dataset training single years previous set experiments expected see figure 9 results worse obtained conjunction grammar extraction algorithm however trained hmm outperforms naive bayes classifier also case 46 effects training set size investigate effects size training set propose set experiments alternative reported section 44 experiments selected variable number sequences journal issues n training randomly chosen dataset tested generalization remaining sequences accuracy reported function n averaging 20 trials trial proportion training test sequences experiments performed american missionary dataset shown figure 10 generalization isolated sequential classifier tends saturate 15 sequences training set slightly figure 9 comparison ergodic hmm hmm based extracted grammar hidden markov models 213 figure 10 learning curve sequential isolated classifiers average number issues single year sequential classifier consistently outperforms isolated page classifier 47 learning partially labeled documents since labeling expensive human activity evaluated system also fraction training documents pages labeled particular interested measuring loss accuracy due missing page labels since structure learning feasible partially labeled documents used case ergodic fully connected hmm ten states one per class performed six different experiments american missionary dataset using different percentages labeled pages experiments issues year 1884 form training set remaining issues form test set table 3 shows detailed results experiment classification accuracy reported single classes entire test set using 30 labeled pages hmm fails learn reliable transition structure naive bayes classifier trained em nigam et al 2000 obtains higher accuracy table 4 however higher percentages known page labels comparison favors sequential classifier using 50 labeled pages hmm outperforms isolated page classifier trained completely labeled data greater percentages labeled documents performances begin saturate reaching maximum 8024 labels known corresponds result obtained section 45 5 conclusions presented text categorization system multipage documents capable effectively taking account contextual information improve accuracy respect traditional isolated page classifiers method smoothly deal unlabeled pages within document although found learning hmm structure 214 frasconi et al table 3 results achieved model trained expectationmaximization varying percentage labeled documents percentage labeled documents category another aspect granularity document structure exploited working level pages straightforward since page boundaries readily available however actual category boundaries may coincide page boundaries pages may contain portions text belonging different articles case page would belong multiple categories although critical singlecolumn journals american missionary case documents typeset two three columns certainly deservesattentionafurtherdirectionofinvestigationisthereforerelatedtothedevelopment algorithms capable performing automatic segmentation continuous stream text without necessarily relying page boundaries finally text categorization methods take document structure account may extremely useful types documents natively available electronic form including web pages documents produced typesetting systems particular hypertexts like documents internet organized directed graphs structure seen generalization sequences however devising classifier capture context inhypertextsbyextendingthearchitecturedescribedinthispaperisstillanopenproblemal though extension hmms sequences trees straightforward see eg diligenti etal2001thegeneralcaseofdirectedgraphsisdifficultbecauseofthepresenceofcycles preliminary research direction based simplified models incorporating graphical transition structure presented diligenti et al 2000 passerini et al 2001 acknowledgments thank cornell university library providing us data collected within making america project research partially supported ec grant ist199920021 metae project notes 1 related formulation would consist assigning global category whole multipage document formulation considered paper 2 observing text 3 bayesian network annotated graph nodes represent random variables missing edges encode conditional independence statements amongst variables given particular state knowledge semantics belief networks determine whether collecting evidence set variables modify ones belief set variables jensen 1996 pearl 1988 4 adopt standard convention denoting variables uppercase letters realizations corresponding lowercase letters moreover use table notation probabilities jensen 1996 example px shorthand table denotes twodimensional table entries 5 course mean category independent context r input output hmm architecture bayesian networks dor data mining introduction bayesian networks text categorization support vector machines learning many relevant fea tures transductive inference text classification using support vector machines experimental evaluation ocr text representations learning document classifiers new probabilistic model text classification retrieval hierarchically classifying documents using words sequential algorithm training text classifiers comparison two learning algorithms text categorization bayesian belief networks tool stochastic parsing automating construction internet portals machine learning machine learning feature selection text classification labeled unlabeled documents using em evaluation methods focused crawling probabilistic reasoning intelligent systems networks plausible inference tutorial hidden markov models selected applications speech recognition online searching page presentation university michigan probabilistic independence networks hidden markov probability models hidden markov model induction bayesian model merging examplebased mapping method text classification retrieval comparative study feature selection text categorization tr probabilistic reasoning intelligent systems networks plausible inference examplebased mapping method text categorization retrieval sequential algorithm training text classifiers bayesian belief networks tool stochastic parsing probabilistic independence networks hidden markov probability models feature selection perception learning usability case study text categorization text classification labeled unlabeled documents using em statistical language learning machine learning introduction bayesian networks bayesian networks data mining automating construction internet portals machine learning text categorization suport vector machines hierarchically classifying documents using words comparative study feature selection text categorization transductive inference text classification using support vector machines hidden markov model induction bayesian model merging focused crawling using context graphs image document categorization using hidden tree markov models structured representations information extraction hmm structures learned stochastic optimization evaluation methods focused crawling new probabilistic model text classification retrieval title2 ctr fabrizio sebastiani machine learning automated text categorization acm computing surveys csur v34 n1 p147 march 2002