data mining sparse grids using simplicial basis functions recently presented new approach 18 classification problem arising data mining based regularization network approach contrast methods employ ansatz functions associated data points use grid usually highdimensional feature space minimization process cope curse dimensionality employ sparse grids 49 thus ohn1nd1 instead ohnd grid points unknowns involved denotes dimension feature space gives mesh size use sparse grid combination technique 28 classification problem discretized solved sequence conventional grids uniform mesh sizes dimension sparse grid solution obtained linear combination contrast former work dlinear functions used apply linear basis functions based simplicial discretization allows handle dimensions algorithm needs less operations per data pointwe describe sparse grid combination technique classification problem give implementational details discuss complexity algorithm turns method scales linearly number given data points finally report quality classifier built new method data sets 10 dimensions turns new method achieves correctness rates competitive best existing methods b introduction data mining process nding patterns relations trends large data sets examples range scien tic applications like postprocessing data medicine evaluation satellite pictures nancial commercial applications eg assessment credit risks selection customers advertising campaign letters overview data mining various tasks approaches see 5 12 paper consider classication problem arising data mining given set data points ddimensional feature space together class label data classier must constructed allows predict class newly given data point future decision making widely used approaches besides others decision tree induction rule learning adaptive multivariate regression splines neural networks support vector machines interestingly techniques interpreted framework regularization networks 21 approach allows direct description important neural networks also allows equivalent description support vector machines nterm approximation schemes 20 classication data interpreted scattered data approximation problem certain additional regularization terms highdimensional spaces 18 presented new approach classication problem also based regularization network approach contrast methods employ mostly global ansatz functions associated data points use independent grid associated local ansatz functions minimization process similar numerical treatment partial dierential equations uniform grid would result oh denotes dimension feature space gives mesh size therefore complexity problem would grow exponentially encounter curse dimensionality probably reason conventional gridbased techniques used data mining however socalled sparse grids approach allows cope complexity problem extent method originally developed solution partial dierential equations 2 8 28 49 used successfully also integral equations 14 27 interpolation approximation 3 26 39 42 eigenvalue problems 16 integration problems 19 information based complexity community also known hyper bolic cross points idea even traced back 41 ddimensional problem sparse grid approach employs oh 1 points dis cretization accuracy approximation however nearly good conventional full grid methods provided certain additional smoothness requirements fullled thus sparse grid discretization method employed also higherdimensional problems curse dimensionality conventional full grid methods affects sparse grids much less paper apply sparse grid combination technique 28 classication problem regularization network problem discretized solved certain sequence conventional grids uniform mesh sizes coordinate direction contrast 18 dlinear functions stemming tensorproduct approach used apply linear basis functions based simplicial discretization comparison approach allows processing dimensions needs less operations per data point sparse grid solution obtained solutions dierent grids linear combination thus classier build sparse grid points data points discussion complexity method gives method scales linearly number instances ie amount data classied therefore method well suited realistic data mining applications dimension feature space moderately high eg preprocessing steps amount data large furthermore quality classier build new method seems good consider standard test problems uci repository problems huge synthetical data sets 10 dimensions turns new method achieves correctness rates competitive best existing methods note combination method simple use parallelized natural straightforward way remainder paper organized follows section 2 describe classication problem framework regularization networks minimization qua dratic functional discretize feature space derive associated linear problem focus gridbased discretization techniques introduce sparse grid combination technique classication problem discuss properties furthermore present new variant based discretization simplices discuss complexity aspects section 3 presents results numerical experiments conducted sparse grid combination method demonstrates quality classier build new method compares results ones 18 ones obtained dierent forms svms 33 nal remarks conclude paper 2 problem classication data interpreted traditional scattered data approximation problem certain additional regularization terms contrast conventional scattered data approximation applications encounter quite highdimensional spaces end approach regularization networks 21 gives good framework approach allows direct description important neural networks also allows equivalent description support vector machines nterm approximation schemes 20 consider given set already classied data training rg assume data obtained sampling unknown function f belongs function space v dened r sampling process disturbed noise aim recover function f given data good possible clearly illposed problem since innitely many solutions possible get wellposed uniquely solvable problem assume knowledge f end regularization theory 43 47 imposes additional smoothness constraint solution approximation problem regularization network approach considers variational problem min f2v c denotes error cost function measures interpolation error f smoothness functional must well dened rst term enforces closeness f data second term enforces smoothness f regularization parameter balances two terms typical examples 2 r denoting gradient laplace operator value chosen according crossvalidation techniques 13 22 37 44 principle structural risk minimization 45 note nd exactly type formulation case scattered data approximation methods see 1 31 regularization term usually physically motivated 21 discretization restrict problem nite dimensional subspace function f replaced ansatz functions f j g n span vn preferably form basis vn coecients f j g n j1 denote degrees freedom note restriction suitably chosen nitedimensional subspace involves additional regularization regularization discretization depends choice vn remainder paper restrict choice given linear operator p way obtain minimization problem feasible linear system thus minimize fn nite dimensional space vn plug 2 4 obtain dierentiation respect k k k equivalent matrix notation end linear system c square n n matrix entries c rectangular n matrix entries b vector contains data labels length unknown vector contains degrees freedom j length n depending regularization operator obtain different minimization problems vn example use gradient fn regularization expression 1 obtain poisson problem additional term resembles interpolation problem natural boundary conditions partial dierential equation neumann conditions discretization 2 gives us linear system 7 c corresponds discrete laplacian obtain classier fn solve system 22 grid based discrete approximation yet specic nite dimensional subspace vn type basis functions want use contrast conventional data mining approaches work ansatz functions associated data points use certain grid attribute space determine classier help grid points similar numerical treatment partial dierential equations reasons simplicity remainder paper restrict ourself case x 0 1 situation always reached proper rescaling data space conventional nite element discretization would employ equidistant grid n mesh size coordinate direction n renement level following always use gradient regularization expression 3 let j denote nite element method piecewise dlinear ie linear dimension test trialfunctions nj x grid would give nj nj x variational procedure 4 6 would result discrete linear system size matrix entries corresponding 7 note fn lives space vn spanf nj discrete problem 8 might principle treated appropriate solver like conjugate gradient method multigrid method suitable ecient iterative method however direct application nite element discretization solution resulting linear system appropriate solver clearly possible ddimensional problem larger four number grid points order oh best case number operations order encounter socalled curse dimensionality complexity problem grows exponentially least 4 reasonable value n arising system stored solved even largest parallel computers today 23 sparse grid combination technique therefore proceed follows discretize solve problem certain sequence grids l l 1 l uniform mesh sizes h tth coordinate direction grids may possess dierent mesh sizes dierent coordinate directions end consider grids l twodimensional case grids needed combination formula level 4 shown figure 1 nite element approach piecewise dlinear test trial functions lj x grid l would give f l l lj lj x variational procedure 4 6 would result discrete system l matrices unknown vector solve c figure 1 combination technique level two dimensions problems feasible method end use diagonally preconditioned conjugate gradient algorithm also appropriate multigrid method partial semi coarsening applied discrete solutions f l contained spaces piecewise dlinear functions grid l note problems substantially reduced size comparison 8 instead one problem size nd deal problems size dimv l moreover problems solved independently allows straightforward parallelization coarse grain level see 23 also simple eective static load balancing strategy available 25 finally linearly combine results f l lj lj x dierent grids l follows f c resulting function f c n lives sparse grid space space dimv spanned piecewise dlinear hierarchical tensor product basis see 8 note summation discrete functions dierent spaces v l 13 involves dlinear interpolation resembles transformation representation hierarchical basis details see 24 28 29 however never explicitly assemble function f c keep instead solutions f l dierent grids l arise combination formula linear operation f f c easily expressed means combination figure 2 twodimensional sparse grid left threedimensional sparse grid acting directly functions f l ie ff c l 1 l nd 1 q ff l therefore want evaluate newly given set data points f test evaluation set form combination associated values f l according 13 evaluation dierent f l test points done completely parallel summation needs basically allreducegather operation second order elliptic pde model problems proven combination solution f c n almost accurate full grid solution fn ie discretization error jje c provided slightly stronger smoothness requirement f full grid approach holds need seminorm 1 bounded furthermore series expansion error necessary combination technique existence shown pde model problems 10 combination technique one various methods solve problems sparse grids note exist also nite dierence 24 38 galerkin nite element approaches 2 8 9 work directly hierarchical product basis sparse grid combination technique conceptually much simpler easier implement moreover allows reuse standard solvers dierent subproblems straightforwardly parallelizable 24 simplicial basis functions far mentioned dlinear basis functions based tensorproduct approach case presented detail 18 grids combination technique linear basis functions based simplicial discretization also possible use socalled kuhns triangulation 15 32 rectangular block see figure 3 summation discrete functions dierent spaces l 13 involves linear interpolation table 1 complexities storage assembly matrixvector multiplication dierent matrices arising combination method one grid l discretization approaches c l g l stored together one matrix structure dlinear basis functions linear basis functions l b l storage o3 n o3 n o2 o2 assembly o3 n od 2 2d od 2 o2 mvmultiplication o3 n o3 n o2 o2 figure 3 kuhns triangulation threedimensional unit cube theroetical properties variant sparse grid technique still investigated detail however results presented section 3 warrant use see slightly worse results linear basis functions dlinear basis functions believe new approach results approximation order since new variant combination technique overlap supports ie regions two basis functions nonzero greatly reduced due use simplicial discretization complexities scale signicantly better concerns costs assembly storage nonzero entries sparsely populated matrices 8 see table 1 note general operators p complexities c l scale o2 n choice zeroentries arise need considered reduce complex ities see table 1 right column c l actual iterative solution process diagonally preconditioned conjugate gradient method scales independent number data points approaches note however storage run time complexities still depend exponentially dimension presently due limitations memory modern workstations 512 mbyte 2 gbyte therefore deal case 8 dlinear basis functions 11 linear basis functions decomposition matrix entries several computers parallel environment would permit dimensions 3 numerical results apply approach dierent test data sets use synthetical data real data practical data mining applications data sets rescaled 0 1 evaluate method give correctness rates testing data sets available tenfold crossvalidation results otherwise details criti figure 4 spiral data set sparse grid level 5 top left 8 bottom right cal discussion evaluation quality classica tion algorithms see 13 37 31 twodimensional problems rst consider synthetic twodimensional problems small sets data correspond certain structures 311 spiral rst example spiral data set proposed alexis wieland mitre corp 48 194 data points describe two intertwined spirals see figure 4 surely articial problem appear practical ap plications however serves hard test case new data mining algorithms known neural networks severe problems data set neural networks separate two spirals 40 table 2 give correctness rates achieved leaveoneout crossvalidation method ie 194fold cross validation best testing correctness achieved level 8 8918 comparison 7720 40 figure 4 show corresponding results obtained sparse grid combination method levels 5 8 level 7 two spirals clearly detected resolved note 1281 grid points contained sparse grid level 8 2817 sparse grid points shape two reconstructed spirals gets smoother table 3 results ripley data set linear basis dlinear basis best possible level tenfold test test data test data linear dlinear 9 877 00015 901 909 911 910 level training correctness testing correctness 9 00006 10000 8814 table 2 leaveoneout crossvalidation results spiral data set reconstruction gets precise 312 ripley data set taken 36 consists 250 training data 1000 test points data set generated synthetically known exhibit 8 error thus better testing correctness 92 expected since training testing data proceed follows first use training set determine best regularization parameter per tenfold crossvalidation best test correctness rate corresponding given dierent levels n rst two columns table 3 compute sparse grid classier 250 training data column three table 3 gives result classier previously unknown test data set see method works well already level 4 sucient obtain results 914 reason surely relative simplicity data see figure 5 hyperplanes enough separate classes quite properly also see much need use higher levels contrary even overtting eect visible figure 5 column 4 show results 18 achieve almost results dlinear functions see kind results could possible sophisticated strategy determing give last two columns table 3 testing correctness achieved best possible end compute discrete values sparse grid classiers 250 data points evaluate test set pick best result clearly see much dierence indicates approach determine value training set crossvalidation works well almost results linear dlinear basis functions note testing correctness figure 5 ripley data set combination technique linear basis functions left level 4 right level 8 906 911 achieved 36 35 respectively data set 32 6dimensional problems 321 bupa liver bupa liver disorders data set irvine machine learning database repository 6 consists 345 data points 6 features selector eld used split data 2 sets 145 instances 200 instances respectively test data therefore report tenfold crossvalidation results compare dlinear results 18 two best results 33 therein introduced smoothed support vector machine ssvm classical support vector machine svm jjjj 2 11 46 results given table 4 expected sparse grid combination approach linear basis functions performs slightly worse linear approach best test result 6960 level 4 new variant sparse grid combination technique performs slightly worse ssvm whereas dlinear variant performs slighly better support vector machines note results svm approaches like support vector machine using 1norm approach svm jjjj 1 reported somewhat worse 33 table 4 results bupa liver disorders data set linear dlinear comparison methods level 1 10fold train correctness 0012 7600 0020 7600 svm 33 10fold test correctness 6900 6787 ssvm svm jjjj 2level 2 10fold train correctness 0040 7613 010 7749 7037 7057 10fold test correctness 6601 6784 7033 6986 level 3 10fold train correctness 0165 7871 0007 8428 10fold test correctness 6641 7034 level 4 10fold train correctness 0075 9201 00004 9027 10fold test correctness 6960 7092 322 synthetic massive data set 6d measure performance massive data set produced datgen 34 6dimensional test case 5 million training points 20 000 points testing used call datgen r1 x0100ro0100ro0100ro o5020000 p e015 results given table 5 note already level 1 testing correctness 90 achieved 001 main observation test case concerns execution time measured pentium iii 700 mhz machine besides total run time also give cpu time needed computation matrices l see linear basis functions really huge data sets 5 million points processed reasonable time note 50 computation time spent data matrix assembly importantly execution time scales linearly number data points latter also case dlinear func tions mentioned approach needs operations per data point results much longer execution time compare also table 5 especially assembly data matrix needs 96 total run time variant present example linear basis approach 40 times faster dlinear approach renement level eg level 2 need 17 minutes linear case 11 hours dlinear case higher dimensions factor even larger 33 10dimensional problems 331 forest cover type forest cover type dataset comes uci kdd archive 4 also used 30 approach similar followed consists cartographic variables meter cells forest cover type pre dicted 12 originally measured attributes resulted 54 attributes data set besides 10 quantitative variables 4 binary wilderness areas 40 binary soil type variables use quantitative variables class label 7 values sprucefir lodgepole pine ponderosa pine cottonwoodwillow aspen douglasr krummholz like 30 report results classi cation ponderosa pine 35754 instances total 581012 since far less 10 instances belong ponderosa pine weigh class factor 5 ie ponderosa pine class value 5 others 1 treshold value separating classes 0 data set randomly separated training set test set evaluation set similar size 30 results 6 dimensions could reported table 6 present results 6 dimensions chosen ie dimensions 14567 10 10 dimensions well give overview behavior several present level n overall correctness results correctness results ponderosa pine correctness result class three values give results evaluation set chosen see table 6 already level 1 testing correctness 9395 ponderosa pine 6 dimensional version higher renement levels give better results result 9352 evaluation set almost corresponding testing correctness note 30 correctness rate 8697 achieved evaluation set usage 10 dimensions improves results slightly get 9381 evaluation result level 1 higher renement levels improve results data set note forest cover example sound enough example classication might strike forest scientists amusingly supercial known years dynamics forest growth dominant eect species present given location 7 yet dynamic variables classier one see warning never assumed available data contains relevant information 332 synthetic massive data set 10d measure performance still higher dimensional massive data set produced datgen 34 10dimen sional test case 5 million training points 50 000 points testing used call datgen r1 x0200ro like synthetical 6dimensional example main observations concern run time measured pentium iii 700 mhz machine besides total run time also give cpu time needed computation matrices g l note highest amount memory needed level 2 case 5 million data points 500 mbytes 250 mbytes matrix 250 mbytes keeping data points memory 50 run time spent assembly table 5 results 6d synthetic massive data set training testing total data matrix points correctness correctness time sec time sec iterations linear basis functions level 1 500 000 905 905 25 8 25 5 million 905 906 242 77 28 level 2 500 000 912 911 110 55 204 5 million 911 912 1086 546 223 50 000 922 914 48 23 869 level 3 500 000 917 917 417 226 966 5 million 916 917 4087 2239 1057 dlinear basis functions level 1 500 000 907 908 597 572 91 5 million 907 907 5897 5658 102 level 2 500 000 915 916 4285 4168 656 5 million 914 915 42690 41596 742 data matrix time needed data matrix scales linearly number data points see table 7 total run time seems scale even better linear 4 conclusions presented sparse grid combination technique linear basis functions based simplices classication data moderatedimensional spaces new method gave good results wide range problems capable handle huge data sets 5 million points run time scales linearly number data important property many practical applications often dimension problem substantially reduced certain preprocessing steps number data extremely huge believe sparse grid combination method possesses great potential practical application problems demonstrated ripley data set best value regularization parameter determined also practical relevance parallel version sparse grid combination technique reduces run time signicantly see 17 note method easily parallelizable already coarse grain level second level parallelization possible grid combination technique standard techniques known numerical treatment partial differential equations since necessarily dimensions need maximum renement level modication combination technique regard dierent renement levels dimension along lines 19 seems promising note furthermore approach delivers continuous classier function approximates data therefore used without modication regression problems well contrast many methods like eg decision trees also two classes handled using isolines dierent values finally reasons simplicity used operator r dierential eg operators employed associated regular nite element ansatz functions 5 acknowledgements part work supported german bundesministerium fur bildung und forschung bmbf within project 03grm6bn work carried cooperation prudential systems software gmbh chemnitz authors thank one referees remarks forest cover data set 6 r adaptive verfahren f uci kdd archive uci repository machine learning databases ecological consequences computer model forest growth tensor product approximation spaces e learning data concepts data mining methods knowledge discovery approximate statistical tests comparing supervised classi information complexity multivariate fredholm integral equations sobolev classes simplizialzerlegungen von beschr computation eigenproblems hydrogen helium strong magnetic electric parallelization sparse grid approach data mining data mining sparse grids numerical integration using sparse grids equivalence sparse approximation support vector machines regularization theory neural networks architectures generalized cross validation method choosing good ridge parameter combination technique sparse grid solution pdes multiprocessor machines adaptive sparse grid multilevel methods elliptic pdes based optimized tensorproduct approximation spaces sparse grids boundary integral equations combination technique solution sparse grid problems high dimensional smoothing based multilevel analysis grundlagen der goemetrischen datenverarbeitung combinatorial lemmas topology ssvm smooth support vector machine classi program creates structured data bayesian neural networks classi neural networks related methods classi comparing classi die methode der finiten di interpolation sparse grids nikolskijbesov spaces dominating mixed smoothness 2d spiral pattern recognition possibilistic measures quadrature interpolation formulas tensor products certain classes functions approximation functions bounded mixed derivative solutios illposed problems estimation dependences based empirical data nature statistical learning theory spline models observational data spiral data set sparse grids tr regularization theory neural networks architectures approximation scattered data using smooth grid functions nature statistical learning theory information complexity multivariate fredholm integral equations sobolev classes 2d spiral pattern recognition possibilistic measures equivalence sparse approximation support vector machines data mining methods knowledge discovery adaptive sparse grid multilevel methods elliptic pdes based finite differences approximate statistical tests comparing supervised classification learning algorithms bayesian neural networks classification computation eigenproblems hydrogen helium strong magnetic electric fields sparse grid combination technique learning data comparing classifiers parallel solution 3d pdes network workstations vector computers ctr jochen garcke regression optimised combination technique proceedings 23rd international conference machine learning p321328 june 2529 2006 pittsburgh pennsylvania deepak k agarwal shrinkage estimator generalizations proximal support vector machines proceedings eighth acm sigkdd international conference knowledge discovery data mining july 2326 2002 edmonton alberta canada j garcke griebel thess data mining sparse grids computing v67 n3 p225253 november 2001