decision tree approximations boolean functions decision trees popular representations boolean functions show given alternative representation boolean function f say readonce branching program one find decision tree approximates f desired amount accuracy moreover size decision tree smallest decision tree represent f construction obtained quasipolynomial time also extend result case one access source random evaluations boolean function f instead complete representation case show similar approximation obtained specified amount confidence opposed absolute certainty former case latter result implies proper paclearnability decision trees uniform distribution without using membership queries b introduction decision trees popular representations boolean func tions form basic inference engine wellknown machine learning programs c45 q86 q96 boolean decision trees also used problem performing reliable computations presence faulty components kk94 medical diagnosis popularity decision trees representing boolean functions may attributed following reasons universality decision trees represent boolean functions amenability manipulation many useful operations boolean functions performed efficiently time polynomial size decision tree rep resentation contrast operations intractable popular representations table 1 gives comparison decision trees dnf formulas readonce branching programs supported nsf grant 9820840 advantages decision tree representation motivate following problem given arbitrary representation boolean function f find equivalent representation f decision tree small size immediately evident problem bound hard stated polynomial time solvability problem would imply satisfiability cnf formulas decided polynomial time impossible unless pnp therefore consider slightly different problem let us say g approximation f fraction assignments g f differ evaluation given arbitrary representation boolean function f find approximation f decision tree small size order fall trap interested solving problem efficiently realis tically may use time polynomial following parameters 1 size given representation f 2 size smallest decision tree representation f given 3 inverse desired error tolerance ie 1 approximations would useful applications small amount error tolerated return gains would accrue decision tree rep resentation indeed case applications machine learning data mining example one could postprocess hypothesis output learning program convert decision tree ensuring much error introduced choosing suitably small note one may use knowledge special properties representation scheme hypothesis constructing decision tree approximation note one may even construct decision tree approximation decision tree hypothesis would useful conjunction programs like c45 output decision trees make special efforts ensure output tree provably smallest desired error tolerance expense sacrificing little error one could achieve desired minimization cases table 1 complexity operations different representation schemes readonce branching decision trees programs universality 2 representations polynomial time polynomial time polynomial time 2 representations polynomial time polynomial time polynomial time complement representation exponential time polynomial time polynomial time deciding satisfiability polynomial time polynomial time polynomial time deciding unsatisfiability np complete b polynomial time polynomial time deciding monotonicity conp complete b open polynomial time c deciding equivalence conp complete b corp polynomial time e deciding symmetry conp complete b polynomial time c polynomial time c deciding relevance conp complete b open polynomial time e variables counting number pcomplete f polynomial time c polynomial time c satisfying assignments making representation np hard b corp polynomial time e irredundant making representation np hard b open nphard g minimum truthtable np hard h open polynomial time minimization straightforward definition representation scheme easy reduction cnfsatisfiability c proved paper result follows one bcw80 e folk theorem proves decision trees testable equivalence polynomial time results follow f proved s75 proved zb98 h result masek cited garey johnsons book gj79 proved glr99 first show case wellknown representation schemes small approximating decision trees obtained quasipolynomial time polynomial factor first parameter listed multiplied factor involves exponent logarithmic second third parameters schemes 1 decision trees 2 ordered binary decision diagrams 3 readonce branching programs 4 olog nheight branching programs 5 satj dnf formulas constant j 6 boolean formulas third item generalization first two result first two follows third quasipolynomial time algorithm actually holds generality classes roughly speaking representation schemes number satisfying assignments input function small projections computed efficientlya property call satcountable paperwould come technique employed indeed present algorithm general way argue required properties hold schemes worth emphasizing although time taken algorithm quasipolynomial size decision tree approximation fact output decision tree smallest size decision tree height level approximation sense optimal certainly size larger smallest decision tree represent boolean function approximated also consider situation evaluations boolean function f available given sample evaluations show previous algorithm modified slightly give quasipolynomial time algorithm produces small approximating decision tree sample decision tree may disagree f evaluating jsj assignments given argue latter result implies proper quasipolynomial time paclearnability decision trees uniform distribution informally learning result may interpreted 8as follows compared absolute certainty approximation first result learning result says given access source random evaluations f instead complete representation f output algorithm approximating decision tree much confidence desired absolute certainty may way obtain decision tree approximations representation schemes like dnf formulas counting number satisfying assignments pcomplete gj79 novel feature learning algorithm occam algorithm behw87 unlike ones known learning theory algorithm may actually make errors even training sample used consequently analysis sample complexity generalization ones normally used may independent interest learning result compared similar ones learning theory bshoutys monotone theory based algorithm b95 deployed learn decision trees arbitrary fixed distribution polynomial time following drawbacks comparison algo rithm algorithm uses membership queries outputs decision tree depth3 formula similarly bshouty mansours algorithm bm95 output decision tree ehrenfeucht haussler eh89 show decision trees rank r learnable time n distribu tion rank decision tree height largest complete binary tree embedded since decision tree nodes rank log first glance result would seem improvement learning result paper since one could learn node decision trees quasipolynomial time distribution difference learning node decision trees n variables algorithm would always produces decision tree size larger using sample size polynomial inverse error confidence parameters contrast algorithm ehrenfeucht haussler may output tree size n olog using sample size quasipolynomial n polynomial inverse error confidence parameters rest paper organized follows section 2 contains definitions lemmas used remaining sec tions section 3 algorithm finding approx imating decision tree given satcountable representation section 4 contains results approximating decision trees given source random evaluations boolean function conclude open problems section 5 preliminaries let f boolean function set n variables total assignment obtained setting n variables either 0 1 assignment may represented nbit vector f0 1g n natural way satisfying assignment f one 1 number satisfying assignments f denoted f partial assignment obtained subset variables v assigned values partial assignment may represented vector length n whose elements either 0 1 vector element corresponding variable assigned value thus total number partial assignments 3 n number partial assignments k variables assigned values n size partial vector denoted jj number elements assigned 0 1 empty partial vector denoted one variables assigned projection f partial assignment denoted f function obtained hardwiring values variables included precisely given total assignment partial assignment let denote total assignment obtained setting variable whose value value variable whose value value f defined interested projectionclosed representation classes boolean functions ie ones given representation boolean function f partial vector boolean function f also represented class moreover representation computed polynomial time say projectionclosed representation class polynomialtime satcountable given representation f value f computed time polynomial size representation n total number variables representation function f use jdj denote size context assures ambiguity treat representation synonymous boolean function represented error errf f 0 f respect another boolean function f 0 defined set n variables total number assignments f 6 f 0 moreover f approximation f 0 consider following projectionclosed representation classes boolean functions paper 1 decision trees decision tree binary tree leaves labeled either 0 1 internal node labeled variable given assignment evaluated starting root iteratively applying following rule leaf reached let variable current node x value position 1 branch right otherwise branch left leaf reached labeled 0 resp 1 1 size decision tree number nodes 2 branching programs bps branching program directed acyclic graph unique node indegree called root two nodes outdegree 0 called leaves one labeled 0 labeled 1 nonleaf node graph contains variable outdegree exactly two every variable appears rootleaf path branching program called readonce robp note decision tree effectively considered robp assigments evaluated following rule decision trees height bp length longest path root leaf node ordered binary decision diagram obdd robp additional property variables appear order path root leaf 3 satj dnf formulas dnf formulas every assignment satisfied j terms formula 4 formulas boolean formulas every variable occurs proposition 1 decision trees obdds robps bps sat j dnf formulas formulas projectionclosed proof bp projection partial vector computed follows redirect incoming edges vertex labeled variable assigned value left respectively right child vertex variable assigned value 0 respectively 1 recursively delete vertices incoming edges using depthfirst search steps achieved linear time note bp decision tree obdd robp h height bp projection also belongs class sat jdnf formulas projection obtained substituting values assigned variable 0 dnf term result deletion term whereas 1 results deletion variable term formula appropriate boolean algebra rules applied eliminate 1s 0s obtained accomplished linear time cases proposition 2 robps olog nheight bps satcount able proof number satisfying assignments robp f computed follows traverse nodes f reverse topological order let fx denote subrobp rooted node x consisting vertices reached x edges joining node x visited fraction x 0 x 1 assignments fx satisfying assignments computed follows x leaf x 0 1 value leaf node otherwise x internal node x yz z left right children x simple inductive argument shows completion r r root robp fraction satisfying assignments f consequently n number variables f next let b olog n height bp representing boolean function f first construct decision tree equivalent f spreading b creating separate copy node whenever needed rather sharing subfunctions branching program decision tree may immediately satisfy readonce property easily converted one eliminating subtrees duplicated variables along path total number nodes resultant decision tree 2 olog finally compute number satisfying assignments decision tree described robp next two propositions used paper proved simply order complete table 1 proposition 3 decision trees tested monotonicity polynomial time proof let given decision tree n variables convenient extend partial order defined boolean lattice set partial vectors implies 6 0 partial vector say every total vector leaf node x determines partial vector px based assignment variables path root leaf node let us say x counterexample monotonicity partial vector px x value 1 essential observation monotone leaf counterexample monotonicity easy test monotonicity using observation leaf node x assigned value 1 let partial vector obtained setting 1 variables px assigned 1 leaving remaining variables projection p 0 x identically 1 x counterexample monotonicity demonstrated path 0 projection boolean function fv permutation v 0 proposition 4 robps tested symmetry polynomial time proof proof inspired central idea bcw80 let f boolean function set variables denote set assignments f evaluates 1 first generalize f realvalued function treating v set real variables precisely redefine f variables v assume values 0 1 value redefinition coincides value boolean true generalization shown bcw80 given robp representation boolean function f value real function real vector v computed linear time visiting robp topological order next let table 2 system linear equations calculating jr k jb set assignments f precisely k ones x computing values g0 mentioned using robp representation f treating jr k j variables leads system linear equations table 2 easily shown rank coefficient matrix therefore system admits unique solution finally observe boolean function f symmetric jr k j either 0 n values k 0 k n proof follows decide symmetry obdds decision trees also polynomial time proposition 5 sat jdnf formulas satcountable proof let us say two terms 0 conflicting contains literal l 0 contains literal l consensus two nonconflicting terms 0 denoted tt 0 term obtained union literals 0 conflicting consensus 0 definition satj dnf formula f implies every set terms formula must least two conflicting terms therefore using principle inclusion exclusion term k literals simply 2 n k comment sum needs consider consensus j terms f constant j total time computation polynomial proposition 6 formulas satcountable proof let f formula set n variables f constant 1 f constant 0 term containing single literal written either f 1 f 2 formulas disjoint sets n 1 n 2 variables respectively easy argue recursive application rules ensures f computed 3 finding decision tree approximation main result section algorithm constructing decision tree approximation boolean function f represented projectionclosed satcountable class heart algorithm procedure find generalization dynamic programming method used glr99 truthtable minimization decision trees find works follows given f boolean function n variables height parameter h size parameter builds precisely one tree set k partial vector size h eac h k 0 k k set decision tree representations function f size k height h jj minimum error respect f among trees minimum size desired approximation therefore tree constructed set w 1g algorithm employs twodimensional array p k hold tree k tree p array represented triple form root left subtree right sub tree unless contains single leaf node case represented leafs value partial vector notation v 1 v 0 respectively denotes partial vector obtained extending setting variable v 1 0 respectively lemma 7 algorithm find correct ie given satcount able representation boolean function f height parameter h size parameter find outputs decision tree 0 height h size among decision trees errt 0 f minimum one decision tree minimum error minimum size among trees proof show induction l p k tree k 0 jj h 01 foreach jj h 02 f 03 else p 0 0 04 05 0 06 foreach 07 foreach 08 p 09 foreach variable v used k 11 errp k f errv 12 13 else errp k f 14 jp v 0 k 15 figure 1 algorithm find mg tree must leaf value 0 1 depending value yields minimum error relative f lines 2 3 algorithm find examine hypercube corresponding f determine whether majority assignments 0 1 also true l l assume p k 0 correctly computed l l k 0 0 minf2 hjj also assume p k 0 correctly computed k 0 0 k 1 show find causes tree k placed p k size trees k less k induction hypothe sis p k initialized tree k line 8 lines 915 cannot modify p k algorithm cor rect therefore let size trees k exactly k let opt tree k let v root v must variable assigned value let sizes opts left right subtrees k 0 respectively observe k 0 k 1 one examined line 9 let let 1 induction hypothe errright subtree opt f1 since error tree sum errors two subtrees algorithm finds tree p k error opt size opt lemma follows lemma 8 let pjf j n denote time complexity computing number satisfying assignments arbitrary projection given satcountable function f time complexity find oh proof since f satcountable representation time required line 2 opjf j nn number partial vectors examined line 1 h lines 14 take opjf j nn oh time lines 5 6 cause oh partial vectors examined variable line 7 takes values n possibilities v possible combinations k 0 k 00 line 9 complexity lines 1015 dominated o1 error computations decision tree p satcountable function f error computation implemented follows leaf node x partial vector corresponding evaluation path leading x contribution total error partial vector either f leaf x value 0 2 njjjj f value 1 total error errt obtained summing errors computed fashion leaf complexity computation bounded opjf j nm lines 515 hence algorithm find bounded oh 3 pjf j n common dynamic programming algorithms memoizing helps reduce overall complexity observe complexity error computation reduced maintaining second twodimensional array e whose elements contains error corresponding element array p first e 0 computed opjf j n time lines 2 3 remaining e ks computed every time p k updated o1 time simply summing error left right subtrees p k timesaving modification time complexity becomes opjf lemma 9 let mnode decision tree exists decision tree height nodes approximation proof restrict height h converting node x level h either 0 1 depending whether 0s 1s respectively hypercube defined path leading x call tree clearly nodes error confined hypercubes converted nodes x level h original tree since dm2e nodes error node 2 n h 1 follows approximation substituting 4 yields desired result theorem 10 given satcountable boolean function representation f whose smallest decision tree representation nodes error parameter find decision tree 0 nodes approximates f time polynomial jf j n log proof given f use standard doubling trick determine olog iterations algorithm least value findf logm returns decision tree approximates f lemma 9 size smallest decision tree represent f correctness time complexity follow lemmas 7 8 respectively 4 learning decision trees uniform distribution show algorithm previous section extended learn decision trees uniform distribution remarked introduction means given access uniformly distributed sample evaluations boolean function f error parameter confidence parameter algorithm output decision tree nodes least number nodes needed represent f decision tree approximates f confidence least 1 algorithm takes time polynomial n logm log1 ie quasipolynomial time algorithm however sample complexity algorithm modest polynomial parameters log n log1 log1 use following additional terminology prove results section let mhn denote class decision trees n variables height h size decision tree let h tree height h obtained converting nonleaf nodes depth h leaf nodes classification 0 1 depending whether majority assignments corresponding hypercube f classified 0 1 respectively recall two boolean functions f 1 f 2 n denotes number assignments f 1 6 f 2 extension sample classified examples form h bi assignment number examples form h bi f 6 b need following wellknown inequalities proposition 11 chernoff bounds let outcomes r identical independent bernoulli trials prob x pr 0 probr p given sample classified examples boolean function form h bi assignment b 2 f0 1g height parameter h size parameter decision tree height h size computed among decision trees errd minimum among minimum error trees minimum size computation requires oh proof let denote assignments extend partial assignment given computed ojsjn time modify condition line 2 algorithm find number assignments whose values 1 0 compared modified line 2 takes ojsjn time references f lines 10 11 13 replaced error computations carried described proof lemma 8 error computation takes ojsjm time since rest algorithm unchanged complexity obtained replacing pjf j n jsj note also true modified algorithm proposed proof lemma 8 correctness follows lemma 7 theorem 13 given uniformly distributed sample size examples mnode decision tree n variables error parameter 0 1 confidence parameter 0 1 find decision tree mhn time orm 2 n oh confidence least 1 error approximating ie proof execute algorithm find modified deal sample described lemma 12 parameters h let call decision tree 0 mhn bad errt fixed bad decision tree 0 outputs mhn least error sample 2here last inequality follows chernoff bounds applied number errors trees 0 probability p find outputs bad tree 0 mhn certainly jt mhn j 2e r 2 8 number binary trees nodes 2 4 number decision trees nodes also upper bound mhn con sequently choice r proposition little bit arithmetic probability p turns conclusions given satcountable representation boolean function uniformly distributed sample evaluations boolean function paper presents quasipolynomial algorithm computing decision tree smallest size approximates function possible achieve polynomial time failing possible obtain decision tree whose size within polynomial factor smallest approximating decision tree polynomial time finding decision tree smallest size equivalent given one nphard zb98 opens question whether least polynomial approximation smallest equivalent decision tree possible polynomial time ideas paper seem enough answer ques tion hope combining ideas results ehrenfeucht haussler eh89 work matter fact results already used give quasipolynomial approximation smallest decision tree equivalent projectionclosed representation allows testing tautology satisfiability polynomial time quasipolynomial time done following way consider sample ehrenfeucht haussler algorithm 2 n assignments however avoid using time polynomial sample size noting operations sample algorithm consist 1 checking assignments evaluate either 0 1 2 computing new sample 0 obtained projecting given variable 0 1 operations time polynomial given representation converts algorithm one whose complexity added factor form r smallest rank equivalent decision tree since r cannot exceed olog size smallest equivalent decision tree get desired quasipolynomial approximation finally ideas paper combined ehrenfeucht haussler properly learn decision trees arbitrary distributions without membership queries 6 acknowledgment thank anonymous referee suggested sharper bound mhn led improvement sample complexity theorem 13 r occams razor equivalence free boolean graphs decided probabilistically polynomial time exact learning boolean functions via monotone theory simple learning algorithms decision trees multivariate polynomials learning decision trees random examples computers intractability exact learning irrelevant variables abound constructing optimal binary decision trees npcomplete boolean decision trees faulty nodes induction decision trees learning decision tree classi fiers central problems computational complexity finding small equivalent decision trees hard tr occams razor learning decision trees random examples needed learning exact learning boolean functions via monotone theory learning decision tree classifiers partial occams razor applications exact learning irrelevant variables abound computers intractability induction decision trees simple learning algorithms decision trees multivariate polynomials central problems computational complexity