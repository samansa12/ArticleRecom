model selection error estimation study model selection strategies based penalized empirical loss minimization point tight relationship error estimation databased complexity penalization good error estimate may converted databased penalty function performance estimate governed quality error estimate consider several penalty functions involving error estimates independent test data empirical vc dimension empirical vc entropy marginbased quantities also consider maximal difference error first half training data second half expected maximal discrepancy closely related capacity estimate calculated monte carlo integration maximal discrepancy penalty functions appealing pattern classification problems since computation equivalent empirical risk minimization training data labels flipped b introduction consider following prediction problem based random observation one estimate 2 prediction rule measurable bounded loss function data consist sequence independent identically distributed samples distribution x n independent x goal choose prediction rule f n restricted class f loss lf close possible best possible loss inmum taken prediction rules empirical risk minimization evaluates performance prediction rule f 2 f terms empirical loss b l n provides estimate whose loss close optimal loss l class f suciently large loss best function f close l ii suciently small nding best candidate f based data still possible two requirements clearly con ict tradeo best understood writing rst term often called estimation error second approximation error often f large enough minimize l possible distributions x f large empirical risk minimiza tion case common x advance sequence smaller model whose union equal f given data n one wishes select good model one classes problem model selection denote b f k function f k minimal empirical risk one hopes select model class fk excess error el close min idea structural risk minimization also known complexity regular ization add complexity penalty b f k compensate overtting eect penalty usually closely related distributionfree upper bound sup f2f k penalty eliminates eect overtting thus structural risk minimization nds best tradeo approximation error distributionfree upper bound estimation error unfortunately distributionfree upper bounds may conservative specic distributions criticism led idea using datadependent penalties next section show approximate upper bound error including datadependent bound used dene possibly data dependent complexity penalty c n k model selection algorithm excess error close min section 3 gives several applications performance bounds section 2 section 31 considers estimates provided independent test sample disadvantage cost data section 32 considers distributionfree estimate based vc dimension datadependent estimate based shatter coecients unfortunately dicult compute section 33 brie considers marginbased error estimates viewed easily computed estimates quantities analogous shatter coecients section 34 looks estimate provided maximizing discrepancy error rst half sample second half classication estimate conveniently computed simply minimizing empirical risk half labels ipped section 35 looks complex estimate expected maximum discrepancy estimate calculated monte carlo integration lead better performance bounds section 4 review concentration inequalities central proofs finally section 5 oer experimental comparison proposed methods clarity include table 1 notation use throughout paper work complexity regularization see akaike 1 barron 23 bar ron birge massart 4 barron cover 5 birge massart 89 buescher kumar 1112 devroye gyor lugosi 14 gallant 16 geman hwang 17 kearns mansour ng ron 20 krzyzak linder 23 lugosi nobel 25 lugosi zeger 27 26 mallows 28 meir 33 modha masry 34 rissanen 35 schwarz 37 shawe taylor bartlett williamson anthony 38 shen wong 39 vapnik f prediction rule sets prediction rules model classes f union model classes f k f k element f k minimal loss element f k minimizing empirical loss prediction rule f minimizing loss loss minimal loss functions f k l b l n empirical loss r nk estimate high condence upper bound loss l b penalized loss estimate l loss optimal prediction rule table 1 notation 42 vapnik chervonenkis 46 yang barron 50 51 datadependent penalties studied bartlett 6 freund 15 kolt chinskii 21 koltchinskii panchenko 22 lozano 24 lugosi nobel 25 massart 30 shawetaylor bartlett williamson anthony 38 penalization error estimates class f k let b f k denote prediction rule selected f k based data goal select among rules one approximately minimal loss key assumption analysis true loss b f k estimated k assumption 1 every n positive numbers c k estimate r nk l b available satises ce 2m 2 1 notice c might depend sample size n dene databased complexity penalty r log k last term required technical reasons become apparent shortly typically small dierence r nk simply estimate right amount penalization l b finally dene prediction rule r log k following theorem summarizes main performance bound f n theorem 1 assume error estimates r nk satisfy 1 positive constants c 0 2ce 2m 2 moreover k b minimizes empirical loss model class f k r logce second part theorem 1 shows prediction rule minimizing penalized empirical loss achieves almost optimal tradeo approximation error expected complexity provided estimate r nk complexity based approximate upper bound loss particular knew advance classes f k contained optimal prediction rule could use error estimates r nk obtain upper bound el upper bound would improve bound theorem 1 range loss function innite set inmum empirical loss might achieved case could dene b f k suitably good approximation inmum however convenience assume throughout minimum always exists suces various proofs assume n closed proof brevity introduce notation 0 sup union bound r log j denition ce 2m assumption 1 ce since prove second inequality k decompose lf n l k rst term may bounded standard integration tail inequality shown see eg 14 page 208 e logce2m choosing f k lf k second term may bounded directly denition minimizes empirical loss f k last step follows fact e summing obtained bounds terms yields k logce2m implies second statement theorem sometimes bounds tighter assumption 1 available assumption bounds may exploited decrease term log km denition complexity penalty assumption 2 every n positive numbers c k estimate r nk l b available satises ce 2 dene modied penalty dene prediction rule trivial modication proof theorem 1 obtain following result theorem 2 assume error estimates r nk satisfy assumption 2 positive constants c 0 moreover k b minimizes empirical loss model class f k log2ec far concentrated expected loss penalized estimate however easy modication proof obtain exponential tail inequalities work one inequality scenario theorem 1 theorem 3 assume error estimates r nk satisfy 1 positive constants c k b minimizes empirical loss model class f k 0 r log k proof note r log k r log k sup r log k rst inequality theorem 1 r log k union bound denition r log k minimizes empirical loss f k e 2n log kn hoedings inequality concludes proof examples shown concentrate expected loss penalized empirical error minimizers tail probability estimates may obtained cases simple application theorem applications 31 independent test sample assume independent sample pairs available simply remove samples training data course attractive may small relative n case estimate l b apply hoedings inequality show assumption 1 satised apply theorem 1 give following result corollary 1 assume model selection algorithm section 2 performed holdout error estimate 3 min r log k words estimate achieves nearly optimal balance approximation error quantity may regarded amount overtting inequality recover main result lugosi nobel 25 much simpler estimate fact bound corollary may substantially improve main result 25 square roots bound corollary 1 removed increasing penalty term small constant factor using bernsteins inequality place hoedings follows choose modied estimate r nk 1 1 positive constant bernsteins inequality see eg 14 yields thus 2 satised replaced 3m1 8 therefore dening obtain performance bound 32 estimated complexity remaining examples consider error estimates r nk avoid splitting data simplicity concentrate section case classication 01 loss dened 0 arguments may carried general case well recall basic vapnikchervonenkis inequality 45 43 sup n empirical shatter coecient f k number dierent ways n points classied elements f k easy show inequality implies estimate r log es k x 2n assumption 1 need estimate quantity log es k x 2n simplest way use fact es k x 2n vc dimension f k substituting theorem min r log 4 r rn type distributionfree result mentioned introduction interesting result involves estimating es k x 2n k x n theorem 4 assume model selection algorithm section 2 used r min r 12e log k x n r log k key ingredient proof concentration inequality 10 random vc entropy log 2 k x n proof need check validity assumption 1 shown 10 n satises conditions theorem 9 first note es k x 2n log es k x 2n log last inequality theorem 9 therefore r 3e log k x n sup r log es k x 2n used vapnikchervonenkis inequality 4 follows r r 3e log k x n r 12 log k x n r 3e log k x n r 12 log k x n r 3e log k x n last term may bounded using theorem 9 follows r r 3e log k x n log expb 9 expb 9 log 2c exp summarizing therefore assumption 1 satised applying theorem 1 nishes proof 33 eective vc dimension margin practice may dicult compute value random shatter coecients k x n alternative way assign complexities may easily obtained observing k x n empirical vc dimension class f k vc dimension restricted points immediate estimate r log 4 assumption 1 way estimate theorem 4 fact careful analysis possible get rid log n factor price increased constant unfortunately computing k general still dicult lot eort devoted obtain upper bounds k simple compute bounds handy framework since upper bound may immediately converted complexity penalty particular marginsbased upper bounds misclassication probability neural networks 6 support vector machines 38 7 44 13 convex combinations classiers 36 29 immediately give complexity penalties theorem 1 performance bounds recall facts basis theory support vector machines see bartlett shawetaylor 7 cristianini shawetaylor 13 vapnik 44 references therein model class f called class generalized linear classiers exists function f class linear classiers r p class prediction rules form w 2 r p weight vector satisfying much theory support vector machines builds fact eective vc dimension generalized linear classiers minimal distance correctly classied data points separating hyperplane larger certain margin may bounded independently linear dimension p function margin constant say linear classier correctly classies x margin recall following result lemma 1 bartlett shawetaylor 7 let f n arbitrary possibly data dependent linear classier form w n 2 r p weight vector satisfying kw 0 positive random variables let k n positive integer valued random variable k x k r correctly classies k n data points x margin 0 sn assume b f minimizes empirical loss class f generalized linear classiers correctly classies least n k data points margin application lemma shows take sn obtain f sn r2m log sn using inequality inequality shows model classes f k classes generalized linear classiers classes error estimate r nk dened condition 1 satised theorem 1 may used result obtain following performance bound theorem 5 sn kk r log k k r k random variables k corresponding class f k importance result lies fact gives computationally feasible way assigning datadependent penalties linear classiers hand estimates r nk may much inferior estimates studied previous section 34 penalization maximal discrepancy section propose alternative way computing penalties improved performance guarantees new penalties may still dicult compute eciently better chance obtain good approximate quantities dened solutions optimization problem assume simplicity n even divide data two equal halves dene predictor f empirical loss two parts l 2 using notation section 2 dene error estimate r nk l 2 loss function 01 loss ie 0 0 maximum discrepancy l 2 may computed using following simple trick rst ip labels rst half data thus obtaining modied data set x 0 next nd f k 2 f k minimizes empirical loss based 0 l 2 clearly function f k maximizes discrepancy therefore algorithm used compute empirical loss minimizer b may used nd f k compute penalty based maximum discrepancy appealing although empirical loss minimization often computationally dicult approximate optimization algorithm used nding prediction rules estimating appropriate penalties particular algorithm approximately minimizes empirical loss class f k minimizes proper subset f k theorem still applicable et al 47 considered similar quantity case pattern classication motivated bounds similar 5 elf n b dened eective vc dimension obtained choosing value vc dimension gives best bound experimental estimates elf n b showed linear classiers xed dimension variety probability distributions good suggests model selection strategy estimates elf n using bounds following theorem justies direct approach using discrepancy training data directly rather using discrepancy range sample sizes estimate eective vc dimension shows independent test sample necessary similar estimate considered 49 although error bound presented 49 theorem 34 nontrivial maximum discrepancy negative theorem 6 penalties dened using maximumdiscrepancy error estimates 6 min l 2 r log k proof check assumption 1 apply theorem 1 introduce ghost sample x 0 n independent data distribution denote empirical loss based sample proof based simple observation k thus k l 2 sup l 2 sup l 2 sup l 2 dierence supremum maximum satises conditions mcdiarmids inequality see theorem 8 c probability exp 2 2 n9 thus assumption 1 satised proof nished 35 randomized complexity estimator section introduce alternative way estimating quantity may serve eective estimate complexity model class f maximum discrepancy estimate previous section splitting data two halves oer alternative allows us derive improved performance bounds consider expectation random split data two sets maximal discrepancy koltchinskii 21 considers similar estimate proves bound analogous theorem 7 improve bound theorem sequence iid random variables pf 1and independent data n introduce quantity sup n use nk measure amount overtting class f k note nk known may computed arbitrary precision montecarlo simulation case pattern classication computation integration involves minimizing empirical loss sample randomly ipped labels oer two dierent ways using estimates model selection rst based theorem 1 second slight modication theorem 2 start simpler version theorem 7 let dene error estimates r nk choose f n minimizing penalized error estimates r log k r log k proof introduce ghost sample proof theorem 6 recall symmetrization trick gine zinn 18 sup sup n sup sup sup rest proof assumption 1 follows easily concentration equalities k sup sup sup last step used mcdiarmids inequality easy verify dierence supremum nk satises condition theorem 8 c assumption 1 holds theorem 1 implies result concentration inequalities concentrationofmeasure results central analysis inequalities guarantee certain functions independent random variables close mean recall three inequalities used proofs theorem 8 mcdiarmid 31 let x independent random variables taking values set assume f n r satises sup c mcdiarmids inequality convenient f variance situations variance f much smaller following inequality might appropriate theorem 9 boucheron lugosi massart 10 suppose independent random variables taking values set r exists function r x 0 moreover log 5 experimental comparison empirical penalization criteria 51 learning problem section report experimental comparison proposed model selection rules setup proposed kearns mansour ng ron 20 toy problem x drawn uniform distribution interval 0 1 class f k dened class functions f 2 f k exists partition 0 1 f constant intervals straightforward check vcdimension f k k1 following 20 assume target function f belongs f k unknown k label example x obtained ipping value denotes noise level clearly function g makes simple learning problem especially convenient experimental study fact computation minima empirical loss min f2f k performed time log n using dynamic programming algorithm described 20 lozano 24 also reports experimental comparison model selection methods problem paper studied several penalized model selection techniques holdout crossvalidation method based independent test sample penalization based empirical vc entropy maximum discrepancy estimator randomized complexity estimator investigated learning problem easy see empirical vc entropy log 2 k x n class f k almost surely constant equal therefore penalization based empirical vc entropy essentially equivalent guaranteed risk minimization grm procedure proposed vapnik 44 thus investigate empirically method note lozano 24 compares grm procedure method based rademacher penalties similar randomized complexity estimator nds rademacher penalties systematically outperform grm procedure 20 grm compared minimum description length principle independent test sample technique regarded simplied crossvalidation technique main message 20 penalization techniques take account empirical loss structural properties models cannot compete crossvalidation sample sizes contrary conclusion based experiments databased penalties perform favorably compared penalties based independent test data gures shown report experiments three methods 1 holdout method holdout bases selection independent samples described section 31 2 maximum discrepancy md method selects model according method section 34 3 rademacher penalization rp performs randomized complexity method proposed section 35 using maximum discrepancy section 34 experiments penalties were2 l 2 found multiplying penalty dened section 34 12 provides superior performance using randomized complexity estimators sec tion 35 penalties sup n note experiments log k log k terms omitted penalties reasons comparison performance oracle selection also shown pictures method selects model minimizing true loss l b among empirical loss minimizers b f k classes f k training error minimization algorithm described 20 implemented using templates priority queues doubly linked lists provided leda library 32 52 results results illustrated gures general conclusion may observe generalization error ie true loss obtained methods mdp rp favorable compared holdout even sample sizes 500 1000 datadependent penalization techniques perform well holdout data dependent penalization techniques exhibit less variance holdout main message paper good error estimation procedures provide good model selection methods hand except holdout method datadependent penalization methods try estimate directly l b gures show accurate noise level high becomes rather inaccurate noise level decreases strong incentive explore datadependent penalization techniques take account fact parts f k equally eligible minimizing empirical loss acknowledgements thanks vincent mirelli alex smola fruitful conversations thanks anonymous reviewers useful suggestions r new look statistical model identi logically smooth density estimation complexity regularization application arti minimum complexity density estimation sample complexity pattern classi generalization performance support vector machines pattern classi sharp concentration inequality applications random combinatorics learning learning canonical smooth estima tion learning canonical smooth estima tion introduction support vector machines bounding learning algorithms nonlinear statistical models nonparametric maximum likelihood estimation method sieves experimental theoretical comparison model selection methods rademacher penalties structural risk minimization rademacher processes bounding risk function learning radial basis function networks complexity regularization function learning model selection using rademacher penalization adaptive model selection using empirical com plexities nonparametric estimation via empirical risk minimization concept learning using complexity regulariza tion comments c p improved generalization explicit optimization margins applications concentration inequalities statis tics method bounded di platform combinatorial geometric computing performance bounds nonlinear time series prediction minimum complexity regression estimation weakly dependent observations universal prior integers estimation minimum description length boosting margin estimating dimension model structural risk minimization datadependent hierarchies convergence rate sieve estimates best constants khintchine inequality concentration measure isoperimetric inequalities product spaces estimation dependencies based empirical data nature statistical learning theory statistical learning theory uniform convergence relative frequencies events probabilities theory pattern recognition measuring vcdimension learning machine weak convergence empirical processes asymptotic property model selection criteria tr ctr davide anguita sandro ridella fabio rivieccio rodolfo zunino quantum optimization training support vector machines neural networks v16 n56 p763770 june clayton scott robert nowak learning minimum volume sets journal machine learning research 7 p665704 1212006 leila mohammadi sara van de geer asymptotics empirical risk minimization journal machine learning research 6 p20272047 1212005 peter l bartlett shahar mendelson rademacher gaussian complexities risk bounds structural results journal machine learning research 3 312003 andrs antos balzs kgl tams linder gbor lugosi datadependent marginbased generalization bounds classification journal machine learning research 3 p7398 312003 joel ratsaby complexity hyperconcepts theoretical computer science v363 n1 p210 25 october 2006 ron meir tong zhang generalization error bounds bayesian mixture algorithms journal machine learning research 4 1212003 joel ratsaby learning multicategory classification sample queries information computation v185 n2 p298327 september 15 magalie fromont model selection bootstrap penalization classification machine learning v66 n23 p165207 march 2007 shie mannor ron meir tong zhang greedy algorithms classificationconsistency convergence rates adaptivity journal machine learning research 4 p713741 1212003 ron meir gunnar rtsch introduction boosting leveraging advanced lectures machine learning springerverlag new york inc new york ny