compileroptimized simulation largescale applications high performance architectures paper propose evaluate practical automatic techniques exploit compiler analysis facilitate simulation large messagepassing systems use compiler techniques compilersynthesized static task graph model identify subset computations whose values significant effect performance program generate symbolic estimates execution times computations programs regular computation communication patterns information allows us avoid executing simulating large portions computational code simulation also allows us avoid performing message data transfers still simulating message performance detail used techniques integrate mpisim parallel simulator ucla rice dhpf compiler infrastructure evaluate accuracy benefits techniques three standard messagepassing benchmarks wide range problem system sizes optimized simulator errors less 16 compared direct program measurement cases studied typically much smaller errors furthermore requires factors 5 2000 less memory factor 10 less time execute original simulator dramatic savings allow us simulate regular messagepassing programs systems problem sizes 10 100 times larger possible original simulator current stateoftheart simulators b introduction predicting parallel application performance essential step developing large applications highly scalable parallel architectures sizing system configurations necessary large problem sizes analyzing alternative architectures systems considerable research done analytical simulation models performance prediction complex scalable systems analytical methods typically require custom solutions problem may tractable complex interconnection networks detailed modeling scenarios simulation models likely primary choices generalpurpose performance prediction well known however detailed simulations large systems computationintensive long execution times significant deterrent widespread use current generation parallel program simulators use two techniques reduce model execution times direct execution parallel simulation direct execution simulator uses available system resources directly execute portions program parallel simulation distributes computational workload among multiple processors using appropriate synchronization algorithms ensure execution model produces result events model executed causal order however current state art even using direct execution parallel simulations simulation large applications designed architectures thousands processors run many orders magnitude slower physical counterparts paper propose implement evaluate practical automatic optimizations exploit compiler support enable efficient simulation large messagepassing parallel programs goal enable simulation target systems thousands processors realistic problem sizes expected large platforms key idea underlying work use compiler analysis isolate fragments local computations message data whose values affect performance program example computations determine loop bounds control flow message patterns volumes effect performance whereas computations many array values significant effect performance computations abstracted away simulating rest program detail predict performance characteristics application similarly also possible avoid performing data transfers many messages whose values affect performance simulating performance messages detail two major aspects compiler analysis required accomplish optimization identifying values within program could affect program performance isolating computations communications determine values perform first step use compilersynthesized static task graph model 4 5 abstract representation identifies sequential computations tasks parallel structure program task scheduling precedences explicit communication controlflow determines parallel structure symbolic expressions task graph control flow conditions communication patterns volumes scaling expressions sequential task execution times directly capture values ie references within expressions impact program performance second step uses compiler technique called program slicing 21 identify portions computation determine values compiler emit simplified mpi code contains exactly computations must actually executed simulation addition communication remaining code fragments abstracted away compiler also needs estimate execution time abstracted code using parameterized direct measurement addition reducing simulation times optimizations dramatically reduce memory requirements simulation major program arrays referenced redundant computations allocated simulation memory savings potentially allow much larger problem sizes architectures studied would otherwise feasible order demonstrate impact optimizations combined mpisim parallel simulator 6 2527 dhpf compiler infrastructure 2 develop program simulation framework incorporates new techniques described original mpisim simulator used direct execution parallel simulation achieve substantial reductions simulation time parallel programs dhpf normal usage compiles hpf program mpi variety shared memory systems provides extensive parallel program analysis capabilities integrated tool allow us evaluate impact preceding optimizations existing mpi hpf programs without requiring changes source code previous work modified dhpf compiler automatically synthesize static task graph model symbolic task time estimates mpi programs compiled hpf source programs 1 work use static task graph plus program slicing perform simulation optimizations described also extended mpisim exploit information compiler avoid executing significant portions computational code hypothesis significantly reduce memory time requirements simulation therefore enable us simulate much larger systems problem sizes previously possible use number widely used benchmarks evaluate utility integrated framework sweep3d 1 benchmark sp npb benchmark suite 8 tomcatv spec92 benchmark simulation models application validated measurements range problem sizes numbers processors errors predicted execution times compared direct measurement 1 future plan synthesize information existing mpi codes well dhpf infrastructure supports general computation partitioning communication analysis symbolic analysis capabilities make feasible wide class mpi programs 16 cases studied often substantially less validation done distributed memory ibm sp architecture well shared memory sgi origin 2000 note mpisim simulates mpi communication communications via shared memory optimizations significant impact performance simulators total memory usage simulator using compiler synthesized model factor 5 2000 less original simulator simulation time typically lower factor 510 dramatic savings allow us simulate systems problem sizes 10100 times larger possible original simulator without significant reductions accuracy simulator example successful simulating execution configuration sweep3d target system 10000 processors many cases simulation time faster original program remainder paper proceeds follows section 2 first describes state art parallel program simulation set stage work section 3 provides brief overview mpisim static task graph model section 4 describes optimization strategy compiler simulator extensions required implement strategy section 5 describes experimental results section 6 presents main conclusions related work analytical performance prediction intractable complex applications program simulations commonly used studies well known simulations large systems tend slow improve simulators directexecution used 20 26 28 direct execution simulators make use available system resources directly execute portions application code simulate architectural features specific interest unavailable example simulators used study various architectural components memory subsystem interconnection network specifically one interested determining faster communication fabric network workstations value given set applications one run application currently available machines simulate projected networks behavior benefits directexecution simulation obvious first one estimate value new hardware without expense purchasing second one simulation fastthere need simulate workstations behavior example level memory references since part hardware readily available many early simulators designed sequential execution 9 13 14 however even use abstract models direct execution sequential program simulators tended slow slowdown factors ranging 2 35 process simulated program 9 several recent efforts exploring use parallel execution 10 16 17 23 24 27 28 reduce model execution times varying degrees success order multiple simulation processes maintain accuracy simulations use protocols synchronize processes one widely used protocols quantum protocol lets processes compute given quantum synchronizing general synchronous simulators use quantum protocol must tradeoff simulation accuracy speedfrequent synchronizations slowdown simulation synchronizing less frequently introduces errors possibly executing statements outoforder lapse 16 17 parallel proteus use form program analysis increase simulation window beyond fixed quantum mpisim uses parallel discrete event simulation conservative protocol 24 27 supported protocols include null message protocol nmp 11 conditional event protocol cep 12 new protocol combination two 22 discussed next section mpisim exploits determinism present communication pattern application reduce many cases completely eliminate synchronization overheads although simulation protocol optimizations reduced simulation times resulting improvements still inadequate simulate large problems interest highend users instance sweep3d kernel application asci benchmark suite released us department energy largest configuration requires computations grid one billion elements memory requirements execution time configuration makes impractical simulate even running simulations high performance computers hundreds processors overcome computational intractability researchers used abstract simulations avoid execution computational code entirely 18 19 however leads major limitations make approach inapplicable many real world applications main problem abstracting away code model essentially independent program control flow even though control flow may affect communication pattern well sequential task times also preceding solution requires significant user modifications source program form special input language order express required information abstracted sequential tasks communication patterns makes difficult apply tool existing programs written widely used standards message passing interface mpi high performance fortran hpf 3 background goals 31 mpisim parallel simulation mpi programs using direct execution starting point work mpisim directexecution parallel simulator performance prediction mpi programs mpisim simulates mpi application running parallel system referred target program system respectively machine simulator executed host machine may either sequential parallel machine general number processors host machine less number processors target architecture simulated simulator must support multithreading simulation kernel processor schedules threads ensures events host processors executed correct timestamp order target thread simulated follows local code simulated directly executing host processor communication commands trapped simulator uses appropriate model predict execution time corresponding communication activity target architecture supports commonly used mpi communication routines pointtopoint collective communications simulator collective communication functions implemented terms pointto point communication functions pointtopoint communication functions implemented using set core nonblocking mpi functions general host architecture fewer processors target machine sequential simulation host machine one processor requires simulator provide capability multithreaded execution since mpi programs execute collection single threaded processes necessary provide capability multithreaded execution mpi programs mpisim memory execution time constraints sequential simulation led development parallel implementations mpisim mpisim ported multiple parallel architectures including distributed memory ibm sp2 well sharedmemory sgi origin 2000 simulation kernel provides support sequential parallel execution simulator parallel execution supported via set conservative parallel simulation protocols 26 typically work follows 2 future plan synthesize information existing mpi codes well dhpf infrastructure supports general computation partitioning communication analysis symbolic analysis capabilities make feasible wide class mpi programs application process simulation modeled logical processes lp 3 lp execute independently without synchronizing lps executes wait operation mpirecv mpibarrier etc synchronization protocol used decide lp proceed briefly describe default protocol used mpisim lp model computes local quantities called earliest output time eot earliest input time eit 7 eot represents earliest future time lp send message lp model similarly eit represents lower bound receive timestamp future messages lp may receive upon executing wait statement lp safely select matching message input buffer receive timestamp less eit different asynchronous protocols differ method computing eit implementation supports variety protocols mentioned previously primary overhead implementing parallel conservative protocols due communications compute eit blocking suffered lp able advance eit suggested implemented number optimizations significantly reduce frequency strength synchronization parallel simulator thus reducing unnecessary blocking execution 26 27 optimizations geared towards exploiting determinism applications instance consider lp blocked receive statement input buffer contains single message general lp cannot proceed removing message buffer might possible another message destined lp transit message lower timestamp however receive statement known process deterministic follows must exist unique message matches receive statement soon lp receives message proceed without need synchronizations lps model best case every receive statement model known deterministic synchronization messages generated model parallel simulation extremely efficient preceding optimizations two limitations first works communications statements priori known deterministic second use direct execution simulator implies memory computation requirements simulator least large target application restricts target systems application problem sizes studied even using parallel host machines compilerdirected optimizations discussed next section primarily aimed alleviating restrictions 32 static task graph representation seen next section compiler analysis performed greatly facilitated exploiting appropriate abstract representation parallel behavior program part poems project 3 15 developed abstract program representation called static task graph stg captures extensive static information parallel program 5 stg designed computed automatically parallelizing compiler compact symbolic representation parallel structure program independent specific program input values number processors node stg represents set possible parallel tasks typically one per process identified symbolic set integer process identifiers illustrate stg example mpi program shown figure 1 compute node loop nest represents set tasks one per process denoted symbolic set process ids 0 p node also includes markers describing corresponding region source code original program node must represent contiguous region code edge graph represents set edges connecting pairs parallel tasks described symbolic integer mapping example communication edge figure labeled mapping indicating process p 1 sends process nodes fall one three categories controlflow computation communication computational node includes symbolic scaling function captures number loop iterations task 3 general lp used simulate multiple application processes scales function arbitrary program variables communication node includes additional symbolic information describing pattern volume communication overall stg serves general language architectureindependent representation messagepassing programs previous work extended dhpf compiler synthesize static dynamic task graphs mpi programs generated dhpf compiler hpf source programs 4 future extract task graphs directly existing mpi codes compiler support extremely valuable enables techniques developed paper applied fully automatically ie without user intervention efficient simulation parallel programs compilersupported techniques efficient largescale simulation section begins motivating overall strategy use address key restriction simulation scalability identified namely time cost required simulating detailed computations target program describe specifically strategy accomplished 41 optimization strategy challenges parallel program simulators used performance evaluation execute simulate actual computations target program two purposes determine execution time computations b determine impact computational results performance program due artifacts like communication patterns loop bounds controlflow many parallel programs however sophisticated compiler extract extensive information target program statically particular identify two types relevant information often available compiletime 1 parallel structure program including sequential portions computation tasks mapping tasks threads communication synchronization patterns threads 2 symbolic estimates execution time isolated sequential portions computation information provided simulator directly may possible avoid executing substantial portions computational code simulation therefore reduce execution time memory requirements simulation illustrate goal consider simple example mpi code fragment figure 1 code performs shift communication operation array every processor sends boundary values left neighbor code executes simple computational loop nest simple example communication pattern number iterations loop nest depend values block size per processor b array size n number processors p local processor identifier myid therefore computation values must executed simulated simulation however communication pattern loop iteration counts depend values stored arrays computed used computational loop nest earlier refer latter values redundant computations point view performance estimation estimate performance computational loop nest analytically could avoid simulating code loop nest still simulating communication behavior detail could achieve optimization using compiler generate simplified code shown right figure code replaced loop nest call special simulatorprovided delay function extended mpisim provide function simply forwards simulation clock double precision anmax 1 double precision call mpicommsizempicommworld p ierr call mpicommrankmpicommworld myid ierr read n myid gt send d2n1 myidb1 processor myid1 endif myid lt p recv d2n1 myid1b1 processor myid1 endif endif integer allocatable dummybuf call mpicommsizempicommworld p ierr call mpicommrankmpicommworld myid ierr call readandbroadcastw1 read n allocate dummybufn22 myid gt send dummybuf processor myid1 endif myid lt p recv dummybuf processor myid1 endif call delayn2 minnmyidbb figure 1 example illustrate simple mpi program b taskgraph mpi program c simplified mpi program efficient simulation original mpi code c simplified mpi code b task graph original mpi code task pairs p q compute controlflow edge c communication edge compute tasks simulation thread specified amount compiler estimates cost loop nest form simple scaling function shown argument delay call function describes computational cost varies retained variables b n p myid plus parameter w 1 representing cost single loop iteration currently obtain value w 1 direct measurement one selected problem sizes number processors use scaling function compute required delay value problem sizes number processors note example compiler avoided allocating arrays significantly reduces memory required simulate program additional optimization compiler prove data transferred message also redundant simulator also avoid performing actual data transfer although simulate message operation detail also avoid allocating memory message buffer message optimization lead savings simulation time memory usage paper develops automatic compilerbased techniques perform optimizations described evaluates potential benefits techniques particular goal use compilergenerated static task graph plus additional compiler analysis avoid simulating executing substantial portions computational code target program sending unnecessary data use task graph identify computational tasks candidates elimination compute scaling expressions delay functions importantly identify values computed program impact performance use additional compiler analysis distinguish computations compute values ie redundant defined specifically four major challenges must address achieving goals first three addressed previous system known us must transform original parallel program simplified legal mpi program simulated mpisim simplified program must include computation communication code needs executed simulator must yield performance estimates original program total execution time individual process total communication computation times well detailed metrics communication behavior b must able abstract away much local computation within task feasible eliminate many data structures original program possible isolating redundant computations program c must identify messages whose contents directly affect computation receiver exploit information reduce simulation time memory usage must estimate execution times abstracted computational tasks given program size number processors accurate performance prediction sequential code challenging problem widely studied literature use fairly straightforward approach described section 45 refining approach part ongoing work poems project following subsections describe techniques use address challenges implementation dhpf mpisim first describe basic process using task graph generate simplified mpi program describe compiler analysis needed identify redundant computations finally discuss approach use estimate performance eliminated code 42 translating static task graph simplified mpi program stg directly identifies local sequential computational tasks control flow communication tasks patterns parallel program using compilergenerated stg basis analysis avoid perform complex ad hoc analysis identify components given information first step identify contiguous regions computational tasks andor controlflow stg collapsed single condensed collapsed task loop nest figure 1 note simply transformation stg simplifying analysis directly imply changes parallel program refer task graph resulting transformation condensed task graph later analysis consider single computational task single collapsed task time deciding simplify code refer either single sequential task criteria collapsing tasks depend goals performance study first general rule collapsed region must include branches exit region ie single exit end region second current work collapsed region must contain communication tasks aim simulate communication precisely finally deciding whether collapse conditional branches involves difficult tradeoff important eliminate controlflow references large arrays order achieve savings memory time desire difficult estimate performance code containing controlflow found however typically branches involve large arrays significant impact program performance example one minor conditional branch loop nest sweep3d depends intermediate values large 3d arrays impact branch execution time relatively negligible detecting fact general difficult within compiler may depend expected problem sizes computation times therefore two possible approaches take precise approach allow user specify directives specific branches eliminated treated analytically program simulation simpler approximate approach eliminate conditional branches inside collapsible loop nest rely statistical average execution time iteration provide good basis estimating total execution time loop nest either approach use profiling estimate branching probabilities eliminated branches currently taken second approach first one difficult implement could provide precise performance estimates condensing task graph also compute scaling expression collapsed task describes number computational operations scales function program variables introduce time variables represent execution time sequence statements single loop iteration denoted w task approach use estimate overall execution time sequential task described section 45 based condensed task graph assuming compiler analysis section 43 needed generate simplified mpi program follows retain controlflow loops branches original mpi code retained condensed task graph ie controlflow collapsed second retain communication code original program particular calls underlying messagepassing library program array otherwise unused referenced communication call replace array reference reference single dummy buffer used communication note without message optimization described later section simulator must still perform actual data transfer processes simulating message message optimization attempts eliminate data transfer use buffer size maximum message sizes communication calls program allocate buffer statically dynamically potentially multiple times depending required message sizes known third replace code sequence sequential task task graph call mpisim delay function pass argument describing estimated execution time task insert sequence calls runtime function one per w parameter start program read value parameter file broadcast processors finally eliminate data variables referenced simplified program 43 program slicing identifying redundant computations data major challenge performing transformations mentioned earlier correctly effectively identify redundant computations ie ones safely eliminated solution propose use program slicing retain parts computational code associated data structures affect program execution time given variable referenced statement program slicing finds isolates subset program computation data affect value variable 21 subset conservative limited precision static program analysis therefore may minimal key requirement applying program slicing identify variable values affect execution time program compilergenerated static task graph captures information directly precisely allowing us avoid complicated ad hoc analysis entire source code particular values affect performance exactly variable references appear retained controlflow condensed graph scaling functions sequential tasks communication events source destination expressions communication descriptors communication calls values identified program slicing used isolate computations data affect variable values program slicing essentially reachability analysis dependence graph program including data control dependences particular given particular target reference use reachability analysis identify statements program affect value reference chain dependences ie feasible path dependence graph wellknown compiler technique omit details stateoftheart algorithm program slicing described 21 used basis implementation applying technique however requires target reference part program appears program dependence graph computed compiler expressions static task graph directly derived corresponding expressions program therefore cannot used starting points program slicing expressions introduce dummy procedure call statements appropriate points target program passing expressions arguments rebuild program dependence graph expressions used starting points slicing dummy procedure calls later eliminated obtaining memory time savings desire requires full interprocedural program slicing completely eliminate uses many large arrays possible general interprocedural slicing challenging feasible compiler technique currently available dhpf infrastructure take limited interprocedural side effects account order correctly handle calls runtime library routines including communication calls runtime routines dhpf compilers runtime library particular assume routines modify arguments passed reference cannot modify global ie common block variables mpi program necessary sufficient support singleprocedure benchmarks expect incorporate full interprocedural slicing future support continuing work poems final output slicing analysis set computations must retained simplified mpi code remaining computations program except io statements communication calls considered redundant code generation simplified mpi program described previous section modified slightly use information sequential task nonredundant computations retained generated program rest task replaced single call simulator delay function precise performance prediction simulator delay calls include time retained computations since simulated time accounted explicitly execution time estimates computed however apply entire task practice found amount nonredundant code small tasks therefore adjust execution time estimates account retained code 44 message optimization simulating redundant messages noted previous section data transferred messages may also redundant point view performance cases identified avoid performing data transfers simulation potentially leading additional time memory savings although conceptually similar redundant computations discuss message optimization separately mechanism achieving optimization somewhat different explained first compiler identify redundant messages direct result program slicing analysis described particular technique described account interprocedural sideeffects slicing directly identifies message receive calls receive redundant values corresponding message send calls already known compiler compiler provides information simulator flagging mpi calls redundant buffers used messages allocated resulting simplified mpi program actual message optimization follows call flagged mpisim simulates call detail sending necessary protocol messages predicting endtoend latency messages sends data receiving simulation thread actual data available simulated application however call flagged compiler redundant mpisim still simulates call detail respect mpi communication protocol sends empty message receiving simulation thread since redundant receives also flagged receiver copy data buffer messages need present simulated application provide information synchronization program although optimization reduce number messages sent size messages reduced memory used messages need allocated results lower latencies incurred messages sent processors well smaller communication overheads due copying data enclosed messages intofrom communication buffers also results lower memory usage simulator 45 estimating task execution times main approximation approach estimate sequential task execution times without direct execution analytical prediction sequential execution times extremely challenging problem particularly modern superscalar processors cache hierarchies variety possible approaches different tradeoffs cost complexity accuracy simplest approach one use paper measure task times specifically w one selected problem sizes number processors use symbolic scaling functions derived compiler estimate delay values problem sizes number processors current scaling functions symbolic functions number loop iterations incorporate dependence cache working sets problem sizes believe extensions scaling function approach capture nonlinear behavior caused memory hierarchy possible performance estimates measured task times simplified mpi code mpi code timers parallel program dhpf parallel system figure 2 compilation parameter measurement simulation parallel program two alternatives direct measurement task time parameters use compiler support estimating sequential task execution times analytically b use separate offline simulation sequential task execution times 15 cases need scaling functions remains including issues mentioned important amortize cost estimating parameters many prediction experiments scaling functions tasks depend intermediate computational results addition program inputs even case may appear compiler example nas benchmark sp grid sizes processor computed stored array used loop bounds use array makes forward propagation symbolic expressions infeasible therefore completely obscures relationship loop bounds program input variables simply retain executable scaling expressions including references arrays simplified code evaluate execution time able automate fully modeling process given hpf application compiled mpi modified dhpf compiler automatically generates two versions mpi program one simplified mpi code delays calls described previously second full mpi code timer calls inserted perform measurements w parameters output timer version directly provided input delay version code complete process illustrated figure 2 performed detailed experimental evaluation compilerbased simulation approach studied three issues experiments 1 accuracy optimized simulator uses compilergenerated information compared original simulator direct measurements target program 2 reduction memory usage achieved optimized simulator compared original resulting improvements overall scalability simulator terms system sizes problem sizes simulated 3 performance optimized simulator compared original terms absolute simulation times terms relative speedup compared sequential model execution simulating large number target processors results categories presented types optimizations considered paper elimination local computations elimination data contents large messages begin description experimental methodology describe results issues turn 51 experimental methodology used three realworld benchmarks tomcatv sweep3d nas sp one synthetic communication kernel sample study tomcatv spec92 floatingpoint benchmark studied hpf version benchmark compiled mpi dhpf compiler sweep3d department energy asci benchmark 1 sp nas parallel benchmark npb23b2 benchmark suite 8 mpi benchmarks written fortran 77 finally designed synthetic kernel benchmark sample evaluate impact compilerdirected optimizations programs varying computation granularity message communication patterns commonly used parallel applications tomcatv dhpf compiler automatically generates three versions output mpi code normal mpi code generated dhpf benchmark key arrays hpf code distributed across processors contiguous blocks second dimension ie using hpf distribution block b simplified mpi code calls mpisim delay function making full use techniques described section 4 c normal mpi code timer calls inserted measure task time parameters described section 45 since dhpf parses emits fortran mpisim supports c use f2c translate version generated code c run mpisim two benchmarks sweep3d nas sp manually modified existing mpi code generate simplified mpi mpi code timers case since task graph synthesis mpi codes implemented yet codes serve show compiler techniques developed applied large range codes good results application measured task times values w 16 processors measured values used experiments problem size different numbers processors exception nas sp measured task single problem size 16 processors used task times problem sizes well recall scaling functions use currently account cache working sets cache performance changing either problem size number processors affects working set size per process therefore cache performance application nevertheless measurement approach provided accurate predictions optimized simulator shown next subsection benchmarks except sample evaluated distributed memory ibm sp 128 processors sample experiments conducted shared memory sgi origin 2000 8 processors 52 validation original mpisim successfully validated number benchmarks architectures 6 26 27 new techniques described section 4 however introduce additional approximations modeling process key new approximation estimating sequential execution times portions computational code tasks abstracted away aim section evaluate accuracy mpisim applying techniques application optimized simulator henceforth denoted mpisimtg validated direct measurements application execution time also compared predictions original simulator studied multiple configurations problem size number processors application cases mpisimtg validated measured system 4 begin tomcatv handled fully automatically steps compilation task measurements simulation shown figure 2 size tomcatv used validation 20482048 figure 3 shows results 4 64 processors even though mpisim analytical model mpisim tg accurate mpisim direct execution mpisimde error performance predicted mpisimtg 16 average error 113 measured system 4 message optimizations introduced modify underlying communication model thus affect validation validation mpisim tomcatv2060100140 number processors runtime sec measured figure 3 validation mpisim 20482048 tomcatv ibm sp figure 4 shows execution time model sweep3d total problem size 150150150 grid cells predicted using mpisimtg mpisimde well measured values 64 processors predicted measured values close differ 98 average mpisimde differed measured value 37 mpisimtg 72 number processors runtime sec measured figure 4 validation sweep3d ibm sp fixed total problem size finally validated mpisimtg nas sp benchmark task times obtained 16 processor run class smallest three builtin sizes b c benchmark used experiments problem sizes figures 5 6 show validation class largest size class c validation class good errors less 7 validation class c also good average error 4 even though task times obtained class result particularly interesting programs size class c average runs 166 times longer class demonstrates compileroptimized simulator capable accurate projections across wide range scaling factors furthermore cache effects appear play great role code two applications examined illustrated fact errors increase noticeably task times obtained small number processors used larger number processors validation sp class a100300500700 number processors runtime sec measured figure 5 validation nas sp class ibm sp validation sp class c5001500250016 36 64 100 number processors runtime seconds measured figure validation nas sp class c ibm sp figure 7 summarizes errors mpisimtg incurred simulating three applications errors within 16 figure emphasizes compilersupported approach combining analytical model simulation accurate range benchmarks system sizes problem sizes hard explore errors without detailed analysis application therefore better quantify errors expected optimized simulator used sample benchmark allows us vary computation communication ratio well communication patterns error mpisimtg predictions measured number processors tomcatv sweep3d150cubed figure 7 percent error incurred mpisimtg predicting application performance validated origin 2000 two common communication patterns selected wavefront nearest neighbor pattern communication computation ratio varied 1 100 ratio 1 1 figure 8 plots total execution time program mpisimtg prediction order demonstrate better impact computation granularity validation figure 8 plots percentage variation predicted time compared measured values seen figure predictions accurate ratio computation communication large typical many realworld applications amount computation granularity program decreases simulator incurs larger errors expected measurement errors task time estimation errors become relatively significant nevertheless graph shows predicted values differ 15 measured values even small communication computation ratios validation sample measured vs predicted optimization origin 2k5001500001 00125 00167 0025 communication computation ratio time seconds wvfrntmeasured nnmeasured figure 8 validation sample origin 2000 percent variation measured time predicted time515001 003 010 030 050 070 090 communication computation ratio difference wvfrnt nn figure 9 effect communication computation ratio predictions accuracy mpisimtg large computation communication ratio 5 error indicates slightly higher errors observed tomcatv sweep3d nas sp must due presence small computation communication ratios 53 expanding simulator larger systems problem sizes main benefit using compilergenerated code decrease memory requirements simplified application code since simulator uses least much memory application decreasing amount memory application decreases simulators memory requirements thus allowing us simulate large problem sizes systems number processors total memory use total memory use memory reduction factor sweep 3d 44255 per proc problem size 4900 2884mb 30mb 96 sweep 3d 661000 per proc problem size 6400 215gb 122mb 1762 tomcatv 20482048 4 236mb 1184kb 1993 table 1 memory usage mpisimde mpisimtg benchmarks table 1 shows total amount memory needed mpisim using analytical mpisimtg direct execution mpisimde models sweep3d 4900 target processors analytical models reduce memory requirements two orders magnitude 44255 per processor problem size similarly 661000 problem size memory requirements target configuration 6400 processors reduced three orders magnitude three orders magnitude reduction also achieved tomcatv smaller reductions achieved sp dramatic reduction memory requirements model allows us simulate much larger target architectures b show significant improvements execution time simulator illustrate improved scalability achieved simulator compilerderived analytical models consider sweep3d paper study small subset problems interest application developers represented 20 million cell total problem size divided 44255 77255 2828255 per processor problem sizes need run 4900 1600 100 processors respectively scalability simulator 44255 problem size seen figure 10 memory requirements direct execution model restricted largest target architecture could simulated 2500 processors analytical model possible simulate target architecture 10000 processors since applications predicted runtime 10000 processors 110955 seconds runtime simulator configuration 148118 seconds simulators slowdown 1335 note instead scaling system size could scale problem size instead increase memory requirements per process order simulate much larger problems validation scalability sweep3d 4x4x255proc26101 10 100 1000 10000 number processors runtime sec measured figure 10 scalability sweep3d 44255 per processor size ibm sp 54 performance mpisim benefits compileroptimized simulation evident memory reduction also improved performance characterize performance simulator four ways 1 performance gains using message optimization mpisimtgmo mpisimtg compared mpisimde 2 absolute performance ie total simulation time mpisimtg vs mpisimde vs application 3 parallel performance mpisimtg terms absolute relative speedups 4 performance mpisimtg simulating large systems given parallel host system effect optimizations simulators performance illustrate performance improvements mpisimde mpisimtg takes advantage local optimizations mpisimtgmo additionally optimizes messages sent conducted experiments three benchmarks case sweep3d compared performance three versions simulator given number host processors available problem size per processor fixed number target processors experiment increased study demonstrates ability simulator efficiently simulate large problem sizes nas sp since problem size application given class c fixed number target processors varied number host processors available simulator study illustrates relative performance simulators also ability use computational resources figures 11 12 13 show performance mpisimtgmo mpisimtg mpisimde simulating sweep3d three sizes per processor sizes 77255 1414255 2828255 simulators use host processors simulate 4900 target processors improvements performance mpi simde mpisimtg sizes average 397 6728 8807 respectively problem size per processor grows larger amount computation per processor increases thus amount computation abstracted away increases resulting runtime savings 7x7x255 per processor size 64 host processors20060010 100 1000 10000 target processors runtime sec mpisimtgmo figure 11 sweep3d 7x7x255 per processor size mpisimtgmo mpisimtg message optimization 14x14x255 per processor size 64 hosts2006001000 target processors runtime sec mpisimtgmo figure 12 sweep3d 1414255 per processor size 28x28x255 per processor size 64 host procs200600100010 100 1000 10000 target procs runtime sec mpisimtgmo figure 13 sweep3d 2828255 per processor size although biggest performance gain computation optimization reducing size messages sent possible beneficial simulation mpisimtgmo runs faster simulation optimizes computation mpisimtg improvements sizes 77255 1414255 2828255 2804 3123 139 respectively benefits message optimizations limited sweep3d application uses large number barrier synchronizations well collective operations mpiallreduce operations either take data single data items also observed great performance improvements nas sp benchmark class c largest size available suite figures 14 15 show performance mpisimtg mpisimtgmo two target processor configurations 16 64 simulations run variety host processors 1 64 first mpisimtg mpisimtgmo ran faster actual application measured runtime application executing 16 processors 262338 seconds whereas running 64 processors 79067 seconds additionally figures 14 15 illustrate simulation run order magnitude faster mpisim message optimization used figure 14 jump runtime mpisimtg 1 2 host processors due large communication costs size messages sent processors 605161 doubles therefore cost sending messages increases considerably one processor used 2 host processors used increased cost compensated increased computational power however number host processors increases better performance achieved since size large messages reduced 0 mpisimtgmo simulation communication overhead significantly reduced simulator performs substantially better mpisimtg number target processors increases 64 figure 15 size messages simulation reduced 370441 target processor code still using message optimization results order magnitude decrease simulators runtime absolute performance local code optimization compare absolute performance mpisim gave simulator many processors available application host processors target processors class c2006001000 host processors runtime sec mp imtg mp imtgmo figure 14 16 target processor simulation nas sp class c running various number host processors processors nas sp c lass c100300500700 host processors runtime seconds mp imtg mp imtgmo figure 15 64 target processor simulation nas sp class c running various number host processors figure shows absolute performance sweep3d total problem size 150 3 mpisimde average 28 times slower actual application measured figure however mpisimtg initially faster measured application starting 13 times faster running 4 processors gradually becoming 22 times faster processors finally twice slow application running 64 processors message optimizations present mpisimtgmo decrease simulators runtime average 18 compared mpisimtg mpisimtg mpisimtgmo always faster average 185 times faster respectively mpisimde showing clear benefits compiler optimizations however number processors increases amount communication relative computation increases thus exposing overhead simulating communications making mpisimtg mpisim tgmo slower application cubed sweep3d total problem size1010000 number processors runtime seconds measured mpisimtgmo figure absolute performance mpisim fixed total problem size sweep3d vertical scale logarithmic figure 17 shows runtime application measured runtime two versions simulator running nas sp class observe mpisimde running twice slower application predicting however mpisimtg able run much faster application even though detailed simulation communication still performed best case 36 processors runs 25 times faster 100 processors runs 15 times faster relative performance mpisimtg decreases number processors increases amount computation application decreases increased number processors thus savings abstracting computation decreased absolute performance mpisim nas sp206010014030 50 70 90 number processors runtime seconds measured figure 17 absolute performance mpisim nas sp benchmark class even dramatic results obtained tomcatv runtime mpisimtg exceed 2 seconds processor configurations compared runtime application ranges 130 seconds figure 18 due ability compiler abstract away computation simulator needs directly execute skeleton code controls flow computation communication patterns absolute performance mpisim tomcatv20601001400 number processors runtime seconds application figure absolute performance mpisim tomcatv 2048x2048 parallel performance evaluate parallel performance simulator study well take advantage increasing system resources processors solve given problem fixed total problem size figures 14 15 indirectly demonstrate performance simulator illustrate performance better speedup achieved 16 target configuration depicted figure 19 although mpisimtgmo smaller runtime mpisimtg scales well 8 host processors number host processors increases communication overhead host begins dominate runtime hand mpi simtg send large messages suffers one host used able distribute overhead among processors 16target nas sp class c05152535 number host processors mpisimtgmo figure 19 speedup mpisim nas sp clearly performance simulator better larger systems simulated 64target processor case figure 15 runtime decreases steadily number processors increased however using host processors actually increases simulators runtime 64 target class c could run single processor due memory constraints direct speedup comparisons possible better scalability seen sweep3d application figure 20 shows performance mpisimtg mpisimde simulating 150 3 sweep3d running 64 target processors number host processors varied 1 64 data single processor mpisimde simulation available simulation exceeds available memory clearly mpisimde mpisimtg scale well speedup mpisimtg also shown figure 21 steep slope curve 8 processors indicates good parallel efficiency 8 processors speedup impressive reaching 15 64 processors due decreased computation communication ratio application still runtime mpisimtg average 54 times faster mpisimde runtime imu lator vs application 150x150x150 sweep3d 64target proc100300500700 number host processors runtime sec mp isim de mp isim tg measured figure 20 parallel performance mpisim speedup mpisimtg 150cubed sweep3d 64 target processors515 berofprocessors speedup mpisimtg figure 21 speedup mpisimtg sweep3d performance large systems quantify performance improvement mpisimtg compared running time simulators predicting performance large system case want simulate billioncell problem sweep3d applications developers envision problem utilize 20000 processors corresponds 661000 per processor problem size figure 22 shows running time simulators function number target processors 64 host processors used problem size fixed per processor problem size increases increased number processors figure clearly shows benefits optimizations best case performance 1600 processors simulated corresponding 576 million problem size runtime optimized simulator nearly half runtime original simulator however even optimizations memory requirements still large able simulate desired target system mpisim runtime 6x6x1000 per processor size host processors20060010000 500 1000 1500 2000 2500 3000 number target host processors runtime seconds figure 22 performance mpisim simulating sweep3d large systems 6 conclusions work developed scalable approach detailed performance evaluation communication behavior message passing interface mpi high performance fortran hpf programs approach based using compiler analysis identify portions computation whose results significant impact program performance therefore simulated detail compiler builds intermediate static task graph representation program enables identify program values impact performance also enables derive scaling functions computational tasks compiler uses program slicing determine portions computations needed determining performance finally compiler abstracts away parts computational code corresponding data structures replacing simple analytical performance estimates also flags messages data transfer performed within simulation communication code retained compiler simulated detail mpisim experimental evaluation shows approach introduces relatively small errors prediction program execution times benefit achieve significantly reduced simulation times typically factor 2 greatly reduced memory usage two three orders magnitude gives us ability accurately simulate detailed performance behavior systems problem sizes 10100 times larger possible current stateoftheart simulation techniques current work also exploring number alternative combinations modeling techniques example use detailed simulation sequential tasks instead analytical modeling measurement allow get accurate estimates task execution times also enable us study applications performance processor memory architecture different currently available platforms within poems aim support combination analytical modeling simulation modeling measurement sequential tasks communication code static task graph provides convenient program representation support flexible modeling environment 5 one potential limitation work benefits would large applications parallelism communication patterns depend extensively intermediate results computations particular socalled irregular applications may property evaluating benefits applications requires research perhaps refinement techniques developed another interesting direction whether techniques described extended types distributed applications ie nonscientific applications use network communication intensively fast simulation techniques could developed applications could prove extremely valuable controlling runtime optimization decisions object migration load balancing adaptation qualityof service requirements critical decisions many distributed applications acknowledgements work supported darpaito contract n6600197c8533 endtoend performance modeling large heterogeneous adaptive paralleldistributed computercommunication systems httpwwwcsutexaseduuserspoems work also supported part asci asap program doellnl subcontract b347884 darpa rome laboratory air force materiel command usaf agreement number f306029610159 wish thank members poems project valuable contributions would also like thank lawrence livermore national laboratory use ibm sp work performed adve sakellariou computer science department rice university r asci sweep3d benchmark code using integer sets dataparallel program analysis optimization poems endtoend performance design large parallel adaptive computational systems compiler synthesis task graphs parallel system performance modeling environment application representations multiparadigm performance modeling environment parallel systems performance prediction large parallel applications using parallel simulations parsec parallel simulation environment complex systems nas parallel benchmarks 20 proteus highperformance parallelarchitecture simulator optimistic simulation parallel architectures using program executables distributed simulation case study design verification distributed programs conditional event approach distributed simulation rice parallel processing testbed multiprocessor simulation tracing using tango poems endtoend performance design large parallel adaptive computational systems distributed memory lapse parallel simulation messagepassing programs parallelized direct execution simulation messagepassing parallel programs fast functional algorithm simulation testbed functional algorithm simulation fast multipole method architectural implications improving accuracy vs speed tradeoff simulating sharedmemory multiprocessors ilp processors interprocedural slicing using dependence graphs transparent implementation conservative algorithms parallel simulation languages reducing synchronization overhead parallel simulation adaptive synchronization method unpredictable communication patterns dataparallel programs parallel simulation data parallel programs mpisim using parallel simulation evaluate mpi programs asynchronous parallel simulation parallel programs wisconsin wind tunnel virtual prototyping parallel computers tr rice parallel processing testbed interprocedural slicing using dependence graphs proteus highperformance parallelarchitecture simulator wisconsin wind tunnel distributed memory lapse reducing synchronization overhead parallel simulation optimistic simulation parallel architectures using program executables parallelized direct execution simulation messagepassing parallel programs transparent implementation conservative algorithms parallel simulation languages using integer sets dataparallel program analysis optimization poems mpisim performance prediction large parallel applications using parallel simulations asynchronous parallel simulation parallel programs improving lookahead parallel discrete event simulations largescale applications using compiler analysis parsec poems adaptive synchronization method unpredictable communication patterns dataparallel programs compiler synthesis task graphs parallel program performance prediction parallel simulation data parallel programs fast improving accuracy vs speed tradeoff simulating sharedmemory multiprocessors ilp processors ctr yasuharu mizutani fumihiko ino kenichi hagihara fast performance prediction masterslave programs partial task execution proceedings 4th wseas international conference software engineering parallel distributed systems p17 february 1315 2005 salzburg austria