abstractions portable scalable parallel programming abstractin parallel programming need manage communication load imbalance irregularities computation puts substantial demands programmer key properties architecture number processors cost communication must exploited achieve good performance coding properties directly program compromises portability flexibility code significant changes needed port enhance program describe parallel programming model supports concise independent description key aspects parallel programincluding data distribution communication boundary conditionswithout reference machine idiosyncrasies independence components improves portability allowing components program tuned independently encourages reuse supporting composition existing components isolation architecturesensitive aspects computation simplifies task porting programs new platforms moreover model effective exploiting data parallelism functional parallelism paper provides programming examples compares work related languages presents performance results b introduction diversity parallel architectures puts goals performance portability conflict programmers tempted exploit machine detailssuch interconnection structure granularity parallelismto maximize performance yet software portability needed reduce high cost software development programmers advised avoid making machinespecific assumptions challenge provide parallel languages minimize tradeoff performance portability 1 languages must allow programmer write code assumes particular architecture allow compiler optimize resulting code machinespecific manner allow programmer perform architecturespecific performance tuning without making extensive modifications source code recent years parallel programming style evolved might termed aggregate dataparallel computing style characterized ffl data parallelism programs parallelism comes executing function many elements collection data parallelism attractive allows parallelism growor scalewith number data elements processors simd architectures exploit parallelism fine grain ffl aggregate execution number data elements typically exceeds number processors multiple elements placed processor manipulated sequentially attractive placing groups interacting elements processor vastly reduces communication costs moreover approach uses good sequential algorithms locally often efficient simply multiplexing parallel algorithms another benefit data passed processors batches amortize communication overhead finally computation one data element delayed waiting communication elements may processed ffl loose synchrony although strict data parallelism applies function every element local variations nature positioning elements require different implementations conceptual function instance data elements boundary computational domain neighbors communicate data parallelism normally assumes interior exterior elements treated executing different function boundaries exceptional cases easily handled features make aggregate dataparallel style programming attractive yield efficient programs executed typical mimd architectures however without linguistic support style programming promotes inflexible programs embedding performancecritical features constants number processors number data elements boundary conditions processor interconnection systemspecific communication syntax machine size problem size changes significant program changes fixed quantities generally required consequence 1 consider program portable respect given machine performance competitive machinespecific programs solving problem 2 several languages introduced support key aspects style however unless aspects style supported performance scalability portability development cost suffer instance good locality reference important aspect programming style lowlevel approaches 25 allow programmers handcode data placement resulting code typically assumes one particular data decomposition program ported platform favors de composition extensive changes must made performance suffers languages 4 5 15 give programmer control data decomposition leaving issues compiler hardware good data decompositions depend characteristics application difficult determine statically compilers make poor data placement decisions many recent languages 6 22 provide support data decompositions hide communication operations programmer thus encourage locality algorithmic level consequently reliance automated means hiding latencymultithreaded hardware multiple lightweight threads caches compiler optimizations overlap communication computationwhich cannot always hide latency trend towards relatively faster processors relatively slower memory access times exacerbates situation languages provide inadequate control granularity parallelism requiring either one data point per process 21 43 assuming larger fixed granularity 14 29 including notion granularity forcing compiler runtime system choose best granularity 15 given diversity parallel computers particular granularity best machines computers cm5 prefer coarse granularities j machine prefer finer granularity mit alewife tera computer benefit multiple threads per process also languages provide sufficient control algorithm applied aggregate data preferring instead multiplex parallel algorithm multiple data points processor 43 44 many language models adequately support loose synchrony boundaries parallel computations often introduce irregularities require significant coding effort processes execute code programs become riddled conditionals increasing code size making difficult understand hard modify potentially inefficient programming typical mimdstyle language much cleaner instance writing slightly different function type boundary process problematic change algorithm likely require versions changed paper describe language abstractionsa programmingmodelthat fully support aggregate dataparallel programming style model serve foundation portable scalable mimd languages preserve performance available underlying machine belief many tasks programmersand compilers runtime systemscan best handle performancesensitive aspects parallel program belief leads three design principles first provide abstractions efficiently implementable mimd architectures along specific mechanisms handle common types parallelism data distribution boundary conditions model based practical mimd computing model called candidate type architecture cta 45 second insignificant diverse aspects computer architectures hidden exposed programmer assumptions based characteristics sprinkled throughout program making portability difficult examples characteristics hidden include details machines communication style processor memory interconnection topology instance one machine might provide shared memory another message passing either implemented software third architectural features essential performance exposed parameterized architectureindependent fashion key characteristic speed latency permessage overhead communication relative computation cost communication increases relative computation communication costs must reduced aggregating processing onto smaller number processors finding ways increase overlap communication computation result phase abstractions parallel programming model provides control granularity parallelism control data partitioning hybrid data function parallel construct supports concise description boundary conditions core solution ensemble construct allows global data structure defined distributed processes allows granularityand location data elementsto controlled loadtime parameters ensemble also code form describing operations execute elements handling boundary conditions likewise interprocessor connections described port ensemble provides similar flexibility using ensembles three components global operationdata code communicationthey scaled together parameters three parts ensemble boundary conditions specified independently reusability enhanced remainder paper organized follows first present solution problem describing architectural model basic language modelthe cta phase abstractions section 3 gives detailed illustration abstractions using jacobi iteration example demonstrate expressiveness programmability abstractions section 4 shows simple array language primitives built top model section 5 discusses advantages programming model respect performance portability section 6 presents experimental evidence phase abstractions support portable parallel programming finally compare phase abstractions related languages models close summary phase abstractions sequential computing languages c pascal fortran successfully combined efficiency portability languages common make successful based model sequence operations manipulate infinite randomaccess memory programming model succeeds preserves characteristics von neumann machine model reasonably faithful representation sequential computers models never literally implementedunitcost access infinite memory illusion provided virtual memory caches backing storethe model accurate vast majority programs rare cases programs perform extreme amounts disk io deviations model costly programmer critical von neumann model capture machine features relevant performance essential machine features ignored better algorithms could developed using accurate machine model together von neumann machine model accompanying programming model allow languages c fortran portable efficient parallel world propose candidate type architecture cta play role von neumann model 2 phase abstractions role programming model finally sequential languages replaced languages based phase abstractions orca c 32 34 cta cta 45 asynchronous mimd model consists p von neumann processors execute independently processor local memory processors communicate sparse otherwise unspecified communication network sparse means network constant degree connectivity network topology intentionally left unbound provide maximum generality finally model includes global controller communicate processors low bandwidth network logically controller provides synchronization low bandwidth communication broadcast single value although premature claim cta effective model von neumann model appear requisite characteristics simple makes minimal architectural assumptions captures enough significant features useful developing efficient algorithms example ctas unbound topology bias model towards particular machine topologies existing parallel computers typically significant performance hand distinction global local memory references key distinction clear cta model finally assumption sparse topology realistic existing medium large scale parallel computers phase abstractions extend cta way sequential imperative programming 2 recent bsp 48 logp 8 models present similar view parallel machine part suggest similar way programming parallel computers model extends von neumann model main components phase abstractions xyz levels programming ensembles 1 19 46 21 xyz programming levels programmers problemsolving abilities improved dividing problem small manageable piecesassuming pieces sufficiently independent considered separately additionally pieces often reused programs saving time future problems one way build parallel program smaller reusable pieces compose sequence independently implemented phases executing parallel algorithm contributes overall solution next conceptual level phase comprised set cooperating sequential processes implements desired parallel algorithm sequential process may developed separately levels problem solvingprogram phase process also called z x levelshave direct analogies cta x level corresponds individual von neumann processors cta x level program specifies sequential code executes one process model mimd process execute different code level analogous set von neumann processors cooperating compute parallel algorithm forming phase ylevel may specify xlevel programs connected interprocess communication examples phases include parallel implementations fft matrix multiplication matrix transposition sort global maximum phase characteristic communication structure induced data dependencies among processes example fft induces butterfly batchers sort induces hypercube 1 finally z level corresponds actions ctas global controller sequences parallel phases invoked synchronized z level program specifies high level logic computation sequential invocation phases although execution may overlap needed solve complex problems example carparrinello molecular dynamics code simulates behavior collection atoms iteratively invoking series phases perform ffts matrix products computations 49 zyx order three levels provide topdown view parallel program example xyz levels jacobi iteration figure 1 illustrates xyz levels programming jacobi iteration z level consists loop invokes two phases one called jacobi performs overrelaxation called max computes maximum difference iterations used test termination level phase collection processes executing concurrently two phases graphically depicted squares representing processes arcs representing communication processes program jacobi data load errortolerance error max z level x level level ij local section newijoldij1oldij1 localmaxmaxlocalmax leftchild localmaxmaxlocalmax rightchild send localmax parent figure 1 xyz illustration jacobi iteration jacobi phase uses mesh interconnection topology max phase uses binary tree details level distribution data shown figure explained next subsection finally sketch x level program two phases shown right figure 1 x level code jacobi phase assigns data point average four neighbors max phase finds data points largest difference current iteration previous iteration 2 z level program basically sequential program provides control flow overall computation x level program primitive form also viewed sequential program additional operations allow communicate processes although parallelism explicitly specified x z levels two levels may still contain parallelism example phase invocation may pipelined x level processes execute superscalar architectures achieve instructionlevel parallelism level specifies scalable parallelism clearly departs sequential program ensembles support definition manipulation parallelism 22 ensembles phase abstractions use ensemble structure describe data structures partitioning process placement process interconnection particular ensemble partitioning set elements data codes port connectionsinto disjoint sections section represents thread execution section unit concurrency degree parallelism modulated increasing decreasing number sections three aspects parallel computationdata code communicationare unified ensemble structure three components reconfigured scaled coherent concise fashion provide flexibility portability data ensemble data structure partitioning z level data ensemble provides logically global view data structure x level portion ensemble mapped section viewed locally defined structure local indexing example 6 theta 6 data ensemble figure 2 global view indices 0 5 theta 0 5 local view 3 theta 3 subarrays indices 2 mapping global view local view performed level described section 3 use local indexing schemes allows x level process refer generic array bounds rather global locations data space thus x level source code used multiple processes global view figure 2 6theta6 array left corresponding data ensemble 2theta2 array sections code ensemble collection procedures partitioning code ensemble gives global view processes performing parallel computation procedures ensemble differ model mimd procedures identical model spmd figure 3 shows code ensemble jacobi phase processes execute xjacobi function figure 3 illustration code ensemble finally port ensemble defines logical communication structure specifying collection port name pairs pair names represents logical communication channel two sections port names bound local port name used x level figure 4 depicts port ensemble jacobi phase example north port n one process bound south port neighboring process figure 4 illustration port ensemble level phase composed three components code ensemble port ensemble connects code ensembles processes data ensembles provide arguments processes code ensemble sections ensemble ordered numerically th section code ensemble bound th section data port ensemble correspondence allows section allocated processor normal sequential execution process executes processor data stored memory local processor ports define connections interprocessor communication consequently th sections ensembles assigned processor maintain locality across phases two phases share data ensemble require different partitionings best performance separate phase may used move data z level logically stores ensembles z level variables composes phases stores results phase invocation interface z x levels encourages modularity level code invoked different ensemble parameters way procedures reused sequential languages ensemble abstraction helps hide diversity parallel architectures however map well individual architectures abstraction must parameterized example number processors size problem parameterization illustrated next section 3 ensemble example jacobi provide better understanding ensembles phase abstractions complete description jacobi program adopt notation proposed orca c language 30 32 languages based phase abstractions possible see section 4 31 define rows 1 constants define shape define cols 2 logical processor array define twod 3 program jacobian shape processors switch shapef configuration computation case rows rows processors break case cols rows break case twod partition2drows cols processors break rows cols processors configuration parameter list data ensemble definitions level port ensemble definitions code ensemble definitions process definitions x level begin z level tolerance delta f newp prepare next iteration figure 5 overall phase abstraction program structure shown figure 5 phase abstractions program consists x z descriptions plus list configuration parameters used program adapt different execution environments case two runtime parameters accepted processors shape first parameter number processors second specifies shape processor array discussed later program uses 2d data decomposition setting shape rows cols choose horizontal strips vertical strips decomposition function partition2d computes values rows cols rows processors difference rows cols minimized configuration computation program use different load time parameters adapt different numbers processors assume three different data decompositions configuration computation executed load time 32 z level jacobi program configured z level program executed initializes program variables reads input data iteratively invokes jacobi max phases convergence reached point output phase invoked data processing communication components jacobi max phases specified defining composing code data port ensembles described 33 level data ensembles program uses two arrays store floating point values point 2d grid parallelism achieved partitioning arrays contiguous 2d blocks partition blockrc float prowscols float newprowscols declaration states p newp dimensions rows cols partitioned onto r c section array process array keyword partition identifies p newp ensemble arrays block names partitioning reused define ensembles partitioning corresponds one figure 2 rows6 cols6 2 ensemble declaration belongs data ensembles metacode figure 5 section 5 shows alternate decomposition declared values r c assumed specified programs configuration parameter list section implicitly defined size r c r divide rows evenly sections r e others r consequently x level processes contain assumptions data decomposition except dimension subarrays processes scale number logical processors problem size 34 jacobi phase ensemble jacobi phase computes point average four nearest neighbors implying section communicate four nearest neighbor sections see figure 4 following level ensemble declaration defines appropriate port ensemble jacobiportnames n e w north east west south first line declares phases port names following bindings specified second third lines define mesh connectivity level port names port ensemble declaration specify connections ports lie boundaries case unbound ports bound derivative functions compute boundary conditions using data local section following binds derivative functions ports edges jacobi jacobi0i portn receive rowzero 0 receive colzero 0 jacobii0 portw receive colzero 0 jacobir1i ports receive rowzero 0 rowzero colzero defined double rowzero static double row1t default initialized 0s return row double colzero static double col01s default initialized 0s return col0 values determined process x level functionin case xjacobi absence derivative functions x level programs could check existence neighbors tests complicate source code increases chance introducing errors section 5 shows even modestly complicated boundary conditions lead proliferation special case code code ensemble define code ensemble jacobi r c sections assigned instance xjacobi code jacobiijcode xjacobi 0 jacobi contains heterogeneity boundaries program handled derivative functions functions general however restriction functions argument types return type must match phase invocation level x level code jacobi shown figure 6 first sends edge values four neighbors receives boundary values neighbors finally uses four point stencil compute average interior point several features x level code noteworthy ffl parametersthe arguments x level code establish correspondence local variables sections ensembles case local value array bound block ensemble values ffl communicationcommunication specified using transmit operator port name left specifies send righthand side port right indicates receive variable lefthand side semantics receive operations block sends ffl uniformitybecause derivative functions used xjacobi function contains tests boundary conditions sending receiving neighbor values ffl border valuesthe values used define bounds value array parameters derived section size data ensemble hold data neighboring sections value declared one element wider side incoming array argument extra storage explicitly specified difference local declaration x0s10t1 formal declaration x1s1t upper bounds array declarations inclusive ffl array slicesslices provide concise way refer entire row general ddimensional block data slices used conjunction transmit operator entire block sent single message thus reducing communication overhead complete phase summarize data ensemble port ensemble code ensemble collectively define jacobi phase upon execution sections declared configuration parameters logically connected nearestneighbor mesh section data manipulated one xjacobi process end result parallel algorithm computes one jacobi iteration 35 max phase max phase finds maximum change grid points uses data ensemble jacobi phase port ensemble shown graphically figure 8 defined maxportnames p l r parent left right xjacobivalue1s1t newvalue1s1t double value0s10t1 extra storage four sides double newvalue0s10t1 port north east west south double newvalue0s10t1 int j send neighbor values north value 11t 1t array slice east value1st west value1s1 south values1t receive neighbor values i1 j1 i1 j1 figure level code jacobi phase derivative functions phase bound receive leaf sections left right port return value computed smallest value function send roots unbound parent port noop maxiportl receive smallestvalue rc2 processors maxiportr receive smallestvalue rc2 processors maxiportp send noop smallest value derivative function simply returns smallest value represented architecture code ensemble phase similar jacobi phase except xmax replaces xjacobi see figure 7 xmaxvalue1s1t newvalue1s1t double value1s1t double newvalue1s1t port parent left right int j double localmax double temp compute local maximum i1 j1 jt j compute global maximum temp left receive temp right receive parent localmax send figure 7 x level code max phase applications complicated jacobi benefit using ensembles increases cost amortized larger program cost using ensembles also decrease libraries figure 8 illustration tree port ensemble ensembles phases derivative functions x level codes built example max phase jacobi common many computations would normally defined programmer 4 high level programming phase abstractions phase abstractions programming language rather foundation development parallel programming languages support creation efficient scalable portable programs orca c used previous section literal textual instantiation phase abstractions clearly shows power phase abstractions may find lowlevel tedious fact departure literal orca c language required achieve elegant programming style adopting certain conventions possible build reusable abstractions directly top orca c staying within orca c framework solution advantage different sublanguages used together single large problem requires diverse abstractions good performance example consider design apllike array sublanguage orca c 3 recall x level procedure receives two kinds parametersglobal data passed arguments port connectionsthat support two basic activities computations data communication however possible constrain x level functions perform one two tasksa local computation communication operation could separate computation phases communication phases example x level computation functions adding integers computing minimum values sorting elements x level communication functions shifting data cyclically ring broadcasting data communicating tree structure reductions naturally combine communication computation notable exceptions separation 3 since submission paper array sublanguage known zpl developed support data parallel computations 35 47 31 37 syntax differs significantly orca c zpl remains true phase abstractions model provides powerful z level language hides x level details user communication computation desirable operations suffices define communicationoriented phase takes additional function parameter combining results communications illustrate reconsider jacobi example rather specify entire jacobi iteration one x level process communication operation constitutes separate phase results combined z level add divide phases convergence test computed z level subtracting old array new one performing maximum reduction differences program skeleton figure 9 illustrates method providing examples x level functions referred operator syntactic style c shift reduce z level code shows data ensembles declared phase structures add leftshift reduce initialized divide subtract phases analogous add shift functions analogous leftshift three consequences approach first interface phase substantially simplified second problems harder describe possible combine computation communication within single x level function finally x level functions phases comprise smaller likely perform one task increasing composability reusability although array sublanguage defined similar apl salient differences significantly orca c functions operate subarrays rather individual elements means fast sequential algorithms applied subarrays solution achieves conciseness reusability apl sacrifice control data decompositions lose ability use separate global local algorithms solution also advantage embedding array language orca c allowing programming styles used needed power phase abstractions comes decomposition parallel programs x z levels encoding key architectural properties simple parameters concept ensembles allows data port code decompositions specified reused individual components three types ensembles work together allow problem machine size scaled addition derivative functions allow single x level program used multiple processes even presence boundary conditions section discusses phase abstractions respect performance expressiveness portability scalability programs moved one platform another must adapt characteristics host machine obtain good performance adaptation automatic requires minor effort portability achieved phase abstractions support portability scalability encoding key architectural characteristics ensemble parameters separating phase definitions several independent components xproc type1s1t operatortype x1s1t type y1s1t type result1s1t int j i1 j1 jt return result xproc void shifttype val1s1t port writeneighbor int i2 xproc int reducetype val1k typeop port parent int i2 ik i1 parent accum begin z double x1j1k oldx1j1k phase operator phase left phase reduce figure 9 jacobi written array style using orca c changes either problem size number processors encapsulated data ensemble declaration section 3 relate size section overall problem size rows cols number sections r c follows problem size scales changing values rows cols machine size scales changing values r c granularity parallelism controlled altering either number processors number sections ensemble declaration flexibility important aspect portability different architectures favor different granularities desirable write programs without making assumptions underlying machine knowledge machine details often used optimize program performance therefore tuning may sometimes necessary example may beneficial logical communication graph match machines communication structure consider embedding binary tree max phase onto mesh architecture logical edges must span multiple physical links edge dilation eliminated connectivity allows comparisons along row processors along single column see figure 10 figure 10 rows columns compute global maximum address edge dilation problem fixed binary tree presented section 3 replaced new port ensemble uses tree variable degree solution shown figure 11 child ports represented array ports new program use either binary tree rows columns approach port ensemble declaration latter approach shown rows columns communication structure code suitably parameterized program execute efficiently variety architectures selecting proper port ensemble xmaxvalue1s1t new1s1t numchildren double value1s1t double newvalue1s1t port parent childnumchildren int j double localmax double temp compute local maximum i1 j1 compute global maximum i0 inumchildren temp childi receive parent localmax send figure parameterized x level code max phase locality best data partitioning depends factors problem machine size hard wares communication computation characteristics applications communication patterns phase abstractions model changes data partitioning encapsulated data ensembles example define 2d block partitioning p processors configuration code define number sections p 1d strip partitioning desired number sections simply defined p strip decomposition requires process eastwest neighbors instead four neighbors used block decomposition using port ensembles bind derivative functions unused portsin case north south portsthe program easily accommodate change number neighbors source level changes required explicit dichotomy local nonlocal access encourages use different algorithms locally globally batchers sort example benefits approach see section 1 contrasts approaches programmer compiler identifies much finegrained parallelism possible compiler aggregates finegrained parallelism granularity appropriate target machine boundary conditions typically processes edge problem space must treated separately 4 jacobi iteration example receive east port must conditionally executed processes east edge eastern neighbors isolated occurrences conditionals pose little problem realistic applications lead convoluted code example simple nine different casesdepending portions boundaries contained within processand conditionals lead code dominated treatment exceptional cases 18 41 example suppose program block decomposition assumes conditional expression process either northeast east southeast section shown northeast special case 1 else east special case 2 else southeast special case 3 problem arises programmer decides vertical strips decomposition would efficient 4 although discuss problem context message passing language shared memory programs must also deal special cases code assumes exactly one three boundary conditions holds vertical strips decomposition one section eastern edge three conditions apply one therefore change data decomposition forces programmer rewrite boundary condition code model attempts insulate port code ensembles changes data decomposition processes send receive data ports cases involve interprocess communication cases invoke derivative functions handling boundary conditions thus decoupled x level source code instead cluttering process code special cases due boundary conditions handled problem level naturally belong reusability characteristics provide flexibility phase abstractions also encourage reusability example carparrinello molecular dynamics program 49 consists several phases one computed using modified gramschmidt mgs method solving qr factorization empirical results shown mgs method performs best 2d data decomposition 36 however phases carparrinello computation require 1d decomposition case 1d decomposition mgs yields best performance since avoids data movement phases illustrates reusable component effective flexible enough accommodate variety execution environments irregular problems paper described statically defined arraybased ensembles however imply phase abstractions ill suited dynamic unstructured problems fact extent lpar 28 set language extensions irregular scientific computations see section 7 described terms phase abstractions key point ensemble set partitioning support dynamic irregular computations envision dynamic irregular partitionings managed runtime consider first statically defined irregular problem finite element analysis programmer begins defining logical data ensemble replaced physical ensemble runtime logical definition includes proper record formats array port names actual data decomposition actual port ensemble runtime phase run determines partitioning creates data port ensembles size contents data ensemble defined interconnection structure determined sections mapped physical processors assume code ensemble spmd since obviates need assign different codes different processes dynamically partitioning phase completed ensembles behave statically defined phases dynamic computations could generalized idea example load balancing phase could move data sections also create revised data port ensembles represent new partitioning technical difficulties remain dynamic ensembles supported concepts change limits nonshared memory model nonshared memory model encourages good locality reference exposing data movement programmer performance advantage model small applications inherently poor locality example direct methods performing sparse factorization poor locality reference sparse irregular nature input data certain solutions problem shared memory model performs better single address space leads better load balance use work queue model 38 shared memory model also provides notational convenience especially pointerbased structures involved 6 portability results experimental evidence suggests phase abstractions provide portability across diverse set mimd computers 32 33 section summarizes results one program simple similar results achieved qr factorization matrix multiplication 30 briefly describe simple machines program run manner portable program implemented significant results simple large computational fluid dynamics benchmark whose importance high performance computing comes substantial body literature already devoted study introduced 1977 sequential benchmark evaluate new computers fortran compilers 7 since creation studied widely sequential parallel forms 3 9 13 16 17 23 24 40 42 hardware portability parallel simple program investigated ipsc2 ipsc2 f ncube7 sequent symmetry bbn butterfly gp1000 transputer simulator machines summarized table 1 two intel machines differ ipsc2 slower intel 80387 floating point coprocessor faster ipsc sx floating point accelerator simulator detailed transputerbased nonshared memory machine using detailed information arithmetic logical communication operators t800 24 simulator executes program written phase abstraction language estimates program execution time implementation simple program written orca c since compiler exists language based phase abstractions program handcompiled straightforward fashion c code uses message passing substrate support phase abstractions resulting c code machine machine sequent intel intel ncube bbn transputer model symmetry ipsc2 ipsc2 f ncube7 butterfly gp1000 simulator nodes 20 processors intel 80386 intel 80386 intel 80386 custom motorola 68020 t800 memory 32mb 4 mbnode 8 mbnode 512 kbnode 4 mbnode na cache 64kb 64 kb 64kb none none network bus hypercube hypercube hypercube omega mesh table 1 machine characteristics independent except process creation dependent operating systems method spawning processes number 1680 points transputer 1680 points intel ipsc2 1680 points butterfly 1680 points ncube7 1680 points symmetry number 28 pingalirogers linsnyder hiromoto et al figure 12 simple speedup various machines b simple 4096 points figure 12a shows similar speedups achieved machines many hardware characteristics affect speedup explain differences among curves discussion concentrate communication costs relative computational speed feature best distinguishes machines example ipsc2 f ncube7 identical interconnection topologies ratio computation speed communication speed greater ipsc2 11 12 effect reducing speedup decreases percentage time spent computing increases fraction time spent noncomputation overhead similarly since message passing latency lowest sequents shared bus sequent shows best speedup claim assumes little bus contention valid assumption considering modest bandwidth required simple figure 12b shows simple results hiromoto et al denelcor hep using 4096 data points 23 indicate portable program roughly competitive machinespecific code many differences resultsincluding different problem sizes different architectures possibly even different problem specificationsmake difficult draw stronger conclusions another reference point figure 12b compares results ipsc2 pingali rogers parallelizing compiler language 42 experiments run ipsc2s 4mb memory 80387 floating point units parameters appear identical largest potential difference lies performance sequential programs speedups computed although results encouraging proponents functional languages point results make use sophisticated compiler type compiler technology developed pingali rogers likely improve performance programs well even though machines differ substantiallyfor example memory structurethe speedups fall roughly within range moreover version simple compares favorably machinespecific implementations results suggest portability achieved application running machines 7 related work many systems support global view parallel computation spmd execution data decompositions similar various aspects phase abstractions none however provide support x level algorithm different zlevel parallel algorithm provide general support handling boundary conditions controlling granularity section discusses systems address scalability portability aggregate data parallel programming style dataparallel c dataparallel c 21 dpc portable sharedmemory simdstyle language similarities c unlike phase abstractions dpc supports pointwise parallelism dpc pointwise processor poly variables distributed across processors machine unlike predecessor c 43 dpc supports data decompositions data improve performance coarsegrained architectures however dpc supports pointwise communication compiler runtime system must detect several point sends processor destined processor bundle also maintain performance simd model mimd machine extra compiler analysis required detect perinstruction simd synchronizations necessary removed pointwise process identical edge effects must coded conditionals determine processes edge computation hard reuse code boundary conditions may change problem problem constant variable boundary conditions supported expanding data space leaving processes idle dino dino 44 clike spmd language like c constructs distributed data structures replicating structures processors executing single procedure element data set dino provides shared address space remote communication specified annotating accesses nonlocal objects symbol default semantics true messagepassing parallel invocations procedure synchronize exit procedure dino allows mapping data processes specified programmerdefined functions ensure fast reads shared data partitioning map individual variable multiple processors writes variables broadcast copies dino handles edge effects fashion c dino supports pointwise communication compiler runtime system must combine messages mehrotra rosendale system described mehrotra rosendale 39 much like dino supports small set data distributions however system provides way control precisely determine points local possible control communication costs algorithm choice based locality hand system require explicit marking external memory references dino instead system infers possible references global algorithms processes dynamically choose neighbors simplifies programming also programs portable written dino communication structure processor visible programmer programmer change partitioning clauses data aggregates spmd processing allowed special facilities handling edge effects fortran dialects recent languages kali 26 vienna fortran 6 hpf 22 focus data decomposition expression parallelism data decompositions similar phase abstractions notion data ensembles overall approach fundamentally different phase abstractions require effort programmer approach relies compiler technology exploit loop level parallelism compilerbased approach guarantee deterministic sequential semantics less potential parallelism since may cases compilers cannot transform sequential algorithm optimal parallel one kali vienna fortran hpf depart sequential languages primarily support data though languages provide mechanisms specifying parallel loops vienna fortran provides form parallel loops forall statement hpf kali specifies loop loop carried dependencies ensure deterministic semantics updates common variables different loop iterations values deterministically merged end loop construct optional hpf compiler may attempt extract parallelism even forall used hpf vienna fortran allow arrays aligned respect abstract partitioning powerful constructs example arrays dynamically remapped procedures define data distribution together features potentially expensive although programmer helps specifying data distribution various points program compiler must determine move data addition data distribution directives kali allows programmer control assignment loop iterations processors use clause help maintaining locality lpar lpar portable language extension supports structured irregular scientific parallel computations 28 27 particular lpar provides mechanisms describing nonrectangular distributed partitions data space manage loadbalancing locality partitions created union intersection set difference arrays support irregular decompositions high cost lpar syntactically distinguishes irregular decompositions faster runtime support used regular decompositions 5 computations invoked group arrays foreach operator executes body parallel array yield coarsegrained parallelism lpar uses overlapping indices distributed subarrays support sharing data elements overlapping domains provide elegant way describing multilevel mesh algorithms computations boundary conditions operator redistributing data elements lpar depends routine written base language compute new decomposition phase abstractions potential support dynamic irregular decompositions discussed section 5 multigrid decompositions sublanguage supporting scaled partitionings communication scaled ensembles would useful phase abstractions support loose synchrony naturally supports use refined grids conjunction base grid splitc splitc sharedmemory spmd language memory reference operations support latencyhiding 10 splitc procedures concurrently applied ownercomputes fashion partitions aggregate data structure array pointerbased graph process reads data global pointer splitc data type hide latency splitc supports asynchronous readakin unsafe multilisp future 20that initiates read global pointer wait data arrive process invoke sync operation block outstanding reads 5 scott baden personal communication complete similar operation global writes operations hide latency providing global namespace reducing copying data message queues copying may necessary bulk communication noncontiguous data column array however operations lead complex programming errors misplaced reference synchronization operation lead incorrect output immediate failure array distribution splitc straightforward somewhat limited number higher order dimensions cyclically distributed remaining dimensions distributed blocks load balance locality irregular decompositions may difficult achieve applications array distribution declarations tied procedures array parameter declarations limit reusability portability declarations code depends must modified distribution changes coupling also incur performance penalty benefit optimal array distribution one procedure invocation may offset cost redistributing array calculations use array splitc provides special support boundary conditions usual trick creating enlarged array possible otherwise irregularities must handled conditional code body spmd procedures 8 conclusion parallelism offers promise great performance thus far hampered lack portability scalability programming convenience unacceptably increase time cost developing efficient programs support required quickly programminga solution easily moving new machines old ones become obsolete rather defining new parallel programming paradigm phase abstractions model supports wellknown techniques achieving highperformancecomputing sequentially local aggregates data elements communicating large groups data unitby allowing programmer partition data across parallel machines scalable manner furthermore separating program reusable partsx level level zlevel ensemble declarations boundary conditionsthe creation subsequent programs significantly simplified approach provides machineindependent lowlevel control parallelism allows programmers write spmd manner without sacrificing efficiency mimd processing message passing often praised efficiency condemned difficult use contribution phase abstractions language model focuses efficiency reducing difficulty nonshared memory programming programmability model exemplified straightforward solution problems simple well ability define specialized highlevel array sublanguages phase abstractions model designed structurally similar mimd architectures performs well variety mimd processors claim supported tests machines intel ipsc sequent symmetry bbn butterfly r flexible communication abstraction nonshared memory parallel computing program structuring effective parallel portability simulator mimd performance prediction application s1 mkiia multiprocessor nesl nested dataparallel language linda context vienna fortran fortran language extension distributed memory multiprocessors simple code logp towards realistic model parallel computation parallel programming splitc hypercube performance performance intel ipsc860 ncube 6400 hypercubes supporting machine independent programmingon diverse parallel architectures report sisal language project simple chip restructuring simple chip architecture simple exercise programming poker scalable abstractions parallel programming language concurrent symbolic computation high performance fortran forum experiences denelcor hep processor element architecture nonshared memory parallel computers ipsc2 users guide compiling global namespace parallel loops distributed execution lattice parallelism parallel programming model nonuniform implementation lpar parallel programming model scientific computations portability parallel programs across mimd computers zpl language reference manual portable implementation simple portable parallel programming cross machine comparisons simple data ensembles orca c zpl array sublanguage accommodating polymorphic data decompositions explicitly parallel programs simple performance results zpl towards machineindependent solution sparse cholesky factorization compiling high level constructs distributed memory architectures analysis simple code dataflow computation experiences poker compiler parallelization simple distributed memory machine dino parallel programming language type architecture xyz abstraction levels pokerlike languages zpl programming guide bridging model parallel computation parallel implementation carparrinello method tr ctr marios dikaiakos daphne manoussaki calvin lin diana e woodward portable parallel implementation two novel mathematical biology algorithms zpl proceedings 9th international conference supercomputing p365374 july 0307 1995 barcelona spain bradford l chamberlain sungeun choi e christopher lewis calvin lin lawrence snyder w derrick weathersby zpl machine independent programming language parallel computers ieee transactions software engineering v26 n3 p197211 march 2000