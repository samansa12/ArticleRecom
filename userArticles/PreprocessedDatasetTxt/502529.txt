mining timechanging data streams statistical machinelearning algorithms assume data random sample drawn stationary distribution unfortunately large databases available mining today violate assumption gathered months years underlying processes generating changed time sometimes radically although number algorithms proposed learning timechanging concepts generally scale well large databases paper propose efficient algorithm mining decision trees continuouslychanging data streams based ultrafast vfdt decision tree learner algorithm called cvfdt stays current making old data growing alternative subtree whenever old one becomes questionable replacing old new new becomes accurate cvfdt learns model similar accuracy one would learned reapplying vfdt moving window examples every time new example arrives o1 complexity per example opposed ow w size window experiments set large timechanging data streams demonstrate utility approach b introduction modern organizations produce data unprecedented rates among large retailers ecommerce sites telecommunications providers scientic projects rates gigabytes per day common data contain valuable knowledge volume increasingly outpaces practitioners ability mine result common practice either mine subsample available data mine models drastically simpler data could support cases volume time span accumulated data storing consistently reliably future use challenge even storage problematic often dicult gather data one place one time format appropriate mining reasons many areas notion mining xedsized database giving way notion mining openended data stream arrives goal research help make possible minimum eort data mining practitioner previous paper 9 presented vfdt decision tree induction system capable learning highspeed data streams incremental anytime fashion producing models asymptotically arbitrarily close would learned traditional decision tree induction systems statistical machinelearning algorithms including vfdt make assumption training data random sample drawn stationary distribution un fortunately large databases data streams available mining today violate assumption exist months years underlying processes generating changes time sometimes radically example new product promotion hackers attack holiday changing weather conditions changing economic conditions poorly calibrated sensor could lead violations assumption classication systems attempt learn discrete function given examples inputs outputs problem takes form changes target function time known concept drift traditional systems assume data generated single concept many cases however accurate assume data generated series concepts concept function timevarying parameters traditional systems learn incorrect models erroneously assume underlying concept stationary fact drifting one common approach learning timechanging data repeatedly apply traditional learner sliding window w examples new examples arrive inserted beginning window corresponding number examples removed end win dow learner reapplied 27 long w small relative rate concept drift procedure assures availability model ecting current concept generating data window small however may result insucient examples satisfactorily learn concept computational cost reapplying learner may prohibitively high especially examples arrive rapid rate concept changes quickly meet challenges propose cvfdt system capable learning decision trees highspeed time changing data streams cvfdt works eciently keeping decision tree uptodate window exam ples particular able keep model consistent window using constant amount time new example precisely time proportional number attributes data depth induced tree cvfdt grows alternate subtree whenever old one seems outofdate replaces old one new one becomes accurate allows make smooth negrained adjustments concept drift occurs eect cvfdt able learn nearly equivalent model one vfdt would learn repeatedly reapplied window examples o1 time instead ow time per new example next section discuss basics vfdt sys tem following section introduce cvfdt system present series experiments synthetic data demonstrate cvfdt outperform traditional systems highspeed timechanging data streams next apply cvfdt mining stream web page requests entire university washington campus conclude discussion related future work 2 vfdt system classication problem generally dened follows set n training examples form x given discrete class label x vector tributes may symbolic numeric goal produce examples model predict classes future examples x high accuracy example x could description clients recent purchases decision send customer catalog x could record cellular telephone call decision whether fraudulent one eective widelyused classi cation methods decision tree learning 4 20 learners type induce models form decision trees node contains test attribute branch node corresponds possible outcome test leaf contains class prediction label example x obtained passing example root leaf testing appropriate attribute node following branch corresponding attributes value example decision tree learned recursively replacing leaves test nodes starting root attribute test node chosen comparing available attributes choosing best one according heuristic measure classic decision tree learners like c45 20 cart sliq 17 sprint 24 table 1 vfdt algorithm inputs stream examples x set symbolic attributes g split evaluation function one minus desired probability choosing correct attribute given node usersupplied tie threshold nmin examples checks growth output ht decision tree procedure vfdt xg let ht tree single leaf l 1 root g g obtained predicting frequent class class yk value x ij attribute x example x sort x leaf l using ht x ij x x increment n ijy l label l majority class among examples seen far l let n l number examples seen l examples seen far l class n l mod nmin 0 compute using counts n ijk l let xa attribute highest g l let x b attribute secondhighest g l compute using equation 1 g l g l replace l internal node splits xa branch split add new leaf l let g obtained predicting frequent class l class yk value x ij attribute return ht use every available training example select best attribute split policy necessary data scarce two problems training examples abundant requires examples available consideration throughout entire runs problematic data ram disk assumes process generating examples remains entire period examples collected mined previous work 9 presented vfdt fast decision tree learner system able learn abundant data within practical time memory constrai nts accomplishes noting catlett 5 others 12 19 may sucient use small sample available examples choosing split attribute given node thus rst examples arrive data stream need used choose split attribute root subsequent ones passed induced portion tree reach leaf used choose split attribute recursively determine number examples needed decision vfdt uses statistical result known hoeding bounds additive cherno bounds 13 n independent observations realvalued random variable r range r hoeding bound ensures condence 1 true mean r least r r observed mean samples r 2n 1 true irrespective probability distribution generated observations let gx heuristic measure used choose test attributes use information gain seeing n samples leaf let xa attribute best heuristic measure x b attribute second best let new random variable dierence observed heuristic val ues applying hoeding bound g see g calculated equation 1 usersupplied condently say dierence gxa gx b larger zero select xa split tribute 12 table 1 contains pseudocode vfdts core al gorithm counts n ijk sucient statistics needed compute heuristic measures quantities required similarly maintained sucient statistics available memory vfdt reduces memory requirements temporarily deactivating learning least promising nodes nodes reactivated later begin look promising currently active nodes vfdt employs tie mechanism precludes spending inordinate time deciding 1 valid long g therefore g viewed average examples seen leaf case commonlyused heuristics example information gain used quantity averaged reduction uncertainty regarding class membership example 2 paper assume thirdbest lower attributes suciently smaller gains probability true best choice negligible plan lift assumption future work attributes given node pessimistically assumed independent simply involves bonferroni correction 18 attributes whose practical dierence negligible vfdt declares tie selects xa split attribute time g usersupplied tie thresh old prepruning carried considering node null attribute x consists splitting node thus split made condence 1 best split found better according g splitting notice tests splits ties executed every nmin user supplied value examples arrive leaf justied observation vfdt unlikely make decision given exam ple wasteful carry calculations one pseudocode shown symbolic attributes currently developing extension numeric ones sequence examples may innite case procedure never terminates point time parallel procedure use current tree ht make class predictions using otheshelf hardware vfdt able learn fast data read disk time incorporate example oldvc l maximum depth ht number attributes v maximum number values per attribute c number classes time independent total number examples already seen assuming size tree depends true concept dataset use hoeding bounds speed gains necessarily lead loss accuracy shown high condence core vfdt system without ties de activations due memory constraints asymptotically induce tree arbitrarily close tree induced traditional batch learner let dt1 tree induced version vfdt using innite data choose nodes split attribute ht tree learned core vfdt system given innite data stream p probability example passed dt1 level fall leaf point probability arbitrary example take dierent path dt1 ht bounded p 9 corollary result states tree learned core vfdt system nite sequence examples correspond subtree dt1 bound p see domingos hulten 9 details vfdt p bound 3 cvfdt system cvfdt conceptadapting fast decision tree learner extension vfdt maintains vfdts speed accuracy advantages adds ability detect respond changes examplegenerating process like systems capability cvfdt works keeping model consistent sliding window ex amples however need learn new model scratch every time new example arrives instead updates sucient statistics nodes incrementing counts corresponding new example decrementing counts corresponding oldest example window needs forgotten statistically eect underlying concept sta tionary concept changing however splits previously passed hoeding test longer alternative attribute higher gain two close tell case cvfdt begins grow alternative subtree new best attribute table 2 cvfdt algorithm inputs sequence examples x set symbolic attributes g split evaluation function one minus desired probability choosing correct attribute given node usersupplied tie threshold w size window nmin examples checks growth f examples checks drift output ht decision tree procedure cvfdtsx g w nmin initialize let ht tree single leaf l 1 root let alt l 1 initially empty set alternate trees l 1 g obtained predicting frequent class g let w window examples initially empty class yk value x ij attribute x process examples example x sort x set leaves l using ht trees alt node x passes let id maximum id leaves l add x id beginning w last element w removed g x nmin f examples since last checking alternate trees checksplitvalidityht n return ht table 3 cvfdtgrow procedure procedure cvfdtgrowht n g x nmin sort x leaf l using ht let p set nodes traversed sort node l pi p x ij x x increment n ijy l p tree ta alt l p g x nmin label l majority class among examples seen far l let n l number examples seen l examples seen far l class n l mod nmin 0 compute using counts n ijk l let xa attribute highest g l let x b attribute secondhighest g l compute using equation 1 g l g l replace l internal node splits xa branch split add new leaf l let let alt l fg g obtained predicting frequent class l class yk value x ij attribute root alternate subtree becomes accurate new data old one old subtree replaced new one table contains pseudocode outline cvfdt algorithm cvfdt initializations processes examples stream indenitely example arrives added window 3 old example forgotten needed x incorporated current model cvfdt periodically scans ht alternate trees looking internal nodes whose sucient statistics indicate new attribute would make better test chosen split attribute alternate subtree started node table 3 contains pseudocode treegrowing portion cvfdt system similar hoeding tree algorithm cvfdt monitors validity old decisions maintaining sucient statistics every node ht instead leaves like vfdt forgetting old example slightly complicated fact ht may grown changed since example initially incorporated therefore nodes assigned unique monotonically increasing id created example added w maximum id leaves reaches ht alternate trees recorded examples eects forgotten decrementing counts sucient statistics every node example reaches 3 window stored ram resources available otherwise kept disk table 4 forgetexample procedure procedure traverses leaves id idw let p set nodes traversed sort node l p x ij x x decrement n ijk l tree alt alt l ht whose id stored id see pseudocode table 4 detail cvfdt forgets examples cvfdt periodically scans internal nodes ht looking ones chosen split attribute would longer selected gxa gx b nds node cvfdt knows either initially made mistake splitting xa happen less time something process generating examples changed either case cvfdt need take action correct ht cvfdt grows alternate subtrees changed subtrees ht modies ht alternate accurate original see needed let l node change detected simple solution replace l leaf predicting common class l sufcient statistics policy assures ht always current possible respect process generating examples however may drastic initially forces single leaf job previously done whole subtree even subtree outdated may still better best single leaf particularly true l near root ht result drastic shortterm reductions ht predictive accuracy clearly acceptable parallel process using ht make critical decisions internal node ht list alternate subtrees considered replacements subtree rooted node table 5 contains pseudocode checksplit procedure checksplitvalidity starts alternate subtree whenever nds new winning attribute node new best attribute g g 2 similar procedure used choose initial splits except tie criteria tighter avoid excessive alternate tree creation cvfdt supports parameter limits total number alternate trees grown one time alternate trees grown way ht via recursive calls cvfdt pro cedures periodically node nonempty set alternate subtrees l test enters testing mode determine replaced one alternate subtrees mode l test collects next training examples arrive instead using grow children alternate trees uses compare accuracy subtree roots accuracies alternate subtrees accurate alternate subtree accurate l test l test replaced alternate test phase cvfdt also prunes alternate subtrees making progress ie whose accuracy table 5 checksplitvalidity procedure procedure checksplitvalidityht n node l ht leaf tree alt alt l let xa split attribute l let xn attribute highest g l xa let x b attribute highest g l xn g l 0 tree alt l already splits root compute using equation 1 g l g l 2 let l new internal node splits xn let alt branch split add new leaf l l new let alt l fg g obtained predicting frequent class l class yk value x ij attribute creasing time alternate subtree l test l alt cvfdt remembers smallest accuracy dierence ever achieved two min l test l alt cvfdt prunes alternate whose current test phase accuracy dierence least min l test l one window size w appropriate every concept every type drift may benecial dynamically change w run example may make sense shrink w many nodes ht become questionable response rapid change data rate events could indicate sudden concept change similarly applications may benet increase w questionable nodes may indicate concept stable good time learn detailed model cvfdt able dynamically adjust size window response usersupplied events events specied form hook functions monitor ht call setwindowsize function appropriate cvfdt changes window size updating w immediately forgetting examples longer w discuss properties cvfdt system brie compare vfdtwindow learner reapplies vfdt w every new example cvfdt requires memory proportional ondvc n number nodes cvfdts main tree alternate trees number attributes v maximum number values per attribute c number classes window examples ram stored 4 ram short cvfdt aggressive pruning unpromising alternate subtrees disk cost disk accesses per example fore cvfdts memory requirements dominated sucient statistics independent total number examples seen point run cvfdt available model ects current concept generating w able keep model uptodate time proportional olcdvc per example l c length longest path example take ht times number alternate trees vfdt window requires olvdvcw time keep model upto date every new example l v maximum depth ht vfdt factor wlv lc worse cvfdt em pirically observed l c smaller l v experiments despite large time dierence cvfdts drift mechanisms allow produce model similar ac curacy structure models induced two may however signicantly dierent following reason vfdtwindow uses information training example one place tree induces leaf example falls arrives means vfdt window uses rst examples w make decision root next make decision rst level tree initial building phase cvfdt fully induced tree available every new example passed induced tree information contains used update statistics every node passes dierence advantage cvfdt allows induction larger trees better probability estimates leaves also disadvantage vfdtwindow may accurate large concept shift partway w vfdtwindows leaf probabilities set examples near end w cvfdts ect w also notice even structure induced tree change cvfdt vfdtwindow outperform vfdt simply leaf probabilities therefore class predictions updated faster without dead weight examples fell leaves current window 4 empirical study conducted series experiments comparing cvfdt vfdt vfdtwindow goals evaluate cvfdts ability scale evaluate cvfdts ability deal varying levels drift identify characterize situations cvfdt outperforms systems 41 synthetic data experiments synthetic data used changing concept based rotating hyperplane hyperplane ddimensional space set points x satisfy x ith coordinate x examples labeled positive examples labeled negative hyperplanes useful simulating timechanging concepts change orientation position hyperplane smooth manner changing relative size weights particular sorting weights magnitudes provides good indication dimensions contain information limit one weights zero dimension associated nonzero weight one contains information concept allows us control relative information content attributes thus change optimal order tests decision tree representing hyperplane simply changing relative sizes weights sought concept maintained advantages hy perplane weights could randomly modied without potentially causing decision frontier move outside range data meet goals used series alternating class bands separated parallel hyper planes start reference hyperplane whose weights initialized 2 except w0 25d label example substitute coordinates left hand side equation 2 obtain sum jsj 1 w0 example labeled positive otherwise jsj 2 w0 example labeled negative examples generated uniformly ddimensional unit hypercube value x ranging 0 1 labeled using concept continuous attributes uniformly discretized bins noise added randomly switching class labels p exam ples unless otherwise stated experiment used following settings million training examples window disk memory limits prepruning test set 50000 examples alternate tree test mode 9000 examples used test samples 1000 examples runs done 1ghz pentium iii machine 512 mb ram running linux rst series experiments compares ability cvfdt vfdt deal large conceptdrifting data sets concept drift added datasets following manner every 50000 examples w1 modied adding 001d test set relabeled updated concept p noise initially 1 multiplied 1 5 drift points also w1 fell 0 rose 25d figure 1 compares accuracy algorithms function dimensionality space reported values obtained testing accuracy learned models every 10000 examples throughout run averaging results drift level reported minor axis average percentage test set changes label point concept changes cvfdt substantially accurate vfdt approximately 10 average cvfdts performance improves slightly increasing figure 2 compares average size models induced run shown figure 1 reported values generated averaging every 10000 examples cvfdts trees substantially smaller vfdts advantage consistent across values tried simultaneous accuracy size advantage derives fact cvfdts tree built 100000 relevant examples vfdts built millions outdated examples next carried detailed evaluation cvfdts concept drift mechanism figure 3 shows detailed view one runs figures 1 2 one 50 minor axis shows portion test figure 1 error rates function number attributes figure 2 tree sizes function number attributes figure 3 error rates learners function number examples seen set labeled negative test point computed noise added test set included illustrate concept drift present dataset cvfdt able quickly respond drift vfdts error rate often rises drastically reacting change vfdts error rate seems peak worse values run goes cvfdts error peaks seem constant height believe happens vfdt trouble responding drift induced larger tree must replicate corrections across outdated struc ture cvfdt face problem replaces subtrees become outdated gathered detailed statistics run cvfdt took 43 times longer vfdt 57 times longer including time disk io needed keep window disk vfdts average memory allocation course run 23 mb cvfdts 165 mb average number nodes vfdts tree 2696 average number cvfdts tree 677 132 alternate trees remainder main tree next examined cvfdt responds changing levels concept drift datasets added using parameter every 75000 examples concept hyperplanes weights selected random updated w 25 chance ipping signs chosen prevent many weights drifting pattern figure 4 shows comparison datasets cvfdt substantially outperformed vfdt every level drift notice vfdts error rate approaches 50 2 variance vfdts data points large cvfdts error rate seems grow smoothly increasing levels concept change suggesting drift adaptations robust eective wanted gain insight way cvfdt starts new alternate subtrees prunes existing ones replaces portions ht alternates purpose instrumented run cvfdt figure 4 output token response events aggregated events chunks 100000 training examples generated data points nonzero values figure 5 shows results experiment large number events run example alternate subtrees swapped ht swaps seem occur examples test set changing labels quickly also wanted see well cvfdt would compare system using traditional drifttracking methods thus compared cvfdt vfdt vfdtwindow simulated vfdtwindow running vfdt w every 100000 examples instead every example dataset experiment used drift settings used generate figure 4 6 shows results cvfdts error rate vfdtwindows except brief period middle run class labels changing rapidly cvfdts average error rate run 163 vfdts 194 vfdtwindows 153 dierence runtimes large vfdt took 10 minutes cvfdt took 46 minutes estimate vfdtwindow would taken 548 days complete run applied every new example put another way vfdtwindow provides 4 accuracy gain compared figure 4 error rates function amount concept drift figure 5 cvfdts drift characteristics figure rates time cvfdt vfdt vfdtwindow vfdt cost increasing running time factor 17000 cvfdt provides 75 vfdtwindows accuracy gain introduces time penalty less 01 vfdtwindows cvfdts alternate trees additional sucient statistics use much ram example none runs ever grew 70mb never observed cvfdt use ram vfdt fact often used little half ram vfdt systems ram requirements dominated sucient statistics kept leaves vfdt every node cvfdt observed vfdt often twice many leaves nodes cvfdts tree alternate trees combined expected vfdt considers many examples forced grow larger trees make fact early decisions become incorrect due concept drift cvfdts alternate tree pruning mechanism seems eective trading memory smooth transitions concepts room aggressive pruning cvfdt exhausts available ram exploring tradeo area future work 42 web data currently applying cvfdt mining stream web page requests emanating whole university washington main campus nature data described detail wolman et al 29 experiments far used oneweek anonymized trace external web accesses made university campus 23000 active clients oneweek trace period entire university population estimated 50000 people students faculty sta trace contains million requests arrive peak rate 17400 per minute size compressed trace le 20 gb 5 request tagged anonymized organization id associates request one organizations colleges departments etc within university one purpose data used improve web caching key predicting accurately possible hosts pages requested near future given recent requests applied decisiontree learning problem following manner split campuswide request log series equal time slices experiments report time slice hour organization o1 244k hosts appearing logs maintained count many times organization accessed host time slice c ijt discretized counts four buckets representing requests 1 12 requests 13 25 requests 26 requests time slice host accessed time slice generated example attributes h j requested time slice t1 0 carried real time using modest resources keeping statistics last current time slices c 1 c memory keeping counts hosts actually appear time slice never needed 30k counts outputting examples c 1 soon c complete using procedure obtained dataset containing 189 million examples 609 la 5 log may 1999 trac may 2000 double size beled common class host appear next time slice exploration designed determine cvfdts concept drift features would provide benet ap plication example arrived tested accuracy learners models allowed learners update models example kept statistics aggregated accuracies changed time vfdt cvfdt run additional parameters achieved 727 accuracy whole dataset cvfdt achieved 723 however cvfdts aggregated accuracy higher rst 70 run times much 10 cvfdts accuracy fell behind near end run believe following reason drift tracking kept ahead throughout rst part run window small learn detailed model data vfdt end experiment shows data indeed contain concept drift cvfdts ability respond drift gives advantage vfdt next step run cvfdt dierent perhaps dynamic window sizes evaluate nature drift also plan evaluate cvfdt traces longer week 5 related work schlimmer grangers 23 stagger system one rst explicitly address problem concept drift salganico 21 studied drift context nearestneighbor learning widmer kubats 27 flora system used window examples also stored old concept descriptions reactivated seemed appropriate systems applied small databases todays standards kelly hand adams 14 addressed issue drifting parameters probability distributions theoretical work concept drift includes 16 3 ganti gehrke ramakrishnans 11 demon framework designed help adapt incremental learning algorithms work eectively timechanging data streams demon diers cvfdt assuming data arrives pe riodically perhaps daily large blocks cvfdt deals example arrives framework uses oline processing time mine interesting subsets available data blocks earlier work 12 gehrke ganti ramakrishnan presented incremental decision tree induction algorithm boat works demon framework boat able incrementally maintain decision tree equivalent one would learned batch decision tree induction system underlying concept stable boat perform maintenance extremely quickly drift present boat must discard regrow portions induced tree expensive drift large aects nodes near root tree cvfdt avoids problem using alternate trees removing restriction learn exactly tree batch system would comparison boat cvfdt area future work great deal work incrementally maintaining association rules cheung han ng wong fazil tansel arkun 2 propose algorithms maintaining sets association rules new transactions added database sarda srinivas 22 also done work area demons contribution 11 particularly relevant addresses association rule maintenance specically highspeed data stream domain blocks transactions added deleted database regular basis aspects concept drift problem also addressed areas activity monitoring 10 active data mining 1 deviation detection 6 main goal explicitly detect changes rather simply maintain uptodate concept techniques latter obviously help former several pieces research concept drift contextsensitive learning collected special issue journal machine learning 28 relevant research appeared icml96 workshop learning contextsensitive domains 15 aaai98 workshop ai approaches timeseries problems 8 nips2000 workshop realtime modeling complex learning tasks 26 turney 25 maintains online bibliography contextsensitive learning 6 future work plan apply cvfdt realworld problems ability adjust concept changes allow perform well broad range tasks cvfdt may useful tool identifying anomalous situations currently cvfdt discards subtrees outofdate concepts change periodically subtrees may become useful identifying situations taking advantage another area study areas study include comparisons related systems continuous attributes weighting examples partially forgetting examples allowing weights decay simulating weights subsampling controlling weight decay function according external information drift 7 conclusion paper introduced cvfdt decisiontree induction system capable learning accurate models demanding highspeed conceptdrifting data streams cvfdt able maintain decisiontree uptodate window examples using small constant amount time new example arrives resulting accuracy similar would obtained reapplying conventional learner entire window every time new example arrives empirical studies show cvfdt eectively able keep model uptodate massive data stream even face large frequent concept shifts preliminary application cvfdt real world domain shows promising results 8 acknowledgments research partly supported gift ford motor company nsf career ibm faculty awards third author 9 r active data mining learning changing concepts exploiting structure change megainduction machine learning large databases mining surprising patterns using temporal description length maintenance discovered association rules large databases incremental updating technique mining highspeed data streams activity monitoring noticing interesting changes behavior demon mining monitoring evolving data boat optimistic decision tree construction impact changing populations classi complexity learning according two models drifting environment sliq fast scalable classi decision theoretic subsampling induction large databases adaptive algorithm incremental mining association rules sprint scalable parallel classi learning presence concept drift hidden contexts special issue context sensitivity concept drift tr c45 programs machine learning learning presence concept drift hidden contexts boatmyampersandmdashoptimistic decision tree construction activity monitoring efficient algorithm update large itemsets early pruning impact changing populations classifier performance complexity learning according two models drifting environment mining highspeed data streams learning changing concepts exploiting structure change maintenance discovered association rules large databases mining surprising patterns using temporal description length adaptive algorithm incremental mining association rules demon ctr ying yang xindong wu xingquan zhu combining proactive reactive predictions data streams proceeding eleventh acm sigkdd international conference knowledge discovery data mining august 2124 2005 chicago illinois usa charu c aggarwal jiawei han jianyong wang philip yu demand classification data streams proceedings tenth acm sigkdd international conference knowledge discovery data mining august 2225 2004 seattle wa usa francisco ferrertroyano jesus aguilarruiz jose c riquelme incremental rule learning based example nearness numerical data streams proceedings 2005 acm symposium applied computing march 1317 2005 santa fe new mexico francisco ferrertroyano jesus aguilarruiz jose c riquelme data streams classification incremental rule learning parameterized generalization proceedings 2006 acm symposium applied computing april 2327 2006 dijon france joong hyuk chang suk lee finding recently frequent itemsets adaptively online transactional data streams information systems v31 n8 p849869 december 2006 joo gama pedro medas pedro rodrigues learning decision trees dynamic data streams proceedings 2005 acm symposium applied computing march 1317 2005 santa fe new mexico joo gama ricardo rocha pedro medas accurate decision trees mining highspeed data streams proceedings ninth acm sigkdd international conference knowledge discovery data mining august 2427 2003 washington dc yi zhang xiaoming jin automatic construction organization strategy ensemble learning data streams acm sigmod record v35 n3 p2833 september 2006 francisco ferrertroyano jess aguilarruiz jos c riquelme prototypebased mining numeric data streams proceedings acm symposium applied computing march 0912 2003 melbourne florida brain babcock mayur datar rajeev motwani liadan ocallaghan maintaining variance kmedians data stream windows proceedings twentysecond acm sigmodsigactsigart symposium principles database systems p234243 june 0911 2003 san diego california francisco ferrertroyano jess aguilarruiz jos c riquelme discovering decision rules numerical data streams proceedings 2004 acm symposium applied computing march 1417 2004 nicosia cyprus nilesh dalvi pedro domingos mausam sumit sanghai deepak verma adversarial classification proceedings tenth acm sigkdd international conference knowledge discovery data mining august 2225 2004 seattle wa usa otey parthasarathy ghoting g li narravula panda towards nicbased intrusion detection proceedings ninth acm sigkdd international conference knowledge discovery data mining august 2427 2003 washington dc george forman tackling concept drift temporal inductive transfer proceedings 29th annual international acm sigir conference research development information retrieval august 0611 2006 seattle washington usa shi zhong efficient streaming text clustering neural networks v18 n56 p790798 june 2005 wei fan streamminer classifier ensemblebased engine mine conceptdrifting data streams proceedings thirtieth international conference large data bases p12571260 august 31september 03 2004 toronto canada geoff hulten pedro domingos mining complex models arbitrarily large databases constant time proceedings eighth acm sigkdd international conference knowledge discovery data mining july 2326 2002 edmonton alberta canada anand narasimhamurthy ludmila kuncheva framework generating data simulate changing environments proceedings 25th conference proceedings 25th iasted international multiconference artificial intelligence applications p384389 february 1214 2007 innsbruck austria orna raz philip koopman mary shaw semantic anomaly detection online data sources proceedings 24th international conference software engineering may 1925 2002 orlando florida yunyue zhu dennis shasha efficient elastic burst detection data streams proceedings ninth acm sigkdd international conference knowledge discovery data mining august 2427 2003 washington dc rouming jin gagan agrawal efficient decision tree construction streaming data proceedings ninth acm sigkdd international conference knowledge discovery data mining august 2427 2003 washington dc graham cormode mayur datar piotr indyk muthukrishnan comparing data streams using hamming norms zero proceedings 28th international conference large data bases p335345 august 2023 2002 hong kong china jimeng sun dacheng tao christos faloutsos beyond streams graphs dynamic tensor analysis proceedings 12th acm sigkdd international conference knowledge discovery data mining august 2023 2006 philadelphia pa usa weiguang teng mingsyan chen philip yu regressionbased temporal pattern mining scheme data streams proceedings 29th international conference large data bases p93104 september 0912 2003 berlin germany haixun wang jian yin jian pei philip yu jeffrey xu yu suppressing model overfitting mining conceptdrifting data streams proceedings 12th acm sigkdd international conference knowledge discovery data mining august 2023 2006 philadelphia pa usa hillol kargupta byunghoon park sweta pittie lei liu deepali kushraj kakali sarkar mobimine monitoring stock market pda acm sigkdd explorations newsletter v3 n2 january 2002 wei fan systematic data selection mine conceptdrifting data streams proceedings tenth acm sigkdd international conference knowledge discovery data mining august 2225 2004 seattle wa usa lilian harada detection complex temporal patterns data streams information systems v29 n6 p439459 september 2004 themistoklis palpanas dimitris papadopoulos vana kalogeraki dimitrios gunopulos distributed deviation detection sensor networks acm sigmod record v32 n4 december joong hyuk chang suk lee efficient mining method retrieving sequential patterns online data streams journal information science v31 n5 p420432 october 2005 sreenivas gollapudi sivakumar framework algorithms trend analysis massive temporal data sets proceedings thirteenth acm international conference information knowledge management november 0813 2004 washington dc usa kevin b pratt gleb tschapek visualizing concept drift proceedings ninth acm sigkdd international conference knowledge discovery data mining august 2427 2003 washington dc joo gama ricardo fernandes ricardo rocha decision trees mining data streams intelligent data analysis v10 n1 p2345 january 2006 charu c aggarwal change diagnosis evolving data streams ieee transactions knowledge data engineering v17 n5 p587600 may 2005 daniel kifer shai bendavid johannes gehrke detecting change data streams proceedings thirtieth international conference large data bases p180191 august 31september 03 2004 toronto canada mohamed medhat gaber shonali krishnaswamy arkady zaslavsky costefficient mining techniques data streams proceedings second workshop australasian information security data mining web intelligence software internationalisation p109114 january 01 2004 dunedin new zealand graham cormode mayur datar piotr indyk muthukrishnan comparing data streams using hamming norms zero ieee transactions knowledge data engineering v15 n3 p529540 march malu castellanos fabio casati umeshwar dayal mingchien comprehensive automated approach intelligent business processes execution analysis distributed parallel databases v16 n3 p239273 november 2004 joong hyuk chang suk lee estwin online data stream mining recent frequent itemsets sliding window method journal information science v31 n2 p7690 april 2005 tho manh nguyen josef schiefer min tjoa sense response service architecture saresa approach towards realtime business intelligence solution use fraud detection application proceedings 8th acm international workshop data warehousing olap november 0405 2005 bremen germany haixun wang wei fan philip yu jiawei han mining conceptdrifting data streams using ensemble classifiers proceedings ninth acm sigkdd international conference knowledge discovery data mining august 2427 2003 washington dc yixin chen guozhu dong jiawei han benjamin w wah jianyong wang multidimensional regression analysis timeseries data streams proceedings 28th international conference large data bases p323334 august 2023 2002 hong kong china jiawei han yixin chen guozhu dong jian pei benjamin w wah jianyong wang dora cai stream cube architecture multidimensional analysis data streams distributed parallel databases v18 n2 p173197 september 2005 wengkeen wong andrew moore gregory cooper michael wagner whats strange recent events wsare algorithm early detection disease outbreaks journal machine learning research 6 p19611998 1212005 yasushi sakurai spiros papadimitriou christos faloutsos braid stream mining group lag correlations proceedings 2005 acm sigmod international conference management data june 1416 2005 baltimore maryland changtien lu yufeng kou jiang zhao li chen detecting tracking regional outliers meteorological data information sciences international journal v177 n7 p16091632 april 2007 yang li lee wynne hsu finding hot query patterns xquery stream vldb journal international journal large data bases v13 n4 p318332 december 2004 marcus maloof ryszard michalski incremental learning partial instance memory artificial intelligence v154 n12 p95126 april 2004 jrgen beringer eyke hllermeier online clustering parallel data streams data knowledge engineering v58 n2 p180204 august 2006 streaming pattern discovery multiple timeseries proceedings 31st international conference large data bases august 30september 02 2005 trondheim norway brian babcock shivnath babu mayur datar rajeev motwani jennifer widom models issues data stream systems proceedings twentyfirst acm sigmodsigactsigart symposium principles database systems june 0305 2002 madison wisconsin zhiyuan chen chen li jian pei yufei tao haixun wang wei wang jiong yang jun yang donghui zhang recent progress selected topics database research report nine young chinese researchers working united states journal computer science technology v18 n5 p538552 september singlepass mining path traversal patterns streaming web clicksequences computer networks international journal computer telecommunications networking v50 n10 p14741487 14 july 2006 mohamed medhat gaber arkady zaslavsky shonali krishnaswamy mining data streams review acm sigmod record v34 n2 june 2005 shivnath babu jennifer widom continuous queries data streams acm sigmod record v30 n3 september 2001 lukasz golab tamer zsu issues data stream management acm sigmod record v32 n2 p514 june venkatesh ganti johannes gehrke raghu ramakrishnan mining data streams block evolution acm sigkdd explorations newsletter v3 n2 january 2002