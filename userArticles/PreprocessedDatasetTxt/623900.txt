lowlatency communication atm networks using active messages recent developments communication architectures parallel machines reduced communication overheads latencies order magnitude paper examines whether techniques carry clusters workstations connected atm network even though clusters use standard operating system software equipped network interfaces optimized stream communication allow direct protected userlevel access network use networks without reliable transmission flow control first part paper describes differences communication characteristics clusters workstations built standard hardware software components stateoftheart multiprocessors second part evaluates prototype implementation lowlatency active messages communication model sun workstation cluster interconnected atm network measurements show applicationlevel roundtrip latencies 50 microseconds small messages roughly comparable active messages implementation thinking machines cm5 multiprocessor b introduction shift slow broadcastbased local area networks high bandwidth switched network architectures making use clusters workstations 1 platforms parallel processing attractive number software packages 56 already support 1 term cluster used refer collections workstationclass machines interconnected lowlatency highbandwidth network parallel processing todays workstations net works communication performance two orders magnitude inferior stateofthe art multiprocessors result embarassingly parallel applications ie parallel applications essentially never communicate make use environments networking technologies atm1 offer opportunity close gap example atm cells roughly size messages multiprocessors takes microseconds send receive cell atm switches configured provide bisection bandwidths comparable parallel machine networks routing latencies order microseconds 3 however date communication potential available application level purely technical point view gap clusters workstations multiprocessors certainly closing distinction two types systems becoming blurred differences remain partic ular design construction multiprocessors allows better integration components 2 paper focuses exclusively scalable multiprocessor architectures specifically excludes busbased sharedmemory multiprocessors 3 current atm switches latencies order magnitude higher comparable multiprocessor net works however difference seem inherent atm networks least local area switches document created framemaker 402 designed fit together addition sharing physical components power supplies cooling cabinets potential reduce cost allow denser packaging debate significance technological differences still open becoming clear two approaches yield qualitatively similar hardware systems indeed possible take cluster workstations load system software making look almost identical multi processor means continuous spectrum platforms spanning entire range workstations ethernet stateoftheart multiprocessors become available distinction multiprocessors clusters arbitrary technical point view pragmatic point view however significant differences likely remain important attraction using cluster workstations instead multiprocessor lies offtheshelf availability major hardware software components means components readily available familiar cost lower economies scale leveraged across entire workstation user community thus even technical point view continuous spectrum clusters multiprocessors use offtheshelf components clusters maintain differences fact use standard components clusters raises question whether reasonably used parallel processing recent advances multiprocessor communication performance principally due tighter integration programming models compilers operating system functions hardware primitives clear whether advances carried clusters whether use standard components squarely odds achieving level integration required enable modern parallel programming mod els specifically new communication architectures distributed shared memory explicit remote memory access active messages reduced costs hundreds thousands microseconds dozen precisely integration system components new communication architectures designed network interfaces implement common primitives directly hardware allow operating system moved critical communication path without compromising protec tion well suited highlevel language implementation paper examines whether techniques developed improve communication performance multiproces sors particular active messages carried clusters workstations standard networks mostly standard system software paper assumes current state art technology clusters using atm networks differ multiprocessors three major aspects clusters use standard operating system software implies less coordination among individual nodes particular respect process scheduling address translation atm networks provide reliable delivery flow control taken granted multiprocessor networks network interfaces workstations optimize stream communication eg tcpip less well integrated overall architecture eg connect io bus instead memory bus comparing communication clusters multiprocessors paper makes two major contributions first analyzes section 2 implications differences clusters multiprocessors design communication layers similar used multiprocessors second describes section 3 design active messages prototype implementation collection sun workstations interconnected atm network yields applicationtoapplica tion latencies order 20s use active messages workstation clusters briefly contrasted approaches section 4 section 5 concludes paper technical issues collections workstations used many different forms run large applications order establish basis comparison multiprocessors paper limits consider collections workstations called clusters consist homogeneous set machines dedicated run parallel applications located close proximity machine room interconnected atm network cluster employed large variety settings cluster could simply provide highperformance compute service user community run large parallel applications typical setting would computational resource distributed application one example stormcast weather monitoring system norway runs large collection machines spread across large portion country uses cluster dozen workstations machine room without high speed network case run computeintensive weather prediction models emit storm warn 1 discussion differences fault isolation characteristics beyond scope paper ings availability lowlatency communication among workstations would enable use parallel programming languages powerful parallel algorithms require closer coupling among processors possible today concentrating compute cluster offers largest potential improvement latency longhaul links dominated speedoflight network congestion issues wide area communication comparatively better served todays distributed computing software note paper argue running concurrent applications heterogeneous environment across large distances workstations happen sitting idle interesting design point fact used success fully set communication issues occurring context cannot compared multiprocessor given applications clusters considered exhibit characteristics similar multiproces sors programming models used would compara ble identical popular parallel computing includes various forms message passing eg sendreceive pvm shared memory eg cache coherent shared memory remote reads writes explicit global memory parallel object oriented languages eg numerous c extensions parallel machines several proposed communication architectures achieved low overheads low latencies high bandwidths required high performance implementations programming models particular cache coherent shared mem remote reads writes active messages offer roundtrip communication within hundred instruction times frequent communication fine granularity object object cache line basis remains compatible high performance settings overhead communication time spent processor initiating communica tion essentially cost pushing message data network interface sending end pulling receiving end virtually cycles spent protocol handling reliability flow control handled hardware operating system need involved every communication operation network interface hardware enforce protection boundaries across network communication architectures cannot moved straightforward manner multiprocessors clusters workstations atm networks three major differences two atm networks offer neither reliable delivery flow control atm network interfaces provide support protected userlevel access network workstation operating systems coordinate process scheduling address translation globally coping differences poses major technical challenges may eventually require integration mul tiprocessorspecific features clusters following three subsections present nature differences detail discuss resulting issues 21 reliability flow control network multiprocessor networks flow control implemented hardware linkbylink basis whenever input buffer router fills output upstream router disabled prevent buffer overflow flow control thus effect blocking messages network eventually backpressure propa gates sending nodes prevented injecting messages mechanism guarantees messages never dropped due buffer space limitations within network receiving end addition electrical characteristics network designed ensure low error rates use simple error detection correction mechanism imple mented hardware offer reliability within network typical processing nodes contrast atm network provide form flow control offer reliable delivery instead higher protocol layers must detect cell loss corruption cause retransmission partitioning responsibilities may acceptable case streambased communication eg tcpip video audio questionable parallel computing setting flow control error detection correction multiprocessor networks serve cover four causes message loss buffer overflow receiving software buffer overflow receiving network interface buffer overflow within network message corruption due hardware errors atm network simple window based endtoend flow control schemes permessage crc used aal5 cover first last cases 1 cell loss addition preventing buffer overflow receiving network interface achieved ensuring rate cells moved interface main memory least large maximal cell arrival rate preventing buffer overflow within network however realistically possible using endtoend flow control particularly problem parallel computing setting nodes tend communicate nodes highly regular irregular patterns 1 although transmission media may cause burst errors cannot corrected crc codes unpredictable intervals degree contention within network therefore cannot measured predicted accuracy either sender receiver communication patterns result high contention result high cell loss rates causing extensive retransmissions traditional flow control schemes used streambased communication avoid fruitless retransmission storms dynamically reducing transmission rate connections experience high cell loss rates works settings following law large numbers contention wide area network tend vary instantaneously therefore degree contention observed recent past good predictor contention near future illustration difficulties parallel computing setting consider implementation parallel sort efficient parallel sort algorithms 3 based alternation local sorts nodes permutation phases nodes exchange data nodes permutation phases serve move elements sorted towards correct position communication patterns observed highly dynamic characteristics depend large degree input data point attempted data rate given node exceeds link rate output buffers upstream switches start filling communication patterns change rapidly essentially every cell futile attempt predict contention given toall communication pattern probability internal contention among seemingly unrelated connections high beyond problems caused contention resulting retransmissions lack reliable delivery guarantee atm networks imposes certain overhead communication primitives specifically sender must keep copy cell sent corresponding acknowledgment received case cell must retransmitted means messages cannot transferred directly processor registers network interface possible cm5 12 rather memory copy must made well 22 userlevel access network interface recently multiprocessor communication architectures achieved significant reduction communication overhead eliminating operating system critical path order compromise security network interface must offer form protection mechanism shared memory models memory management unit extended map remote memory local virtual user address space operating system enforce security managing address translation tables messagebased network interfaces contain node address translation table maps users virtual node numbers onto physical node address space operating system enforces security controlling address translation thereby preventing process sending message arbitrary node current generation message based network interfaces control destination node address therefore require processes parallel program run time next generation adds sending process id message allowing receiving network interface discriminate messages destined currently running process retrieve message directly messages dormant processes must queued typically operating system later retrieval contrast network interfaces available workstations yet incorporate form protection mechanism instead operating system must involved sending reception every message connection based nature atm networks would principally allow design protection mechanism limit virtual circuits user process access operating system would still control virtual circuit setup architecture networking layers current operating systems seem setup allow userlevel network interface access appears unlikely network interfaces features become commonplace soon challenge highperformance communication layer clusters thus minimize path kernel judiciously coordinating userkernel interactions 23 coordination system software across communicating nodes almost communication architectures message reception logic critical performance bottleneck order able handle incoming messages full network bandwidth processing required arriving message must minimized carefully trick used multiprocessor systems ensure rapid message handling constrain sender send messages easy handle shared memory systems done coordinating address translation tables among processing nodes originating node translate virtual memory address remote access directly place corresponding physical memory address message set communication primitives small fixed eg read write forcing sender perform complicated part remote memory access namely protection checks address translation handling request relatively simple implement 1 virtual address sent receiving node could discover requested virtual memory location paged disk result handling message would become rather involved active messages multiprocessors scheduling processes assumed coordinated among nodes communicating processes execute simultaneously respective nodes guarantees messages handled immediately arrival destination process order accomplish sender active message specifies userlevel handler destination whose role extract message network integrate ongoing computation handler also implement simple remote service send reply active message back however order prevent deadlock communication patterns limited requests replies eg handler reply message allowed send messages implementation active messages typically reserves first word message handler address handler receiving end dispatched immediately message arrival dispose message fact message layer call upon handlers deal messages fifo order simplifies buffering considerably required traditional message passing models pvm mpi nx models allow processes consume messages arbitrary order arbitrary times forcing communication architecture implement general buffer message matching mechanisms high cost clusters fact operating systems individual nodes nearly coordinated contradicts assumption messages always consumed quickly upon arrival case active messages destination process might suspended cannot run handler shared memory model memory location requested might mapped although exact coordination possible without major changes operating system core implementation either communication model likely able perform coordination among nodes influence local operating system accord ingly may allow communication layer assume common case everything works fine must able handle difficult cases well 24 summary even though superficially cluster workstations appears technically comparable multiproces 1 cache coherent shared memory stretch characterization given cache receiving node essentially performs another address translation may miss require additional communication nodes complete request sor reality key characteristics different cause significant implementation difficulties comparable raw hardware link bandwidths bisection bandwidths routing latencies conceal lack clusters flow control reliability userlevel network access operating system coordination shortcomings inevitably result lower communication performance quantitative effect performance evaluated next section presents prototype implementation active messages cluster sun workstations however lack flowcontrol atm networks poses fundamental problem catastrophic performance degradation occur due significant cell loss particular communication patterns 3 ssam sparcstation active messages prototype ssam prototype implements critical parts active messages communication architecture cluster sparcstations connected atm network primary goal evaluate whether possible provide parallel programming environment cluster comparable found multipro cessors prototype primarily concerned providing performance par parallel machines addressing handicaps atm networks identified previous section particular prototype provides reliable communication evaluate cost performing necessary flowcontrol error checking software minimizes kernel intervention determine cost providing protection software buffering designed tolerate arbitrary context switching nodes time limited experimental setup described available prototype cannot provide information neither cell losses due contention within network affect perfor mance scheduling processes coordinated improve overall performance parallel applications 31 active messages communication architecture active messages communication architecture 4 offers simple general purpose communication primitives thin veneer raw hardware intended serve substrate building libraries provide higherlevel communication abstractions generating communication code directly par allellanguage compiler unlike communication layers intended direct use application programmers really provides lowerlevel services communication libraries runtime systems built basic communication primitive message associated small amount computation form handler receiving end typically first word active message points handler message message arrival computation node interrupted handler executed role handler get message network integrating ongoing computation andor sending reply message back buffering scheduling provided active messages extremely primitive thereby fast buffering involved actual transport scheduling required activate handler sufficient support many higherlevel abstractions general buffering scheduling easily constructed layers active messages needed minimalist approach avoids paying performance penalty unneeded functionality order prevent deadlock livelock active message restricts communication patterns requests replies ie handler request message allowed send reply message reply handler allowed send replies 311 ssam functionality current implementation geared towards sending small messages fit payload single atm cell eight 48 available bytes payload atm cell used ssam hold flowcontrol information 16 bits handler address 32 bits aal34 compatible checksum 16 bits remaining 40 bytes hold active message data c header file interface ssam shown figure 1 send request active message user places message data perconnection buffer provided ssam calls ssam10 connection identifier remote handler address ssam10 adds flowcontrol information traps kernel message injected net work also polls receiver processes incoming messages receiving end network polled ssam10 ssampoll latter polls net work messages accumulated receive fifo moved buffer ssam calls appropriate handler message passing arguments originating connection identifier address buffer holding message address buffer reply message handler processes message may send reply message back placing data buffer provided returning address reply handler null reply sent current prototype use interrupts instead network polled every time message sent means long process sending messages also handle incoming ones explicit polling function provided program parts communicate using interrupts planned implemented yet 312 example implementing remote read ssam sample implementation splitphase remote doubleword read shown figure 2 readdouble function increments counter outstanding reads formats request active message address read well information reply sends message readdoubleh handler fetches remote location sends reply back read doublerh reply handler stores data memory decrements counter originating processor waits completion read busywaiting counter end readdouble splitphase read could constructed easily exposing counter caller could proceed computation initiating read wait counter data required 32 experimental setup experimental setup used evaluate performance prototype ssam implementation consists 60mhz sparcstation20 25mhz sparcsta running sunos 41 two machines connected via fore systems sba100 atm interfaces using 140mbs taxi fiber interfaces located sbus 32bit io bus running 20 25mhz pro figure 1 c interface sparcstation active messages sparcstation atm active messages initialize active messages extern int ssaminitvoid active message handlers int connection void inbuf ssamreqhandlerint connection void inbuf void replybuf buffers send messages define ssammaxconn 32 extern void ssamreqbufssammaxconn extern void ssam10int connection ssamreqhandler handler poll network explicitly extern void ssampollvoid vide 36cell deep output fifo well 292cell input fifo send cell processor stores 56 bytes memorymapped output fifo receive cell reads 56 bytes input fifo register interface indicates number cells available input fifo figure 2 sample remote read implementation using ssam remote read static volatile int readcnt 0 struct double src dest double data4 read 32 bytes remote node void read32int conn double src double dest read32msg readcnt ssam10conn read32h read request handler static ssamreplyhandler read32hint conn read32msg read32msg double outdest else non doubleword aligned code omitted return read32rh read reply handler static void read32rhint conn read32msg double dest indest else non doubleword aligned code omitted readcnt note network interface used much simpler closer multiprocessor nis secondgeneration atm interfaces available today function performed hardware beyond simply moving cells ontooff fiber checksum generation checking atm header aal34 compatible payload particular dma segmentation reassembly multicell packets provided 33 ssam implementation implementation sparcstation atm active messages layer consists two parts device driver dynamically loaded kernel userlevel library linked applications using ssam driver implements standard functionality open close atm device provides two paths send receive cells fast path described consists three trap instructions lead directly code sending receiving individual atm cells traps relatively generic functionality specific active messages userlevel library also performs flowcontrol buffer man agement conventional readwrite system call interface provided comparison purposes allows send receive cells using pure device driver approach traps send receive cells consist carefully crafted assembly language routines routine small 28 43 instructions send receive traps respectively uses available registers care fully register usage simplified sparc archi tectures use circular register file provides clean 8register window trap interfacing program traps via function call arguments passed another 8 registers become available trap following paragraphs describe critical parts ssam implementation detail 331 flowcontrol simple sliding window flow control scheme used prevent overrun receive buffers detect cell losses window size dimensioned allow close full bandwidth communication among pairs processors order implement flow control window size w process preallocates memory hold 4w cells per every process communi cates algorithm send request message polls receiver free window slot available injects cell network saving one buffers well case retransmitted upon receipt request message message layer moves cell buffer soon corresponding process running calls active message handler handler issues reply sent copy held buffer handler generate reply explicit acknowledgment sent upon receipt reply acknowledgment buffer holding original request message reused note distinction requests replies made active messages allows acknowledgments piggybacked onto replies recovery scheme used case lost duplicate cells standard except reception duplicate request messages may indicate lost replies retransmitted important realize flow control mechanism really attempt minimize message losses due congestion within net work lack flowcontrol atm networks effectively precludes simple congestion avoidance scheme larger testbeds become available atm community agrees routers handle buffer overflows seems futile invest sophisticated flowcontrol mechanisms nevertheless bursty nature parallel computing communication patterns likely require solution performance characteristics atm network become robust multiprocessor networks 332 userkernel interface buffer management streamlining userkernel interface important factor contributing performance ssam prototype kernel preallocates buffers process device opened pages pinned prevent pageouts mapped using mmap processes address space every message send userlevel library chooses buffer next message places pointer exported variable application program moves message data buffer passes connection id handler address ssam finishes formatting cell adding flow control handler traps kernel trap passes message offset within buffer area connection id registers kernel protection ensured simple masks limit connection id offset ranges lookup maps current process connection ids virtual circuit kernel finally moves cell output fifo receiving end readatm kernel trap delivers batch incoming cells predetermined shared memory buffer number cells received returned register cell kernel performs four tasks loads first half cell registers uses vci index table obtain address appropriate processes input buffer moves full cell buffer checks integrity cell using three flag bits set ni last byte upon return trap ssam library loops received cells checking flowcontrol information calling appropriate handlers request reply messages sending explicit acknowledgments needed 34 ssam performance following paragraphs describe performance measurements ssam made number synthetic benchmarks following terminology used overhead consists processor cycles spent preparing send receive message latency time message send routine called time message handled remote end bandwidth rate user data transferred performance goal ssam fiber rate 140mbits transmits cell every 314s 532 bytes atm payload bandwidth 152mbs 1 341 atm traps detailed cost breakdown operations occurring traps send receive cells shown table 1 two timing columns refer measurements taken sparcstation 1 sparcstation 20 respectively times obtained measuring repeated executions trap gettimeofday uses microsecondaccu rate clock takes 95s ss20 time breakdown trap measured commenting appropriate instructions somewhat approximate due pipeline overlap occurring successive instructions write trap cost broken 5 parts cost trap return protection checks overhead fetching addresses loading cell registers pushing cell network interface ss show clearly fiber saturated sending cell time user level also indicates majority cost 75 lies access network interface across sbus cost trap surprisingly low even though second largest item fact could reduced slightly current implementation adds level indirection trap dispatch simplify dynamic loading device driver 2 read trap itemized similarly cost trap return fetching device register count available cells additional overhead settingup addresses loading cell network interface 1 bandwidths measured megabytes per second 2 kernel writeprotects trap vectors bootup ssam prototype uses permanently loaded trap performs indirect jump via kernel variable allow simple dynamic driver loading demultiplexing among processes storing cell away total cost shows trap receives single cell well percell cost trap receives cells access device dominates due fact doubleword load incurs full latency sbus access total time 461s ss20 falls short fibers cell time limit achievable bandwidth 68 fiber writeread trap first sends cell receives chunk cells amortizes cost trap across functions overlaps checking cell count slightly sending last item table shows cost null system call comparison purposes write file descriptor 1 used clear system call approach would yield performance far inferior traps would achieve fraction fiber bandwidth 342 atm readwrite system calls addition direct traps device driver allows cells sent received using traditional read system calls device file descriptor time conventional path provided comparison purposes read write entry points device driver limited sending receiving single cells multicell reads writes could supported easily read write entry points perform following operations check appropriateness file descriptor transfer data user space internal buffer using uiomove transfer data internal buffer fifos network interface internal buffer used data cannot transferred directly user space device using uiomove due fact device fifos word addressable use internal buffer also allows doubleword accesses device fifos improves access times considerably table 2 shows costs various parts read write system calls syscall overhead entries reflect time taken read respectively write system call empty read write device driver rou tine measures kernel overhead associated system calls check fd uiomove entry reflects time spent checking validity file descriptor performing uiomove case read also includes time check device register holding number cells available input fifo pushpull cell entries reflect time spent transfer contents one cell internal buffer device fifos write read 1 cell totals reflect cost full system call read 0 cells entry time taken unsuccessful poll includes system call table 2 cost sending receiving cells using read system calls operation ss20 ss1 write system call syscall overhead 226s 100s check fd uiomove 34s 16s push cell ni 22s 8s write total 282s 124s read system call syscall overhead 221s 99s pull cell ni 50s 13s check fd recv ready uiomove 70s 25s read total 1 cell 341s 137s read total 0 cells 288s 113s table 1 cost breakdown traps send receive cells operation ss20 ss1 traprett 044s 203s check pid connection id addtl kernel ovhd 005s 050s load cell push 013s 387s push cell ni 205s 317s total 272s 1011s read trap traprett 044s 203s check cell count 081s 108s addtl kernel ovhd 018s 080s per cell pull ni 427s 368s per cell demux 009s 023s per cell store away 017s 350s total 1 cell 587s 1132s per cell total total 0 cells read 37s 112s total 1 cell read 82s 214s null system call 69s 40s head file descriptor checks reading receiveready register timings show clearly overhead read write system call interface prohibitive small mes sages larger messages however may well viable choice portable traps 343 ssam measurements active messages layer built cell send receive traps shown table 3 cases one word active message payload carries data handlers simply return send request uses writereadtrap adds little 1s overhead ss20 cell formatting flowcontrol handling times roughly cost read trap reading 16 cells per trap plus little 1s flow control handler dispatch reply sent adds time writetrap measurements show supporting singlecell active messages optimal longer messages required achieve peak bulk transfer rates one cellatatime prototype yield 56mbs simpler interface shorter messages eg bytes payload might well useful well accelerate small requests acknowledgments often found higherlevel protocols unfortunately given trap cost dominated network interface access time sba100 requires 56 bytes cell transferred processor unlikely significant benefit realized 344 splitc full implementation splitc 2 still progress timings remote memory access primitives show roundtrip time remote read aligned bytes takes 32s ss 20 oneway remote store takes 22s payload 1 remote accesses smaller payloads noticeably cheaper bulk write implemented current ssam layer transfers 55mbytess 1 note realistic setting fore asx100 switch add roughly 10s latency write time 20s roundtrip read time 7 table 3 cost breakdown sparcstation active messages operation ss20 ss1 send request 50s 15s handle request reply sent 56s 15s handle request send reply 77s 25s handle ack 50s 11s handle reply 52s 12s experiments show using long messages could improved 9mbytess using full atm payload simplifying handling slightly unresolved issues current ssam prototype influence process scheduling given current buffering scheme ssam layer operation influenced process running performance applica tions however likely highly influenced scheduling best influence scheduler semiportable fashion requires investigation promising approach appears use realtime thread scheduling priorities available solaris 2 amount memory allocated ssam prototype somewhat excessive fact simplicity current prototype uses twice many buffers strictly necessary example assuming flowcontrol window 32 cells used kernel allocates pins 8kbytes memory per process per connec tion 64node cluster 10 parallel applications running represents 5mb memory per processor number preallocated buffers could reduced without affecting peak bulk transfer rates adjusting flow control window size dynamically idea first cell long message contain flag requests larger window size receiver extra buffers would allocated purpose receiver grants larger window one sender time using first acknowledgment cell bulk transfer larger window size remains effect end long message scheme two benefits request larger window overlapped first cells long message receiver prevent many senders transferring large data blocks simultaneously would suboptimal cache however fundamentally appears memory alternatively low performance price pay neither flowcontrol network coordinated process scheduling subtle problem atm payload alignment used sba100 interface surface future 53 bytes atm cell padded sba100 56 bytes 48byte payload starts 6th byte ie halfword aligned effect bulk transfer payload formats designed sba100 mind supporting doubleword moves data memory clash network interfaces doubleword align atm payload 36 summary prototype active messages implementation sparcstation atm cluster provides preliminary demonstration communication architecture developed multiprocessors adapted peculiarities workstation cluster performance achieved roughly comparable multiprocessor cm5 oneway latency roughly 6s clear without network interface closer processor performance gap cannot closed time taken flowcontrol protection software surprisingly low least comparison network interface access times cost effect shifted large preallocated pinned buff ers prototypes memory usage somewhat excessive schemes comparable performance also require large buffers speed comes careful integration layers language level kernel traps key issues avoiding copies application place data directly kernel picks move device passing easy check information kernel particular pass arbitrary virtual address 4 comparison approaches atm network communication layer directly comparable ssam remote memory access model proposed thekkath et al 1011 implementation similar ssam uses traps reserved opcodes mips instruction set implement remote read write instructions 1 major difference two models remote memory operations separate data control transfer active messages unifies remote memory accesses data transferred user memory kernel without corresponding process run model used allow remote reads writes full address space process rather communicating process must allocate special communication memory segments pinned operating system buffers used ssam communication segments flexible ssams buffers directly hold data structures limited fact segments pinned advantage ssam remote memory accesses coupling data control message causes small amount user code executed 1 one could easily describe traps employed ssam additional emulated communication instructions allows data scattered complex data structures scheduling computation directly influenced arrival data remote memory access model limited control transfer offered persegment notification flags order cause file descriptor become ready finally ssam provides reliable transport mechanism remote memory access primitives unreliable provide flowcontrol table 4 compares performance two approaches thekkaths implementation uses two decstation 5000 interconnected turbochannel version fore100 atm interface used ssam performs little worse ssam data transfer significantly worse control transfer remote reads writes directly comparable transfer payload per cell performance traditional communication layers atm network evaluated lin et al 7 shows two orders magnitude higher communication latencies ssam offers table 5 summarizes best roundtrip latencies oneway bandwidths attained sun 4690s sparcstation 2s connected fore sba100 interfaces without switch millisecond scale reflects costs traditional networking architecture used layers although clear fores aal5 api slower readwrite system call interface described 342 note tcpip implementation welloptimized fastpath yield submillisecond latencies table 4 comparison ssam remote memory accesses 2 decstation 5000s atm 11 operation ssam remote mem access read latency 32s 45s latency 22s 30s addtl control transfer ovhd none 260s block write 55mbs 44mbs table 5 performance traditional communication layers sun4690s sparcstation 2s atm 7 communication layer roundtrip latency peak bandwidth fore aal5 api 17ms 4mbs bsd tcpip sockets 39ms 2mbs pvm tcpip 54ms 15mbs sun rpc 39ms 16mbs conclusions emergence highbandwidth lowlatency networks making use clusters workstations attractive parallel computing style applications technical point view continuous spectrum systems conceived ranging collections ethernetbased workstations tightly integrated custom multiprocessors however paper argues clusters characterized use offtheshelf components handicap respect multiprocessors hardware software customized allow tighter integration network overall architecture use standard components particular atm networking technology results three major disadvantages clusters respect multiprocessors atm networks offer reliable delivery flow control ii current network interfaces well integrated workstation architecture iii operating systems nodes cluster coordinate process scheduling address translations prototype implementation active messages communication model described paper achieves two orders magnitude better performance traditional networking layers table 6 shows resulting communication latencies bandwidths ballpark stateoftheart multiprocessors key success use large memory buffers careful design lean userkernel interface major obstacle towards closing remaining performance gap slow access network interface across io bus reducing buffer memory usage requires coordination process scheduling across nodes taking care flow control software dominate performance study behavior atm networks parallel computing communication loads remains open question table comparison ssams performance recent parallel machines machine peak bandwidth roundtrip latency paragon active mesg 4 10mbs 12s cluster 6 r fast parallel sorting logp splitc active messages mechanism integrated communication computation pvm 30 users guide reference manual memory coherence shared virtual memory systems distributed network computing local atm networks paragon implementation nx message passing interface sp1 highperformance switch efficient support multicomputing atm networks separating data control transfer distributed operating systems tr ctr jarek nieplocha robert harrison shared memory programming metacomputing environments global array approach journal supercomputing v11 n2 p119136 oct 1997 david e culler lok tin liu richard p martin chad yoshikawa assessing fast network interfaces ieee micro v16 n1 p3543 february 1996 thomas e anderson david e culler david patterson team case networks workstations ieee micro v15 n1 p5464 february 1995 boris roussev jie wu distributed computing using java comparison two server designs journal systems architecture euromicro journal v52 n7 p432440 july 2006 takashi matsumoto kei hiraki mbcf protected virtualized highspeed userlevel memorybased communication facility proceedings 12th international conference supercomputing p259266 july 1998 melbourne australia huang c c huang p k mckinley multicast virtual topologies collective communication mpcs atm clusters proceedings 1995 acmieee conference supercomputing cdrom p9es december 0408 1995 san diego california united states dawson r engler frans kaashoek dpf fast flexible message demultiplexing using dynamic code generation acm sigcomm computer communication review v26 n4 p5359 oct 1996 von eicken basu v buch w vogels unet userlevel network interface parallel distributed computing includes url acm sigops operating systems review v29 n5 p4053 dec 3 1995 scott pakin mario lauria andrew chien high performance messaging workstations illinois fast messages fm myrinet proceedings 1995 acmieee conference supercomputing cdrom p55es december 0408 1995 san diego california united states thomas sterling daniel savaresse peter macneice kevin olson clark mobarry bruce fryxell phillip merkey performance evaluation convex spp1000 scalable shared memory parallel computer proceedings 1995 acmieee conference supercomputing cdrom p55 december 0408 1995 san diego california united states wooyoung kim gul agha efficient support location transparency concurrent objectoriented programming languages proceedings 1995 acmieee conference supercomputing cdrom p39es december 0408 1995 san diego california united states k l johnson f kaashoek wallach crl highperformance allsoftware distributed shared memory acm sigops operating systems review v29 n5 p213226 dec 3 1995