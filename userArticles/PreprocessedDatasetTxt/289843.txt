accelerated inexact newton schemes large systems nonlinear equations classical iteration methods linear systems jacobi iteration accelerated considerably krylov subspace methods like gmres paper describe inexact newton methods nonlinear problems accelerated similar way leads general framework includes many wellknown techniques solving linear nonlinear systems well new ones inexact newton methods frequently used practice avoid expensive exact solution large linear system arising possibly also inexact linearization step newtons process framework includes acceleration techniques linear steps well nonlinear steps newtons process described class methods accelerated inexact newton methods contains methods like gmres gmresr linear systems arnoldi jacdav linear eigenproblems many variants newtons method like damped newton general nonlinear problems numerical experiments suggest approach may useful construction efficient schemes solving nonlinear problems b introduction goal paper twofold number iterative solvers linear systems equations fom 23 gmres 26 gcr 31 flexible gmres 25 gmresr 29 gcro 7 structure similar iterative methods linear eigenproblems like shift invert arnoldi 1 24 davidson 6 24 jacobidavidson 28 show algorithms viewed instances accelerated inexact newton scheme cf alg 3 applied either linear equations linear eigenproblems observation may help us design analysis algorithms transporting algorithmic approaches one application area another moreover aim identify efficient schemes nonlinear problems well show learn algorithms linear problems specific interested numerical approximation solution u nonlinear equation 1 f smooth nonlinear map domain r n c n contains solution u r n c n n typically large special types systems equations play important motivating role paper first type linear system equations 2 mathematical institute utrecht university p box 80010 nl3508 utrecht nether lands email fokkemamathruunl sleijpenmathruunl vorstmathruunl r fokkema g l g sleijpen h van der vorst nonsingular matrix b x vectors appropriate size b given x unknown dimension n problem typically large often sparse equivalent 1 type serve main source inspiration ideas second type concerns generalized linear eigenproblem normalized v f u av gamma bv equation 3 equivalent 1 type example mildly nonlinear system serve illustration similarity various algorithms seen instances see section 6 however schemes discuss applicable general nonlinear problems like instance equations arise discretizing nonlinear partial differential equations form whereomega domain r 2 r 3 u satisfies suitable boundary conditions example 4 instance whereomega domain r 2 omega see also section 8 guided known approaches linear system cf 25 29 7 eigenproblem cf 28 27 define accelerated inexact newton schemes general nonlinear systems leads combination krylov subspace methods inexact newton cf 16 4 also 8 acceleration techniques 2 offers us overwhelming choice techniques improving efficiency newton type methods sideeffect leads surprisingly simple framework identification many wellknown methods linear eigen nonlinear problems numerical experiments nonlinear problems like problem 5 serve illustration usefulness approach rest paper organized follows section 2 briefly review ideas behind inexact newton method section 3 introduce accelerated inexact newton methods examine iterative methods linear problems accelerated distinguish galerkin approach minimal residual approach concepts extended nonlinear case section 4 make comments implementation schemes section 5 show many wellknown iterative methods linear problems fit framework section 6 section 7 consider instances mildly nonlinear generalized eigenproblem general nonlinear problems section 8 present numerical results concluding remarks section 9 accelerated inexact newton schemes 3 1 choose initial approximation u 0 2 repeat u k accurate b compute residual r c compute approximation j k jacobian f 0 u k solve correction equation approximately compute approximate solution p k correction equation update compute new approximation algorithm 1 inexact newton 2 inexact newton methods newton type methods popular solving systems nonlinear equations instance represented 4 u k approximate solution iteration number k newtons method requires next approximate solution 1 evaluation jacobian j k f 0 solution correction equation unfortunately may expensive even practically impossible determine jacobian andor solve correction equation exactly especially larger systems situations one aims approximate solution correction equa tion possibly approximation jacobian see eg 8 alg 1 algorithmical representation resulting inexact newton scheme instance krylov subspace methods used approximate solution correction equation 6 directional derivatives required cf 8 4 need evaluate jacobian explicitly v given vector vector f 0 uv approximated using fact combination krylov subspace method directional derivatives combines steps 2c 2d alg 1 initial guess u 0 sufficiently close solution newtons method asymptotically least quadratic convergence behavior however quadratic convergence usually lost one uses inexact variants often convergence much faster linear next section make suggestions linear speed convergence may improved note aim restore much possible asymptotic convergence behavior exact newton address question global convergence 4 r fokkema g l g sleijpen h van der vorst 1 choose initial approximation x 0 2 repeat x k accurate c x algorithm 2 jacobi iteration 3 accelerating inexact newton methods newtons method one step method step newtons method updates approximate solution information previous step however computational process subspaces built gradually contain useful information concerning problem information may exploited improve current approximate solution propose precisely consider alternative update strategies step 2e inexact newton algorithm alg 1 31 acceleration linear case linear system written f x gammaa jacobian f approximate solution p k computed preconditioning matrix approximating inexact newton algorithm alg 1 reduces standard richardson type iteration process splitting r instance choice leads jacobi iteration see alg 2 one may improve convergence behavior standard iterations schemes ffl using sophisticated preconditioners andor ffl applying acceleration techniques update step different preconditioners different acceleration techniques lead different algo rithms wellknown examples iterations schemes use sophisticated preconditioners instance gaussseidel iteration l strict lower triangular part relaxation parameter examples iterations schemes use acceleration techniques algorithms take updates approximate solution linear combination previous directions p j preferable updates b gamma ax k1 x k minimal sense eg kb gamma ax k1 k 2 minimal gmres 26 gcr 31 b gamma ax k1 orthogonal p j j k fom gencg 23 b gamma ax k1 quasiminimal bicg 17 qmr 11 course distinction preconditioning acceleration clear one acceleration techniques limited number steps seen kind dynamic preconditioning opposed static preconditioning fixed view one free choose acceleration technique examples iteration schemes flexible gmres 25 gmresr 29 gcro 7 accelerated iteration schemes linear problems construct approximations solution smaller accelerated inexact newton schemes 5 easier projected problem example gmres computes k equivalently av k b gamma v solution larger tridiagonal problem obtained oblique projections stability efficiency reasons one usually constructs another basis span v k certain orthogonality properties depending selected approach 32 acceleration nonlinear case interested methods finding zero general nonlinear mapping f linear case methods mentioned apart computation residual essentially mix two components 1 computation new search direction involves residual 2 update approximation involves current search direction possibly previous search directions solution k projected problem first component may interpreted preconditioning second component acceleration looking carefully k linear case computed distinguish two approaches based two different conditions g k f fom oblique approaches lead methods compute appropriate w k gmres approach leads methods compute kg k yk 2 minimal minimal residual condition observations linear case formulate iteration schemes nonlinear case inexact newton iteration accelerated similar way standard linear iteration acceleration accomplished updating solution correction k subspace spanned correction directions p j j k precise update k approximate solution given search space v k spanned furthermore g k propose determine ffl galerkin condition g k solution w k matrix dimensions v k ffl minimal residual mr condition g k solution min ffl mix restricted minimal residual rmr condition g k solution min equation 7 generalizes fom approach equation 8 generalizes gmres approach solving 7 means component residual r k1 subspace w k spanned w k vanishes w k one may choose instance w 6 r fokkema g l g sleijpen h van der vorst fom w component j k p k orthogonal w gmres w linear equations minimal residual galerkin approach coincide last choice known linear case complication galerkin approach equation 7 may solution means approach may lead breakdown method order circumvent shortcoming extend formulated restricted minimal residual approach 9 compared 7 formulation also attractive another reason one apply standard gaussnewton 9 schemes solving general nonlinear least squares problems one might argue drawback gaussnewton scheme may converge slowly however least squares problems zero residual solutions asymptotic speed convergence gaussnewton method newtons method means galerkin problem 7 solution gaussnewton scheme applied 9 find fast efficient see also section 8 note equations 79 represent nonlinear problems k variables may much easier solve original problem smaller nonlinear problems formulated cheaply costs update step may considered relatively small note also since equations 79 nonlinear may one solution fact may exploited steer computational process specific preferable solution original problem accelerated inexact newton galerkin approach step 2e inexact newtons algorithm alg 1 replaced four steps ffl search subspace v kgamma1 expanded approximate newton correction suitable basis constructed subspace ffl shadow space w k selected project original problem ffl projected problem 7 solved ffl solution updated represented steps 3e3h alg 3 minimal residual approach restricted minimal residual approach represented similar way 4 computational considerations section make comments implementation details mainly focus limiting computational work memory space 41 restart small k problems 79 small dimension may often solved relatively low computational costs eg variant newtons method larger k may become serious problem situation one may wish restrict subspaces v w subspaces smaller dimension see alg 3 step 3i approach limits computational costs per iteration may also negative effect speed convergence example simplest choice restricting search subspace 1dim subspace leads damped inexact newton methods instance damping parameter ff solution min ff kg k ffk 2 g k accelerated inexact newton schemes 7 1 choose initial approximation u 0 2 3 repeat u k accurate b compute residual r c compute approximation j k jacobian f 0 u k solve correction equation approximately compute approximate solution p k correction equation expand search space select v k spanv linearly independent v kgamma1 update f expand shadow space select w k linearly independent w kgamma1 update w g solve projected problem compute nontrivial solutions projected system update select k set solutions update approximation restart large select select theta 0 matrices r v rw compute suitable combinations columns algorithm 3 accelerated inexact newton course complete restart also feasible say mth step cf step 3i alg 3 disadvantage complete restart rebuild subspace information usually leads slower speed convergence seems like open door suggest parts subspaces better retained restart practical situations difficult predict parts meaningful choice would depend spectral properties jacobian well current approximation solving linear equations gmresr 29 good results reported 30 selecting number first last columns cf step 3i alg 3 eg 8 r fokkema g l g sleijpen h van der vorst 7 variant gmresr called gcro proposed implements another choice subspaces dimension l first l columns retained together combination last columns combination taken approximate solution induced minimal residual condition subspaces dimension l l 1 specific u k solves min replaced v k l v km km denoting 42 update update step step 3h alg 3 solution k projected problem selected set solutions selection may necessary since many nonlinear problems one solution sometimes may reason poor convergence inexact newton sequence approximate solutions wavers different exact solutions larger search subspaces search subspace may contain good approximations one solution may exploited steer sequence approximate solution wanted solution may help avoid wavering convergence behavior selection k based additional properties solution u k1 instance may look solution largest norm case eigenvalue problems solution one component close specific value instance one interested eigenvalues close say 0 ritz vector ritz value closest 0 chosen 43 projected problem even though problems small dimension solved relatively low computational costs step 3g alg 3 necessarily inexpensive projected problem embedded large subspace may require quite computational effort represent problem small subspace belongs dimension dimspanv k case linear equations linear eigenvalue problems computation theta matrix w products type problems many others well one may save computational costs reusing information previous iterations 44 expanding search subspace algorithm breaks search subspace expanded happens p k belongs span finite precision arithmetic angle p k subspace small similar gmres one may replace p k j k v v last column vector matrix v kgamma1 approximate solution correction equation breakdown also occur new residual r k equal previous residual r kgamma1 situation instead modifying expansion process iteration number k one may also take measures iteration number order avoid 29 steps lsqr suggested linear solver krylov subspace method may already cure stagnation 5 linear solvers fit framework section show wellknown iterative methods solution linear systems fit framework methods follow specific choices equivalent wellknown methods sense least exact arithmetic accelerated inexact newton schemes 9 produce basis vectors search spaces approximate solutions newton corrections sense gmres orthodir equivalent linear equation 2 equivalent one 1 j gammaa section denotes preconditioning matrix ie vector v gamma1 v easy compute approximates gamma1 v 51 gcr choice alg 3 without restart equivalent preconditioned gcr 31 52 fom gmres choice algorithms related fom gmres 26 additional choice w alg 3 fom choice gives algorithm equivalent gmres 53 gmresr taking w k w k perpendicular gcr taking p k approximate solution equation alg 3 equivalent gmresr algorithms 29 one might compute p k steps gmres instance 6 schemes mildly nonlinear problems section discuss numerical methods iterative solution generalized eigenproblem 3 show also fit general framework alg 3 methods already mentioned methods consist two parts one part approximate solution correction equation cf step 3d alg 3 used extend search space part solution projected problem cf step 3g alg 3 used construct update approximate solution start derivation suitable form newton correction equation generalized eigenproblem make comments solve projected problem correction equation order avoid complications go complex differentiation mainly focus numerical computation eigenvectors fixed component given direction rather computation eigenvectors fixed norm first let u fixed vector nontrivial component direction desired eigenvector x want compute approximations u k x normalized component udirection u select k residual r k orthogonal w w another fixed nontrivial vector ie approximate eigenvalue k given k w au k w bu k consider map f given f u u belongs hyperplane fy 2 c 1g jacobian j given r fokkema g l g sleijpen h van der vorst correction equation reads u equation equivalent u 0 gammar k p solution 10 p solution 11 projected problem generalized eigenvalue problem fortunate position solutions problems moderate size computed standard methods instance qz 19 method however apply methods reformulate projected problem exceptional position u k w f key reformulation observation methods consider affine subspace u k spanv k equal v k v k contains u k alternative step 3g alg 3 may also compute solutions problem solved instance qz method selecting k new approximation u k1 given u 61 arnoldis method consider simplified case ie standard eigenproblem one step krylov subspace method krylov dimension 1 solution correction equation 10 obtain correction hence note may poor approximation general r k 6 u approximate eigenvector u k belongs search subspace spanv expanding search subspace component p k orthogonal spanv kgamma1 equivalent expanding space orthogonal component au k would expansion vector arnoldis method hence search subspace precisely krylov subspace generated u 0 ap parently arnoldis method method inexact newton step without restart choice w corresponds standard one arnoldi produces called ritz values choice w leads harmonic ritz values 22 62 davidsons method arnoldis method davidsons method 6 also carries one step krylov subspace method solution correction equation however contrast arnoldis method davidson also incorporates preconditioner accelerated inexact newton schemes 11 suggests solve 10 approximately p k inverse diagonal gamma k b choices suggested well cf eg 5 21 preconditioner even search space simply krylov subspace generated u 0 may lead advantage davidsons method arnoldis method none choices preconditioner proper care taken projections see 10 preconditioner approximate inverse projected matrix see 10 map rather gamma k b however diagonal gamma b choose u w equal arbitrary standard basis vector davidson 6 note p diagonal r k w therefore particular choice w u diagonal may expected good preconditioner correction equation including projections cases good preconditioner gamma k b observe argument hold nondiagonal preconditioners 63 jacobidavidson davidson methods nondiagonal preconditioner take care properly projections correction equation 10 observation made 28 new algorithm proposed eigenproblems including projections davidson scheme addition modified schemes allow general approximate solutions p k instance use steps preconditioned krylov subspace method correction equation suggested leading arnoldi type methods variable polynomial preconditioning determined efficiently projections included correctly new methods called jacobidavidson methods jacobi took proper care projections build search subspace davidson see 28 details references analysis results 3 27 show jacobidavidson methods also effective solving generalized eigenproblems even without matrix inversion jacobidavidson methods allow variety choices may improve efficiency steps speed convergence good examples methods projected problem 7 used steer computation extensive discussion refer 27 7 schemes general nonlinear problems section summarize iterative methods solution nonlinear problems proposed different authors show methods fit framework brown saad 4 describe family methods solving nonlinear problems refer methods nonlinear krylov subspace projection methods 12 r fokkema g l g sleijpen h van der vorst modifications newtons method intended enhance robustness heavily influenced ideas presented 9 one methods variant damped inexact newton approximate solution correction equation steps arnoldi gmres determine damping parameter ff linesearch backtracking technique another scheme special 1dimensional subspace acceleration also propose model trust region approach take update approximation krylov subspace e vm generated steps preconditioned arnoldi gmres vm k point dogleg curve ky k k trust region size k approximation min vm could considered block version previous method 2 axelsson chronopoulos propose two nonlinear versions truncated generalized conjugate gradient type method methods fit frame work first method ngcg minimal residual method v k orthonormal words correction equation solved second method nngcg differs ngcg p k computed approximate solution method correction equation 6 accuracy nonincreasing sequence 8 method nngcg minimal residual method viewed generalization gmresr 29 certain conditions map f prove global convergence 15 kaporin axelsson propose class nonlinear equation solvers gnks ideas presented 4 2 combined direction vectors obtained linear combinations columns e vm v k precise problem solved special gaussnewton iteration scheme avoids excessive computational work taking account acute angle r k j k p k rate convergence method generalizes gcro 7 8 numerical experiments section test several schemes present results numerical experiments three different nonlinear problems tests test results methods linear eigenproblems refer references purpose presentation show acceleration may useful also nonlinear case useful mean additional computational cost compensated faster convergence different schemes distinguish way approximately solve correction equation projected problem cf section 32 7 overwhelming variety choices selected possible combinations lead schemes equivalent already proposed methods lead new methods compare following existing minimal residual schemes ffl linesearch backtracking linesearch technique 4 pp 458 ffl dogleg model trust region approach proposed 4 pp 462 ffl nngcg variant method proposed 2 solving 8 levenbergmarquardt algorithm 20 accelerated inexact newton schemes 13 ffl gnks method proposed 15 new restricted minimal residual schemes ffl rmr choosing w ffl rmr b choosing w component j k p k orthogonal w kgamma1 last two schemes minimization problem solved gaussnewton variant described 15 necessary subspaces direction p k projected problem obtained 10 steps gmres third example also 50 iterations generalized cgs variant cgs2 10 cases exact jacobian used furthermore used orthonormal matrices v k w k obtained modified gramschmidt process restricted last 10 columns attempt save computational work computations done sun sparc 20 double precision iterations stopped method failed either convergence slow ie number nonlinear iterations per step exceeded 200 since computational cost methods approximately proportional costs number function evaluations matrix multiplications following counters given tables ffl ni number nonlinear iterations ffl fe number function evaluations ffl mv number multiplications jacobian ffl pre number applications preconditioner ffl total sum fe mv pre 81 1d burgers equation first test problem consider following 1d burgers equation 14 x discretized spatial variable x finite differences 64 grid points time derivative used deltat denotes solution time ndeltat test solution u n computed initial guess u n1 took u n preconditioning used table tab 1 show results problem 12 14 r fokkema g l g sleijpen h van der vorst method ni fe mv total linesearch 594 1818 4853 6671 dogleg 644 3559 6140 9699 nngcg 229 4426 11769 16195 gnks 187 852 7067 7919 rmr 230 926 4471 5397 table 1 results burgers equation plot solutions given fig 1 table shows cumulative value counters method completing computation u look number nonlinear iterations ni see acceleration indeed reduces number however case gnks result less work number matrix multiplications mv increases much galerkin approaches rmr rmr b less expensive methods rmr winner x figure 1 solution burgers equation 82 bratu problem second test problem consider bratu problem 12 4 seek solution u nonlinear boundary value problem foromega took unit square discretized finite differences 31 theta 31 regular grid known cf 12 exist critical value problem 13 two solutions problem 13 solutions order locate critical value use arc length continuation method described 12 section 23 24 problem 13 replaced problem accelerated inexact newton schemes 15 method ni fe mv pre total linesearch 391 1013 3732 3421 8166 dogleg 381 2664 3010 3010 8684 nngcg 361 1297 4243 3091 8631 gnks 358 1056 6896 2780 10732 rmr 389 539 4005 3399 7943 table 2 results bratu problem solved arc length continuation method method ni fe mv pre total linesearch 29 85 336 308 729 dogleg gnks 38 119 1806 370 2295 rmr 6 13 77 55 145 table 3 single solve bratu problem u form scalar valued function chosen arc length solution branch u solution 13 preconditioned gmres ilu0 18 discretized laplace operator delta first table tab 2 shows results full continuation run starting smallest solution u solution branch followed along discretized arc see acceleration may useful spite fact little room average approximately 45 newton iterations necessary compute solution per continuation step example rmr b performs better rmr table tab 3 shows results case solve 13 fixed near critical value case galerkin acceleration even useful differences pronounced sup norm solution different values plotted fig 2 two solutions 4 along diagonal unit square shown fig 3 83 driven cavity problem section present test results classical driven cavity problem incompressible fluid flow follow closely r fokkema g l g sleijpen h van der vorst figure 2 sup norms solution u along arc figure 3 solutions u 4 along diagonal domain presentations 12 4 stream functionvorticity formulation equations n whereomega unit square viscosity reciprocal reynolds number terms alone written subject boundary conditions equation discretized finite differences 25 theta 25 grid see fig 4 grid lines distributed roots chebychev polynomial degree 25 preconditioner used modified 13 decomposition biharmonic operator starting solution computed several solutions using arc length continuation method cf previous example 12 step sizes deltas 100 0 tab 4 shows results test using 10 steps gmres cgs2 10 correction equation case cgs2 approximately solved correction equation relative residual norm precision 2 gammak k current newton step 8 maximum 50 steps clearly methods using basis produced steps gmres perform poorly example gnks able complete full continuation run requires large number newton steps look results schemes cgs2 used see except linesearch method methods perform much better restricted minimal residual methods efficient ones accelerated inexact newton schemes 17 gmres method ni fe mv pre total linesearch fails 400 total 545 dogleg fails 100 total 113 nngcg fails 2200 total 19875 gnks 641 2315 30206 6210 38731 rmr fails 2000 total 13078 rmr b fails 800 total 7728 method ni fe mv pre total linesearch fails 1300 total 4342 rmr 137 297 6266 5969 12532 table 4 results driven cavity problem solved arc length continuation method figure 4 grid driven cavity problem 25 theta 25 figure 5 stream lines driven cavity problem 100 test also reveals possible practical drawback methods like dogleg gnks methods exploit affine subspace find suitable update approximation may fail problem hard preconditioner good enough case dimension affine subspace must large may storage requirements computational overhead fea sible schemes use approximate solutions correction equation delivered arbitrary iterative method eg cgs2 one easily adapt precision leaves freedom r fokkema g l g sleijpen h van der vorst figure lines driven cavity problem 400 figure 7 stream lines driven cavity problem 1600 figure 8 stream lines driven cavity problem 2000 figure 9 stream lines driven cavity problem 3000 plots stream lines values 00 00025 0001 00005 00001 000005 cf 12 given fig 59 plots show virtually solutions 12 9 conclusions shown classical newton iteration scheme nonlinear problems accelerated similar way standard richardsontype iteration schemes linear equations leads framework accelerated inexact newton schemes 19 many wellknown iterative methods linear eigen general nonlinear problems fit framework overwhelming number possible iterations schemes formulated selected shown numerical experiments especially restricted minimal residual methods useful reducing computational costs r principle minimized iterations solution matrix eigenvalue problem nonlinear generalized conjugate gradient methods te riele hybrid krylov methods nonlinear systems equations davidson method iterative calculation lowest eigenvalues corresponding eigenvectors large real symmetric matrices nested krylov methods preserving orthogonality generalized conjugate gradient squared qmr quasi minimal residual method nonhermitian linear systems class first order factorizations methods partial differential equation u class nonlinear equation solvers based residual norm reduction sequence affine subspaces acceleration techniques decoupling algorithms semiconductor simulation solution systems linear equations minimized iteration iterative solution method linear systems coefficient matrix symmetric mmatrix algorithm generalized matrix eigenvalue problems generalizations davidsons method computing eigenvalues large nonsymmetric matrices approximate solutions eigenvalue bounds krylov subspaces krylov subspace method solving large unsymmetric linear systems gmres generalized minimum residual algorithm solving nonsymmetric linear systems jacobidavidson iteration method linear eigenvalue problems gmresr family nested gmres methods experiences gmresr generalized conjugategradient acceleration nonsymmetrizable iterative methods tr ctr keith miller nonlinear krylov moving nodes method lines journal computational applied mathematics v183 n2 p275287 15 november 2005 p r gravesmorris bicgstab vpastab adaptation mildly nonlinear systems journal computational applied mathematics v201 n1 p284299 april 2007 hengbin zeyao mo xingping liu choice forcing terms inexact newton method journal computational applied mathematics v200 n1 p4760 march 2007 hengbin zhongzhi bai globally convergent newtongmres method large sparse systems nonlinear equations applied numerical mathematics v57 n3 p235252 march 2007 monica bianchini stefano fanelli marco gori optimal algorithms wellconditioned nonlinear systems equations ieee transactions computers v50 n7 p689698 july 2001