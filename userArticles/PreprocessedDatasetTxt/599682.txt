metricbased methods adaptive model selection regularization present general approach model selection regularization exploits unlabeled data adaptively control hypothesis complexity supervised learning tasks idea impose metric structure hypotheses determining discrepancy predictions across distribution unlabeled data show metric used detect untrustworthy training error estimates devise novel model selection strategies exhibit theoretical guarantees overfitting still avoiding underfitting extend approach derive general training criterion supervised learningyielding adaptive regularization method uses unlabeled data automatically set regularization parameters new criterion adjusts regularization level specific set training data received performs well variety regression conditional density estimation tasks proviso methods sufficient unlabeled training data available b introduction supervised learning one takes sequence training pairs hx attempts infer hypothesis function prediction error errhx future test examples basic paradigm covers many tasks studied machine learning research including gression typically ir measure prediction error squared dierence similar loss classication typically small discrete set measure prediction error misclassication loss erry assume example classication label f0 1g probabilistic prediction 0 1 measure prediction error using log loss erry also known crossentropy error bis95 regardless specics scenarios one always faces classical overtting versus undertting dilemma supervised learning hypothesis chosen class complex data good chance exhibit large test error even though training error small occurs complex classes generally contain several hypotheses behave similarly training data yet behave quite dierently parts domainthus diminishing ability distinguish good hypotheses bad note signicantly different hypotheses cannot simultaneously accurate therefore one must restrict set hypotheses able reliably dierentiate accurate inaccurate predictors hand selecting hypotheses overly restricted class prevent one able express good approximation ideal predictor thereby causing important structure training data ignored since undertting overtting result large test error must avoided simultaneously tradeo overtting undertting fundamental dilemma machine learning statistics paper primarily interested investigating automated methods calibrating hypothesis complexity given training data techniques developed process fall one three basic categories model se lection regularization model combination model selection one rst takes base hypothesis class h decomposes discrete collection subclasses h 0 h 1 organized nested chain lattice given training data attempts identify optimal subclass choose nal hypothesis variety methods proposed choosing optimal subclass techniques fall one two basic categories complexity penalization eg minimum description length principle ris86 various statistical selection criteria fg94 holdout testing eg crossvalidation bootstrapping efr79 regularization similar model selection except one impose discrete decomposition base hypothesis class instead penalty criterion imposed individual hypotheses either penalizes parametric form eg ridge regression weight decay neural network training cm98 rip96 bis95 penalizes global smoothness properties eg minimizing curvature pg90 model combination methods select single hypothesis rather take weighted combination base hypotheses form composite predic tor composing base functions way eect smoothing erratic hypotheses eg bayesian model averaging mac92 bagging bre96 increasing representation power base hypothesis class linear combinations eg boosting fs97 neural network ensemble methods kv95 methods shown impressive improvements naive learning algorithms every area supervised learning research however one diculty techniques usually require expertise apply properly often involve free parameters must set informed practitioner paper introduce alternative methods model selection regularization attempt improve robustness standard ap proaches idea use unlabeled data automatically penalize hypotheses behave erratically labeled training set section 3 rst investigate unlabeled data used perform model selection nested sequences hypothesis spaces strategies develop shown experimentally outperform standard model selection methods proved robust theory section 4 consider regularization show proposed model selection strategies extended generalized training objective supervised learning idea use unlabeled data automatically tune degree regularization given task without set free parameters hand show resulting regularization technique adapts behavior given training set outperform standard xed regularizers given problem note however address model combination methods paper kv95 instead leaving future work work reported extends earlier conference papers sch97 ss00 metric structure supervised learning paper consider metric structure space hypothesis functions arises simple statistical model supervised learning problem assume examples hx yi generated stationary joint distribution p xy x learning hypothesis function primarily interested modeling conditional distribution p yjx however investigate utility using extra information marginal domain distribution p x choose good hypothesis note information p x obtained collection unlabeled training examples x often abundant supply many applicationsfor example text processing computer perception signicance information domain distribution p x denes natural pseudo metric space hypotheses two hypothesis functions f g obtain measure distance computing expected disagreement predictions z 1 erry natural measure prediction error problem hand eg regression classication associated normalization function recovers standard metric axioms specically interested obtaining metric properties nonnegativity df g 0 symmetry triangle inequality df g df dh g turns typical prediction error functions admit metric type example regression measure distance two prediction functions z normalization function establishes metric proper ties classication measure distance two classiers z normalization required achieve metric conditional density estimation one measure distance two conditional probability models kullbackleibler divergence technically metric nevertheless supplies useful measure ct91 cases resulting distances eciently calculated making single pass list unlabeled examples importantly denitions generalized include target conditional distribution analogous manner z z 2 interpret true error hypothesis function h respect target conditional p yjx distance h p yjx signicance denition consistent previous denition 1 therefore embed entire supervised learning problem common metric space structure illustrate regression denition 2 yields root mean squared error hypothesis z z classication gives true misclassication probability z z conditional probability modeling gives expected log lossor kldivergence p yjx yields useful measure although metric together denitions 1 2 show impose global metric space view supervised learning problem figure 1 given labeled training examples hx goal nd hypothesis h space h closest target conditional p yjx distance measure 2 also given large set auxiliary unlabeled exam figure 1 ples x 0 r also accurately estimate distances alternative hypotheses f g within h eectively giving us 1 r r suciently large r distances dened 3 close distances dened 1 however distances hypotheses target conditional p yjx 2 weakly estimated using presumably much smaller set labeled training data need close 2 challenge approximate closest hypothesis target conditional accurately possible using available information 3 4 place true distances 1 2 use metric space perspective devise novel model selection regularization strategies exploit interhypothesis distances measured auxiliary set unlabeled examples approach applicable supervised learning problem admits reasonable metric structure particular strategies expressed terms generic distance measure depend aspects problem however sake concreteness focus regression source demonstration problems initially return classication conditional density estimation examples near end paper 3 model selection rst consider process using model selection choose appropriate level hypothesis complexity data conceptually simplest approach automatic complexity control supervised learning idea stratify hypothesis class h sequence lattice nested subclasses h 0 h 1 training data somehow choose class proper complexity given data understand one might make choice note given training sample principle obtain corresponding sequence empirically optimal functions h problem select one functions based observed training errors however errors monotonically decreasing assuming fully optimize class therefore choosing function smallest training error inevitably leads overtting trick invoke criterion beyond figure 2 mere empirical error minimization make nal selection mentioned two basic model selection strategies currently predomi nate complexity penalization holdout testing however neither approaches attends metric distances hypotheses oer obvious way exploit auxiliary unlabeled data adopting metric space view section 2 obtain useful new perspective model selection setting chain h 0 h 1 h interpreted sequence hypothesis spaces wherein measure distance candidate hypotheses using unlabeled data unfortunately still cannot directly measure distances hypotheses target conditional p yjx therefore must estimate based small labeled training sample however exploit fact distances functions sequence hence attempt use additional information make better choice figure 2 31 rst intuition explore interhypothesis distances help us detect overtting simple manner consider two hypotheses h k h k1 small estimated distance p yjx yet large true distance situation clear concerned selecting second hypothesis true distance h k h k1 indeed large functions cannot simultaneously close p yjx simple geometry implies least one distance estimates p yjx must inaccurate know intuitively trust earlier estimate latter since h k1 chosen larger class fact really accurate estimates would satisfy triangle inequality known distance dh k h k1 since empirical distances eventually become signicant underestimates general h explicitly chosen minimize empirical distance labeled training set triangle inequality provides useful test detect estimates become inaccurate fact basic test forms basis simple model selection strategy tri works surprisingly well many situations figure 3 figure 3 32 example polynomial regression demonstrate method subsequent methods develop paper rst consider problem polynomial curve tting supervised learning problem goal minimize squared prediction error erry specically consider polynomial hypotheses natural stratication polynomials degree 0 etc motivation studying task classical wellstudied problem still attracts lot interest cmv97 grv96 vap96 moreover polynomials create dicult model selection problem strong tendency produce catastrophic overtting eects figure 4 another benet polynomials interesting nontrivial class ecient techniques computing best hypotheses figure 4 apply metric based approach task dene metric terms squared prediction error erry square root normalization discussed section 2 evaluate ecacy tri problem compared performance number standard model selection strategies including structural risk minimization srm cmv97 vap96 ric fg94 sms shi81 gcv cw79 bic sch78 aic aka74 cp mal73 fpe aka70 also compared 10fold cross validation cvt standard holdout method efr79 wk91 koh95 conducted simple series experiments xing domain distribution p x xing various target functions f specic target functions used experiments shown figure 5 generate training samples rst drew sequence values computed target function values fx 1 fx added independent gaussian noise obtain labeled training sequence given training sample computed series figure 5 best polynomials h etc given sequence model selection strategy choose hypothesis h k basis observed empirical errors implement tri gave access auxiliary unlabeled examples x 0 r order compute true distances polynomials sequence main emphasis experiments minimize true distance nal hypothesis target conditional p yjx primarily concerned choosing hypothesis obtains small prediction error future test examples independent complexity level 1 determine eectiveness various selection strategies therefore measured ratio true error distance polynomial selected best true error among polynomials sequence h etc means optimum achievable ratio 1 rationale wish measure model selection strategys ability approximate best hypothesis given sequencenot nd better function outside sequence 2 table 1 shows results obtained approximating step function corrupted gaussian noise strategy adj tables explained section 33 obtained results repeatedly generating training samples xed size recording approximation ratio achieved strategy tables record dis table 1 tribution ratios produced strategy training sample sizes respectively using unlabeled examples measure interhypothesis distancesrepeated 1000 trials initial results appear quite positive tri achieves median approximation ratios 106 prediction error criteria one could imagine optimizing model selec tion example one could interested nding simple model underlying phenomenon gives insight fundamental nature rather simply producing function predicts well future test examples hc96 however focus traditional machine learning goal minimizing prediction error one could consider elaborate strategies choose hypotheses outside sequence eg averaging several hypotheses together kv95 os96 bre96 however mentioned pursue idea paper 108 training sample sizes 20 30 respectively compares favorably median approximation ratios 139 154 achieved rm 117 achieved cvt cases remaining complexity penalization strategies gcv fpe etc performed signicantly worse trials however notable dierence tris robustness overtting fact although penalization strategy srm performed reasonably well much time prone making periodic catastrophic overtting errors even normally wellbehaved crossvalidation strategy cvt made signicant overtting errors time time evidenced fact 1000 trials training sample size table 1 tri produced maximum approximation ratio 218 whereas cvt produced worst case approximation ratio 643 penalization strategies srm gcv produced worst case ratios percentiles tri 145 cvt 611 srm 419 gcv fact tris robustness overtting surprise one prove tri cannot produce approximation ratio greater 3 make two simple assumptions tri makes best hypothesis hm sequence ii empirical error hm underesti mate note second assumption likely hold choosing hypotheses explicitly minimizing proposition 1 let hm optimal hypothesis sequence h hypothesis selected tri ii proof consider hypothesis h n follows hm sequence assume show h n must fail triangle test 5 hm therefore tri select h n first notice initial assumption h n error along triangle inequality imply 3dh 3 although one might suspect large failures could due measuring relative instead absolute error turns large relative errors also correspond large absolute errorswhich verify section 41 recall since training errors monotonically decreasing also assumption therefore dh contradicts 5 thus tri consider h n finally since h cannot precede hm assumption h must satisfy note proposition 1 well propositions 2 3 implicitly assume true interhypothesis distances dh principle must measured unlimited amounts unlabeled data discuss relaxing assumption section 34 continuing experimental investigation nd basic avor results remains unchanged dierent noise levels different domain distributions p x fact much stronger results obtained wider tailed domain distributions like gaussian table 2 dicult target functions like sin1x table 3 complexity penalization table 2 table methods srm gcv etc forced regime constant catas trophe cvt noticeably degrades yet tri retains similar performance levels shown table 1 course results might due considering pathological target function perspective polynomial curve tting therefore important consider natural targets might better suited polynomial approximation fact repeating previous experiments benign target function dierent results table 4 shows procedure tri fare well caseobtaining median approximation ratios 311 351 training sample sizes 20 respectively compared 133 103 srm 137 116 cvt closer inspection tris behavior reveals table 4 reason performance drop tri systematically gets stuck low evendegree polynomials cf table 6 fact simple geometric explanation evendegree polynomials degree give reasonable ts sin 2 2x whereas odddegree ts tail wrong direction creates signicant distance successive polynomials causes triangle inequality test fail even odd degree ts even though larger evendegree polynomials give good approximation therefore although metricbased tri strategy robust overtting prone systematic undertting seemingly benign cases similar results obtained tting fth degree target polynomial corrupted level gaussian noise table 5 problem demonstrates rst assumption used proposition 1 violated natural situations see table 6 consideration table 5 table diculty leads us develop reformulated procedure 33 strategy 2 adjusted distance estimates nal idea explore model selection observe actually dealing two metrics true metric dened joint distribution p xy empirical metric determined labeled training sequence note previous model selection strategy tri ignored fact could measure empirical distance hypotheses labeled training data well measure true distance dh k h unlabeled data however fact measure interhypothesis distances actually gives us observable local vicinity exploit observation attempt derive improved model selection procedure given two metrics consider triangle formed two hypotheses h k h target conditional p yjx figure 6 notice six distances involvedthree real three estimated true distances p yjx two care yet two however exploit observed figure 6 relationship adjust empirical training error estimate fact one could rst consider simplest possible adjustment based naive assumption observed relationship metrics h k h also holds h p yjx note actually case would obtain better estimate dh simply rescaling training distance according observed ratio since expect underestimate general expect ratio larger 1 fact adopting simple heuristic obtain another model selection procedure adj also surprisingly eective figure 7 simple procedure overcomes undertting problems associated tri yet retains much tris robustness overtting figure 7 although rst glance procedure might seem ad hoc turns one prove overtting bound adj analogous established tri particular assume adj makes best hypothesis hm sequence ii adjusted error estimate underestimate adj cannot overt factor much greater 3 proposition 2 let hm optimal hypothesis sequence h let h hypothesis selected adj ii proof denition adj since adj selects h favor hm show implies bound test error dh terms optimum available test error first triangle inequality dh well note denition adj since yields 9 8 assumption obtain simple algebraic manipulation shows respect adj exhibit robustness overtting also weak theoretical guarantee undertting make assumptions empirical distance estimates underestimates ii adjusted distance estimates strictly increase empirical distance estimates true error successor hypothesis hm improves true error predecessors h signicant factor hm selected lieu predecessors proposition 3 consider hypotheses hm assume 0 dh suciently small follows therefore adj choose predecessor lieu hm proof triangle inequality recall denition b b 0 specically leading largest therefore applying 11 particular obtain second step follows assumption fact dh applying 10 occurrences dh since assumption assumption ii therefore although adj might originally appeared well motivated possesses worst case bounds overtting tting cannot established conventional methods however bounds remain somewhat weak table 6 shows adj tri systematically undert experiments even though assumption ii proposition 1 almost always satised expected assumption ii proposition 2 true one quarter time therefore propositions 1 2 provide loose characterization quality methods however metricbased procedures remain robust overtting demonstrate adj indeed eective repeated previous experiments adj new competitor results show adj robustly outperformed standard complexity penalization holdout methods cases consideredspanning wide variety target functions noise levels domain distributions p x tables 15 show previous data along performance characteristics adj particular tables 4 5 6 show adj avoids extreme undertting problems hamper tri appears responsively select high order approximations supported data moreover tables 13 show adj still extremely robust overtting even situations standard approaches make catastrophic errors overall best model selection strategy observed polynomial regression tasks even though possesses weaker guarantee overtting tri note model selection procedures propose add little computational overhead traditional methods since computing interhypothesis distances involves making single pass reference list unlabeled examples advantage standard holdout techniques like cvt repeatedly call hypothesis generating mechanism generate pseudohypothesesan extremely expensive operation many applications finally note adj possesses subtle limitation multiplicative rescaling employs cannot penalize hypotheses zero training error therefore limit degree polynomials 2 experiments avoid null training errors however despite shortcoming adj procedure turns perform well practice often outperforms straightforward tri strategy 34 robustness unlabeled data moving regularization brie investigate robustness model selection techniques limited amounts auxiliary unlabeled data principle one always argue preceding empirical results useful metricbased strategies tri adj might require signicant amounts unlabeled data perform well practice however 200 unlabeled examples used previous experiments seem onerous fact previous theoretical results propositions assumed innite unlabeled data explore issue robustness limited amounts unlabeled data repeated previous experiments gave tri adj small auxiliary sample unlabeled data estimate interhypothesis distances experiment found strategies actually quite robust using approximate distances table 7 shows small numbers unlabeled examples still sucient tri adj perform nearly well moreover table 7 shows techniques seem signicantly degrade consider fewer unlabeled labeled training examples robustness observed table 7 across range problems considered fact straightforward exercise theoretically analyze robustness procedures tri adj approximation errors estimated interhypothesis distances model selection sequence h kk 12 pairwise distances need estimated unlabeled data means straightforward union bound combined standard uniform convergence results ab99 obtain r error bar estimates 1 condence level error bars could easily used suitably adjust propositions 13 account estimation errors however pursue analysis since straightforward unrevealing although empirical results section anecdotal paper suf97 pursues systematic investigation robustness procedures reaches similar conclusions also based articial data rather present detailed investigation model selection strategies serious case studies rst consider improvement basic method regularization one diculties model selection generalization behavior depends specic decomposition base hypothesis class one con siders dierent decompositions h lead dierent outcomes avoid issue extend previous ideas general training criterion uses unlabeled data decide penalize individual hypotheses global space h main contribution section simple generic training objective applied wide range supervised learning problems continuing assume access sizable collection unlabeled data use globally penalize complex hypotheses specically formulate alternative training criterion measures behavior individual hypotheses labeled unlabeled data intuition behind criterion simpleinstead minimizing empirical training error alone addition seek hypotheses behave similarly labeled training data objective arises observation hypothesis ts training data well behaves erratically labeled training set likely generalize unseen examples detect erratic behavior measure distance hypothesis exhibits xed origin function chosen arbitrarily data sets hypothesis behaving erratically labeled training set likely distances disagree eect demonstrated figure 8 two large degree polynomials labeled training data well dier dramatically true error dierences training set distance simple origin function note figure 8 use trivial origin functions throughout section zero function constant function mean labels formulate concrete training objective rst propose following tentative measures empirical training error plus additive penalty empirical error times multiplicative penalty case compare behavior candidate hypothesis h xed origin thus cases seek minimize empirical training error times penalty measures discrepancy distance origin labeled training data distance origin unlabeled data regularization eect criteria illustrated figure 8 somewhat surprisingly found multiplicative objective 13 generally performs much better 12 harshly penalizes discrepancies training set behavior therefore form adopt although training criteria might appear ad hoc entirely unprincipled one useful property origin function happens equal target conditional p yjx minimizing 12 13 becomes equivalent minimizing true prediction error however despite utility technique turns initial training objectives inherent drawback subtly bias nal hypotheses towards origin function 12 13 allow minima articially large origin distances labeled data simultaneously small distances unlabeled data example illustrated figure 8 hypothesis function g minimizes 13 clearly attracted origin right end domain labeled training data course bias towards desirable happens near target conditional p yjx sense could serve useful prior hypotheses however reason expect anywhere near p yjx practice especially considering trivial constant functions used paper nevertheless intuitive way counter diculty avoid bias towards introduce symmetric forms previous criteria also penalize hypotheses unnaturally close origin labeled data one could consider symmetrized form additive penalty 12 well symmetrized form multiplicative penalty 13 penalties work directions hypotheses much origin training data penalized hypotheses signicantly closer origin training data rationale behind symmetric criterion types erratic behavior indicate observed training error likely unrepresentative ection hypothesiss true error value intuition demonstrated figure 9 hypothesis f minimizes symmetric criterion 15 drawn towards origin inappropriately thereby achieves smaller true prediction error hypothesis g minimizes 13 figure 9 symmetric training criteria also given technical justi cation first origin function happens equal target conditional minimizing either 14 15 comes close minimizing true prediction error dh p yjx see multiplicative criterion 15 let h hypothesis achieves minimum note criterion becomes equivalent criterion becomes equivalent dh p yjx r 2 latter case since h minimizes 15 must dh p yjx bayes optimal hypothesis h since h directly optimized training set remains xed usually means dh p yjx tend close dh p yjx thus minimizing 15 result near optimal generalization performance scenario note property would hold naively smoothed versions objective general case origin match target symmetric criteria also still provably penalize hypotheses small training error large test error see 15 note hypothesis h triangle inequality since p yjx optimized training set expect sizes thus 16 shows greater k p yjx k 3 hs training error must penalized signicant ratio least k 1 contrast alternative hypothesis g achieves comparable training error yet exhibits balanced behavior labeled training set strongly preferred fact g cannot overt amount h without violating 16 importantly bayes optimal hypothesis h also tend depend training set thus h typically achieve small value objective force hypothesis large overtting error relative p yjx exhibit objective value greater minimum note sensitivity lower bound 16 clearly depends distance origin target origin far target lower bound weakened criterion 15 becomes less sensitive overtting however experiments show objective unduly sensitive choice long far data fact even simple constant functions generally suce 4 outcome new regularization procedure uses training objective 15 penalize hypotheses based given training data unlabeled data resulting procedure eect uses unlabeled data automatically set level regularization given problem goal apply new training objective various hypothesis classes see regularizes eectively across dierent data sets demonstrate several classes however regularization behavior even sub tler since penalization factor 15 also depends specic labeled training set consideration resulting procedure regularizes data dependent way procedure adapts penalization particular set observed data raises possibility outperforming regularization scheme keeps xed penalization level across dierent training samples drawn problem fact demonstrate improvement achieved realistic hypothesis classes real data sets 41 example polynomial regression rst supervised learning task consider polynomial regression problem considered section 32 regularizer introduced 15 turns perform well problems case training 4 one could easily imagine trying complex origin functions low dimensional polynomials smooth interpolant functions explore ideas paper primarily wished emphasize robustness method even simple choices origin however one extension investigate use set origin functions penalize according maximum ratiobut yield signicant improvements objective expressed choosing hypothesis minimize set labeled training data fhx j ig r j1 set unlabeled examples xed origin usually set constant function mean labels note training objective seeks hypotheses labeled training data well simultaneously behaving similarly labeled unlabeled data test basic eectiveness approach repeated experiments section 32 rst class methods compared model selection methods considered 10fold cross validation cvt structural risk minimization srm cmv97 ric fg94 sms shi81 gcv cw79 bic sch78 aic aka74 cp mal73 fpe aka70 metric based model selection strategy adj introduced section 33 ever since none statistical methods ric sms gcv bic aic cp fpe performed competitively experiments report results gcv performed best among comparison also report results optimal model selector opt makes oracle choice best available hypothesis given model selection sequence experiments model selection methods considered polynomials degree 0 2 5 second class methods compared regularization methods consider polynomials maximum degree 2 penalize individual polynomials based size coecients smoothness properties specic methods considered standard form ridge penalization weight decay places penalty k 2 polynomial coecients k cm98 bayesian maximum posteriori inference zeromean gaussian priors polynomial coecients k diagonal covariance matrix mac92 6 methods require regularization parameter set hand refer methods reg map respectively 5 note restricted degree less 1 prevent maximum degree polynomials achieving zero training error discussed section 3 destroys regularization eect multiplicative penalty 6 test elaborate approach bayesian learning polynomials described you77 test ability technique automatically set regularization level tried range fourteen regularization parameters xed regularization methods reg map comparison purposes also report results oracle regularizers reg map select best value training set experiments conducted repeating experimental conditions section 32 specically table 8 repeats table 1 tting step function table 9 repeats table 3 tting sin1x table repeats table 4 tting sin 2 2x table 11 repeats table 5 tting fth degree polynomial regularization criterion based table 8 table table table minimizing 15 listed ada gures adaptive regular ization 7 also tested ada using dierent origin functions 2 max 4 8 max examine robustness also tested onesided version ada 13 verify benets symmetrized criterion 15 13 results quite positive rst observation model selection methods generally fare well regularization techniques problems model selection seems prone making catastrophic overtting errors polynomial regression problems whereas regularization appears retain robust control noted even frequently trusted 10fold cross validation procedure cvt fare well experiments model selection strategy perform reasonably well besides oracle model selector opt metricbased method adj also exploits unlabeled data new adaptive regularization scheme ada performed best among procedures experiments tables 811 show outperforms xed regularization strategies reg map xed choices regularization parameter even though optimal choice varies across problems map inferior reg experiments therefore report detailed results demonstrates ada able eectively tune penalization behavior problem hand moreover since outperforms even best choice data set ada also demonstrates ability adapt penalization behavior specic 7 used standard optimization routine matlab 53 fminunc determine co ecients minimize 14 15 although nondierentiability 15 creates diculty optimizer prevent reasonable results achieved another potential problem could arise h gets close origin however since chose simple origins never near p yjx h drawn near experiments thus resultant numerical instability arise training set given problem fact ada competitive oracle regularizers reg map experiments even outperformed oracle model selection strategy opt two problems clear ada fairly robust choice since moving distant constant origin even eight times max value completely damage performance results also show onesided version ada based 13 inferior symmetrized version experiments conrming prior expectations 42 example radial basis function regression test approach realistic task considered problem regularizing radial basis function rbf networks regression rbf networks natural generalization interpolation spline tting tech niques given set prototype centers c 1 c k rbf representation prediction function h given euclidean distance x center c g response function width parameter experiment use standard local gaussian basis function fitting rbf networks straightforward simplest approach place prototype center training example determine weight vector w allows network training labels best weight vector obtained solving w in6 6 6 4 kx1 x1k g g solution guaranteed exist unique distinct training points natural basis functions g including gaussian basis used bis95 although exactly tting data rbf networks natural problem generally overts training data process replicating labels many approaches therefore exist regularizing rbf networks however techniques often hard apply involve setting various free parameters controlling complex methods choosing prototype centers etc cm98 bis95 simplest regularization approaches add ridge penalty weight vector minimize h given 17 cm98 alternative approach add nonparametric penalty curvature pg90 resulting procedure similar apply methods practice one make intelligent choice width parameter regularization parameter un fortunately choices interact often hard set hand without extensive visualization experimentation data set section investigate eectively ada regularizer able automatically select width parameter regularization parameter rbf network real regression problems basic idea use unlabeled data make choices automatically adaptively compare ada 15 large number ridge regularization procedures corresponding penalty 18 dierent xed choices thirty total apply ada case simply ran standard optimizer parameter space explicitly solving w vector minimizes 18 choice involves solving linear system cm98 bis95 thus given w could calculate supply resulting value optimizer objective minimized cf footnote 7 conduct experiment investigated number regression problems statlib uci machine learning repositories 8 ex periments data set randomly split training 110 unlabeled 710 test set 210 methods run split repeated random splits 100 times obtain results tables 1215 show ada regularization able choose width regularization parameters achieve eective generalization performance across table 12 table table table range data sets ada performs better xed regularizer every problem except bodyfat even beats oracle regularizer reg one problem shows adaptive criterion 8 urls libstatcmuedu wwwicsuciedumlearnmlrepositoryhtml eective choosing good regularization parameters given prob lem choose adaptively based given training data yield improvements xed regularizers 5 classication finally note regularization approach developed paper also easily applied classication conditional density estimation problems conditional density estimation one use kl divergence proxy distance measure still achieve interesting results however report experiments classication label set usually small discrete set measure prediction error misclassication loss erry distances measured disagreement probability df gx using metric generic regularization objective 15 directly applied classication problems fact applied 15 problem decision tree pruning classication obtaining results shown table 16 unfortunately results achieved experiment strong appears techniques proposed paper may work decisively classication problems regression conditional density estimation problems table believe weakness proposed methods classication might intuitive explanation however since classication functions essentially histogramlike ie piecewise constant limit ability unlabeled data detect erratic behavior labeled training sample histograms across large regions tend behave similarly large neighborhoods around training pointsto extent distances labeled unlabeled data points often similar even complex histograms coping apparent limitation approach remains grounds future research 6 conclusion introduced new approach classical complexitycontrol problem based exploiting intrinsic geometry function learning task new techniques seem outperform standard approaches wide range regression problems primary source advantage proposed metricbased strategies able detect dangerous situations avoid making catastrophic overtting errors still responsive enough adopt reasonably complex models supported data accomplish attending real distances hypotheses standard complexitypenalization strategies completely ignore information holdout methods implicitly take information account indirectly less eectively metricbased strategies introduced although free lunch general sch94 cannot claim obtain universal improvement every complexitycontrol problem sch93 claim one able exploit additional information task knowledge p x obtain signicant improvements across wide range problem types conditions empirical results regression support view substantial body literature investigated unlabeled data context supervised learning although way considered paper work area adopts perspective parametric probability modeling uses unlabeled data part maximum likelihood em discriminative training procedure mu97 cc96 rv95 gs91 on78 another common idea supply articial labels unlabeled examples use data directly supervised training procedure bm98 tow96 unlabeled examples also used construct cover hypothesis space improve worst case bounds generalization error lp96 however none previous research explicitly uses unlabeled data automated complexity control perhaps closest work spirit kv95 uses unlabeled examples calculate optimal combination weights ensemble regressors emphasis kv95 model combination rather model selection regulariza tion nevertheless appears close relationship ideas important direction future research develop theoretical support strategiesin particular stronger theoretical justication regularization methods proposed section 4 improved analysis model selection methods proposed section 3 remains open whether proposed methods tri adj ada fact best possible ways exploit hypothesis distances provided p x plan continue investigating alternative strategies could potentially eective regard example remains future work extend multiplicative adj ada methods cope zero training errors finally would interesting adapt approach model combination methods extending ideas kv95 combination strategies including boosting fs97 bagging bre96 acknowledgements research supported nserc mitacs cito bul thanks yoshua bengio adam grove rob holte john laerty joel martin john platt lyle ungar jason weston anonymous referees helpful comments various stages research r neural network learning theoretical foundations statistical predictor information new look statistical model identi neural networks pattern recognition combining labeled unlabeled data cotraining bagging predictors relative value labeled unlabeled samples pattern recognition unknown mixing pa rameter learning data concepts comparison vc method classical methods model selection elements information theory smoothing noisy data spline func tions computers theory statistics thinking unthinkable risk ation criterion multiple regression decisiontheoretic generalization online learning application boosting applications model selection techniques polynomial approximation comparison scienti study crossvalidation bootstrap accuracy estimation model selection neural network ensembles bayesian interpolation comments c p mixture experts classi generating accurate diverse members neuralnetwork ensemble regularization algorithms learning equivalent multilayer networks pattern recognition neural networks stochastic complexity modeling learning mixture labeled unlabeled examples parametric side information estimating dimension model new metricbased approach model selec tion optimal selection regression variables adaptive regularization criterion supervised learning characterizing generalization performance model selection strategies using unlabeled data supervised learning nature statistical learning theory computer systems learn bayesian approach prediction using polynomials tr ctr yoshua bengio nicolas chapados extensions metric based model selection journal machine learning research 3 312003 antonio bahamonde gustavo f bayn jorge dez jos ramn quevedo oscar luaces juan jos del coz jaime alonso flix goyache feature subset selection learning preferences case study proceedings twentyfirst international conference machine learning p7 july 0408 2004 banff alberta canada