inducing features random fields abstractwe present technique constructing random fields set training samples learning paradigm builds increasingly complex fields allowing potential functions features supported increasingly large subgraphs feature weight trained minimizing kullbackleibler divergence model empirical distribution training data greedy algorithm determines features incrementally added field iterative scaling algorithm used estimate optimal values weights random field models techniques introduced paper differ common much computer vision literature underlying random fields nonmarkovian large number parameters must estimated relations learning approaches including decision trees given demonstration method describe application problem automatic word classification natural language processing b introduction paper present method incrementally constructing random fields method builds increasingly complex fields approximate empirical distribution set training examples allowing potential functions features supported increasingly large subgraphs feature assigned weight weights trained minimize kullbackleibler divergence field empirical distribution training data features incrementally added field using topdown greedy algorithm intent capturing salient properties empirical sample allowing generalization new configurations general problem methods propose address discovering structure inherent set sample patterns one fundamental aims statistical inference learn ing problem central wide range tasks including classification compression prediction illustrate nature approach suppose wish automatically characterize spellings words according statistical model application develop section 5 field features simply uniform distribution ascii strings take distribution string lengths given conspicuous feature english spellings commonly comprised lowercase letters induction algorithm makes observation first constructing field e indicator function weight agammaz associated feature character lowercase chosen approximately 1944 means string lowercase letter position 7 e 1944 times likely stephen vincent della pietra renaissance technologies stony brook ny 11790 email sdellavdellarenteccom john lafferty computer science department school computer science carnegie mellon university pittsburgh pa 15213 email laffertycscmuedu string without lowercase letter position following collection strings generated resulting field gibbs sampling examples shown sample generated annealing concentrate distribution probable strings r xevo ijjiir b jz gsr wq vf x ga msmgh pcp ozivlal hzagh yzop io advzmxnv ijvbolft x emx kayerf mlj rawzyb jp ag ctdnnnbg wgdw kguv cy spxcq uzflbbf dxtkkn cxwx jpd ztzh lv zhpkvnu l r qee nynrx atze4n ik se w lrh hp yrqykah zcngotcnx igcump zjcjs lqpwiqu cefmfhc lb fdcy tzby yopxmvk fz govyccm ijyiduwfzo 6xr duh ejv pk pjw l fl w second important feature according algorithm two adjacent lowercase characters extremely common secondorder field becomes e weight agammazagammaz associated adjacent lowercase letters approximately 180 first 1000 features algorithm induces include strings ly ing character denotes beginningofstring character denotes endof string addition first 1000 features include regular expressions weight 915 azaz weight gamma581 addition first two features az azaz set strings obtained gibbs sampling resulting field shown reaser homes thing reloverated ther conists fores anditing mr proveral ont prolling prothere mento yaou 1 chestraing intrally qut best compers cluseliment uster deveral thise offect inatever thifer constranded stater vill thase youse menttering verate examples discussed detail section 5 induction algorithm present two parts feature selection parameter estimation greediness algorithm arises feature selection step feature pool candidate features evaluated estimating reduction kullbackleibler divergence would result adding feature field reduction approximated function single parameter largest value function called gain candidate approximation one key elements approach making practical evaluate large number candidate features stage induction algorithm candidate largest gain added field parameter estimation step parameters field estimated using iterative scaling algorithm algorithm use new statistical estimation algorithm ieee transactions pattern analysis machine intelligence vol 19 4 april 1997 call improved iterative scaling improvement generalized iterative scaling algorithm darroch ratcliff 12 require features sum constant improved algorithm easier implement darroch ratcliff algorithm lead increase rate convergence increasing size step taken toward maximum iteration section 4 give simple selfcontained proof convergence improved algorithm make use kuhntucker theorem machinery constrained optimization moreover proof rely convergence alternating iprojection csiszars proof 10 darrochratcliff procedure feature selection step parameter estimation step require solution certain algebraic equations whose coefficients determined expectation values respect field many applications expectations cannot computed exactly involve sum exponentially large number configurations true application develop section 5 cases possible approximate equations must solved using monte carlo techniques compute expectations random variables application present uses gibbs sampling compute expectations resulting equations solved using newtons method method viewed terms principle maximum entropy 19 instructs us assume exponential form distributions parameters viewed lagrange multipliers techniques develop paper apply exponential models general formulate approach terms random fields provides convenient framework within work main application naturally cast terms method differs common applications statistical techniques computer vision natural language processing contrast many applications computer vision involve free parameters typical application method involves estimation thousands free parameters addition methods apply general exponential models random fieldsthere underlying markov assumption made contrast statistical techniques common natural language processing typical applications method probabilistic finitestate pushdown automaton statistical model built following section describe form random field models considered paper general learning algorithm section 3 discuss feature selection step algorithm briefly address cases equations need estimated using monte carlo methods section 4 present improved iterative scaling algorithm estimating parameters prove convergence algorithm section 5 present application inducing features spellings finally section 6we discuss relation methods learning approaches well possible extensions method ii learning paradigm section present basic algorithm building random field elementary features basic idea incrementally construct increasingly detailed field approximate reference distribution p typically distribution p obtained empirical distribution set training examples establishing notation defining form random field models consider present training problem statement two equivalent optimization problems discuss notions candidate feature gain candidate finally give statement induction algorithm form random field models finite graph vertex set v edge set e let finite alphabet configuration space w set labelings vertices v letters c ae v 2 w configuration c denotes configuration restricted c random field g probability distribution w set random fields nothing simplex probability distributions w f support f written suppf smallest vertex subset c ae v property c consider random fields given gibbs distributions form e functions suppv c c field markov whenever vc 6 0 c clique totally connected subset v property expressed terms conditional probabilities e u v arbitrary vertices assume c pathconnected subset v 1g say values c parameters field functions f c features field following often convenient use notation disregards dependence features parameters vertex subset c expressing field e every random field e v f form field markovian obtained completing edge set e ensure subgraph generated vertex subset totally connected impose constraint two parameters say parameters tied j tied write nonbinary feature general collapse number tied parameters onto single parameter della pietra della pietra lafferty inducing features random fields 3 associated nonbinary feature tied parameters often natural particular problem presence nonbinary features generally makes estimation parameters difficult automorphism oe graph permutation vertices takes edges edges u v 2 e oeu oev 2 e random field e v f said homogeneous features feature f automorphism oe graph feature f j f j w addition field said homogeneous roughly speaking homogeneous feature contributes weight distribution matter graph appears homogeneous features arise naturally application section 5 methods describe paper apply exponential models general essential underlying graph structure however convenient express approach terms random field models described b two optimization problems suppose given initial model q 0 2 reference distribution p set features practice often case p empirical distribution set training samples 1 thus given c number times configuration appears among training samples wish construct probability distribution q 2 accounts data sense approximates deviate far q 0 measure distance probability distributions p q using kullbackleibler divergence p log p throughout paper use notation expectation function g w r respect probability distribution p function h w r distribution q use notation h ffi q q h denote generalized gibbs distribution given note z q h usual partition function normalization constant determined requirement h ffi q sums 1 written expectation two natural sets probability distributions determined data first set pf p distributions agree p expected value feature function f second set qf q 0 generalized gibbs distributions based q 0 feature function f let closure qf q 0 respect topology inherits subset euclidean space two natural criteria choosing element q sets ffl maximum likelihood gibbs distribution choose q distribution likelihood respect p ffl maximum entropy constrained distribution choose q distribution pf p maximum entropy relative q although criteria different determine moreover distribution unique element intersection pf discuss detail section 41 appendix p empirical distribution set training examples equivalent maximizing probability field p assigns training data given sufficiently many parameters simple matter construct field arbitrarily small classic problem training idea behind method proposed paper incrementally construct field captures salient properties incorporating increasingly detailed collection features allowing generalization new configurations resulting distributions absolutely continuous respect empirical distribution training sample maximum entropy framework parameter estimation tempers training problem however basic problem remains scope present paper present random field induction paradigm c inducing field interactions begin supposing set atomic features supported single vertex use atomic features incrementally build complicated features following definition specifies shall allow field incrementally constructed induced 4 ieee transactions pattern analysis machine intelligence vol 19 4 april 1997 definition 1 suppose field q given f features f called active features q feature g candidate q either g 2 f atomic g atomic feature active set candidate features q denoted cq words candidate features obtained conjoining atomic features existing features condition supports ensures feature supported pathconnected subset g g 2 cq candidate feature q call 1 parameter family random fields q induction q g also define g q ff think g q ff g improvement feature g brings model weight ff show following section g q ff g convex ff use suggestive notation convex convex place less mnemonic concave convex terminology define g q g greatest improvement feature g give model keeping features parameters fixed ff refer g q g gain candidate g incremental construction random fields describe algorithm incrementally constructing fields field induction algorithm initial reference distribution p initial model q 0 output field q active features f arg min algorithm 1 candidate g 2 cq n compute gain g q n g g q n g feature largest gain 3 compute go step 1 induction algorithm two parts feature selection parameter estimation feature selection carried steps 1 2 feature yielding largest gain incorporated model parameter estimation carried step 3 parameters adjusted best represent reference distribution two computations discussed detail following two sections iii feature selection feature selection step induction algorithm based upon approximation approximate improvement due adding single candidate feature measured reduction kullbackleibler divergence adjusting weight candidate keeping parameters field fixed general estimate since may well adding feature require significant adjustments parameters new model computational perspective approximating improvement way enable simultaneous evaluation thousands candidate features makes algorithm practical section present explain feature selection step detail proposition 1 let g q ff g defined 2 approximate improvement obtained adding feature g parameter ff field q g constant g q ff g strictly convex ff attains maximum unique point ff satisfying proof using definition 1 kullbackleibler divergence write g q ff p log theta e ffg theta e ffg thus ff g q ff moreover qge ffg hence 2 g q ff g convex ff g constant 2 minus variance g respect q ffg strictly negative g q ff g strictly convex g binaryvalued gain expressed particularly nice form stated following proposition whose proof simple calculation proposition 2 suppose candidate g binaryvalued g q ff g maximized value della pietra della pietra lafferty inducing features random fields 5 bernoulli random variables given features binaryvalued instead take values nonnegative integers parameter ff solves 3 thus maximizes g q ff g cannot general determined closed form case tied binary features applies application describe section 5 cases convenient rewrite 3 slightly let total probability assigned event feature g takes value k 3 becomes g q log fi equation lends well numerical solution general shape curve fi 7 fifi g q log fi g shown figure 1 fig 1 derivative gain limiting value fig q log fi gfi n solution equation 4 found using newtons method practice converges rapidly functions configuration space w large coefficients k cannot calculated summing configura tions monte carlo techniques may used estimate important emphasize set random configurations used estimate coefficients g k candidate g simultaneously rather discuss details monte carlo techniques problem refer extensive literature topic obtained good results using standard technique gibbs sampling 17 problem describe section 5 iv parameter estimation section present algorithm selecting parameters associated features random field algorithm generalization generalized iterative scaling algorithm darroch ratcliff 12 reduces algorithm features sum constant however new algorithm make restriction throughout section hold set features initial model q 0 reference distribution fixed simplify notation accordingly particular write fl ffi q instead fl delta f ffi q fl 2 r n assume condition commonly written equivalent description algorithm requires additional piece notation let features binary f total number features configuration improved iterative scaling initial reference distribution p initial model q 0 nonnegative features f output distribution q algorithm 1 let fl k unique solution 3 q k converged set q go step 1 words algorithm constructs distribution q lim determined solution equation used nth iteration field induction algorithm candidate feature added field choose initial distribution q 0 q ffg ff parameter maximizes gain g practice provides good starting point begin iterative scaling fact view distribution result applying one iteration iterative proportional fitting procedure 5 9 project q ffg onto linear family distributions g marginals constrained pg main result section proposition 3 suppose q k sequence determined improved iterative scaling algorithm decreases monotonically converges q arg min remainder section present selfcontained proof convergence algorithm key idea proof express incremental step algorithm terms auxiliary function bounds loglikelihood objective function technique standard means analyzing em algorithm 13 previously applied iterative scaling analysis iterative scaling different simpler previous treatments particular contrast csiszars proof darrochratcliff 6 ieee transactions pattern analysis machine intelligence vol 19 4 april 1997 procedure 10 proof rely upon convergence alternating iprojection 9 begin formulating basic duality theorem states maximum likelihood problem gibbs distribution maximum entropy problem subject linear constraints solution turn task computing solution introducing auxiliary functions general setting apply method prove convergence improved iterative scaling algorithm finish section discussing monte carlo methods estimating equations size configuration space prevents explicit calculation feature expectations duality duality maximum likelihood maximum entropy problems expressed following proposition proposition 4 suppose exists moreover four properties determines q uniquely result well known although perhaps quite packaging language constrained optimization expresses fact maximum likelihood problem gibbs distributions convex dual maximum entropy problem linear constraints property 2 called pythagorean property since resembles pythagorean theorem imagine dp k q square euclidean distance p q vertices right triangle include proof result appendix make paper selfcontained also carefully address technical issues arising fact q closed proposition would true replaced q q fact might empty proof elementary rely kuhntucker theorem machinery constrained optimization b auxiliary functions turn task computing q fix p let r loglikelihood objective function definition 2 function r n theta r auxiliary function l 1 q 2 fl 2 r n 2 afl q continuous q 2 c 1 fl 2 r n dt dt use auxiliary function construct iterative algorithm maximizing l start q recursively define q k1 clear property 1 definition step procedure increases l following proposition implies fact sequence q k reach maximum l proposition 5 suppose q k sequence increases monotonically max lq q k converges q lq equation 6 assumes supremum sup fl afl q k achieved finite fl appendix b slightly stronger assumptions present extension allows components fl k take value gamma1 use proposition construct practical algorithm must determine auxiliary function afl q fl k satisfying required condition determined efficiently section 43 present choice auxiliary function yields improved iterative scaling updates prove proposition 5 first prove three lemmas lemma 1 2 cluster point q k afl proof let q k l subsequence converging fl first inequality follows property 6 fl nk second third inequalities consequence monotonicity lq k lemma follows taking limits using fact l continuous lemma 2 2 cluster point q k proof previous lemma afl 0 fl since means maximum afl dt dt lemma 3 supposefq k g sequence one cluster point q q k converges q proof suppose exists open set b containing q subsequence q nk 62 b since compact q nk cluster point q 0 62 b contradicts assumption unique cluster point proof proposition 5 suppose cluster point q k follows lemma 2 0 q lemma 2 appendix q point proposition 4 follows lemma 3 q k converges q appendix b prove extension proposition 5 allows components fl equal gamma1 extension assume components feature function f nonnegative practical restriction since replace f c improved iterative scaling prove monotonicity convergence improved iterative scaling algorithm applying proposition 5 particular choice auxiliary function assume component feature function f nonnegative easy check extends continuous function r gamma1 n theta lemma 4 afl q extended auxiliary function lq key ingredient proof lemma convexity logarithm convexity exponential expressed inequalities e log x proof lemma 4 extends continuous function r gamma1 n theta suffices prove satisfies properties 1 2 definition 2 prove property 1 note equality 10 simple calculation inequality 11 follows inequality 9 inequality 12 follows definition f jensens inequality 8 property 2 definition 2 straightforward verify proposition 3 follows immediately lemma extended proposition 5 indeed easy check fl k defined proposition 3 achieves maximum afl q k satisfies condition proposition 5 appendix b monte carlo methods improved iterative scaling algorithm described previous section wellsuited numerical techniques since features take nonnegative values iteration algorithm necessary solve polynomial equation feature f express equation 5 form largest value f q k field kth iteration fi equation solution precisely k otherwise efficiently solved using newtons method since coefficients k nonnegative monte carlo methods used configuration space w large coefficients k mi simultaneously estimated generating single set samples distribution q k v application word morphology word clustering algorithms useful many natural language processing tasks one algorithm 6 called mutual information clustering based upon construction simple bigram language models using maximum likelihood crite rion algorithm gives hierarchical binary classification words used variety purposes including construction decision tree language parsing models sense disambiguation machine translation 7 fundamental shortcoming mutual information word clustering algorithm given 6 takes fundamental word spellings increases severity problem small counts present virtually every statistical learning algorithm example word hamil tonianism appears 365893263word corpus used collect bigrams clustering experiments described 6 clearly insufficient evidence base statistical clustering decision basic motivation behind featurebased approach querying features spellings clustering algorithm could notice word begins capital letter ends ism contains ian profit features used words similar contexts section describe applied random field induction algorithm discover morphological features words present sample results application demonstrates technique gradually sharpens probability mass enormous set possible configurations case ascii strings onto set configurations increasingly similar training sample achieves introducing positive features many training samples exhibit well negative features appear sample appear rarely description resulting features 8 ieee transactions pattern analysis machine intelligence vol 19 4 april 1997 used improve mutual information clustering given 20 beyond scope present paper refer reader 6 20 detailed treatment topic section 51 formulate problem terms notation results sections 2 3 4 section 52 describe field induction algorithm actually carried application section 53 explain results induction algorithm presenting series examples problem formulation discover features spellings take configuration space set strings ascii alphabet construct probability distribution p w first predicting length j j predicting actual spelling thus l length distribution p spelling distribution take length distribution given model spelling distribution p length l random field let w l configuration space ascii strings length l j w l since ascii character reduce number parameters tie features described section 21 feature weight independent appears string natural view graph underlyingw l regular lgon group automorphisms graph set rotations resulting field homogeneous defined section 2 field p homogeneous addition tie features across fields different values l thus weight f feature independent l introduce dependence length well whether feature applies beginning end string adopt following artificial construction take graph w l l 1gon rather lgon label distinguished vertex length keeping label held fixed complete description fields induced need specify set atomic features atomic features allow fall three types first type class features form c ascii character v denotes arbitrary character position string second type atomic features involve special vertex carries length string features atomic feature f v introduces dependence whether string characters lies beginning end string atomic features f vl introduce dependence length string tie together length dependence long strings also introduce atomic feature f v7 strings length 7 greater final type atomic feature asks whether character lies one four sets az az 09 denoting arbitrary lowercase letters uppercase letters digits punctu ation example atomic feature tests whether character lowercase illustrate notation use let us suppose following features active field ends ism string least 7 characters beginning capital letter con tains ian probability word hamiltonianism would given l l 14z 14 parameters appropriate features use characters denote beginning ending string common regular expression notation would notation 7az thus means string least 7 characters begins capital letter corresponding feature u v adjacent positions string recalling definition 21 require support feature connected subgraph similarly ism means ends ism corresponds feature u v w x adjacent positions string ian means contains ian corresponding feature b description algorithm begin random field induction algorithm model assigns uniform probabilityto strings incrementally add features random field model order minimize kullbackleibler divergence field unigram distribution vocabulary obtained training corpus length distribution taken according lengths words empirical distribution training data improvement model made candidate feature evaluated reduction relative entropy respect unigram distribution adding new feature yields keeping parameters model fixed learning algorithm incrementally constructs random field describe features spellings informative stage induction algorithm set candidate features constructed fields homogeneous set candidate features viewed follows active feature expressed form substring appears string extended alphabet ascii characters together macros az az 09 della pietra della pietra lafferty inducing features random fields 9 length labels ff g s2s set active features including ffl empty string using repre sentation set candidate features precisely set concatenation strings required definition 2 candidate increases support active feature single adjacent vertex since model assigns probability arbitrary word strings partition function z l computed exactly smallest string lengths l therefore compute feature expectations using random sampling algorithm specifically use gibbs sampler generate 10000 spellings random lengths computing gain g q g candidate fea ture use spellings estimate probability g k candidate feature g occurs k times spelling see equation 4for example feature f vaz occurs two times string solve corresponding fi using newtons method candidate feature emphasized single set random spellings needs generated set used estimate g k candidate g adding best candidate field feature weights readjusted using improved iterative scaling algorithm carry algorithm random spellings generated time incorporating new feature yielding monte carlo estimates coefficients k mi recall k mi expected number times feature appears substring representation homogeneous features string total active features see equation 14 given estimates coefficients newtons method used solve equation 14 complete single iteration iterative scaling algorithm convergence kullbackleibler divergence inductive step complete new set candidate features considered c sample results began uniform field field features field ascii strings given length equally likely lengths drawn fixed distribution sample strings drawn distribution mo zp mtll kscm 3 lqdr awf 5tl4 tc sneio who8zbr pqlv h ydu 1xcl 1jfu widhnm 2 2lew2 soc12ade np9oh 6 qgo xev u o83cof b7cr mqq mv n7g i9gaj 5 u6i9 2evz3nus 3xj gdweql r3r 7v fxy 4p cy2hu comes surprise first feature induction algorithm chooses az simply observes characters lowercase maximum likelihood maximum en tropy weight feature means string lowercase letter position 7 times likely string without lowercase letter position draw strings new distribution using annealing concentrate distribution probable strings obtain spellings primarily made lowercase letters certainly resemble english words r xevo ijjiir b jz gsr wq vf x ga msmgh pcp ozivlal hzagh yzop io advzmxnv ijv bolft x emx kayerf mlj rawzyb jp ag ctdnnnbg wgdw kguv cy spxcq uzflbbf dxtkkn cxwx jpd ztzh lv zhpkvnu l r qee nynrx atze4n ik se w lrh hp yrqykah zcngotcnx igcump zjcjs lqpwiqu cefmfhc lb fdcy tzby yopxmvk fz govyccm ijyiduwfzo 6xr duh ejv pk pjw l fl w following table show first 10 features algorithm induced together associated parameters several things worth noticing second feature chosen azaz denotes adjacent lowercase characters third feature added letter e common letter weight feature next feature introduces first dependence length string az1 denotes feature one character word ending lowercase letter notice feature small weight 004 corresponding intuition words uncommon similarly features z q j x uncommon thus receive small weights appearance feature explained fact vocabulary corpus restricted frequent 100000 spellings words receive unknown word spelling rather frequent endofsentence marker makes appearance later given spelling feature az azaz e az1 feature z q j x shown spellings obtained gibbs sampling resulting collection fields frk et egeit edet eutdmeeet ppge dtgd falawe etci eese ye epemtbn tegoeed ee mp temou enrteunt ore erveelew heyu rht lkaeu lutoee tee mmo eobwtit weethtw 7 ee teet gre eeeteetue hgtte om stmenu ec ter eedgtue iu ec reett ivtcmeee vt eets tidpt lttv etttvti ecte x see pi rlet tt eot leef ke tet iwteeiwbeie yeee et etf ov inducing 100 features model finally begins concentrated spellings resemble actual words extent particularly short strings point algorithm discovered example common 3letter word many words end ed long words often end ion sample 10 first 100 features induced appropriate weights shown table 1 3the tion 4th ed ion7 ent 7c 2236 thed thed toftion ieention cention ceetion ant seieeet cinention tloned uointe feredten iined sonention inathed id lcers ceeecion roferented ioner centention ionent asers ctention thed uentie ttentt rerey sotth cheent thed rontion seoftr sample first 1000 features induced shown table together randomly generated spellings tice example feature 0909 appears surprisingly high weight 938293 due fact string contains one digit likely contain two digits since digits relatively rare general feature 09 assigned small weight 0038 also according model lowercase letter followed uppercase letter rare ght 3az ly al7 ing azaz ed7 er7 ity ent7 0909 qu ex ae ment ies wh ate reaser homes thing reloverated ther conists fores anditing mr proveral ont prolling prothere mento yaou 1 chestraing intrally qut best compers cluseliment uster deveral thise offect inatever thifer constranded stater vill thase youse menttering verate finally visit state model inducing 1500 features describe words point model making refined judgements regarding considered word appearance features explained fact preparing corpus certain characters assigned special macro strings example punctuation characters represented corpus following sampled spellings demonstrate model point recognized existence macros yet discerned proper use 7inte prov der wh 19 ons7 ugh ic 423 508 003 205 259 449 584 776 sys ally 7con ide nal qui 478 610 525 439 291 12056 1818 91322 iz ib inc im iong ive7 un conthing devened f intering ation said prouned suparthere mentter prement intever b gover producits alase conting comment incements contive evined agents thent distements said resting intevent ibm whree acalinate herned 1980 becoment recall nother ments stounicallity camanfined intations conanament clearly model still much learn point compiled significant collection morphological observations traveled long way toward goal statistically characterizing english spellings vi extensions relations approaches section briefly discuss relations incremental feature induction algorithm random fields statistical learning paradigms also present possible extensions improvements method conditional exponential models almost presented carries general setting conditional exponential models including improved iterative scaling algorithm general conditional may underlyingran dom field features defined binary functions fx general approach applicable feature induction method conditional exponential models demonstrated several problems statistical machine translation 3 presented terms principle maximum entropy b decision trees feature induction paradigm also bears resemblence various methods growing classification regression trees like decision trees method builds topdown classification refines features however decision trees correspond constructing features disjoint support explain recall decision tree determines partition context random variable x 2 x order predict actual class context represented random variable 2 leaf tree corresponds sequence binary features n denotes parent node n feature f n question splits x f n negation fn question asked sibling node distribution assigned leaf l simply empirical distribution determined training samples x 2 x theta leaf l characterized conjunction features different leaves correspond conjunctionswith disjoint support contrast feature induction algorithm generally results features overlapping support criterion evaluating questions terms amount reduce conditional entropy corresponds criterion maximizing reduction kullbackleibler divergence g q g candidate features g field q modifying induction algorithm following way obtain algorithm closely related standard methods growing binary decision trees instead considering 1 parameter family fields q g determine best candidate consider 2parameter family fields given since features f f disjoint support improvement obtained adding given byg q general resulting distribution absolutely continuous respect empirical distribution random variable take values standard decision tree algorithm obtained nth stage della pietra della pietra lafferty inducing features random fields 11 add 2m disjoint features f n maximum likelihood training parameters features recovers empirical distribution data node n c extensions mentioned section 1 approach differs common applications statistical techniques computer vision since typical application method involves estimation thousands free parameters yet induction technique may scale well large 2dimensional image prob lems one potential difficulty degree polynomials improved iterative scaling algorithm could quite large could difficult obtain reliable estimates coefficients since monte carlo sampling might exhibit sufficiently many instances desired features extent significant problem primarily empirical issue dependent particular domain method applied random field induction method presented paper definitive many possible variations basic theme incrementally construct increasingly detailed exponential model approximate reference distribution p basic technique based greedy algo rithm course many ways improving search good set features algorithm presented section 2 respects simple possible within general framework also computationally intensive natural modification would add several top candidates stage increase overall speed induction algorithm would also potentially result redundancy among features since top candidates could correlated another modification algorithm would add best candidate step carry parameter estimation several new features added field would also natural establish bayesian framework prior distribution features parameters incorporated could enable principled approach deciding feature induction complete natural class conjugate priors class exponential models use 14 problem incorporating prior knowledge set candiate features challenging duality appendix prove proposition 4 restated proposition 4 suppose exists moreover four properties determines q uniquely proof proposition use lemmas first two lemmas state without proof lemma 1 1 dp k q nonnegative extended realvalued function theta strictly convex p q separately lemma 2 1 map fl p 7 fl ffi p smooth fl p 2 r n theta 2 derivative dp k ffi q respect dt lemma 3 q nonempty proof define q property 3 proposition 4 see makes sense note since k q identically 1 q also continuous strictly convex function q thus since q closed attains minimum unique point q show q also p since q closed action r n q thus definition q minimum function taking derivatives respect using lemma a2 conclude q f lemma 4 q q p 2 p q 2 proof straightforward calculation shows p 1 follows identity continuity q lemma follows taking proof proposition 4 choose q point q q exists lemma a3 satisfies property 1 definition satisfies property 2 lemma a4 consequence property 2 also satisfies properties 3 4 check property 3 instance note q point q remains prove four properties 14 determines q uniquely words need show point satisfying four properties suppose satisfies property 1 property 2 q argument q reversed proves suppose satisfies property 3 second equality follows property 2 q thus similar proof shows ii dealing 1 appendix prove extension proposition 5 allows components fl equal gamma1 extension assume components feature function f nonnegative f 0 assumed loss generality since replace f necessary denote partially extended real numbers usual topology operations addition exponentiation extend continuously r gamma1 let open subset r gamma1 n theta defined observe r n theta dense subset map fl q 7 point defined finite fl extends uniquely continuous map condition fl q 2 ensures normalization definition even fl finite definition 3 call function extended auxiliary function l restricted r n theta ordinary auxiliary function sense definition 2 addition satisfies property 1 definition 2 even fl finite note ordinary auxiliary function extends continuous function extension extended auxiliary function following extension proposition 5 proposition 5 suppose feature function f satisfies nonnegativity condition 7 suppose extended auxiliary function l conclusion proposition5 continues hold condition fl k replaced fl afl lemma 1 valid altered condition since afl q satisfies property 1 definition 2 consequence lemma 2 also valid proof proposition 5 goes without change iii acknowledgements part research presented paper carried authors ibm thomas j watson research center yorktown heights new york stephen della pietra vincent della pietras work partially supported arpa grant n0001491c0135 john laffertys work partially supported nsf arpa grants iri 9314969 n0001492c0189 r variational method estimating parameters mrf complete incomplete data noncausal gauss markovrandom fields parameter structure estimation maximum entropy approach natural language processing classification regression trees note approximations discrete probability distributions classbased ngram models natural language statistical approach machine translation iterative gibbsian technique reconstruction mary images idivergence geometry probability distributions minimization problems geometric interpretation darroch ratcliffs generalized iterative scaling information geometry alternating minimization procedures generalized iterative scaling loglinear models maximum likelihood incomplete data via em algorithm conjugate priors exponential families convergence partially parallel gibbs samplers annealing optimal spectral structure reversible stochastic matrices monte carlo methods simulation markov random fields stochastic relaxation gibbs distributions bayesian restoration images constrainedmonte carlo maximum likelihood dependent data discussion automatic word classification using features spellings partition function estimation gibbs random field images using monte carlo simulations estimation annealing gibbsian fields tr ctr wei li andrew mccallum rapid development hindi named entity recognition using conditional random fields feature induction acm transactions asian language information processing talip v2 n3 p290294 september victor lavrenko jeremy pickens music modeling random fields proceedings 26th annual international acm sigir conference research development informaion retrieval july 28august 01 2003 toronto canada dharanipragada franz j mccarley ward wj zhu segmentation detection ibm hybrid statistical models twotiered clustering topic detection tracking eventbased information organization kluwer academic publishers norwell 2002 fuchun peng fangfang feng andrew mccallum chinese segmentation new word detection using conditional random fields proceedings 20th international conference computational linguistics p562es august 2327 2004 geneva switzerland iain murray zoubin ghahramani bayesian learning undirected graphical models approximate mcmc algorithms proceedings 20th conference uncertainty artificial intelligence p392399 july 0711 2004 banff canada andrew mccallum wei li early results named entity recognition conditional random fields feature induction webenhanced lexicons proceedings seventh conference natural language learning hltnaacl 2003 p188191 may 31 2003 edmonton canada takehito utsuro manabu sassano kiyotaka uchimoto combining outputs multiple japanese named entity chunkers stacking proceedings acl02 conference empirical methods natural language processing p281288 july 06 2002 kishore papineni inverse document frequency second meeting north american chapter association computational linguistics language technologies 2001 p18 june 0107 2001 pittsburgh pennsylvania rob koeling chunking maximum entropy models proceedings 2nd workshop learning language logic 4th conference computational natural language learning september 1314 2000 lisbon portugal karlmichael schneider information extraction calls papers conditional random fields layout features artificial intelligence review v25 n12 p6777 april 2006 jeremy pickens andrew macfarlane term context models information retrieval proceedings 15th acm international conference information knowledge management november 0611 2006 arlington virginia usa jason eisner parameter estimation probabilistic finitestate transducers proceedings 40th annual meeting association computational linguistics july 0712 2002 philadelphia pennsylvania andrew smith trevor cohn miles osborne logarithmic opinion pools conditional random fields proceedings 43rd annual meeting association computational linguistics p1825 june 2530 2005 ann arbor michigan doug beeferman adam berger john lafferty model lexical attraction repulsion proceedings eighth conference european chapter association computational linguistics p373380 july 0712 1997 madrid spain iain bancarz miles osborne improved iterative scaling yield multiple globally optimal models radically differing performance levels proceedings 19th international conference computational linguistics p17 august 24september 01 2002 taipei taiwan jianfeng gao andi wu mu li changning huang hongqiao li xinsong xia haowei qin adaptive chinese word segmentation proceedings 42nd annual meeting association computational linguistics p462es july 2126 2004 barcelona spain takehito utsuro takashi miyata yuji matsumoto generaltospecific model selection subcategorization preference proceedings 17th international conference computational linguistics p13141320 august 1014 1998 montreal quebec canada l yuille anand rangarajan concaveconvex procedure neural computation v15 n4 p915936 april robert malouf markov models languageindependent named entity recognition proceeding 6th conference natural language learning p14 august 31 2002 yumao lu fuchun peng xin li nawaaz ahmed coupling feature selection machine learning methods navigational query identification proceedings 15th acm international conference information knowledge management november 0611 2006 arlington virginia usa nicola ueffing hermann ney using pos information statistical machine translation morphologically rich languages proceedings tenth conference european chapter association computational linguistics april 1217 2003 budapest hungary amir globerson naftali tishby minimum information principle discriminative learning proceedings 20th conference uncertainty artificial intelligence p193200 july 0711 2004 banff canada doug beeferman adam berger john lafferty statistical models text segmentation machine learning v34 n13 p177210 feb 1999 vincent ng learning noun phrase anaphoricity improve coreference resolution issues representation optimization proceedings 42nd annual meeting association computational linguistics p151es july 2126 2004 barcelona spain stefan riezler jonas kuhn detlef prescher mark johnson lexicalized stochastic modeling constraintbased grammars using loglinear measures em training proceedings 38th annual meeting association computational linguistics p480487 october 0306 2000 hong kong victor lavrenko jeremy pickens polyphonic music modeling random fields proceedings eleventh acm international conference multimedia november 0208 2003 berkeley ca usa hai leong chieu hwee tou ng named entity recognition maximum entropy approach proceedings seventh conference natural language learning hltnaacl 2003 p160163 may 31 2003 edmonton canada hai leong chieu hwee tou ng named entity recognition maximum entropy approach using global information proceedings 19th international conference computational linguistics p17 august 24september 01 2002 taipei taiwan stanley kok pedro domingos learning structure markov logic networks proceedings 22nd international conference machine learning p441448 august 0711 2005 bonn germany john lafferty xiaojin zhu yan liu kernel conditional random fields representation clique selection proceedings twentyfirst international conference machine learning p64 july 0408 2004 banff alberta canada zhihua zhang james kwok dityan yeung surrogate maximizationminimization algorithms adaboost logistic regression model proceedings twentyfirst international conference machine learning p117 july 0408 2004 banff alberta canada pang lillian lee shivakumar vaithyanathan thumbs sentiment classification using machine learning techniques proceedings acl02 conference empirical methods natural language processing p7986 july 06 2002 hai leong chieu hwee tou ng maximum entropy approach information extraction semistructured free text eighteenth national conference artificial intelligence p786791 july 28august 01 2002 edmonton alberta canada qi zhang fuliang weng zhe feng progressive feature selection algorithm ultra large feature spaces proceedings 21st international conference computational linguistics 44th annual meeting acl p561568 july 1718 2006 sydney australia mark johnson joint conditional estimation tagging parsing models proceedings 39th annual meeting association computational linguistics p322329 july 0611 2001 toulouse france donald metzler w bruce croft analysis statistical question classification factbased questions information retrieval v8 n3 p481504 may 2005 shenghuo zhu xiang ji wei xu yihong gong multilabelled classification using maximum entropy method proceedings 28th annual international acm sigir conference research development information retrieval august 1519 2005 salvador brazil david j miller siddharth pal transductive methods distributed ensemble classification problem neural computation v19 n3 p856884 march 2007 steven j phillips miroslav dudk robert e schapire maximum entropy approach species distribution modeling proceedings twentyfirst international conference machine learning p83 july 0408 2004 banff alberta canada michael collins brian roark incremental parsing perceptron algorithm proceedings 42nd annual meeting association computational linguistics p111es july 2126 2004 barcelona spain tzukuo huang chihjen lin ruby c weng ranking individuals group comparisons proceedings 23rd international conference machine learning p425432 june 2529 2006 pittsburgh pennsylvania ismael garcavarea francisco casacuberta maximum entropy modeling suitable framework learn contextdependent lexicon models statistical machine translation machine learning v60 n13 p135158 september 2005 joshua goodman sequential conditional generalized iterative scaling proceedings 40th annual meeting association computational linguistics july 0712 2002 philadelphia pennsylvania hai leong chieu hwee tou ng teaching weaker classifier named entity recognition upper case text proceedings 40th annual meeting association computational linguistics july 0712 2002 philadelphia pennsylvania james cussens parameter estimation stochastic logic programs machine learning v44 n3 p245271 september 2001 zhiyi chi statistical properties probabilistic contextfree grammars computational linguistics v25 n1 p131160 march 1999 stephen clark james r curran loglinear models widecoverage ccg parsing proceedings conference empirical methods natural language processing p97104 july 11 minwoo jeong gary geunbae lee exploiting nonlocal features spoken language understanding proceedings colingacl main conference poster sessions p412419 july 1718 2006 sydney australia jenny rose finkel trond grenager christopher manning incorporating nonlocal information information extraction systems gibbs sampling proceedings 43rd annual meeting association computational linguistics p363370 june 2530 2005 ann arbor michigan amir globerson naftali tishby informative dimension reduction eighteenth national conference artificial intelligence p10241029 july 28august 01 2002 edmonton alberta canada jyrki kivinen manfred k warmuth boosting entropy projection proceedings twelfth annual conference computational learning theory p134144 july 0709 1999 santa cruz california united states ella bingham heikki mannila jouni k seppnen topics 01 data proceedings eighth acm sigkdd international conference knowledge discovery data mining july 2326 2002 edmonton alberta canada miles osborne estimation stochastic attributevalue grammars using informative sample proceedings 18th conference computational linguistics p586592 july 31august le zhang jingbo zhu tianshun yao evaluation statistical spam filtering techniques acm transactions asian language information processing talip v3 n4 p243269 december 2004 david mcallester michael collins fernando pereira casefactor diagrams structured probabilistic modeling proceedings 20th conference uncertainty artificial intelligence p382391 july 0711 2004 banff canada yee whye teh max welling simon osindero geoffrey e hinton energybased models sparse overcomplete representations journal machine learning research v4 n78 p12351260 october 1 november 15 2004 james r curran stephen clark investigating gis smoothing maximum entropy taggers proceedings tenth conference european chapter association computational linguistics april 1217 2003 budapest hungary erin l allwein robert e schapire yoram singer reducing multiclass binary unifying approach margin classifiers journal machine learning research 1 p113141 912001 zhihua zhang james kwok dityan yeung surrogate maximizationminimization algorithms extensions machine learning v69 n1 p133 october 2007 yee whye teh max welling simon osindero geoffrey e hinton energybased models sparse overcomplete representations journal machine learning research 4 1212003 john debenham simeon simoff intelligent agents multiissue auctions bidding proceedings 24th iasted international conference artificial intelligence applications p468473 february 1316 2006 innsbruck austria christoph tillmann tong zhang block bigram prediction model statistical machine translation acm transactions speech language processing tslp v4 n3 p6es july 2007 siddharth pal david j miller extension iterative scaling decision data aggregation ensemble classification journal vlsi signal processing systems v48 n12 p2137 august 2007 amir globerson naftali tishby sufficient dimensionality reduction journal machine learning research 3 312003 changki lee gary geunbae lee information gain divergencebased feature selection machine learningbased text categorization information processing management international journal v42 n1 p155165 january 2006 miles osborne using maximum entropy sentence extraction proceedings acl02 workshop automatic summarization p18 july 1112 2002 phildadelphia pennsylvania michael collins ranking algorithms namedentity extraction boosting voted perceptron proceedings 40th annual meeting association computational linguistics july 0712 2002 philadelphia pennsylvania rong jin huan liu robust feature induction support vector machines proceedings twentyfirst international conference machine learning p57 july 0408 2004 banff alberta canada warren r greiff jay ponte maximum entropy approach probabilistic ir models acm transactions information systems tois v18 n3 p246287 july 2000 junichi kazama junichi tsujii maximum entropy models inequality constraints case study text categorization machine learning v60 n13 p159194 september 2005 hyojung oh sung hyon myaeng myunggil jang semantic passage segmentation based sentence topics question answering information sciences international journal v177 n18 p36963717 september 2007 yu gu andrew mccallum towsley detecting anomalies network traffic using maximum entropy estimation proceedings internet measurement conference 2005 internet measurement conference p3232 october 1921 2005 berkeley ca james r curran stephen clark david vadas multitagging lexicalizedgrammar parsing proceedings 21st international conference computational linguistics 44th annual meeting acl p697704 july 1718 2006 sydney australia ismael garca varea franz j och hermann ney francisco casacuberta improving alignment quality statistical machine translation using contextdependent maximum entropy models proceedings 19th international conference computational linguistics p17 august 24september 01 2002 taipei taiwan fei sha fernando pereira shallow parsing conditional random fields proceedings conference north american chapter association computational linguistics human language technology p134141 may 27june 01 2003 edmonton canada robert malouf comparison algorithms maximum entropy parameter estimation proceeding 6th conference natural language learning p17 august 31 2002 john browning david j miller maximum entropy approach collaborative filtering journal vlsi signal processing systems v37 n23 p199209 junejuly 2004 noam slonim gill bejerano shai fine naftali tishby discriminative feature selection via multiclass variable memory markov model eurasip journal applied signal processing v2003 n1 p93102 january chihjen lin ruby c weng sathiya keerthi trust region newton methods largescale logistic regression proceedings 24th international conference machine learning p561568 june 2024 2007 corvalis oregon john lafferty additive models boosting inference generalized divergences proceedings twelfth annual conference computational learning theory p125133 july 0709 1999 santa cruz california united states michael collins robert e schapire yoram singer logistic regression adaboost bregman distances machine learning v48 n13 p253285 2002 shaojun wang dale schuurmans fuchun peng yunxin zhao combining statistical language models via latent maximum entropy principle machine learning v60 n13 p229250 september 2005 adwait ratnaparkhi learning parse natural language maximum entropy models machine learning v34 n13 p151175 feb 1999 ling tan david taniar adaptive estimated maximumentropy distribution model information sciences international journal v177 n15 p31103128 august 2007 junichi kazama junichi tsujii evaluation extension maximum entropy models inequality constraints proceedings conference empirical methods natural language processing p137144 july 11 ismael garca varea franz j och hermann ney francisco casacuberta refined lexicon models statistical machine translation using maximum entropy approach proceedings 39th annual meeting association computational linguistics p204211 july 0611 2001 toulouse france selection englishkorean statistical machine translation proceedings 18th conference computational linguistics p439445 july 31august matthew richardson pedro domingos markov logic networks machine learning v62 n12 p107136 february 2006 sunita sarawagi usercognizant multidimensional analysis vldb journal international journal large data bases v10 n23 p224239 september 2001 jianfeng gao mu li andi wu changning huang chinese word segmentation named entity recognition pragmatic approach computational linguistics v31 n4 p531574 december 2005 michael collins parameter estimation statistical parsing models theory practice distributionfree methods new developments parsing technology kluwer academic publishers norwell 2004 michael collins terry koo discriminative reranking natural language parsing computational linguistics v31 n1 p2570 march 2005 fuchun peng andrew mccallum information extraction research papers using conditional random fields information processing management international journal v42 n4 p963979 july 2006 pieter abbeel daphne koller andrew ng learning factor graphs polynomial time sample complexity journal machine learning research 7 p17431788 1212006 martin j wainwright estimating wrong graphical model benefits computationlimited setting journal machine learning research 7 p18291859 1212006 bingjun sun qingzhao tan prasenjit mitra c lee giles extraction search chemical formulae text documents web proceedings 16th international conference world wide web may 0812 2007 banff alberta canada gang liang nina taft bin yu fast lightweight approach origindestination ip traffic estimation using partial measurements ieeeacm transactions networking ton v14 nsi p26342648 june 2006 phan leminh nguyen tubao ho susumu horiguchi improving discriminative sequential learning rarebutimportant associations proceeding eleventh acm sigkdd international conference knowledge discovery data mining august 2124 2005 chicago illinois usa donald metzler w bruce croft linear featurebased models information retrieval information retrieval v10 n3 p257274 june 2007 ruofei zhang ramesh sarukkai jyhherng chow wei dai zhongfei zhang joint categorization queries clips webbased video search proceedings 8th acm international workshop multimedia information retrieval october 2627 2006 santa barbara california usa phan leminh nguyen yasushi inoguchi tubao ho susumu horiguchi improving discriminative sequential learning discovering important association statistics acm transactions asian language information processing talip v5 n4 p413438 december 2006 daniel gildea daniel jurafsky automatic labeling semantic roles computational linguistics v28 n3 p245288 september 2002 ron meir gunnar rtsch introduction boosting leveraging advanced lectures machine learning springerverlag new york inc new york ny gunnar rtsch sebastian mika bernhard schlkopf klausrobert mller constructing boosting algorithms svms application oneclass classification ieee transactions pattern analysis machine intelligence v24 n9 p11841199 september 2002 nicola orio music retrieval tutorial review foundations trends information retrieval v1 n1 p196 january 2006 david forsyth okan arikan leslie ikemoto james obrien deva ramanan computational studies human motion part 1 tracking motion synthesis foundations trends computer graphics vision v1 n2 p77254 july 2006