task assignment unknown duration consider distributed server system ask policy used assigning jobs tasks hosts server jobs preemptible also jobs service demand known priori particularly concerned case workload heavytailed characteristic many empirically measured computer workloads analyze several natural task assignment policies propose new one tags task assignment based guessing size tags algorithm counterintuitive many respects including load unbalancing nonworkconserving fairness find heavytailed workloads tags outperform task assignment policies known us several orders magnitude respect mean response time mean slowdown provided system load high also introduce new practical performance metric distributed servers called server expansion server expansion metric tags significantly outperforms task assignment policies regardless system load b introduction recent years distributed servers become commonplace allow increased computing power costeffective easily scalable distributed server system requests service tasks arrive must assigned one host machines processing rule assigning tasks host machines known task assignment policy choice task assignment policy significant effect performance perceived users designing distributed server system often comes choosing best task assignment policy given model user requirements question task assignment policy best ageold question still remains open many models paper consider particular model distributed server system tasks preemptible ie concerned applications context switches costly example one application batch computing environments hosts parallel processors tasks parallel context switching tasks involves reloading processors memory return state context switch context switching expensive environment tasks always simply run completion note fact context switches expensive preclude possibility killing job restarting scratch assume furthermore priori information known task time task arrives particular service demand task known assume hosts identical cost time required assigning tasks hosts figure 1 one illustration distributed server illustration arriving tasks immediately dispatched central dispatcher one hosts queue host waiting service served firstcomefirstserved fcfs order observe however model general preclude possibility central queue dispatcher tasks might wait dispatched also preclude possibility alternative scheduling discipline hosts long scheduling discipline require preempting tasks rely priori knowledge tasks main performance goal choosing task assignment policy minimize mean waiting time importantly mean slowdown tasks slowdown waiting time divided service demand means pertask averages consider mean slowdown important mean waiting time desirable tasks delay proportional size system task sizes highly variable users likely anticipate short delays short tasks likely tolerate long delays longer tasks later paper introduce new performance metric called server expansion related mean slowdown secondary performance goal fairness adopt standard definition fairness says tasks large small experience expected slowdown particular large tasks shouldnt penalized slowed greater factor small tasks 1 consider task assignment policies commonly proposed distributed server systems random task assignment policy incoming task sent host probability 1h h number hosts policy equalizes expected number tasks host roundrobin task signment tasks assigned hosts cyclical fashion ith task assigned host mod h policy also equalizes expected number tasks host slightly less variability interarrival times random shortestqueue task assignment incoming task immediately dispatched host fewest number tasks policy benefit trying equalize instantaneous number tasks host rather expected number tasks policies property tasks arriving host serviced fcfs order literature tells us shortestqueue fact best task assignment policy model following conditions met 1 priori knowledge tasks 2 tasks preemptible 3 host services tasks fcfs order 4 incoming tasks immediately dispatched host 5 task size distribution exponential see section 2 one removes restriction 4 possible even better wed really like send task host least total outstanding work work sum task sizes host host would afford task smallest waiting time however dont know priori host currently least work since dont know task sizes turns actually easy get around simply hold tasks dispatcher fcfs queue host free request next task easy prove holding method exactly equivalent immediately dispatching arriving tasks host least outstanding work see 6 proof figure 2 illustration refer policy leastworkremaining since effect sending task host currently least remaining work observe leastworkremaining comes closest obtaining instantaneous load balance may seem leastworkremaining best possible task assignment policy previous literature shows leastworkremaining outperforms previouslydiscussed policies general conditions see section 2 previous literature also suggests leastworkremaining 1 example processorsharing requires infinitelymany preemptions ultimately fair every task experiences expected slowdown dispatcher outside arrivals fcfs fcfs fcfs fcfs figure 1 illustration distributed server dispatcher outside arrivals fcfs fcfs fcfs fcfs send host least work fcfs b figure 2 two equivalent ways implementing leastworkremaining task assignment policy shows incoming tasks immediately dispatched host least remaining work requires knowing priori sizes tasks hosts b shows incoming tasks pooled fcfs queue dispatcher queues individual hosts host free request next task implementation require priori knowledge task sizes yet achieves effect may optimal best possible task assignment policy case task size distribution exponential see section 2 detailed statement previous literature task size distribution exponential motivated respect increasing evidence high variability task size distri butions seen many measurements computer workloads particular measurements many computer workloads shown fit heavytailed distributions high variance described section 3 much higher variance exponential distribution better task assignment policy leastworkremaining task size variability characteristic empirical workloads evaluating various task assignment policies interested understanding influence task size variability decision task assignment policy best analytical tractability assume arrival process poisson simulations indicate variability arrival process much less critical choosing task assignment policy variability task size distribution paper propose new algorithm called tags task assignment guessing size specifically designed high variability workloads prove analytically task sizes show degree variability characteristic empirical measured workloads tags algorithm outperform mentioned algorithms several orders magnitude fact show heavytailed task size distribution greater improvement tags task assignment algorithms improvements contingent system load high 2 case system load high show task assignment policies poor performance become impractical tags especially negatively affected practice system load high achieve reasonable performance one adds new hosts server without increasing outside arrival rate thus dropping system load system behaves desired refer number new hosts must added server expansion requirement show tags outperforms previouslymentioned task assignment policies respect server expansion metric ie given initial load tags requires far fewer additional hosts perform well describe three flavors tags first called tagsoptmeanslowdown designed minimize mean slowdown second called tagsoptmeanwaitingtime 2 distributed server system load defined follows system load outside arrival rate delta mean task size number hosts example system 2 hosts system load 5 outside arrival rate system 4 hosts system load 25 observe 4 host system system load ae twice outside arrival rate 2 host system system load ae designed minimize mean waiting time although effective algorithms fair treatment tasks third flavor called tagsoptfairness designed optimize fairness managing fair tagsoptfairness still achieves mean slowdown mean waiting time close flavors tags section 2 elaborates detail previous work area section 3 provides necessary background measured task size distributions heavytails section 4 describes tags algorithm flavors section 5 shows results analysis case 2 hosts section 6 shows results analysis multiplehost case section 7 explores effect lessvariable job size distributions lastly conclude section 8 details analysis tags described appendix previous work task assignment 21 task assignment preemption problem task assignment model like preemption priori knowledge extensively studied many basic questions remain open one subproblem solved task assignment restriction tasks immediately dispatched host upon arrival host services tasks fcfs order restricted model shown task size distribution exponential arrival process poisson shortestqueue task assignment policy optimal winston 19 result optimality defined maximizing discounted number tasks complete fixed time ephremides varaiya walrand 5 showed shortestqueue task assignment policy also minimizes expected total time completion tasks arriving fixed time exponential task size distribution arbitrary arrival process actual performance shortestqueue policy known exactly mean response time approximated nelson phillips 11 12 whitt shown variability task size distribution grows shortestqueue policy longer optimal 18 whitt suggest policy optimal scenario also considered restricted model described paragraph ages time service tasks currently serving known possible compute arriving tasks expected delay queue scenario weber 17 considers shortestexpecteddelay rule sends task host least expected work note similarity leastworkremaining policy weber shows rule optimal task size distributions increasing failure rate including exponential whitt 18 shows exist task size distributions rule optimal wolff 20 proven leastworkremaining best possible task assignment policy policies make use task size result holds distribution task sizes arrival process another model considered case preemption size task known time arrival task within model sitae algorithm see 7 shown outperform random roundrobin shortestqueue leastworkremaining algorithms several orders magnitude task size distribution heavy tailed contrast sitae tags algorithm require knowledge task size nevertheless nottoohigh system loads 5 tags improves upon performance sitae several orders magnitude heavytailed workloads 22 preemption allowed generalizations throughout paper maintain assumption tasks pre emptible task starts running stopped continued left contrast exists considerable work different problem tasks preemptible see 8 many citations generalizations task assignment problem include scenario hosts heterogeneous multiple resources contention idea purposely unbalancing load suggested previously 3 1 different contexts paper papers assumed task sizes known priori 3 distributed system preemptible tasks considered shown preemptible model mean waiting time minimized balancing load however mean slowdown minimized unbalancing load 1 realtime scheduling considered tasks firm deadlines context authors propose load profiling distributes load way probability satisfying utilization requirements incoming tasks maximized 3 heavy tails described section 1 concerned distribution task sizes affects decision task assignment policy use many application environments show mixture task sizes spanning many orders magnitude environments typically many small tasks fewer large tasks much previous work used exponential distribution capture variability described section 2 however recent measurements indicate many applications exponential distribution poor model heavytailed distribution accurate general heavytailed distribution one 2 simplest heavytailed distribution pareto distribu tion probability mass function cumulative distribution function set task sizes following heavytailed distribution following properties 1 decreasing failure rate particular longer task run longer expected continue running 2 infinite variance ff 1 infinite mean 3 property small fraction 1 largest tasks make large fraction half load refer important property throughout paper heavytailed property lower parameter ff variable distribution pronounced heavytailed property ie smaller fraction large tasks comprise half load concrete example figure 3 depicts graphically loglog plot measured distribution cpu requirements million unix processes taken paper 8 distribution closely fits curve prfprocess lifetime 8 shown distribution present variety computing en vironments including instructional research administrative environments fact heavytailed distributions appear fit many recent measurements computing systems include example ffl unix process cpu requirements measured bellcore 1 ff 125 10 ffl unix process cpu requirements measured uc berkeley ff 1 8 ffl sizes files transferred web 11 ff 13 2 4 ffl sizes files stored unix filesystems 9 ffl io times 14 ffl sizes ftp transfers internet 9 ff 11 13 cases estimates ff made ff tends close 1 represents high variability task service requirements practice upper bound maximum size task files finite lengths throughout paper therefore model task sizes generated iid distribution follows power law upper bound high one refer distribution bounded pareto characterized three parameters ff exponent power law k smallest possible observation p largest possible observation probability mass function bounded pareto bk p ff defined paper vary ffparameter range 0 2 order observe effect changing variability distribution focus effect changing variance keep distributional mean fixed 3000 maximum value fixed correspond typical values taken 2 order keep mean constant adjust k slightly ff note bounded pareto distribution moments finite thus heavytailed distribution sense defined ever distribution still show high variability k p example figure 4 right shows second moment e psi distribution function ff chosen keep e fxg constant 3000 1500 figure shows second moment explodes exponentially ff declines furthermore bounded pareto distribution also still exhibits heavytailed property extent decreasing failure rate property unbounded pareto distribution mention properties important determining choice best task assignment policy distribution process lifetimes log plot fraction processes duration duration secs12 figure 3 measured distribution unix process cpu lifetimes taken hd97 data indicates fraction jobs whose cpu service demands exceed seconds function power law w exponent second moment bounded pareto distribution alpha figure 4 parameters bounded pareto distribution left second moment function ff e outside arrivals figure 5 illustration flow tasks tags algorithm 4 tags algorithm section describes tags algorithm let h number hosts distributed server think hosts numbered h ith host number associated tags works shown figure 5 incoming tasks immediately dispatched host 1 serviced fcfs order complete using 1 amount cpu simply leave system however task used 1 amount cpu host 1 still hasnt completed killed remember tasks cannot preempted expensive model task put end queue host 2 restarted scratch 3 host services tasks queue fcfs order task host uses amount cpu still hasnt completed killed put end queue host 1 way tags algorithm guesses size task hence name tags algorithm may sound counterintuitive reasons first theres sense highernumbered hosts underutilized 3 note although task restarted still task course therefore careful analysis assign new service requirement first host overcrowded since incoming tasks sent host 1 even vital concern tags algorithm wastes large amount resources killing tasks restarting scratch 4 theres also sense big tasks especially penalized since theyre ones restarted tags comes 3 flavors differ chosen tagsoptmeanslowdown chosen optimize mean slowdown tagsoptmeanwaitingtime chosen optimize mean waiting time well see tagsoptmeanslowdownand tagsoptmeanwaitingtime necessarily fair tagsoptfairness chosen optimize fairness specifically tasks whose final destination host experience expected slowdown tagsoptfairness tasks whose final destination host j j tags may seem reminiscent multilevel feedback queueing related multilevel feedback queueing single host many virtual queues host timeshared tasks preemptible task uses amount service time transferred killed restarted lower priority queue also multilevel feedback queueing tasks lower priority queue allowed run tasks higher priority queues 5 analysis results case 2 hosts section contains results analysis tags task assignment policy task assignment policies order clearly explain effect tags algorithm limit discussion section case 2 hosts case refer tasks whose final destination host 1 small tasks tasks whose final destination host 2 big tasks section 53 always assume system load 05 2 hosts section 53 consider system loads still stick case 2 hosts finally section 6 consider distributed servers multiple hosts evaluate several task assignment policies function ff ff varianceparameter bounded pareto task size distribution ff ranges 0 2 recall section 3 lower ff higher variance task size distribution recall also empirical measurements task size distributions often show ff 1 4 dad micha harchol would add theres also psychological concern angry user might hes told tasks killed help general good evaluate random leastworkremaining tags policies roundrobin policy see section 1 evaluated directly showed previous paper 7 random roundrobin almost identical performance well explain section 51 analysis leastworkremaining approximation however confidence approximation extensive simulation paper 7 showed quite accurate setting well discuss section 51 analysis tags also approximation though lesser degree figure 6a shows mean slowdown tagsoptslowdown compared task assignment policies yaxis shown log scale observe high ff performance task assignment policies comparable good however ff decreases performance policies degrades leastworkremaining policy consistently outperforms random policy order magnitude however tagsoptslowdown policy offers several orders magnitude improvement 15 tagsoptslowdown policy outperforms leastworkremaining policy 2 orders magnitude ff 1 tagsoptslowdown policy outperforms leastworkremaining policy 4 orders magnitude 4 tagsoptslowdown policy outperforms leastworkremaining policy 9 orders magnitude increases 15 orders magnitude figures 6b c show mean slowdown tagsoptwaitingtime tagsoptfairness respectively compared task assignment policies since tagsoptwaitingtime optimized mean waiting time rather mean slowdown understandable performance improvements respect mean slowdown dramatic tagsoptslowdown however whats interesting performance tagsoptfairness close tagsoptslowdownand yet tagsoptfairness additional benefit fairness figure 7 identical figure 6 except case performance metric mean waiting time rather mean slowdown tags al gorithm especially tagsoptwaitingtime shows several orders magnitude improvement task assignment policies tags algorithm work well intuitively seems leastworkremaining best performer since leastworkremaining sends task individually experience lowest waiting time reason tags works well 2fold first part variance reduction section 51 second part load unbalancing section 52 using tagsoptslowdown 2 hosts load 5 alpha leastworkleft approx b results mean slowdown using tagsoptwaitingtime 2 hosts load 5 alpha leastworkleft approx c using tagsoptfairness 2 hosts load 5 alpha leastworkleft approx tagsoptfairness figure mean slowdown distributed server 2 hosts system using tagsoptslowdown 2 hosts load 5 alpha leastworkleft approx tagsoptfairness b results mean waiting time using tagsoptwaitingtime 2 hosts load 5 alpha leastworkleft approx tagsoptfairness c using tagsoptfairness 2 hosts load 5 alpha leastworkleft approx tagsoptfairness figure 7 mean waiting time distributed server 2 hosts system 51 variance reduction variance reduction refers reducing variance task sizes share queue intuitively variance reduction important improving performance reduces chance small task getting stuck behind big task queue stated formally theorem 1 derived pollaczekkinchin formula theorem 1 given mg1 fcfs queue arrival process rate x denotes service time distribution ae denotes utilization fxg let w tasks waiting time queue slowdown q queue length arrival proof slowdown formulas follow fact w x independent fcfs queue queue size follows littles formula observe every metric simple fcfs queue dependent psi second moment service time recall workload heavytailed second moment service time explodes shown figure 4 discuss effect high variability task sizes distributed server system various task assignment policies random task assignment random policy simply performs bernoulli splitting input stream result host becomes independent queue load ith host ae equal system load ae arrival rate ith host 1hfraction total outside arrival rate theorem 1 applies directly performance metrics proportional second moment bk p ff performance generally poor second moment bk p ff high round robin round robin policy splits incoming stream host sees e h bk p ff1 queue utilization ae system performance close random policy since still sees high variability service times dominates performance leastworkremaining leastworkremaining policy equivalent mgh queue exist known approximations 1621 qmgh qmmh x denotes service time distribution q denotes queue length whats important observe mean queue length therefore mean waiting time mean slowdown proportional second moment service time distribution case random roundrobin task assignment policies fact performance metrics proportional squared coefficient variation c service time distribution tags tags policy one reduces variance task sizes individual hosts let p fraction tasks whose final destination host consider tasks queue host first tasks destined host task size distribution bs original task size distribution bounded pareto tasks destined hosts numbered greater tasks capped size thus second moment task size distribution host lower second moment original bk p ff distribution hosts except highestnumbered host turns full analysis tags policy presented appendix relatively straightforward except one point fudge explain analytic convenience need able assume tasks arriving host form poisson process course true host 1 however arrivals host departures host exceed size form less bursty process poisson process since spaced apart least igamma1 throughout analysis tags make assumption arrival process host poisson process 52 load unbalancing second reason tags performs well load unbalancing observe task assignment policies described specifically try balance load hosts random roundrobin balance expected load hosts leastworkremaining goes even trying balance instantaneous load hosts tags opposite figure 8 shows load host 1 load host 2 tagsoptslowdown tagsoptwaitingtime tagsoptfairness function ff observe 3 flavors tags purposely severely underload host 1 ff low higher ff actually overload host 1 somewhat middle range ff 1 load balanced two hosts first explain load unbalancing desirable optimizing overall mean slowdown system later explain happens optimizing fairness understand desirable operate unbalanced loads need go back heavytailed property heavytailed property says distribution heavytailed low ff miniscule fraction tasks largest ones needed make half total load example case turns less 10 gamma6 fraction tasks needed make half load fact many tasks still less 10 gamma4 fraction tasks needed make 99999 fraction load suggests load game played choose cutoff point 1 tasks fraction host 1 final destination tasks largest fraction tasks host 2 final destination heavytailed property load host 2 extremely high 99999 load host 1 low 00001 since tasks get run reduced load overall mean slowdown low distribution little less heavytailed eg ff 1 cant play load unbalancing game well would like severely underload host 1 send 999999 fraction load go host 2 able making small fraction tasks 10 gamma4 fraction go host 2 however distribution heavytailed larger fraction tasks must host 2 final destination create high load host 2 turn means tasks destination host 2 count determining overall mean slowdown system bad since tasks destination host 2 experience larger slowdowns thus afford go far overloading host 2 turns us get ff 1 turns actually pays overload host 1 little seems counterintuitive since host 1 counts determining overall mean slowdown system fraction tasks destination host 1 greater however point impossible create wonderful state almost tasks host 1 yet host 1 underloaded tail heavy enough matter choose cutoff significant portion tasks host 2 destination thus host 2 inevitably figure overall mean slowdown need keep performance host 2 check turns need slightly underload host 2 make fact task size variability much greater host 2 host 1 explanation load unbalancing important respect optimizing system mean slowdown however clear load unbalancing also optimizes fairness tagsoptfairness 20103050709loads hosts tagsoptslowdown 2 hosts load 5 alpha host 1 load host 2 b 20103050709loads hosts tagsoptwaitingtime 2 hosts load 5 alpha host 1 load host 2 c 20103050709loads hosts tagsoptfairness 2 hosts load 5 alpha host 1 load host 2 figure 8 load host 1 compared host 2 distributed server 2 hosts system load 5 tagsoptslowdown b tagsoptwaitingtime c tagsoptfairness observe low ff host 1 run load close zero host 2 run load close 1 whereas high ff host 1 somewhat overloaded system load 03 using tagsoptslowdown 2 hosts load 3 alpha leastworkleft approx b system load 05 results mean slowdown using tagsoptslowdown 2 hosts load 5 alpha leastworkleft approx c system load 07 using tagsoptslowdown 2 hosts load 7 alpha leastworkleft approx figure 9 mean slowdown tagsoptslowdown distributed server 2 hosts system load 03 b 05 c 07 figure mean slowdown tagsoptslowdown compared performance random leastworkremaining observe figures tags outperforms task assignment policies ff however tags effective lower system loads mean slowdown experienced small tasks equal mean slowdown experienced big tasks however seems fact treating big tasks unfairly 3 counts 1 small tasks run host 1 low load low ff 2 small tasks run host 1 low e 3 small tasks dont restarted scratch wait second line possibly fair help small tasks much answer simply small tasks small thus need low waiting times keep slowdown low big tasks hand afford lot waiting time better able amortize punishment long lifetimes important mention though would case distributions task size distribution low ff heavytailed big tasks truly elephants way bigger smalls thus afford suffer 5 53 different loads studied model distributed server two hosts system load 5 section consider effect system load performance tags continue assume 2 host model figure 9 shows performance tagsoptslowdown distributed server 2 hosts run system load 03 b 05 c 07 three figures tagsoptslowdown improves upon performance leastworkremaining random full range ff however improvement tagsoptslowdown much better system lightly loaded fact task assignment policies improve system load dropped however improvement tags dramatic case system load 03 tagsoptslowdown improves upon leastworkremaining 4 orders magnitude 6 7 orders magnitude almost 20 orders magnitude system load 07 5 may interest reader understand degree unfairness exhibited tagsoptslowdown tagsoptwaitingtime tagsoptslowdown analysis shows expected slowdown big tasks always exceeds small tasks ratio increases exponentially ff drops efslowdownsmallsg contrast tagsoptwaitingtime expected slowdown big tasks approximately equal small tasks ff drops 1 point expected slowdown big tasks drops way small tasks ratio bigs smalls decreasing superexponentially ff drops hand tagsoptslowdown behaves comparably leastworkremaining ff improves upon leastworkremaining narrower range however ff 1 improvement tagsoptslowdown still 4 orders magnitude performance tags correlated load 2 reasons understood looking figure 10 shows loads 2 hosts tagsoptslowdown case system load 03 b 05 c 07 first reason ineffectiveness tags high loads higher load less able tags play loadunbalancing game described section 52 lower ff tags reaps much benefit lower ff moving load onto host 2 system load 05 tags easily able pile load host 2 without exceeding load 1 host 2 however system load 07 restriction load host 2 must exceed 1 becomes bottleneck tags since means host 1 underloaded tags would like seen comparing figure 10b figure 10c c load host 1 much higher lower ff b second reason ineffectiveness tags high loads call excess excess extra work created tags tasks killed restarted 2host case excess simply equal outside arrival rate p 2 fraction tasks whose final destination host 2 1 cutoff differentiating small tasks big tasks equivalent definition excess difference actual sum loads hosts h times system load h number hosts notice dotted line figure 10abc shows sum loads hosts weve considered distributed servers 2 hosts system load 05 scenario excess problem reason low ff need severe load unbalancing excess basically nonexistent loads 05 since p 2 small due heavytailed property since 1 could forced high ff excess present however task assignment policies already well high ff region low task size variability excess much handicap look case system load 07 however excess much problem evidenced dotted line figure 10c one reason excess worse simply overall excess increases load excess proportional turn proportional load reason excess worse higher loads 1 low ff range although p 2 still low due heavytailed property 1 cannot forced low load host 2 capped 1 thus excess low ff high make matters worse excess must heaped host 1 high ff range excess high p 2 high fortunately observe higher loads excess lowest point ff 1 fact barely existent region observe also region region balancing load optimal thing respect minimizing mean slowdown regardless system load sweet spot fortunate ff 1 characteristic many empirically measured computer workloads see section 3 6 analytic results case multiple hosts considered distributed servers 2 hosts case 2 hosts saw performance tagsoptslowdown amazingly good system load 05 less nearly good system load 05 section consider case 2 hosts phrase adding hosts ambiguous clear whether arrival rate increased well example given system hosts system load 07 could increase number hosts 4 hosts without changing arrival rate system load would drop 035 hand could increase number hosts 4 hosts increase arrival rate appropriately double maintain system load 07 discussions attempt clear view mind one claim made straight h host system h system load ae always configured produce performance least good 2 host system system load ae see observe use h host system assuming h even simulate 2 host system illustrated figure 11 rename hosts 1 2 subsystem 1 rename hosts 3 4 subsystem 2 rename hosts 5 6 subsystem 3 etc split traffic entering h host system 2hth tasks go h2 subsystems apply favorite task assignment policy subsystem independently case choose tags subsystem behave like 2 host system load ae running tags since subsystem identical performance performance whole host system equal performance one subsystem observe cute argument works task assignment policy figure 12 shows mean slowdown tagsoptslowdown case 4 host distributed server system load 03 comparing results 2 host system system load 03 figure 9a see system load 03 loads hosts tagsoptslowdown 2 hosts load 3 alpha host 1 load host 2 sum b system load 05 loads hosts tagsoptslowdown 2 hosts load 5 alpha host 1 load host 2 sum c system load 07 20611418loads hosts tagsoptslowdown 2 hosts load 7 alpha host 1 load host 2 sum figure 10 load host 1 host 2 tagsoptslowdown shown distributed server 2 hosts system load 03 b 05 c 07 dotted line shows sum loads 2 hosts excess dotted line would 06 b 10 c 14 graphs respectively figures b see excess higher ff range figure c see excess low ff high ff range around ff 1 dispatcher outside arrivals tags subsystem tags subsystem tags figure 11 illustration claim h host system h 2 system load ae always configured produce performance least good 2 host system system load ae although h host system much higher arrival rate results mean slowdown using tagsoptslowdown 4 hosts load 3 alpha leastworkleft approx figure 12 mean slowdown tagsoptslowdown compared task assignment policies case distributed server 4 hosts system load 03 cutoffs tagsoptslowdown optimized hand many cases possible improve upon results shown adjusting cutoffs slight bend graph may meaningful observe mean slowdown tags almost never exceeds 1 1 performance random stayed 2 performance leastworkremaining improved couple orders magnitude higher ff region less lower ff region leastworkremaining task assignment policy helped increasing number hosts although system load stayed hosts increases chances one free 3 performance tagsoptslowdown improved lot much mean slowdown tagsoptslowdown never 6 almost always 1 ff 1 tagsoptslowdown improves upon leastworkremainingby 45 orders magnitude improves upon leastworkremaining 89 orders magnitude improves upon leastworkremaining 25 orders magnitude enhanced performance tags hosts may come fact hosts allow greater flexibility choosing cutoffs however hard say sure difficult compute results case 2 hosts cutoffs case 2 hosts optimized mathematica case 4 hosts necessary perform optimizations hand know may possible even better case system load 07 4 hosts ran type problems 2 host case system load 07 61 server expansion performance metric one thing seems artificial current comparison task assignment policies one would ever willing run system whose expected mean slowdown practice system operating mean slowdown number hosts would increased without increasing arrival rate thus dropping system load systems performance improved reasonable mean slowdown like 3 less consider following example suppose 2host system running system load 7 variability parameter 6 system mean slowdown tagsoptslowdown order 10 9 task assignment policy know better suppose however desire system mean slowdown 3 less double number hosts without increasing outside arrival rate 4 hosts system load 035 tagsoptslowdown mean slowdown around 1 whereas leastworkremainings slowdown improved around 10 8 turns would increase number hosts 13 performance leastworkremaining improve point mean slowdown 3 random reach level would require additional 10 9 hosts example suggests new practical performance metric distributed servers call server expansion metric server expansion metric asks many additional hosts must added existing server without increasing outside arrival rate bring mean slowdown reasonable level well arbitrarily define reasonable 3 less figure 13 compares performance task assignment policies according server expansion metric given start 2 host system system load 07 tagsoptslowdown server expansion 3 2 ff leastworkremaining hand number hosts need add ranges 1 27 ff decreases still leastworkremaining bad least performance improves somewhat quickly hosts added load decreased reason effects increase probability task finding idle host contrast random shown figure 13b exponentially worse others requiring many 10 5 additional hosts ff 1 although random benefit increasing number hosts effect nonlog scale alpha tagsoptslowdown leastworkremaining b log scale server expansion requirement random leastworkremaining tagsoptslowdown alpha figure 13 server expansion requirement task assignment policies given start 2 host system system load 07 shows leastworkremaining tagsoptslowdown nonlog scale b shows leastworkremaining tagsoptslowdown random log scale second moment bounded pareto distribution bkpalpha alpha figure 14 second moment bk p ff distribution upper bound p set mean held fixed 3000 ff varied observe coefficient variation ranges 2 isnt nearly strong tags leastworkremaining 7 effect range task sizes purpose section investigate happens range task sizes difference biggest smallest possible task sizes smaller heretofore assumed resulting smaller coefficient variation task size distribution always assumed task sizes distributed according bounded pareto distribution upper bound fixed mean 3000 means example ff 1 agrees empirical data need set lower bound task sizes 167 however implies range task sizes spans 8 orders magnitude clear applications task sizes ranging 8 orders mag nitude section rederive performance task assignment policies upper bound p set still holding mean task size distribution 3000 means example ff 1 using tagsoptslowdown 2 hosts load 5p 10 7 alpha figure 15 mean slowdown tagsoptslowdown distributed server 2 hosts system load 05 compared performance random leastworkremaining set results task size distribution agrees empirical data need set lower bound task sizes implies range task sizes spans 5 orders magnitude figure 14 shows second moment bounded pareto task size distribution function ff comparing figure figure 4 see task size variability far lower therfore coefficient variation lower variance task size distribution suggests improvement tags task assignment policies dramatic higher variability setting fact case interesting however even lower variability setting improvement tags task assignment policies still impressive shown figure 15 figure 15 shows mean slowdown tagsoptslowdown compared random leastworkleft case two hosts system load 05 observe ff 1 tags improves upon task assignment policies 2 orders magnitude ff drops improvement increases figure contrasted figure 9b shows scenario 8 conclusion future work paper interesting proposes powerful new task assignment policy challenges natural intuitions come adopt time common knowledge traditionally area task assignment load balancing load sharing consisted heuristics seek balance load among multiple hosts tags hand specifically seeks unbalance load sometimes severely unbalance load traditionally idea killing task restarting scratch different machine viewed skepticism possibly tolerable new host idle tags hand kills tasks restarts target host typically operating extremely high load much higher load original source host furthermore tags proposes restarting task multiple times interesting consider implications results outside scope task assignment consider example question scheduling cpubound tasks single cpu tasks preemptible priori knowledge given tasks first seems fcfs scheduling option however fact high task size variability fcfs may wise paper suggests killing restarting tasks may worth investigating alternative load cpu low enough tolerate extra work created task assignment also applications outside context distributed server system described paper interesting recent paper shaikh rexford shin 15 discusses routing ip flows also heavytailed size distributions recommends routing long flows differently short flows r load profiling methodology scheduling realtime tasks distributed system task assignment distributed system improving performance unbalancing load simple dynamic routing problem task assignment distributed server task assignment distributed server exploiting process lifetime distributions dynamic load balancing unix file size survey year1993year approximation response time shortest queue routing approximation mean response time shortest queue routing general interarrival service times fractal patterns dasd io traffic approximations finite capacity multiserver queues poisson arrivals optimal assignment customers parallel servers deciding queue join counterexamples optimality shortest line discipline upper bound multichannel queues stochastic modeling theory queues tr deciding queue join counterexamples approximation response time shortest queue routing approximation mean response time shortest queue routing general interarrival service times area traffic exploiting process lifetime distributions dynamic load balancing selfsimilarity world wide web traffic task assignment distributed system extended abstract heavytailed probability distributions world wide web loadsensitive routing longlived ip flows loadbalancing heuristics process behavior choosing task assignment policy distributed server system implementing multiprocessor scheduling disciplines theory practice parallel job scheduling improved utilization responsiveness gang scheduling valuation ultrascale computing systems parallel workload model implications processor allocation evaluation task assignment policies supercomputing servers load profiling ctr jianbin wei chengzhong xu design implementation feedback controller slowdown differentiation internet servers special interest tracks posters 14th international conference world wide web may 1014 2005 chiba japan konstantinos psounis pablo molinerofernndez balaji prabhakar fragkiskos papadopoulos systems multiple servers heavytailed workloads performance evaluation v62 n14 p456474 october 2005 victoria ungureanu benjamin melamed phillip g bradford michael katehakis classdependent assignment clusterbased servers proceedings 2004 acm symposium applied computing march 1417 2004 nicosia cyprus victoria ungureanu benjamin melamed michael katehakis phillip g bradford deferred assignment scheduling clusterbased servers cluster computing v9 n1 p5765 january 2006 mor harcholbalter cuihong li takayuki osogami alan schellerwolf mark squillante cycle stealing immediate dispatch task assignment proceedings fifteenth annual acm symposium parallel algorithms architectures june 0709 2003 san diego california usa jianbin wei xiaobo zhou chengzhong xu robust processing rate allocation proportional slowdown differentiation internet servers ieee transactions computers v54 n8 p964977 august 2005 james broberg zahir tari panlop zeephongsekul task assignment workconserving migration parallel computing v32 n1112 p808830 december 2006 madhusudan youngjun son simulationbased approach dynamic process management web service platforms computers industrial engineering v49 n2 p287317 september 2005