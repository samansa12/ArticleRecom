block distributed memory model abstractwe introduce computation model developing analyzing parallel algorithms distributed memory machines model allows design algorithms using single address space assume particular interconnection topology capture performance incorporating cost measure interprocessor communication induced remote memory accesses cost measure includes parameters reflecting memory latency communication bandwidth spatial locality model allows initial placement input data pipelined prefetchingwe use model develop parallel algorithms various data rearrangement problems load balancing sorting fft matrix multiplication show algorithms achieve optimal near optimal communication complexity simultaneously guaranteeing optimal speedup computational complexity ongoing experimental work testing evaluating algorithms thus far shown promising results b introduction parallel processing promises offer quantum leap computational power likely substantial impact various aspects computing field particular exploited investigate wide range called grand challenge problems science engineering widely recognized 23 important ingredient success technology emergence computational models used algorithms development accurately predicting performance algorithms real machines take similar view 24 computation model bridging model links two layers hardware software existing computation models tend biased towards one layer except exceptions bulk synchronous parallel bsp model advocated valiant 24 one exceptions paper introduce computation model specifically attempts bridging model shared memory single address programming model distributedmemory message passing architectures distributed memory systems configured single address space usually referred scalable shared memory multiprocessors machines achieve scalability distributed memory architectures simple programming style provided single address space model also used predicting performance data parallel algorithms running distributed memory architectures since computation model predict performance real machines start discussion basis measure communication costs incurred accessing remote data indicated 8 hardware organizations massively parallel processors mpps seem converging towards collection powerful processors connected communication network modeled complete graph communication subject restrictions imposed latency bandwidth properties network according common ganization communication different processors handled point topoint messages whose routing times controlled parameters related network latency processor communication bandwidth overhead preparing mes sage network capacity model avoids description exact structure network since algorithms exploiting specific features network less likely robust enough work well variety architectures adapt easily possible future technological changes however programming machine messagepassing level imposes heavy burden programmer makes algorithms development evaluation quite complicated hand dataparallel sharedmemory programming models appealing terms ease use terms close relationship sequential programming models assume single address space block distributed memory bdm model introduced next section captures performance shared memory single address space algorithms incorporating cost measure interprocessor communication caused remote memory accesses cost modeled using latency communication bandwidth processor since remote memory access involves transmission packet typically contains number consecutive words model encourages use spatial locality incorporating parameter represents cost associated accessing consecutive words cost incurred even single word needed model allows initial placement input data includes memory latency hiding technique pipelined prefetching since measure amount local computation amount communication separately able normalize communication cost drop one parameter make analysis corresponding algorithms simpler use model develop parallel algorithms various data rearrangement problems load balancing sort ing fast fourier transform fft computation matrix multiplication show algorithms achieve optimal near optimal communication complexity simultaneously guaranteeing optimal speedup computational complexity next section provide details model section 3 describes collection algorithms handling data rearrangements occur frequently shared memory algorithms load balancing problem addressed section 4 communication efficient algorithm presented section 5 devoted presentation efficient algorithms sorting fft matrix multiplication resulting algorithms seem share common structure highperformance algorithms tested real machines 2 block distributed memory bdm model computation model block distributed memory bdm defined terms four parameters p oe see later parameter oe dropped without loss generality parameter p refers number pro cessors processor viewed unit cost random access machine ram addition processor interface unit interconnection network handles communication among different processors data communicated processors via pointtopoint messages message consists packet holds words consecutive locations local processor memory since assuming shared memory programming model request remote location involves preparation request packet injection packet network reception packet destination processor finally sending packet containing contents consecutive locations including requested value back requesting processor model cost handling request remote location read write formula maximum latency time takes requesting processor receive appropriate packet oe rate processor inject receive word network moreover processor send receive one packet time result note following two observations first permutation p elements remote memory request issued processor destined processor p completed moe time processors simultaneously second k remote access requests issued k distinct processors destined processor require k moe time completed addition make assumption relative order requests completed current interconnection networks multiprocessors use several hardware software techniques hiding memory latency model allow pipelined prefetching hiding memory latency particular k prefetch read operations issued processor completed underlying communication model bdm consistent logp postal models 8 13 5 addition parameter incorporates spatial locality however model allow lowlevel handling message passing primitives except implicitly data accesses particular algorithm written model specify initial data placement among local memories p processors use processor id refer specific data items use synchronization barriers synchronize activities various processors whenever necessary remote data accesses charged according communication model specified synchronization barriers make assumption bdm model provided primitive operations two main reasons making assumption first barriers implemented hardware efficiently relatively small cost second make latency parameter large enough account synchronization costs resulting communication costs conservative side affect overall structure resulting algorithms complexity parallel algorithm bdm model evaluated terms two measures computation time comp communication time tcomm measure comp refers maximum local computation performed processor measured standard sequential ram model communication time tcomm refers total amount communication time spent overall algorithm accessing remote data main goal design parallel algorithms achieve optimal nearoptimal computational speedups sequential complexity problem consideration way total communication time tcomm minimized since tcomm treated separately comp normalize measure dividing oe underlying communication model bdm viewed postal model 5 added parameter reflecting spatial locality hence access operation remote location takes time k prefetch read operations executed time note parameter viewed upper bound capacity interconnection network ie upper bound maximum number words transit processor estimates bounds communication time make simplifying reasonable assumption integral multiple believe locality important factor taken consideration designing parallel algorithms large scale multiprocessors incorporated parameter model emphasize importance spatial locality notion processor locality also seems important current multiprocessor architectures architectures tend hierarchical hence latency much higher accessing processors hierarchy close feature incorporated model modifying value reflect cost associated level hierarchy needs used remote memory access done similar fashion memory hierarchy model studied 2 sequential processors however paper opted simplicity decided include processor locality consideration several models discussed literature logp postal models referred earlier related bdm model however significant differences model models example asynchronous pram 9 block pram 1 assume presence shared memory intermediate results held particular assume data initially stored shared memory makes data movement operations considerably simpler model another example direct connection machine dcm latency 14 uses message passing primitives particular model allow pipelined prefetching bdm model 3 basic algorithms data movements design communication efficient parallel algorithms depends existence efficient schemes handling frequently occurring transformations data layouts section consider data layouts specified twodimensional array say size q theta p column contains subarray stored local memory processor transformation pi layout map elements layout pia necessarily size present optimal near optimal algorithms handle several transformations including broadcasting operations matrix transposition data permutation algorithms described deterministic except algorithm perform general permutation start addressing several broadcasting operations simplest case broadcast single item number remote locations hence layout described onedimensional array assume element a0 copied remaining entries viewed concurrent read operation location a0 executed processors next lemma provides simple algorithm solve problem later use algorithm derive optimal broadcasting algorithm lemma 31 given pprocessor bdm array a0 resides processor p j element a0 copied remaining entries time proof simple algorithm consists rounds pipelined rth round processor p j reads aj a0 copied aj since rounds realized prefetch read operations resulting communication complexity ready following theorem essentially establishes fact kary balanced tree broadcasting algorithm best possible recall earlier made assumption integral multiple theorem 31 given pprocessor bdm item processor broadcast remaining processors 2d log p log e communication time hand broadcasting algorithm uses read write synchronization barrier instructions requires least log p log log p communication complexity 4 proof start describing algorithm let k integer determined later algorithm viewed kary tree rooted location a0 dlog k pe rounds first round a0 broadcast locations using algorithm described lemma 31 followed synchronization barrier second round element locations broadcast distinct set locations communication cost incurred round given 31 therefore total communication cost tcomm set md log p log log e next establish lower bound stated theorem broadcasting algorithm using read write synchronization barrier instructions viewed operating phases phase ends synchronization barrier whenever single phase suppose phases amount communication execute phase least maximum number copies read processor phase hence total amount communication required least note end phase desired item reached remote locations follows end phase desired item reached processors must 1 communication time minimized k logk1 communication time least logk1 complete proof theorem proving following claim logk1 log k 1 proof claim let logk1 logk1 logk1 case decreasing f 2 k increasing range claims follows easily noting f 1 log 4 logarithms base 2 unless otherwise stated case show fk increasing k r1 showing 1 note since k least large positive nonzero integer values k hence fk f claim follows 2 sum p elements pprocessor bdm computed 2d log p log e communication time using similar strategy based observation easy show following theorem theorem 32 given n numbers distributed equally among p processors bdm compute sum n log time 2d log p log e communication time computation time reduces n another simple broadcasting operation processor broadcast item remaining processors operation executed minf time shown next lemma lemma 32 given pprocessor bdm array a0 distributed one element per processor problem broadcasting element processors done minf communication time proof bound follows simple algorithm described lemma 31 p significantly larger use following strategy use previous algorithm processor elements next block elements broadcast circular fashion appropriate p processors one verify resulting communication complexity 2 next data movement operation matrix transposition defined follows let q p let p divide q evenly without loss generality data layout described supposed rearranged layout 0 first column 0 contains first qp consecutive rows laid row major order form second column 0 contains second set qp consecutive rows clearly corresponds usual notion matrix transpose efficient algorithm perform matrix transposition bdm model similar algorithm reported 8 rounds fully pipelined using prefetch read operations first round appropriate block qp elements ith column read processor p i1modp appropriate locations second round appropriate block data column read processor p i2modp resulting total communication time given pm amount local computation oq clearly algorithm optimal whenever pm divides q hence following lemma lemma 33 q theta p matrix transposition performed pprocessor bdm pm e bound reduces next discuss broadcasting operation block n elements residing single processor p processors describe two algorithms first suitable number n elements relatively small second suitable large values n algorithms based circular data movement used matrix transposition algorithm details given proof next theorem theorem 33 problem broadcasting n elements processor p processors completed 22d log p log using kary balanced tree algorithm hand problem solved 2 pm e communication time using matrix transposition algorithm proof first algorithm use kary tree single item broadcasting algorithm described theorem 31 1 using matrix transposition strategy distribute n elements broadcast among k processors processor receives contiguous block size n k view p processors partitioned k groups group includes exactly one processors contains block items broadcast procedure repeated within group similar reverse process gradually read n items processor forward backward phase carried using cyclic data movement matrix transposition algorithm one check communication time bounded follows km log p log pm broadcast n elements 2 pm communication time using matrix transposition algorithm lemma 33 twice distribute n elements among p processors processor receives block size second time circulate blocks processors 2 problem distributing n elements single processor solved using first half either two broadcasting algorithms hence following corollary corollary 31 problem distributing n elements one processor processors processor receives n p elements completed minf2d log p log pm eg communication time 2 finally address following general routing problem let n array n elements initially stored one column per processor pprocessor bdm machine element consists pair datai index processor data relocated assume ff n elements routed single processor constant ff 1 describe follows randomized algorithm completes routing 2 n computation time c constant larger maxf1 1 ffg complexity bounds guaranteed hold high probability probability positive constant ffl long 6 logarithm base e overall idea algorithm used various randomized routing algorithms mesh follow closely scheme described 20 randomized routing mesh bounded queue size describing algorithm introduce terminology use auxiliary array 0 size cn theta p manipulating data intermediate stages holding final output c ffg column 0 held processor array 0 divided p equal size slices slice consisting cn consecutive rows 0 hence slice contains set cn consecutive elements column set referred slot ready describe algorithm algorithm randomized routing input input array a0 n element consists pair datai processor index data routed processor destination ff n p elements constant ff output output array holding routed data c constant larger maxf1 ffg begin processor p j distributes randomly n elements p slots jth column 0 jth slice stored jth processor processor p j distributes locally cn elements every element form resides slot perform matrix transposition 0 hence jth slice layout generated end step 3 resides p j next two facts allow us derive complexity bounds randomized routing algorithm analysis assume lemma 34 completion step 1 number elements slot cn high probability c pproof procedure performed processor similar experiment throwing n bins hence probability exactly cn balls placed particular bin given binomial distribution using following chernoff bound estimating tail binomial distribution obtain probability particular bin cn balls upper bounded therefore probability bins cn balls bounded lemma follows 2 lemma 35 completion step 3 number elements processor destined processor cn high probability ffproof probability element assigned jth slice end step 1 1 hence probability cn elements destined single processor fall jth slice bounded b cn processor destination ffn elements since p slices probability cn elements processor destined processor bounded pe gamma c hence lemma follows 2 previous two lemmas easy show following theorem theorem 34 routing n elements stored initially n array pprocessor bdm ff n p elements destined processor completed high probability 2 computation time c constant larger maxf1 ffg remark since assuming 6 effect parameter dominated bound c n n balancing balancing load among processors important since poor balance load generally causes poor processor utilization 19 load balancing problem also important developing fast solutions basic combinatorial problems sorting selection list ranking graph problems 12 21 problem defined follows load processor p given array represents number useful elements p max n supposed redistribute elements p processors n p elements stored processor assumed without loss generality p divides n section develop simple efficient load balancing algorithm bdm model corresponding communication time given tcomm 5 overall strategy described next overall strategy load balancing algorithm described follows first load balancing problem n 01 n stored p arrays considered hence output array 0 array 0 may km pm steps 2 3 next processors elements appropriate processors step 4 details given next algorithm assume simplicity n powers two algorithm load balancing processor p contains input array 0 elements redistributed way n p elements stored output array 0 begin processor p reads held remaining processors step performed 2 communication time lemma 32 performs following local computations 21 22 compute prefix sums 23 else remark index chosen way read n elements p read n elements indices l r used next step determine locations n p n elements moved p notice step takes op computation time reads appropriate numbers elements l r respectively remark step needs special attention since cases set consecutive processors read elements one processor say p h processors p read appropriate elements p notice h npgammam 1 step divided two substeps follows first substep read elements substep done communication time applying corollary 31 second substep rest routing performed using sequence read prefetch operations since remaining elements processor accessed single processor hence total communication time required step n reads remaining elements appropriate processors corresponding indices l 0 computed locally step 2 remark step completed processor reads elements processors reads prefetched thus one show following theorem theorem 41 load balancing n elements p processors elements reside processor realized 5 communication time 2 5 sorting fft matrix multiplication section consider three basic computational problems sorting fft matrix multiplication present communication efficient algorithms solve problems bdm model basic strategies used wellknown implementations model require careful attention several technical details 51 sorting first consider sorting problem bdm model three strategies seem perform best model 1 column sort 15 2 sample sort see eg 6 related references 3 rotate sort 18 turns column sort algorithm best n sample sort rotate sort better column sort algorithm particularly useful n implemented 4 time column sort algorithm practical since constant term grows exponentially n decreases sample sort algorithm provably efficient 6 implemented 3 p gamma 1d 5 time n log n probability rotate sort algorithm implemented 8 computation time whenever n 6p 2 begin description column sort algorithm column sort column sort algorithm generalization oddeven mergesort described series elementary matrix operations let q theta p matrix elements initially entry matrix one n elements sorted completion algorithm sorted column major order form column sort algorithm eight steps steps 1 3 5 7 elements within column sorted steps 2 4 entries matrix permuted permutations similar matrix transposition lemma 33 since case two steps done 2 communication time steps 6 8 consists qshift operation clearly done communication time hence column sort algorithm implemented model within 4 thus following theorem theorem 51 given n elements n p elements stored local memories set p processors n elements sorted column major order form 4 computation time 2 second sorting algorithm consider section sample sort algorithm randomized algorithm whose running time depend input distribution keys depends output random number generator describe version sample sort algorithm sorts bdm model 3 em computation time whenever complexity bounds guaranteed high probability use randomized routing algorithm described section 3 overall idea algorithm used various sample sort algorithms algorithm described follows closely scheme described 6 sorting connection machine cm2 however first three steps different algorithm sample sort input n elements distributed evenly pprocessor bdm output n elements sorted column major order begin processor p randomly picks list 5 ln n elements local memory processor p reads samples processors hence processor 5p ln n samples execution step processor p sorts list 5p ln n samples pick 5 samples processor p partitions n elements set ij belong interval jth pivot j 1st pivot 0th pivot gamma1 pth pivot 1 processor p reads elements p sets 0i using algorithm randomized routing processor p sorts elements local memory following lemma immediately deduced results 6 lemma 51 ff 0 probability processor contains elements step 5 ne next show following theorem theorem 52 algorithm sample sort implemented pprocessor bdm n log n 3 em time high probability 6 proof step 2 done em communication time using technique similar used prove lemma 32 lemma 51 total number elements processor reads step 5 2 n elements high probability hence step 5 implemented communication time high probability using theorem 34 computation time steps clearly n log n 6 theorem follows 2 rotate sort rotate sort algorithm 18 sorts elements mesh alternately applying transformations rows columns algorithm runs constant number rowtransformation columntransformation phases 16 phases assume n 6p 2 naive implementation original algorithm model requires 14 simple permutations similar matrix transpositions 14 local sortings within pro cessor slightly modify algorithm algorithm implemented model 8 simple permutations 14 local sortings within processor since simple permutation performed model communication time algorithm implemented 8 communication time n log n computation time bdm model simplicity assume n 2t results generalized values n p slice subarray size p p theta p consisting rows l block subarray size p p theta p p consisting positions j l r describe algorithm briefly details appear 18 begin specifying three procedures serve building blocks main algo rithm procedure consists sequence phases accomplish specific transformation array procedure balance input array size v theta w sort columns downwards b rotate row mod w positions right c sort columns downwards procedure unblock rotate row mod p p p p positions right b sort columns downwards procedure shear sort evennumbered columns downwards oddnumbered columns upwards b sort rows right overall sorting algorithm following algorithm rotatesort 1 balance input array size n p theta p 2 sort rows right 3 unblock array 4 balance slice p theta p p array lying side 5 unblock array 6 transpose array 7 shear array 8 sort columns downwards complete correctness proof algorithm see 18 easily prove algorithm performed 14 local sorting steps within processor also prove simple permutations done time similar way lemma 33 steps 1 3 5 6 done one simple permutation steps 2 4 also done one simple permutation overlapping second permutations first permutations steps 3 5 respectively originally step 7 repeat shear three times designed removing six dirty rows left step 5 hence step requires 6 simple permutations model since assumed 6p length column larger row reduce number applications shear procedure step 7 transposing matrix step 6 thus since assumption n 6p implies two dirty columns execution step 6 one application procedure shear enough step 7 removing two dirty columns following theorem theorem 53 given n 6p 2 elements n elements p processors bdm model n elements sorted column major order form 8 communication time n log n notice need repeat shear dlog1 e times step 7 removing dirty columns communication time algorithm rotatesort k e sorting algorithms given elements integers 0 p o1 local sorting needed previous algorithms done n applying radix sort hence following corollary corollary 51 pprocessor bdm machine n integers 0 p o1 sorted column major order form n computation time 1 4 communication time n e 3 3 em communication time high probability two sorting algorithms worth considering 1 radix sort see eg 6 related references 2 approximate median sort 2 22 radix sort performed model b r r tcomm n p communication time b number bits representation keys r algorithm examines keys sorted rbits time tcomm n p communication time routing general permutation n elements hence bounds corollary apply approximate median sort similar sample sort randomization used processor sorting elements processor done model n log n 3 communication time n 52 fast fourier transform next consider fast fourier transform fft computation algorithm computes discrete fourier transform dft ndimensional complex vector x defined cos 2 gamma1 log n arithmetic operations implementation bdm model based following wellknown fact eg see 17 let ndimensional vector x stored n p theta p matrix x rowmajor order form p arbitrary integer divides n dft vector x given w n submatrix w n consisting first n rows first p columns twiddlefactor scaling elementwise multiplication notice resulting output p theta n matrix holding vector w n x column major order form equation 1 interpreted computing dft n p column x followed twiddlefactor scaling finally computing dftp row resulting matrix let bdm machine p processors p divides n n p 2 initial data layout corresponds row major order form data ie local memory processor p hold x 1 algorithm suggested 1 performed following three stages first stage involves local computation dft size n p processor followed twiddlefactor scaling elementwise multiplication w n second stage communication step involves matrix transposition outlined lemma 33 finally n local ffts size p sufficient complete overall fft computation n points therefore following theorem theorem 54 computing npoint fft done n log n divides n communication time reduces remark algorithm described 8 also implemented model within complexity bounds however algorithm somewhat simpler implement 2 53 matrix multiplication finally consider problem multiplying two nthetan matrices b assume log n partition matrices b p 2 3 submatrices say ij b ij size theta n assuming without loss generality p 1 3 integer divides n simplicity view processors indices arranged cube size p 1 3 theta p 1 3 theta p 1 given p ijk 0 show product computed n 3 computation time 62d log p communication time overall strategy similar one used 1 10 related experimental results supporting efficiency algorithm appear 10 presenting algorithm establish following lemma lemma 52 given p matrices size n theta n distributed one matrix per proces sor sum computed 2 computation time 22d log p log communication time proof partition p processors p k groups group contains processors 1 using matrix transposition algorithm put first set nk rows matrix group first processor group second set nk rows second processor processor add k submatrices locally point processors group holds n k theta n portion sum matrix corresponding initial matrices stored processors continue strategy adding set k submatrices within group k processors however time submatrices partitioned along columns resulting submatrices size n theta n repeat procedure log p log e times time processor n portion overall sum matrix collect submatrices single processor complexity bounds follow proof theorem 33 2 algorithm matrix multiplication input two n theta n matrices b p n 3 log n initially submatrices ij b ij stored processor p ij0 0 output processor p ij0 holds submatrix c ij c ij size n theta n begin blocks ij b jk initially stored processors respectively block read concurrently p 13 processors step performed 42d log p communication time using first algorithm described theorem 33 processor multiplies two submatrices stored local memory step done n 3 computation time 0 sum product submatrices p 1processors p ijk computed stored processor p ij0 step done n 2 computation time 22d log p communication time using algorithm described lemma 52 therefore following theorem theorem 55 multiplying two n theta n matrices completed n 3 time 62d log p communication time pprocessor bdm model log n 2 remark could used second algorithm described theorem 33 execute steps 1 3 matrix multiplication algorithm resulting communication bound would 6 r communication latency pram computations hierarchical memory block transfer april processor architecture multiprocessing designing broadcasting algorithms postal model messagepassing systems multiple message broadcasting postal model comparison sorting algorithms connection machine cm2 overview ksri computer system logp toward realistic model parallel computation asynchronous pram algorithms scalability parallel algorithms matrix mul tiplication cachecoherence protocol data diffusion machine load balancing routing hypercube related networks optimal broadcast summation logp model complexity theory efficient parallel algorithms tight bounds complexity parallel sorting stanford dash multiprocessor computational frameworks fast fourier transform sorting constant number row column phases mesh probabilistic analysis locality maintaining load balancing algorithm optimal routing algorithms meshconnected processor arrays efficient algorithms list ranking solving graph problems hypercube parallel sorting regular sampling report purdue workshop grand challenges computer architecture support high performance computing bridging model parallel computation tr ctr david r helman david bader joseph jj parallel algorithms personalized communication sorting experimental study extended abstract proceedings eighth annual acm symposium parallel algorithms architectures p211222 june 2426 1996 padua italy assefaw hadish gebremedhin mohamed essadi isabelle gurin lassous jens gustedt jan arne telle pro model design analysis efficient scalable parallel algorithms nordic journal computing v13 n4 p215239 december 2006