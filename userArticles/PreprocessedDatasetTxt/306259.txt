elevator group control using multiple reinforcement learning agents recent algorithmic theoretical advances reinforcement learning rl attracted widespread interest rl algorithms appeared approximate dynamic programming incremental basis trained basis real simulated experiences focusing computation areas state space actually visited control making computationally tractable large problems member team agents employs one algorithms new collective learning algorithm emerges team whole paper demonstrate collective rl algorithms powerful heuristic methods addressing largescale control problemselevator group control serves testbed difficult domain posing combination challenges seen multiagent learning research date use team rl agents responsible controlling one elevator car team receives global reward signal appears noisy agent due effects actions agents random nature arrivals incomplete observation state spite complications show results simulation surpass best heuristic elevator control algorithms aware results demonstrate power multiagent rl large scale stochastic dynamic optimization problem practical utility b introduction interest developing capable learning systems increasing within multiagent ai research communities eg weiss sen 1996 learning enables systems flexible robust makes better able handle uncertainty changing circumstances especially important multiagent systems designers systems often faced extremely difficult task trying anticipate possible contingencies interactions among agents ahead time much could said concerning field decentralized control policies control stations developed global vantage point learning play role even though executing policies depends information available control station policies designed centralized way access complete description problem research focused constitutes optimal policy given information pattern policies might learned constraints reinforcement learning rl barto sutton forthcoming bertsekas tsitsik lis 1996 applies naturally case autonomous agents receive sensations inputs take actions affect environment order achieve goals rl based idea tendency produce action strengthened reinforced produces favorable results weakened produces unfavorable results framework appealing biological point view since animal certain builtin preferences pleasure always teacher tell exactly action take every situation members group agents employ rl algorithm resulting collective algorithm allows control policies learned decentralized way even situations centralized information available may advantageous develop control policies decentralized way order simplify search policy space although may possible synthesize system whose goals achieved agents conflicting objectives paper focuses teams agents share identical objectives corresponding directly goals system whole demonstrate power multiagent rl focus difficult problem elevator group supervisory control elevator systems operate highdimensional continuous state spaces continuous time discrete event dynamic systems states fully observable nonstationary due changing passenger arrival rates use team rl agents responsible controlling one elevator car agent uses artificial neural networks store action value estimates compare parallel architecture agents share networks decentralized architecture agents independent networks either case team receives global reinforcement signal noisy perspective agent due part effects actions agents despite difficulties system outperforms heuristic elevator control algorithms known us also analyze policies learned agents show learning relatively robust even face increasingly incomplete state information results suggest approaches decentralized control using multiagent rl considerable promise following sections give additional background rl introduce elevator domain describe detail multiagent rl algorithm network architecture used present discuss results finally draw conclusions details topics see crites 1996 2 reinforcement learning symbolic connectionist learning researchers focused primarily supervised learning teacher provides learning system set training examples form inputoutput pairs supervised learning techniques useful wide variety problems involving pattern classification function approximation however many situations training examples costly even impossible obtain rl applicable difficult situa tions help available critic provides scalar evaluation output selected rather specifying best output direction change output rl one faces difficulties supervised learning combined additional difficulty exploration determining best output given input rl tasks divided naturally two types nonsequential tasks agents must learn mappings situations actions maximize expected immediate payoff sequential tasks agents must learn mappings situations actions maximize expected longterm payoffs sequential tasks difficult actions selected agents may influence future situations thus future payoffs case agents interact environment extended period time need evaluate actions basis longterm consequences perspective control theory rl techniques ways finding approximate solutions stochastic optimal control problems agent controller environment system controlled objective maximize performance measure time given model state transition probabilities reward structure environment problems solved principle using dynamic programming dp algorithms however even though dp requires time polynomial number states many problems interest many states amount time required solution infeasible recent rl algorithms designed perform dp incremental manner unlike traditional dp algorithms require priori knowledge state transition probabilities reward structure environment used improve performance online interacting environment online learning focuses computation areas state space actually visited control thus algorithms computationally tractable way approximating dp large problems focusing phenomenon also achieved simulated online train ing one often construct simulation model without ever explicitly determining state transition probabilities environment barto sutton forthcoming crites barto 1996 example simulation model see section 33 several advantages use simulation model sufficiently ac curate possible generate huge amounts simulated experience quickly potentially speeding training process many orders magnitude would possible using actual experience addition one need concerned performance level simulated system training successful example simulated online training found tesauros tdgammon system 1992 1994 1995 used rl techniques learn play strong masterlevel backgammon 21 multiagent reinforcement learning variety disciplines contributed study multiagent systems many researchers focused topdown approaches building distributed systems creating global vantage point one drawback topdown approach extraordinary complexity designing agents since extremely difficult anticipate possible interactions contingencies ahead time complex systems researchers recently taken opposite approach combining large numbers relatively unsophisticated agents bottomup manner seeing emerges put together group amounts sort iterative procedure designing set agents observing group behavior repeatedly adjusting design noting effect group behavior although groups simple agents often exhibit interesting complex dynamics little understanding yet create bottomup designs achieve complex predefined goals multiagent rl attempts combine advantages approaches achieves simplicity bottomup approach allowing use relatively unsophisticated agents learn basis experiences time rl agents adapt topdown global reinforcement signal guides behavior toward achievement complex predefined goals result robust systems complex problems created minimum human effort crites barto 1996 research multiagent rl dates back least work russian mathematician tsetlin 1973 others field learning automata see narendra thathachar 1989 number theoretical results obtained context nonsequential rl certain types learning automata converge equilibrium point zerosum nonzerosum repeated games see narendra thathachar 1989 details teams equilibrium point local maximum element game matrix maximum row column however general nonzerosum games equilibrium points often provide poor payoffs players good example prisoners dilemma equilibrium point produces lowest total payoff axelrod 1984 starting approximately 1993 number researchers began investigate applying sequential rl algorithms multiagent contexts although much work simplistic domains grid worlds several interesting applications appeared pointed promise sequential multiagent rl markey 1994 applies parallel qlearning problem controlling vocal tract model 10 degrees freedom discusses two architectures equivalent distributed parallel architectures described section 44 agent controls one degree freedom action space distinguishes qvalues based action selections bradtke 1993 describes initial experiments using rl decentralized control flexible beam task efficiently damp disturbances beam applying forces discrete locations times uses 10 independent adaptive controllers distributed along beam controller attempts minimize local costs observes local portion state information dayan hinton 1993 propose managerial hierarchy call feudal rl scheme higherlevel managers set tasks lower level managers reward see fit since rewards may different different levels hierarchy team furthermore single action selected lowest level actually affects environment sense hierarchical architecture single agent tan 1993 reports simple hunterprey experiments multiagent rl focus sharing sensory information policies experience among agents shoham tennenholtz 1993 investigate social behavior emerge agents simple learning rules focus two simple nkg iterative games n agents meet k time randomly play game g littman boyan 1993 describe distributed reinforcement learning algorithm packet routing based asynchronous bellmanford algorithm scheme uses single qfunction state entry qfunction assigned node network responsible storing updating value entry differs work distributed rl entire qfunction single entry must stored node addition multiagent rl research concerned team problems significant amount work focused zerosum games single agent learns play opponent one earliest examples samuels checkerplaying program recent example tesauros tdgammon program 1992 1994 1995 learned play strong master level backgam mon types programs often trained using selfplay generally viewed single agents littman 1994 1996 provides detailed discussion rl applied zerosum games case agents alternate actions take simultaneously little work done multiagent rl general nonzero sum games sandholm crites 1996 study behavior multiagent rl context iterated prisoners dilemma show qlearning agents able learn optimal strategy fixed opponent titfortat addition investigate behavior results two qlearning agents face 3 elevator group control section introduces problem elevator group control serves testbed multiagent reinforcement learning familiar problem anyone ever used elevator system spite conceptual simplicity poses significant difficulties elevator systems operate highdimensional continuous state spaces continuous time discrete event dynamic systems states fully observable nonstationary due changing passenger arrival rates optimal policy elevator group control known use existing control algorithms standard comparison elevator domain provides opportunity compare parallel distributed control architectures agent controls one elevator car monitor amount degradation occurs agents face increasing levels incomplete state information buttons dn figure 1 elevator system schematic diagram schematic diagram elevator system lewis 1991 presented figure 1 elevators cars represented filled boxes diagram represents hall call someone wanting enter car gamma represents car call someone wanting leave car left side shaft represents upward moving cars calls right side shaft represents downward moving cars calls cars therefore move clockwise direction around shafts section 31 considers nature different passenger arrival patterns implications section 32 reviews variety elevator control strategies literature section 33 describes particular simulated elevator system focus remainder paper 31 passenger arrival patterns elevator systems driven passenger arrivals arrival patterns vary course day typical office building morning rush hour brings peak level traffic peak traffic occurs afternoon parts day characteristic patterns different arrival patterns different effects pattern requires analysis peak downpeak elevator traffic simply equivalent patterns opposite directions one might initially guess downpeak traffic many arrival floors single destination uppeak traffic single arrival floor many destinations distinction significant implications example light traffic average passenger waiting times kept low keeping idle cars lobby immediately available arriving passengers light traffic waiting times longer since possible keep idle car every upper floor building therefore additional waiting time incurred cars move service hall calls situation reversed heavy traffic heavy traffic car may fill lobby passengers desiring stop many different upper floors large number stops cause significantly longer roundtrip times heavy traffic car may fill stops upper floors reason downpeak handling capacity much greater uppeak capacity siikonen 1993 illustrates differences excellent graph obtained extensive simulations since uppeak handling capacity limiting factor elevator systems designed predicting heaviest likely uppeak demand building determining configuration accomodate demand uppeak capacity sufficient downpeak generally also uppeak traffic easiest type analyze since passengers enter cars lobby destination floors serviced ascending order empty cars return lobby standard capacity calculations strakosch 1983 siikonen 1993 assume car leaves lobby passengers 80 100 percent capacity average passengers likelihood selecting destination floor known probability theory used determine average number stops needed round trip one estimate average round trip time represents average amount time car arrivals lobby l number cars assuming cars evenly spaced average waiting time one half interval reality average wait somewhat longer control decisions pure traffic determine open close elevator doors lobby decisions affect many passengers board elevator lobby doors closed really choice next actions car calls registered passengers must serviced ascending order empty car must return lobby pepyne cassandras 1996 show optimal policy handling pure traffic thresholdbased policy closes doors optimal number passengers entered car optimal threshold depends upon traffic intensity may also affected number car calls already registered state cars course traffic seldom completely pure method must used assigning hall calls general two way traffic comes two varieties two way lobby traffic upmoving passengers arrive lobby downmoving passengers depart lobby compared pure traffic round trip times longer passengers served two way interfloor traffic passengers travel floors lobby interfloor traffic complex lobby traffic requires almost twice many stops per passenger lengthening round trip times two way downpeak traffic patterns require many decisions pure traffic leaving lobby car must decide high travel building turning floors make additional pickups decisions required wider variety contexts control strategies also possible two way downpeak traffic situations reason peak traffic pattern chosen testbed research describing testbed detail review various elevator control strategies literature 32 elevator control strategies oldest relaybased automatic controllers used principle collective control strakosch 1983 siikonen 1993 cars always stop nearest call running direction one drawback scheme means avoid phenomenon called bunching several cars arrive floor time making interval thus average waiting time much longer advances electronics including advent microprocessors made possible sophisticated control policies approaches elevator control discussed literature generally fit following categories often one category unfortunately descriptions proprietary algorithms often rather vague since written marketing purposes specifically intended benefit competitors reason difficult ascertain relative performance levels many algorithms accepted definition current state art ovaska 1992 321 zoning approaches otis elevator company used zoning starting point dealing various traffic patterns strakosch 1983 car assigned zone build ing answers hall calls within zone parks idle goal zoning approach keep cars reasonably well separated thus keep interval approach quite robust heavy traffic gives significant amount flexibility sakai kurosawa 1984 hitachi describe concept called area control related zoning possible assigns hall call car already must stop floor due car call otherwise car within area ff hall call assigned possible area ff control parameter affects average wait time power consumption 322 searchbased approaches another control strategy search space possible car assignments selecting one optimizes criterion average waiting time greedy search strategies perform immediate call assignment assign hall calls cars first registered never reconsider assign ments nongreedy algorithms postpone assignments reconsider light updated information may receive additional hall calls passenger destinations greedy algorithms give measure performance due lack flexibility also require less computation time western countries arriving car generally signals waiting passengers begins decelerate si ikonen 1993 allowing use nongreedy algorithm custom japan signal car assignment immediately upon call registration type signalling requires use greedy algorithm tobita et al 1991 hitachi describe system car assignment occurs hall button pressed assign car minimizes weighted sum predicted wait time travel time number riders fuzzy rulebased system used pick coefficients estimating functions simulations used verify effectiveness receding horizon controllers examples nongreedy searchbased approaches every event perform expensive search best assignment hall calls assuming new passenger arrivals closedloop control achieved recalculating new openloop plan every event weaknesses approach computational demands lack consideration future arrivals examples receding horizon controllers finite intervisit minimization empty system algorithm esa bao et al 1994 fim attempts minimize squared waiting times esa attempts minimize length current busy period 323 rulebased approaches sense control policies could considered rulebased situation action however narrowly considering type production systems commonly used artificial intelligence ujihara tsuji 1988 mitsubishi describe ai2100 system uses expertsystem fuzzylogic technologies claim experts groupsupervisory control experience knowledge necessary shorten waiting times various traffic conditions admit expert knowledge fragmentary hard organize difficult incorporate created rule base comparing decisions made conventional algorithm decisions determined simulated annealing discrepancies analyzed experts whose knowledge solving problems used create fuzzy control rules fuzziness lies part rules ujihara amano 1994 describe latest changes system previous version used fixed evaluation formula based current car positions call locations recent version considers future car positions probable future hall calls example one rule hall call registered upper floor large number cars ascending towards upper floors assign one ascending cars basis estimated time arrival note immediate call allocation algorithm consequent particular rule assigning cars basis estimated time arrival bears similarity greedy searchbased algorithms described 324 heuristic approaches longest queue first lqf algorithm assigns upward moving cars longest waiting queue highest unanswered floor first huff algorithm assigns upward moving cars highest queue people waiting bao et al 1994 algorithms designed specifically downpeak traffic assign downward moving cars unassigned hall calls encounter dynamic load balancing dlb algorithm attempts keep cars evenly spaced assigning contiguous nonoverlapping sectors car way balances loads lewis 1991 dlb nongreedy algorithm reassigns sectors every event 325 adaptive learning approaches imasaki et al 1991 toshiba use fuzzy neural network predict passenger waiting time distributions various sets control parameters system adjusts parameters evaluating alternative candidate parameters neural network explain control algorithm actually used parameters network trained hitachi researchers fujino et al 1992 tobita et al 1991 use greedy control algorithm combines multiple objectives wait time travel time crowd ing power consumption weighting objectives accomplished using parameters tuned online module called learning function unit collects traffic statistics attempts classify current traffic pattern tuning function unit generates parameter sets current traffic pattern tests using builtin simulator best parameters used control system searching entire parameter space would prohibitively expensive heuristics used parameter sets test levy et al 1977 use dynamic programming dp offline minimize expected time needed completion current busy period discount factor used since assumed values finite major difference qlearning must performed offline since uses model transition probabilities system performs sweeps state space trouble using dp calculate optimal policy state space large requiring drastic simplification levy et al use several methods keep size state space manageable consider building cars 8 floors number buttons simultaneously restricted state buttons restricted binary values ie elapsed times discarded cars unlimited capacity construction transition probability matrix principle part procedure assumes intensity poisson arrivals floor known value iteration policy iteration performed obtain solution markon et al 1994 devised system trains neural network perform immediate call allocation three phases training phase one system controlled existing controller flex8820 fuzzyai group control system fujitec supervised learning used train network predict hall call service times first phase training used learn appropriate internal representation ie weights input layer hidden layer network end first phase training weights fixed phase two output layer network retrained emulate existing controller phase three single weights output layer network perturbed resulting performance measured traffic sample weights modified direction improved performance viewed form nonsequential reinforcement learning singlestage reward determined measuring systems performance traffic sample input representation uses 25 units car output representation uses one unit car hall calls allocated car corresponding output unit highest activation also describe clever way incorporating permutational symmetry problem architecture network say states two cars interchanged outputs also interchanged done many sets hidden units cars explicitly linking together appropriate weights system tested simulation 6 cars 15 floors typical building trained 900 passengers per hour small improvement around 1 second average wait time existing controller untypical building uniformly distributed origin destination floors 1500 passengers per hour improvement average wait time almost 4 seconds one advantage system maintain adequate service level beginning since starts preexisting controller hand clear whether also may trap controller suboptimal region policy space would interesting use centralized immediate call allocation network architecture part sequential reinforcement learning algorithm 33 elevator testbed particular elevator system study paper simulated 10story building 4 elevator cars simulator written lewis 1991 passenger arrivals floor assumed poisson arrival rates vary course day simulations use traffic profile bao et al 1994 dictates arrival rates every 5minute interval typical afternoon peak rush hour table 1 shows mean number passengers arriving floors 5minute interval headed lobby addition interfloor traffic varies 0 10 traffic lobby table 1 downpeak traffic profile time 331 system dynamics system dynamics approximated following parameters ffl floor time time move one floor maximum speed 145 secs ffl stop time time needed decelerate open close doors accelerate secs ffl turn time time needed stopped car change direction 1 sec ffl load time time one passenger enter exit car random variable 20th order truncated erlang distribution range 06 60 secs mean 1 sec ffl car capacity 20 passengers simulator quite detailed certainly realistic enough purposes however minor deviations reality noted simulator car accelerate full speed decelerate full speed distance one half floor distances would somewhat longer real system thus simulated acceleration deceleration times always real system vary depending speed elevator example express car descending tenth floor top speed take longer decelerate first floor car descending second floor simulator also allows cars commit stopping floor one half floor away though realistic cars moving top speed concept making decisions regarding next floor car could commit stopping valid although elevator cars system homogeneous learning techniques described paper also used general situations eg several express cars cars service subset floors 332 state space state space continuous includes elapsed times since hall calls registered realvalued even real values approximated binary values size state space still immense components include 2 possible combinations buttons buttons landing except top bottom 2 40 possible combinations 40 car buttons possible combinations positions directions cars rounding nearest floor parts state fully ob servable example exact number passengers waiting floor exact arrival times desired destinations ignoring everything except configuration hall car call buttons approximate position direction cars obtain extremely conservative estimate size discrete approximation continuous state space states 333 control actions car small set primitive actions stopped floor must either move move motion floors must either stop next floor continue past next floor due passenger expectations two constraints actions car cannot pass floor passenger wants get cannot turn serviced car buttons present direction also added three additional heuristic constraints attempt build primitive prior knowledge car cannot stop floor unless someone wants get cannot stop pick passengers floor another car already stopped given choice moving prefer move since downpeak traffic tends push cars toward bottom building last constraint real choices left car stop continue actions actions elevator cars executed asynchronously since may take different amounts time complete 334 performance criteria performance objectives elevator system defined many ways one possible objective minimize average wait time time arrival passenger entry car another possible objective minimize average system time sum wait time travel time third possible objective minimize percentage passengers wait longer dissatisfaction threshold usually 60 seconds another common objective minimize average squared wait time chose latter performance objective since tends keep wait times low also encouraging fair service example wait times 2 8 seconds average 5 seconds wait times 4 6 seconds average squared wait times different 34 2 8 versus 26 4 6 4 algorithm network architecture section describes multiagent reinforcement learning algorithm applied elevator group control scheme agent responsible controlling one elevator car agent uses modification qlearning discreteevent systems together employ collective form reinforcement learning begin describing modifications needed extend qlearning discreteevent framework derive method determining appropriate reinforcement signals face uncertainty exact passenger arrival times describe algorithm feedforward networks used store qvalues distinction parallel distributed versions algorithm 41 discreteevent reinforcement learning elevator systems modeled discrete event systems cassandras 1993 significant events passenger arrivals occur discrete times amount time events realvalued variable systems constant discount factor fl used discretetime reinforcement learning algorithms inadequate problem approached using variable discount factor depends amount time events bradtke duff 1995 case costtogo defined integral rather infinite sum c immediate cost discrete time c instantaneous cost continuous time sum squared wait times currently waiting pas sengers fi controls rate exponential decay experiments described paper since wait times measured seconds scale instantaneous costs c factor 10 6 keep costtogo values becoming exceedingly large elevator system events occur randomly continuous time branching factor effectively infinite complicates use algorithms require explicit lookahead therefore employ discrete event version qlearning algorithm since considers events encountered actual system trajectories require model state transition probabilities qlearning update rule watkins 1989 takes following discrete event form e gammafi gammat x action taken state x time x next decision required state time ff learning rate parameter c fi defined e gammafit gammat x acts variable discount factor depends amount time events consider case c constant events extend formulation case c quadratic since goal minimize squared wait times integral qlearning update rule takes form e gammafi gammat x w p amount time passenger p waiting time already waited time x special care needed handle passengers begin waiting x see section 421 integral solved parts yield difficulty arises using formula since requires knowledge waiting times waiting passengers however waiting times passengers press hall call buttons known real elevator system number subsequent passengers arrive exact waiting times available examine two ways dealing problem call omniscient online reinforcement schemes simulator access waiting times passengers could use information produce necessary reinforcement signals call omniscient reinforcements since require information available real system note controller receives extra information however rather critic evaluating controller reason even omniscient reinforcements used design phase elevator controller simulated system resulting trained controller installed real system without requiring extra knowledge possibility train using information would available real system online online reinforcements assume waiting time first passenger queue known elapsed button time poisson arrival rate queue known estimated gamma distribution used estimate arrival times subsequent passengers time n th subsequent arrival follows gamma distribution gamman 1 queue subsequent arrivals generate following expected costs first b seconds hall button pressedx z bprob n th arrival occurs time delta cost given arrival time z bgamma z bz bgamma integral also solved parts yield expected costs general solution provided section 422 described section 54 using online reinforcements produces results almost good obtained omniscient reinforcements 42 collective discreteevent qlearning elevator system events divided two types events first type important calculation waiting times therefore also reinforcements include passenger arrivals transfers cars omniscient case hall button events online case second type car arrival events potential decision points rl agents controlling car car motion floors generates car arrival event reaches point must decide whether stop next floor continue past next floor cases cars constrained take particular action example stopping next floor passenger wants get agent faces decision point unconstrained choice actions 421 calculating omniscient reinforcements omniscient reinforcements updated incrementally every passenger arrival event passenger arrives queue passenger transfer event passenger gets car car arrival event control decision made incremental updates natural way dealing discontinuities reinforcement arise passengers begin end waiting cars decisions eg another car picks waiting passengers amount reinforcement events cars since share objective function amount reinforcement car receives decisions different since cars make decisions asynchronously fore car associated storage location ri total discounted reinforcement received since last decision time di accumulated time event following computations performed let 0 time last event 1 time current event passenger p waiting 0 1 let w 0 p w 1 p total time passenger p waited 0 1 respectively car 422 calculating online reinforcements online reinforcements updated incrementally every hall button event sig naling arrival first waiting passenger queue arrival car pick waiting passengers queue car arrival event control decision made assume online reinforcements caused passengers waiting queue end immediately car arrives service queue since possible know exactly passenger boards car poisson arrival rate queue estimated reciprocal last interbutton time queue ie amount time last service button pushed however ceiling 004 passengers per second placed estimated arrival rates prevent small interbutton times creating huge penalties might destabilize costtogo estimates time event following computations performed let 0 time last event 1 time current event hall call button b active 0 1 let w 0 b w 1 b elapsed time button b 0 1 respectively car f g 423 making decisions updating qvalues car motion floors generates car arrival event reaches point must decide whether stop next floor continue past next floor cases cars constrained take particular action example stopping next floor passenger wants get agent faces decision point unconstrained choice actions algorithm used agent making decisions updating qvalue estimates follows 1 time x observing state x car arrives decision point selects action using boltzmann distribution qvalue estimates positive temperature parameter annealed decreased training value controls amount randomness selection actions beginning training qvalue estimates inaccurate high values used give nearly equal probabilities action later training qvalue estimates accurate lower values used give higher probabilities actions thought superior still allowing exploration gather information actions discussed section 53 choosing slow enough annealing schedule particularly important multiagent settings 2 let next decision point car time state cars including car updated rdelta values described last two sections car adjusts estimate qx toward following target value fstopcontg car resets reinforcement accumulator ri zero 3 let x x go step 1 43 networks used store qvalues using lookup tables store qvalues ruled large system stead used feedforward neural networks trained error backpropagation algorithm rumelhart et al 1986 networks receive state information input produce qvalue estimates output qvalue estimates written qx oe oe vector parameters weights networks exact weight update equation fstopcontg ff positive learning rate stepsize parameter gradient 5 oe vector partial derivatives qx oe respect component oe start training weights network initialized uniform random numbers gamma1 1 experiments paper use separate singleoutput networks actionvalue estimate others use one network multiple output units one action basic network architecture pure traffic uses 47 input units 20 hidden sigmoid units 1 2 linear output units input units follows ffl units two units encode information nine hall tons realvalued unit encodes elapsed time button pushed binary unit button pushed units units represents possible location direction car whose decision required exactly one units given time note car different egocentric view state system units units represent one 10 floors cars may located car footprint depends direction speed example stopped car causes activation unit corresponding current floor moving car causes activation several units corresponding floors approaching highest activations closest floors information provided one cars particular location unit car whose decision required highest floor waiting passenger unit car whose decision required floor passenger waiting longest amount time unit bias unit always section 4 introduce representations including restricted state information 44 parallel distributed implementations elevator car controlled separate qlearning agent experimented parallel decentralized implementations parallel implementations agents use central set shared networks allowing learn others experiences forcing learn identical policies totally decentralized implementations agents networks allowing specialize control policies either case none agents given explicit access actions agents cooperation learned indirectly via global reinforcement signal agent faces added stochasticity nonstationarity environment contains learning agents 5 results discussion 51 basic results versus algorithms since optimal policy elevator group control problem unknown measured performance algorithm heuristic algorithms including best aware algorithms sector sectorbased algorithm similar used many actual elevator systems dlb dynamic load balancing attempts equalize load cars huff highest unanswered floor first gives priority highest floor people waiting lqf longest queue first gives priority queue person waiting longest amount time fim finite intervisit minimization receding horizon controller searches space admissible car assignments minimize load function esa empty system algorithm receding horizon controller searches fastest way empty system assuming new passenger arrivals fim computationally intensive would difficult implement real time present form esa uses queue length information would available real elevator system esanq version esa uses arrival rate information estimate queue lengths details see bao et al 1994 rlp rld denote rl controllers parallel decentralized rl controllers trained 60000 hours simulated elevator time took four days 100 mips workstation results algorithms averaged hours simulated elevator time ensure statistical significance average waiting times listed trained rl algorithms correct within sigma013 95 confidence level average squared waiting times correct within sigma53 average system times correct within sigma027 table 2 shows results traffic profile traffic table 3 shows results downpeak traffic profile traffic including average 2 passengers per minute lobby algorithm trained downonly traffic yet generalizes well traffic added upward moving cars forced stop upward hall calls table 4 shows results downpeak traffic profile traffic including average 4 passengers per minute lobby time twice much traffic rl agents generalize extremely well new situation table 2 results downpeak profile traffic algorithm avgwait squaredwait systemtime percent60 secs dlb 194 658 532 274 esa 151 338 471 025 rld 147 313 417 007 table 3 results downpeak profile traffic algorithm avgwait squaredwait systemtime percent60 secs huff 196 608 505 199 rld 169 468 427 140 table 4 results downpeak profile twice much traffic algorithm avgwait squaredwait systemtime percent60 secs basic huff 232 875 547 494 fim 208 685 534 310 esa 201 667 523 312 rld 188 593 454 240 one see rl systems achieved good performance notably measured system time sum wait travel time measure directly minimized surprisingly decentralized rl system able achieve good level performance parallel rl system 52 analysis decentralized results view outstanding success decentralized rl algorithm several questions suggest similar policies agents learned one another policy learned parallel algorithm results improved using voting scheme happens one agents policy used control cars section addresses questions first simulator modified poll four decentralized qnetwork agents well parallel qnetwork every decision every car compare action selections one hour simulated elevator time total 573 decisions required four agents unanimous 505 decisions 1 47 decisions 8 percent split evenly 21 decisions 4 percent parallel network agreed 493 505 unanimous decisions 98 percent reason parallel network tended favor stop action decentralized networks though apparently little impact overall performance complete results listed table 5 table 5 amount agreement decentralized agents agents saying agents saying number parallel parallel stop continue instances says stop says cont results show considerable agreement minority situations agents disagree next experiment agents vote actions selected cars cases agents evenly split examine three ways resolving ties favor stop rls favor continue rlc randomly rlr following table shows results voting scheme compared original decentralized algorithm rld results averaged hours simulated elevator time pure traffic results show significant improvement voting situations agents evenly split breaking ties randomly produced results almost identical original decentralized algorithm seems imply agents generally agree important decisions table 6 comparison several voting schemes algorithm avgwait squaredwait systemtime percent60 secs rlc 150 325 417 009 rls 149 322 417 010 rlr 148 314 417 012 rld 147 313 417 007 disagree decisions little consequence action values similar next experiment agent single car selects actions cars rl1 uses agent car 1 control cars rl2 uses agent car 2 following table compares controllers original decentralized algorithm rld results averaged hours simulated elevator time pure traffic table 7 letting single agent control four cars algorithm avgwait squaredwait systemtime percent60 secs rld 147 313 417 007 agent 1 outperformed agents agents performed well relative nonrl controllers discussed summary appears decentralized parallel agents learned similar policies similarity learned policies may caused part symmetry elevator system input representation selected distinguish among cars future work would interesting see whether agents input representations distinguish among cars would still arrive similar policies 53 annealing schedules one important factors performance algorithms annealing schedule used control amount exploration performed agent slower annealing process better final result illustrated table 8 figure show results one training run number annealing rates temperature annealed according schedule represents hours training completed results measured hours simulated elevator time even though somewhat noisy due averaged multiple training runs trend still quite clear schedules tested shared starting ending temper atures although annealing process ended time current qvalue estimates used determine control policy amount time available training known advance one select annealing schedule covers full range temperatures table 8 effect varying annealing rate factor hours avgwait squaredwait systemtime pct60 secs gradual annealing important singleagent rl even important multiagent rl tradeoff exploration exploitation agent must also balanced need agents learn stationary environment agent best beginning learning process agents extremely inept gradual annealing able raise performance levels parallel tesauro 1992 1994 1995 notes slightly different related phenomenon context zerosum games training selfplay allows agent learn wellmatched opponent stage development 54 omniscient versus online reinforcements section examines relative performance omniscient online reinforcements described section 41 given network structure temperature learning rate schedule shown table 9 omniscient reinforcements led slightly better performance online reinforcements little concern regarding application rl real elevator system since one would want perform initial training simulation case huge amount experience needed also performance would poor early stages training real elevator system initial training could performed using simulator networks could finetuned real system final average squared wait hours training freezing figure 2 effect varying annealing rate table 9 omniscient versus online reinforcements avgwait squaredwait systemtime pct60 secs omniscient 152 332 421 007 online 153 342 416 016 55 levels incomplete state information parallel decentralized rl implemented real elevator system would problem providing whatever state information available agents however truly decentralized control situation might possible section looks performance degrades agents receive less state information experiments amount information available agents varied along two dimensions information hall call buttons information location direction status cars input representations hall call buttons real consisting input units two units encode information nine hall buttons realvalued unit encodes elapsed time button pushed binary unit button pushed binary consisting 9 binary input units corresponding nine hall buttons quantity consisting two input units measuring number hall calls current decisionmaking car none input units conveying information hall buttons input representations configuration cars foot prints consisting 10 input units unit represents one 10 floors cars may located car footprint depends direction speed example stopped car causes activation unit corresponding current floor moving car causes activation several units corresponding floors approaching highest activations closest floors activations caused various cars addi quantity consisting 4 input units represent number upward downward moving cars decisionmaking car none consisting input units conveying information hall buttons networks also possessed bias unit always activated 20 hidden units 2 output units stop continue actions used decentralized rl algorithm trained 12000 hours simulated elevator time using downpeak profile omniscient reinforcements temperature annealed according schedule hours training learning rate parameter decreased according schedule results shown table 10 measured terms average squared passenger waiting times hours simulated elevator time considered fairly noisy averaged multiple training runs nevertheless show interesting trends table 10 average squared wait times various levels incomplete state information hall location cars buttons footprints quantity none real 370 428 474 binary 471 409 553 quantity 449 390 530 none 1161 778 827 clearly information hall calls important information configuration cars fact performance still remarkably good even without information cars technically speaking information always available cars constraint prevents car stopping pick passengers floor another car already stopped doubt constraint helped performance considerably hall call information completely missing network weights increased tendency become unstable grow without bound learning rate parameter lowered cases discussion network instability see section 57 way information presented important example supplied number hall calls decisionmaking car useful networks potentially informative binary button information also appears information along one dimension helpful utilizing information along dimension example footprints representation made performance much worse car information absence hall call information time footprints outperformed representations maximum amount hall call information overall performance quite good except complete absence hall call information significant handicap indeed could improved slower annealing seems reasonable say algorithm degrades gracefully presence incomplete state information problem final experiment two binary features added realfootprints input representation activated decisionmaking car highest floor waiting passenger floor longest waiting pas senger respectively addition features average squared wait time decreased 370 359 appear value 56 practical issues one biggest difficulties applying rl elevator control problem finding correct temperature learning rate parameters helpful start scaled version consisting 1 car 4 floors lookup table qvalues made easier determine rough values temperature learning rate schedules importance focusing experience learner appropriate areas state space cannot overstressed training trajectories system important start adding reasonable constraints described section 333 also helps evidence supporting importance focusing given choice training heavier lighter traffic one expects face testing better train heavier traffic type training gives system experience states queue lengths long thus making correct decision crucial 57 instability weights neural networks become unstable magnitude increasing without bound two particular situations seem lead instability first occurs learning algorithm makes updates large happen learning rate large network inputs large happen heavy traffic situations second occurs network weights initialized random values producing excessively inconsistent qvalues example learning rate 10 gamma2 suitable training random initial network moderate traffic 700 passengershour consistently brings instability heavy traffic 1900 passengershour however learning rate 10 gamma3 keeps network stable even heavy traffic train network way several hundred hours elevator time leading weights represent consistent set qvalues learning rate safely raised back 10 gamma2 without causing instability 58 linear networks one may ask whether nonlinear function approximators feedforward sigmoidal networks necessary good performance elevator control prob lem test run using linear network trained delta rule linear network much greater tendency unstable order keep weights blowing learning rate lowered several orders magnitude 10 gamma3 10 gamma6 initial improvement linear network unable reduce average td error resulting extremely poor performance failure linear networks lends support contention elevator control difficult problem 6 discussion parallel distributed multiagent rl architectures able outperform elevator algorithms tested two architectures learned similar policies gradual annealing appeared crucial factor success training accomplished effectively using omniscient online reinforcements algorithms robust easily generalizing new situations added traffic finally degraded gracefully face increasing levels incomplete state information although networks became unstable certain circumstances techniques discussed prevented instabilities practice taken together results demonstrate multiagent rl algorithms powerful techniques addressing large scale stochastic dynamic optimization problems crucial ingredient success multiagent rl careful control amount exploration performed agent exploration context means trying action believed suboptimal order gather additional information potential value beginning learning process rl agent chooses actions randomly without knowledge relative values thus agents extremely inept however spite noise reinforcement signal caused actions agents actions begin appear better others gradually annealing lowering amount exploration performed agents better actions taken greater frequency gradually changes environment agents continue explore raise performance levels parallel even though rl agents team face added stochasticity nonstationarity due changing stochastic policies agents team display exceptional ability cooperate one another learning maximize rewards many areas research elevator group control general multiagent rl deserve investigation implementing rl controller real elevator system would require training several traffic profiles including uppeak interfloor traffic patterns additional actions would needed order handle traffic patterns example uppeak traffic would useful actions specifically open close doors control dwell time lobby interfloor traffic unconstrained actions would needed sake flexibility cars also ability park various floors periods light traffic would interesting try something uniform annealing schedule agents example coordinated exploration strategy roundrobin type annealing might way reducing noise generated agents however coordinated exploration strategy may greater tendency become stuck suboptimal policies theoretical results sequential multiagent rl needed supplement results nonsequential multiagent rl described section 21 another area needs study rl architectures reinforcements tailored individual agents possibly using hierarchy advanced organizational structure local reinforcement architectures potential greatly increase speed learning require much knowledge part whatever producing reinforcement signals barto 1989 fi nally important find effective methods allowing possibility explicit communication among agents 7 conclusions multiagent control systems often required spatial geographic distribution situations centralized information available practical even distributed approach required multiple agents may still provide excellent way scaling approximate solutions large problems streamlining search space possible policies multiagent rl combines advantages bottomup topdown approaches design multiagent systems achieves simplicity bottomup approach allowing use relatively unsophisticated agents learn basis experiences time rl agents adapt topdown global reinforcement signal guides behavior toward achievement complex specific goals result robust systems complex problems created minimum human effort rl algorithms trained using actual simulated experiences allowing focus computation areas state space actually visited control making computationally tractable large problems members team agents employs rl algorithm new collective algorithm emerges group whole type collective algorithm allows control policies learned decentralized way even though rl agents team face added stochasticity nonstationarity due changing stochastic policies agents team display exceptional ability cooperate one another maximizing rewards order demonstrate power multiagent rl focused difficult problem elevator group supervisory control used team rl agents responsible controlling one elevator car results obtained simulation surpassed best heuristic elevator control algorithms aware performance also robust face increased levels incomplete state information acknowledgments thank john mcnulty christos cassandras asif gandhi dave pepyne kevin markey victor lesser rod grupen rich sutton steve bradtke anw group assistance simulator helpful discussions research supported air force office scientific research grant f49620 9310269 r evolution cooperation elevator dispatchers peak traffic chemotaxis cooperativity abstract exercises neuronal learning strategies learning interaction introduction modern reinforcement learning distributed adaptive optimal control flexible structures reinforcement learning methods continuoustime markov decision problems discrete event systems modeling performance analysis phd thesis forming control policies simulation models using reinforcement learning improving elevator performance using reinforcement learning feudal reinforcement learning fuzzy neural network application elevator group control optimal control elevators dynamic load balancing approach control multiserver polling systems applications elevator system dispatching distributed reinforcement learning scheme network routing technical report cmucs93165 markov games framework multiagent reinforcement learning algorithms sequential decision making efficient learning multiple degreeoffreedom control problems quasiindependent qagents adaptive optimal elevator group control use neural networks learning automata introduction electronics information technology highrange elevator systems optimal dispatching control elevator systems uppeak traffic pdp research group development elevator supervisory group control system artificial intelligence studies machine learning using game checkers multiagent reinforcement learning iterated prisoners dilemma elevator traffic simulation vertical transportation elevators escalators neural computation temporal difference learning tdgammon elevator characterized group supervisory control system automaton theory modeling biological systems latest elevator groupcontrol system revolutionary ai2100 elevatorgroup control system new intelligent option series learning delayed rewards adaptation learning multiagent systems received date accepted date final manuscript date tr ctr shingo mabu kotaro hirasawa jinglu hu graphbased evolutionary algorithm genetic network programming gnp extension using reinforcement learning evolutionary computation v15 n3 p369398 fall 2007 rajbala makar sridhar mahadevan mohammad ghavamzadeh hierarchical multiagent reinforcement learning proceedings fifth international conference autonomous agents p246253 may 2001 montreal quebec canada shin ishii hajime fujita masaoki mitsutake tatsuya yamazaki jun matsuda yoichiro matsuno reinforcement learning scheme partiallyobservable multiagent game machine learning v59 n12 p3154 may 2005 mohammad ghavamzadeh sridhar mahadevan learning communicate act using hierarchical reinforcement learning proceedings third international joint conference autonomous agents multiagent systems p11141121 july 1923 2004 new york new york theodore j perkins andrew g barto lyapunov design safe reinforcement learning journal machine learning research 3 312003 shimon whiteson matthew e taylor peter stone empirical studies action selection reinforcement learning adaptive behavior animals animats software agents robots adaptive systems v15 n1 p3350 march 2007 tadhg omeara ahmed patel topicspecific web robot model based restless bandits ieee internet computing v5 n2 p2735 march 2001 hajime fujita shin ishii modelbased reinforcement learning partially observable games samplingbased state estimation neural computation v19 n11 p30513087 november 2007 andrew g barto sridhar mahadevan recent advances hierarchical reinforcement learning discrete event dynamic systems v13 n12 p4177 januaryapril andrew g barto sridhar mahadevan recent advances hierarchical reinforcement learning discrete event dynamic systems v13 n4 p341379 october philipp friese jrg rambau onlineoptimization multielevator transport systems reoptimization algorithms based setpartitioning models discrete applied mathematics v154 n13 p19081931 15 august 2006 shimon whiteson peter stone evolutionary function approximation reinforcement learning journal machine learning research 7 p877917 1212006 gang chen zhonghua yang hao kiah mok goh coordinating multiple agents via reinforcement learning autonomous agents multiagent systems v10 n3 p273328 may 2005 pasquale fiengo giovanni giambene edmondo trentin neuralbased downlink scheduling algorithm broadband wireless networks computer communications v30 n2 p207218 january 2007 darse billings lourdes pea jonathan schaeffer duane szafron learning play strong poker machines learn play games nova science publishers inc commack ny 2001