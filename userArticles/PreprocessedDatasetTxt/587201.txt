solution equality constrained quadratic programming problems arising optimization consider application conjugate gradient method solution large equality constrained quadratic programs arising nonlinear optimization approach based implicitly reduced linear system generates iterates null space constraints instead computing basis null space choose work directly matrix constraint gradients computing projections null space either normal equations augmented system approach unfortunately practice projections result significant rounding errors propose iterative refinement techniques well adaptive reformulation quadratic problem greatly reduce errors without incurring high computational overheads numerical results illustrating efficacy proposed approaches presented b introduction variety algorithms nonlinearly constrained optimization 7 8 12 29 31 use conjugate gradient cg method 25 solve subproblems form minimize x subject nonlinear optimization nvector c usually represents gradient rf objective function gradient lagrangian n theta n symmetric matrix h stands either hessian lagrangian approximation solution x represents search direction equality constraints 12 obtained linearizing constraints optimization problem current iterate assume theta n full row rank constraints 12 constitute linearly independent equations also assume convenience h positive definite null space constraints guarantees 1112 unique solution positive definiteness assumption needed trust region methods discussion also valid context trust region methods normally terminate cg iteration soon negative curvature encountered see 36 38 contrast 23 use iterative method cg attractive large scale optimization number variables large cost effective solve 11 12 approximately increase accuracy solution iterates optimization algorithm approach minimizer addition properties cg method merge well requirements globally convergent optimization methods see eg 36 paper study apply preconditioned cg method 11 12 keep computational cost reasonable level ensuring rounding errors degrade performance optimization algorithm quadratic program 1112 solved computing basis z null space using basis eliminate constraints applying cg method reduced problem argue however due form preconditioners used practice explicit use z cause iteration expensive significant savings achieved means approaches bypass computation z altogether price pay alternatives give rise excessive roundoff errors slow optimization iteration may even prevent converging shall see errors cause constraints 12 satisfied desired accuracy describe iterative refinement techniques improve accuracy solution highly illconditioned problems also propose mechanism redefining vector c adaptively change solution quadratic problem favorable numerical properties notation throughout paper k delta k stands 2 matrix vector norm gnorm vector x defined x gx g given positivedefinite matrix denote floatingpoint unit roundoff machine precision ffl let denote condition number ie nonzero singular values 2 cg method linear constraints common approach solving linearly constrained problems eliminate constraints solve reduced problem cf 17 20 specifically suppose z n theta n gamma matrix spanning null space columns together columns z span r n solution x linear equations 12 written vectors x constraints 12 yield determines vector x substituting 21 11 omitting constant terms constant see x z solves reduced problem minimize x z2 assumed reduced hessian h zz positive definite 23 equivalent linear system apply conjugate gradient method compute approximate solution problem 23 equivalently system 24 substitute 21 obtain approximate solution quadratic program 1112 strategy computing normal component x exactly tangential component zx z inexactly compatible requirements many nonlinear optimization algorithms need ensure linear constraints satisfied remain throughout remainder optimization calculation cf 20 let us consider practical application cg method reduced system 24 well known preconditioning improve rate convergence cg iteration cf 1 therefore assume preconditioner w zz given w zz symmetric positive definite matrix dimension might chosen reduce span cluster eigenvalues w gamma1 could result automatic scaling variables 7 29 regardless w zz defined preconditioned conjugate gradient method applied 24 follows see eg 20 algorithm preconditioned cg reduced systems choose initial point x z compute r zz r z p gammag z repeat following steps termination test satisfied r z z zz r z gammag z z g z r z r z iteration may terminated example r z zz r z sufficiently small approximate solution obtained must multiplied z substituted 21 give approximate solution quadratic program 1112 alternatively may rewrite algorithm multiplication z addition term x performed explicitly cg iteration introduce following algorithm nvectors x algorithm ii preconditioned cg expanded form reduced systems choose initial point x satisfying 12 compute gammag repeat following steps convergence test satisfied x main algorithm studied paper several types stopping tests used since choice depends requirements optimization method shall discuss numerical tests reported paper use quantity r zz r z terminate cg iteration note vector g call preconditioned residual explicitly defined range z result exact arithmetic search directions generated algorithm ii also lie range z thus iterates x satisfy 12 rounding errors computing 217 may cause p component outside range z component normally small cause difficulties 3 implementation projected cg method algorithm ii constitutes effective method computing solution 1112 successfully used various algorithms large scale optimization cf 16 28 39 main drawback need nullspace basis matrix z whose computation manipulation costly sometimes give rise unnecessary illconditioning 9 10 18 24 33 37 difficulties become apparent describe practical procedures computing z consider types preconditioners w zz used practice let us begin first issue 31 computing basis null space many possible choices nullspace matrix z possibly best strategy choose z orthonormal columns provides well conditioned representation null space however computing nullspace matrix expensive number variables large essentially requires computation sparse lq factorization implicit explicit generation q always believed rather expensive compared alternatives described 24 recent research 30 35 suggested fact possible generate q product sparse householder matrices cost may reasonable experimented approach however knowledge general purpose software implementing yet available another possibility try compute basis nullspace involves nonzeros possible although problem computationally hard 9 suboptimal heuristics possible still rather expensive 10 18 33 37 economical alternative based simple elimination variables 17 20 define z first group components x basic dependent variables simplicity assumed first variables partition theta basis matrix b assumed nonsingular define gammab clearly satisfies linearly independent columns practice z formed explicitly instead compute store sparse lu factors 13 b compute products form zv z v means solves using lu factors ideally would like choose basis b sparse possible whose condition number significantly worse requirements difficult achieve simply ensuring b well conditioned difficult task choosing basis delegated sparse lu factorization algorithm ma48 15 recent codes see eg 19 designed compute wellconditioned basis known us extent reach objective 32 preconditioning potential drawbacks nullspace basis 31 sufficiently serious prevent effective use algorithm ii however considering practical choices preconditioning matrix w zz one exposes weaknesses approach ideally one would like choose w zz w gamma1 thus zz perfect preconditioner however unlikely z hz inverse sparse matrices even z hz small dimension forming quite costly therefore operating ideal preconditioner normally question paper consider preconditioners form zz g symmetric matrix z gz positive definite suggestions choose g made 32 two particularly simple choices first choice appropriate h dominated diagonal case example barrier methods constrained optimization handle bound constraints l x u adding terms form gamma objective function positive barrier parameter choice arises several trust region methods constrained optimization 7 12 29 preconditioner derives change variables thus given zz regardless choice g preconditioner 33 requires operations inverse matrix z gz applications 16 39 z defined 31 simple enough structure forming factorizing n gamma theta n gamma matrix z gz expensive g simple form lu factors b sparse number constraints large forming z gz may rather costly even requires solution 2m triangular systems lu factors case preferable form z gz rather compute products form z solving z using cg method inner cg iteration employed 29 effective problemsparticularly number degrees freedom small fail z badly conditioned tends expensive moreover since matrix z gz known explicitly difficult construct effective preconditioners accelerating inner cg iteration summary preconditioner form 33 z defined means 31 computation 215 preconditioned residual g often expensive dominate cost optimization algorithm goal paper consider alternative implementations algorithm ii whose computational cost moderate predictable approach avoid use nullspace basis z altogether 33 computing projections see bypass computation z let us begin considering simple case preconditioner w zz given 34 p z denotes orthogonal projection operator onto null space preconditioned residual 215 written projection performed two alternative ways first replace p z equivalent formula thus replace 36 express solution noting 310 normal equations follows v solution least squares problem minimize desired projection g corresponding residual approach implemented using cholesky factorization aa second possibility express projection 36 solution augmented system r system solved means symmetric indefinite factorization uses 1 theta 1 2 theta 2 pivots 21 let us suppose preconditioner general form 33 preconditioned residual 215 requires computation may expressed g nonsingular found solution g r whenever z gz nonsingular see eg 20 section 541 314 far appealing g gamma1 simple form 315 useful generalization 312 clearly system 312 may obtained 315 setting perfect preconditioner results choices g also possible required z gz positive definite idea using projection 37 cg method dates back least 34 alternative 315 special case 312 proposed 8 although 8 unnecessarily requires g positive definite recent study preconditioning projected cg method 11 hereafter shall write 215 p projection operators mentioned note 38 312 315 make use null space matrix z require factorization matrices involving unfortunately give rise significant roundoff errors particularly cg iterates approach solution difficulties caused fact iterations proceed projected vector increasingly small r indeed optimality conditions quadratic program 1112 state solution x satisfies lagrange multiplier vector vector hx c denoted r algorithm ii generally stay bounded away zero indicated 316 become increasingly closer range words r tend become orthogonal z hence 313 preconditioned residual g converge zero long smallest eigenvalue z gz bounded away zero discrepancy magnitudes cause numerical difficulties apparent 39 shows significant cancellation digits usually take place generation harmful roundoff errors also apparent 312315 small remaining components v remain large since magnitude errors generated solution 312315 governed size large component v vector g contain large relative errors arguments made precise next section example 1 applied algorithm ii solve problem cvxeqp3 cute collection 4 subsequent experiments use simple preconditioner 34 corresponding choice used normal equations 38 augmented system 312 approaches compute projection results given figure 1 plots residual r g function iteration number cases cg iteration terminated r g became negative indicates severe errors occurred since r must positivecontinuing iteration past point resulted oscillations norm gradient without significant improvement iteration 50 runs r order 10 5 whereas projection g figure also plots cosine angle preconditioned residual g rows precisely define ith row note cosine zero exact arithmetic increases indicating cg iterates leave constraint manifold severe errors uncommon optimization calculations see x7 27 grave concern may cause underlying optimization algorithms behave erratically fail paper propose several remedies one based adaptive redefinition r attempts minimize differences magnitudes also describe several forms iterative refinement projection operation techniques motivated roundoff error analysis given next 4 analysis errors present error bounds support arguments made previous section particularly claim problematic situation occurs latter stages pcg augmented system iteration resid cos pcg normal equations iteration resid cos figure 1 conjugate gradient method two options projection iteration g converging zero r simplicity shall assume henceforth scaled shall consider simplest possible preconditioner opposed exact quantity denoted subscript c let us first consider normal equations approach given 39 310 solved means cholesky factorization aa finite precision instead exact solution v normal equations obtain v error deltav recall ffl denotes unit roundoff condition number study total error projection vector g simplify analysis ignore errors arise computation matrixvector product v subtraction given 39 errors dominated error v whose magnitude estimated 41 assumptions 39 computed projection exact projection 1 bound 41 assumes errors formation aa ar backsolves using cholesky factors reasonable assumption context also note 41 sharpened replacing term possible diagonal scalings thus error projection lies entirely range 41 relative error projection satisfies error significant large large let us consider ratio 44 case kr much larger projection 39 kr assumption suppose inequality achieved 44 gives simpler interpret 44 thus conclude error projection 43 large either ratio kr large condition number moderate contribution ratio 44 relative error 43 normally large enough cause failure optimization calculation condition number grows loss significant digits becomes severe especially since appears squared 43 example 1 mentioned ratio 44 order o10 6 iteration 50 bound 43 indicates could correct digits g stage cg iteration agreement test point cg iteration could make progress let us consider augmented system approach 315 focus choice preconditioned residual computed solving r using direct method number methods strategies bunch kaufman 5 duff reid 14 best known examples dense sparse matrices respectively form ldl factorization augmented matrix ie matrix appearing left hand side 45 l unit lower triangular block diagonal 1 theta 1 2 theta 2 blocks approach usually always stable normal equations ap proach improve stability method bjorck 2 suggests introducing parameter ff solving equivalent system r error analysis 3 shows j depends n growth factor factorization oe 1 nonzero singular values important notice aand 2 aenters bound ff oe method give solution never much worse obtained tight perturbation analysis therefore considered stable practical purposes approximating oe difficult common simply use case concerns us kg converges zero kv term inside last square brackets 47 approximately kv k obtain assumed ff 1 interesting compare bound 43 see ratio 44 plays crucial role analysis augmented system approach likely give accurate solution method normal equations case cannot stated categorically however since size factor j difficult predict residual update strategy described x6 aims minimizing contribution ratio 44 see highly beneficial effect algorithm ii presenting discuss various iterative refinement techniques designed improve accuracy projection operation 5 iterative refinement iterative refinement known effective procedure improving accuracy solution obtained method backwards stable consider use context normal equations augmented system approaches 51 normal equations approach let us suppose choose compute projection p r normal equations approach 39310 appealing idea trying improve accuracy computation apply projection repeatedly therefore rather computing 215 let projection applied many times necessary keep errors small motivation multiple projections technique stems fact computed projection small component consisting entirely rounding errors outside null space described 42 therefore applying projection p first projection c give improved estimate ratio 44 much smaller repeating process may hope obtain improvement accuracy multiple projection technique may simply described setting g performing following steps solve set l cholesky factor aa note method appropriate although simple variant possible g diagonal example 2 solved problem given example 1 using multiple projections every cg iteration measure cosine 317 angle g columns cosine greater 10 gamma12 multiple projections applied cosine less value results given figure 2 show residual r g reduced much plane cg iteration figure 1 indeed ratio final initial values r g 10 gamma16 satisfactory straightforward analyze multiple projections strategy 5152 provided make simplifying assumption rounding errors make forming l solving 51 obtain following result proved induction 41 simple consequence 5354 assumption norm one thus error converges rlinearly zero rate course rate sustained indefinitely errors ignored 5152 become important nonetheless one would expect 55 reflect true behaviour kg small multiple unit roundoff ffl iteration residual figure 2 cg method using multiple projections normal equations approach stressed however approach still limited fact condition number appears squared 55 improvement guaranteed also note multiple projections almost identical form numerical properties fixed precision iterative refinement least squares problem 3 p125 fixed precision iterative refinement appropriate approach chosen compute projections stable see compare 43 perturbation analysis least squares problem 3 theorem 146 gives dependence condition number linearnot quadratic moreover since multiplied kg small effect condition number much smaller 57 43 mention two iterative refinement techniques one might consider either effective practical context first use fixedprecision iterative refinement 3 section 29 attempt improve solution v normal equations 310 however generally unsuccessful fixedprecision iterative refinement improves measure backward stability 21 p126 cholesky factorization already backward stable method performed numerical tests found improvement strategy however well known iterative refinement often succeed extendedprecision used evaluate residuals could therefore consider using extended precision iterative refinement improve solution v normal equations 310 long residuals 310 smaller one norm expect error solution 310 decrease factor ffl 2 reaches offl since optimization algorithms normally use double precision arithmetic computations extending precision may simple efficient strategy suitable general purpose software reason consider use extended precision 5152 iterative refinement least squares problem 52 augmented system approach apply fixed precision iterative refinement solution obtained augmented system 315 gives following iteration compute solve g deltag ae g ae v update note method applicable general preconditioners g appropriate value ff hand incorporate iteration described 46 general analysis higham 26 theorem 32 indicates condition number large expect high accuracy v good accuracy g cases example 3 solved problem given example 1 using iterative refinement technique case multiple projections discussed example 2 measure angle g columns every cg iteration iterative refinement applied long cosine angle greater 10 gamma12 results given figure 3 observe residual r g decreased almost much multiple projections approach attains acceptably small value point however residual increases reaches value 10 gamma10 cg iteration continued hundred iterations residual exhibits large oscillations return x61 experience 1 iterative refinement step normally enough provide good accu racy encountered cases 2 3 steps beneficial 6 residual update strategy seen significant roundoff errors occur computation projected residual vector much smaller residual r describe procedure iteration residual figure 3 cg method using iterative refinement augmented system approach redefining r norm closer g dramatically reduce roundoff errors projection operation begin noting algorithm ii theoretically unaffected immediately computing r 214 redefine equivalence due condition fact r used 215 216 follows redefine r means 61 either normal equations approach 38313 augmented system approach 312315 results would theory unaffected freedom redefine r seek value minimizes g symmetric matrix z gz positive definite g gamma1 generalized inverse g vector solves 62 obtained gives rise following modification cg iteration algorithm iii preconditioned cg residual update choose initial point x satisfying 12 compute find vector minimizes krgammaa compute set gammag repeat following steps convergence test satisfied x procedure works well practice improved adding iterative refinement projection operation case 1 2 iterative refinement steps used notice simple interpretation steps 66 67 first obtain solving 62 indicated required value 315 315 may rewritten g thus obtain g step 67 instead found solving 611 advantage using 611 compared 315 solution latter may dominated large components v former g large componentsof course floating point arithmetic zero component solution 611 instead tiny rounded values provided 611 solved stable fashion viewed way see steps 66 67 actually limited form iterative refinement computed v computed g discarded used refine solution iterative semirefinement used contexts 6 22 another interesting interpretation reset r r gamma performed start algorithm iii parlance optimization c gradient objective function 11 r gamma gradient lagrangian problem 1112 vector computed 62 called least squares lagrange multiplier estimate common always case optimization algorithms set 62 compute multipliers thus algorithm iii propose initial residual set current value gradient lagrangian opposed gradient objective function one could ask whether sufficient resetting r beginning algorithm iii omit step 66 subsequent iterations computational experience shows even though initial resetting r reduces magnitude sufficiently avoid errors first cg iteration subsequent values r grow rounding errors may reappear strategy proposed algorithm iii safe ensures r small every iteration one think various alternatives one monitor norm r apply residual update seems growing 61 case particularly efficient implementation residual update strategy note 62 precisely objective least squares problem 311 occurs computing via normal equations approach therefore desired value nothing vector v 310 312 furthermore first block equations 312 shows r therefore case 66 replaced r 67 words applied projection operation twice special case multiple projections approach described previous section based observations propose following variation algorithm iii requires one projection per iteration noted 66 written rather performing projection define r g projected residual computed previous iteration resulting iteration given algorithm iii following two changes omit replace 610 g g r strategy performed well numerical experiments avoids extra storage computation required algorithm iii show mathematically equivalent algorithm iii turn mathematically equivalent algorithm ii arguments follow make use fact first iteration clearly algorithm iii except value store r last step r us consider effect next iteration numerator definition 63 ff becomes g equals r p g thus formula ff theoretically unchanged advantage never negative case 63 rounding errors dominate projection operation next step different value calculated algorithm iii step 66 omitted new variant algorithm iii projected residual calculated 67 p p r ffhp mathematically equivalent value pp r ffhp calculated algorithm iii recall 66 written new strategy applies double projection r finally let us consider numerator 68 new variant given whereas algorithm iii given expanding expressions see formula fi mathematically equivalent cases new variant projection applied selectively example 4 solved problem given example 1 using residual update strategy results given figure 4 show normal equations augmented system approaches equally effective case plot cosine 317 angle preconditioned residual columns small approaches tend grow iteration progressed normal equations approach cosine order 10 gamma14 throughout cg iteration augmented system approach order 10 gamma15 note obtained higher accuracy iterative refinement strategies described previous section compare figures 2 3 augmented system iteration residual normal equations iteration residual figure 4 conjugate gradient method residual update strategy obtain highly reliable algorithm case combine residual update strategy described iterative refinement projection operation gives rise following iteration used numerical tests reported x7 algorithm iv residual update iterative refinement choose initial point x satisfying 12 compute projection computed normal equations 38 augmented system 312 approaches set gammag choose tolerance max repeat following steps convergence test satisfied x apply iterative refinement p r 317 less conclude discussion elaborating point made example 4 concerning computation steplength parameter ff noted formula preferable 612 since cannot give rise cancellation similarly stopping test based g g rather g r residual update implemented algorithm iv change automatically believe expressions recommended implementations cg iteration provided preconditioner based test repeated computation reported example using augmented system approach see figure 1 change algorithm ii used new ff stopping test cg iteration able continue past iteration 70 able reach value also repeated calculation made example 3 residual reached level large oscillations residual mentioned example 3 longer took place thus cases alternative expressions ff stopping test beneficial 62 general g also improve upon efficiency algorithm iii general g using slightly outdated information idea simply use obtained computing g 67 suitable rather waiting following step 65 obtain slightly uptodate version resulting iteration given algorithm iii following two changes omit replace 610 g g r r obtained biproduct 67 notice however general g extra matrixvector product v required since longer relationship exploited although experimented idea proved beneficial similar circumstances 22 7 numerical results test efficacy techniques proposed paper collection quadratic programs form 1112 problems generated last iteration interior point method nonlinear programming described 7 method applied set test problems cute 4 collection apply cg method preconditioner 34 ie solve quadratic programs use augmented system normal equations approaches compute projections compare standard cg iteration stand iterative refinement ir techniques described x5 residual update strategy combined iterative refinement update given algorithm iv results given table 1 first column gives problem name second dimension quadratic program test reliability techniques proposed paper used demanding stopping test cg iteration terminated experiments included several stopping tests cg iteration typically used trust region methods optimization terminate number iterations exceeds 2n gamma denotes dimension reduced system 24 superscript 1 table 1 indicates limit reached cg iteration also stopped length solution vector greater trust region radius set optimization method see 7 us superscript 2 indicate safeguard activated note problems excessive rounding errors trigger finally terminate p hp 0 indicated 3 r g 0 indicated 4 note standard cg iteration able meet stopping test problems table 1 iterative refinement update residual successful cases table 2 reports cpu time problems table 1 note times standard cg approach stand interpreted caution since problems terminated prematurely include times standard cg iteration show iterative refinement residual update strategies greatly increase cost cg iteration next report 3 problems stopping test could met variants three problems table 3 provides least residual norm attained strategy final indirect test techniques proposed paper report results obtained interior point nonlinear optimization code described 7 29 nonlinear programming problems cute collection code applies cg method solve quadratic program iteration used augmented system augmented system normal equations problem dim stand ir update stand ir update corkscrw 147 coshfun optctrl6 table 1 number cg iterations different approaches 1 indicates iteration limit reached 2 indicates termination trust region bound 3 indicates negative curvature detected 4 indicates r augmented system normal equations problem dim stand ir update stand ir update coshfun optctrl6 table 2 cpu time seconds 1 indicates iteration limit reached 2 indicates termination trust region bound 3 indicates negative curvature detected 4 indicated r augmented system normal equations problem dim stand ir update stand ir update obstclae 900 23d07 15d07 55d08 23d07 99d08 42d08 table 3 least residual norm r g attained option normal equations approaches compute projections strategies tried standard cg iteration stand residual update strategy update iterative refinement described algorithm iv results given table 4 fevals denotes total number evaluations objective function nonlinear problem projections represents total number times projection operation performed optimization indicates optimization algorithm unable locate solution note total number function evaluations roughly strategies cases differences cg iteration cause algorithm follow different path solution expected solving nonlinear problems note augmented system approach residual update strategy changes number projections significantly problems improvements substantial hand observe normal equations approach sensitive condition number residual update strategy gives substantial reduction number projections half problems interesting residual update performance augmented system normal equations approaches similar 8 conclusions studied properties projected cg method solving quadratic programming problems form 1112 due form preconditioners used nonlinear programming algorithms opted computing basis z null space constraints instead projecting cg iterates using normal equations augmented system approach given examples showing either case significant roundoff errors occur presented explanation proposed several remedies one use iterative refinement augmented system normal equations approaches alternative update residual every iteration cg iteration described x6 latter implemented particularly efficiently preconditioner given 33 numerical experience indicates updating residual almost always suffices keep errors tolerable level iterative refinement techniques effective update residual used conjunction numerical results reported paper indicate combined strategy economical accurate 9 acknowledgements authors would like thank andy conn philippe toint helpful input early stages research augmented system normal equations f evals projections f evals projections problem n stand update stand update stand update stand update corkscrw 456 350 64 61 458 422 coshfun gausselm 14 11 25 26 92 93 28 41 85 97 hager4 2001 1000 obstclae 1024 0 26 26 6233 6068 26 26 6236 6080 optcntrl optctrl6 122 table 4 number function evaluations projections required optimization method different implementations cg iteration r iterative solution methods cute constrained unconstrained testing environment stable methods calculating inertia solving symmetric linear equations linear least squares solutions housholder transfor mations primal primaldual methods nonlinear programming linearly constrained optimization projected preconditioned conjugate gradients null space problem complexity null space problem ii algorithms preconditioned conjugate gradient approach linear equality constrained minimization global convergence theory general trustregion based algorithms equality constrained optimization direct methods sparse matrices multifrontal solution indefinite sparse symmetric linear equations design ma48 practical methods optimization computing sparse basis nullspace snopt sqp algorithm largescale constrained optimization practical optimization matrix computations iterative methods illconditioned linear systems optimiza tion solving trustregion subproblem using lanczos method sparse orthogonal schemes structural optimization using force method methods conjugate gradients solving linear systems iterative refinement lapack implicit nullspace iterative methods constrained least squares problems implementation algorithm largescale equality constrained optimization multifrontal computation orthogonal factors sparse matrices indefinitely preconditioned inexact newton method large sparse equality constrained nonlinear programming problems preconditioning reduced matrices substructuring methods computing null space equilibrium matrices conjugate gradient method extremal problems qr factorization large sparse overdetermined square matrices multifrontal method multiprocessing environment conjugate gradient method trust regions large scale optimiza tion nested dissection sparse nullspace bases towards efficient sparsity exploiting newton method minimization largescale nonlinear network optimization tr ctr luca bergamaschi jacek gondzio manolo venturin giovanni zilli inexact constraint preconditioners linear systems arising interior point methods computational optimization applications v36 n23 p137147 april 2007 h dollar n gould w h schilders j wathen using constraint preconditioners regularized saddlepoint problems computational optimization applications v36 n23 p249270 april 2007 luca bergamaschi jacek gondzio giovanni zilli preconditioning indefinite systems interior point methods optimization computational optimization applications v28 n2 p149171 july 2004 bocanegra f f campos r oliveira using hybrid preconditioner solving largescale linear systems arising interior point methods computational optimization applications v36 n23 p149164 april 2007 nicholas gould dominique orban philippe l toint galahad library threadsafe fortran 90 packages largescale nonlinear optimization acm transactions mathematical software toms v29 n4 p353372 december cafieri dapuzzo v simone serafino iterative solution kkt systems potential reduction software largescale quadratic problems computational optimization applications v38 n1 p2745 september 2007 nicholas gould philippe l toint iterative workingset method largescale nonconvex quadratic programming applied numerical mathematics v43 n12 p109128 october 2002 meizhong dai david p schmidt adaptive tetrahedral meshing freesurface flow journal computational physics v208 n1 p228252 1 september 2005 silvia bonettini emanuele galligani valeria ruggiero inner solvers interior point methods large scale nonlinear programming computational optimization applications v37 n1 p134 may 2007