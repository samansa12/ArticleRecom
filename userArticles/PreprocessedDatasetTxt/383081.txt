extending equationbased congestion control multicast applications paper introduce tfmcc equationbased multicast congestion control mechanism extends tcpfriendly tfrc protocol unicast multicast domain key challenges design tfmcc lie scalable roundtrip time measurements appropriate feedback suppression ensuring feedback delays control loop adversely affect fairness towards competing flows major contribution feedback mechanism key component endtoend multicast congestion control schemes improve upon wellknown approach using exponentially weighted random timers biasing feedback favor lowrate receivers still preventing response implosion evaluate design using simulation demonstrate tfmcc tcpfriendly scales well multicast groups thousands receivers also investigate tfmccs weaknesses scaling limits provide guidance application domains well suited b introduction widely accepted one several factors inhibiting usage ip multicast lack good deployable welltested multicast congestion control mechanisms quote 10 success internet relies fact besteffort traffic responds congestion link reducing load presented network congestion collapse todays internet prevented congestion control mechanisms tcp believe multicast successful crucial multicast congestion control mechanisms deployed coexist tcp fifo queues current internet university mannheim germany att center internet research icsi aciri precise requirements multicast congestion control perhaps open discussion given efficiency savings multicast take conservative position multicast flow acceptable achieves greater mediumterm throughput receiver multicast group would achieved tcp flow multicast sender receiver requirement satisfied either single multicast group sender transmits rate dictated slowest receiver group layered multicast scheme allows different receivers receive different numbers layers different rates much work done latter class 12 18 4 jury still whether mechanisms made safe deploy paper describes tcpfriendly multicast congestion control tfmcc belongs class single rate congestion control schemes schemes inevitably scale well layered schemes however much simpler match requirements applications well demonstrate scale applications many thousands receivers schemes also suffer degradation face badly broken links receivers deal situations policy deci sion expect applications using singlerate scheme applicationspecific thresholds receiver compelled leave multicast group tfmcc singlerate multicast congestion control scheme available particular pragmatic general multicast congestion control pgmcc 17 also viable solution nice properties certain elegant simplic ity however tfmcc pgmcc differ considerably smoothness predictability transmission argue appropriate solutions applications better suited one 11 tfmcc tfrc tcpfriendly rate control protocol tfrc 5 unicast congestion control mechanism intended applications require smoother predictable transmission rate tcp achieve tfmcc extends basic mechanisms tfrc multicast domain tfrc equationbased congestion control scheme uses control equation derived model tcps longterm throughput directly control senders transmission rate basically tfrc functions follows 1 receiver measures packet loss rate feeds information back sender 2 sender uses feedback messages measure roundtrip time receiver 3 sender uses control equation derive acceptable transmission rate measured loss rate roundtrip time rtt 4 senders transmission rate adjusted directly match calculated transmission rate full details tfrc refer reader 5 tfmcc follows similar design multicast congestion control primary differences receivers measure rtt sender perform calculation acceptable rate rate fed back sender challenge manner ensures feedback receiver lowest calculated rate reaches sender whilst avoiding feedback im plosions moreover need make sure additional delay imposed avoid feedback implosion adversely affect fairness towards competing protocols 2 tfmcc protocol building equationbased multicast congestion control mechanism requires following problems solved control equation must chosen defines target throughput terms measurable parameters case loss event rate rtt receiver must measure loss event rate thus filter packet loss history needs chosen good stable measure current network condi tions sufficiently responsive conditions change receiver must measure estimate rtt sender devising way without causing excessive network traffic key challenge receiver uses control equation calculate acceptable sending rate sender feedback scheme must devised feedback receiver calculating slowest transmission rate always reaches sender feedback implosions occur network conditions change filtering algorithm needs devised sender determine feedback take account adjusts transmission rate clearly parts closely coupled example altering feedback suppression mechanisms impact sender deals feedback many design choices heavily influenced tfrc mechanisms fairly well understood tested paper expend efforts focusing parts tfmcc differ tfrc 21 determining acceptable sending rate control equation used tfrc tfmcc derived model longterm tcp throughput bytessec 15 q 1 expected throughput tcp tcp flow calculated function steadystate loss event rate p roundtrip time rtt packet size tfmcc receiver measures loss event rate estimates rtt sender uses equation 1 calculate tcp estimate throughput tcp flow would achieve network path receiver network conditions sender exceed rate receiver tcpfriendly affect tcp flow bottlenecks another tcp flow would following section elaborate necessary parameters model computed deal potentially large receiver sets 22 adjusting sending rate sender continuously receive feedback ceivers receiver sends feedback indicates rate lower senders current rate sender immediately reduce rate feedback message order eliminate large number unnecessary messages receivers send feedback unless calculated rate less current sending rate however leaves us problem increase transmission rate cannot afford increase transmission rate absence feedback feedback path slowest receiver may congested lossy solution introduce concept current limiting receiver clr clr receiver sender believes currently lowest expected throughput group 1 clr permitted 1 respect clr comparable representative used congestion control schemes pgmcc send immediate feedback without form suppression sender use clrs feedback increase transmission rate clr change another receiver sends feedback indicating lower transmission rate required also change clr leaves multicast group normally signaled clr additional timeout mechanism serves backup case clr crashes becomes unreachable normally way loss measurement performed limits possible rate increase roughly 03 packets per rtt shown 5 however clr leaves group new clr may significantly higher calculated rate cannot afford increase directly rate loss rate currently measured may predictor loss rate new transmission rate instead impose rate increase limit one packet per rtt tcps additive increase constant rate gradually increases new clrs rate 23 measuring loss event rate loss event rate scalably measured ceivers measurement mechanism closely matches used tfrc receiver aggregates packet losses loss events defined one packets lost roundtrip time number packets consecutive loss events called loss interval average loss interval size computed weighted average recent loss intervals l l avg weights w chosen recent loss intervals receive high weights weights gradually decrease 0 older loss intervals example eight weights might use f5 5 5 5 4 3 2 1g allows smooth changes l avg loss events age large values improve smoothness estimate long loss history also reduces responsiveness thus fairness protocol values around 8 32 appear good compromise loss event rate p used input tcp model defined inverse l avg interval since recent loss event end loss event thus may reflect loss event rate interval included calculation loss event rate reduces p thorough discussion loss measurement mechanism see 5 24 roundtrip time measurements key challenge tfmcc receiver able measure rtt sender without causing excessive traffic sender practice problem primarily one getting initial rtt measurement use timestamps data packets receiver see changes delay forward path simply packets arrival time discuss section 243 241 rtt estimate initialization ideally would like receiver able initialize rtt measurement without exchange feedback packets sender possible sender receiver synchronized clocks might achieved using gps receivers less accurately also done using clocks synchronized ntp 13 either case data packets timestamped sender receiver compute oneway delay rtt estimated twice oneway delay sr case ntp errors accumulate stratum 1 server local host must taken account ntp server knows rtt dispersion stratum1 server synchronized sum gives worstcase error synchronization conservative practice ntp provides average timer accuracy 2030 ms 13 cases gives us estimate rtt accurate least nearest 100 ms although perfect still useful first estimate many cases though reliable form clock synchronization available receiver must initialize rtt estimate value larger highest rtt receivers assume networks value 500 ms appropriate 1 initial value used real measurement made appendix reason safe also use value aggregate losses loss events low rtt value would conservative option 242 rtt measurement receiver gets measure instantaneous rtt inst rtt sending timestamped feedback sender echoes timestamp receiver id header data packet feedback messages arrive data packets sent prioritize senders report echoes following order 1 receiver whose report causes selected new clr 2 receivers yet measured rtt 3 nonclr receivers previous rtt measurements 4 existing clr ties broken favor receiver lowest reported rate normally number data packets larger number feedback packets clrs last report echoed remaining data packets 2 prevent single spurious rtt value excessive effect sending rate smooth values using exponentially weighted moving average ewma inst clr set 005 given receivers get frequent rtt measurements thus old measurements likely outdated higher value non used 243 oneway delay rtt adjustments due infrequent rtt measurements would also possible large increases rtt go unnoticed receiver clr avoid adjust rtt estimate actual measurements since data packets carry send timestamp data receiver gets rtt measurement time also compute oneway delay sender receiver including clock skew data oneway receiver sender inst rtt dsr due clock skew values directly meaningful drs used modify rtt estimate real rtt measurements later data packet oneway delay sender receiver determined 0 sr possible compute uptodate rtt estimate inst rtt clock skew sender receiver cancels provided clock drift real rtt measurements neg ligible modified rtt estimates smoothed ewma like normal rtt measurements albeit smaller decay factor ewma since oneway delay adjustments possible new data packet oneway delay adjustments used indicator rtt may changed significantly thus real rtt measurement necessary receiver selected clr measures rtt next packet interim oneway delay adjustments discarded reason proved unnecessary filter flawed oneway delay estimates 2 able infer accurate rtt timestamps necessary also take account offset receipt timestamp echoing back 244 senderside rtt measurements preconfigured initial rtt value used receiver loss aggregation rate computation used set sending rate using high initial rtt would result low sending rate followed high sending rate clr gets first rtt measurement clr change receiver previous rtt mea surement rate oscillations avoided hand sender accepted receiver valid rtt clr receivers high loss rate might never receive feedback echo never become clr reasons tfmcc supports additional senderbased rtt measurements receiver report also echoes timestamp last data packet sender receivers able measure rtt sender computes rtt react receiver report without valid rtt uses adjust calculated rate receiver report 25 receiver feedback tfmcc designed used receiver sets perhaps several thousand receivers critical ensure sender gets feedback receivers experiencing worst network conditions without overwhelmed feedback receivers congestion may occur point distribution tree senders access link single receivers tail circuit thus mechanism must able cope conditions change single receiver lightly congested receivers equally heavily congested similarly pathological cases time would like feedback delay relatively small steady state latter achieved concept clr send feedback immediately however clr help change network conditions affect receivers clr thus ignore influence clr feedback process section note clr generates relatively little feedback traffic strictly improves responsiveness congestion reduces amount feedback sent receivers various reliable multicast protocols incorporate feedback trees receivers organized tree hierarchy internal nodes tree aggregate feedback trees largely solve feedback implosion problem difficult build maintain tree exists clearly used paper assume case examine pure endtoend suppression mechanisms several mechanisms using randomized timers feedback suppression multicast protocols proposed 6 7 9 14 time divided feedback rounds either implicitly explicitly indicated receivers start feedback round receiver sets randomized timer receiver hears feedback another receiver makes unnecessary send feed back cancels timer otherwise timer expires feedback message sent tfmcc use mechanism based exponentially distributed random timers feedback timer expires receiver unicasts current calculated sending rate sender rate lower previous feedback received sender echoes feedback receivers respect intended application finding correct clr improve upon original concept biasing feedback favor lowrate receivers dynamics mechanism depend way timers initialized one receivers feedback suppresses anothers 251 randomized timer values basic exponentially distributed random timer mechanism initializes feedback timer expire seconds x uniformly distributed random variable 0 1 upper limit delay sending feedback n estimated upper bound number receivers set multiple maximum rtt receivers rtt choice b determines number feedback packets per round sent worstcase conditions feedback delay normal conditions section 254 show purpose useful values b lie 3 6 use default value 4 mechanism relatively insensitive overestimation receiver set size n underestimation may result feedback implosion thus sufficiently large value n chosen simulations use seems reasonable given scaling goals whilst basic algorithm sufficient prevent feedback implosion ensure receivers low expected rates likely respond receivers high rates even receiver respond rate less current sending rate ensure lowestrate receiver respond quickly congestion worsens rapidly 3 thus sender would insufficiently responsive increased congestion avoid problem bias feedback timers favor receivers lower rates still allowing sufficient 3 fact receivers lower rtts incorrectly favored since receive feedback request earlier randomization avoid implosion receivers calculate low rate since receiver knows sending rate calculated rate receivers good measure importance feedback ratio r calculated rate current sending rate 4 several ways use r bias timers modify n reduce upper bound receiver set offset subtract offset value feedback time modify x reduce random value x three alternatives cause lowrate receivers report earlier differ respect degree biasing cause circumstances feedback implosion might possible modifying n value never reduced less actual number receivers n since receivers send immediate response probability 1n case n n number feedback responses increases linearly relation n n known large thus possible safely reduce n makes sense always use reduced n feedback suppression instead using biasing using offset decreases time congested receivers respond probability short timer value greatly increased suppression still works determines fraction used spread feedback responses respect reported rate care taken ensure 1 sufficiently large prevent feedback implosion log n x log n r third case similar second case different offset value also important bound impact r feedback time figure shows cumulative distribution function cdf feedback time changes original cdf biasing feedback decrease n corresponds shifting cdf thus increasing probability early responses cannot suppressed contrast using fraction offset reduces time responses spread assuming worst case receivers reporting optimal value thus tfmcc feedback timers biased favor lowrate receivers offset equation 3 clarify method affects feedback time timevalue distribution receiver set without biasing timers biased offset depicted figure 2 suppressed feedback marked dot feedback received sender 4 note 0 r 1 since receivers lower rates current rate send reports cumulative probability feedback time rtts exponential offset modified n figure 1 different feedback biasing methods marked cross best value feedback received marked square note uniform distribution feedback value r used graph unlikely occur reality used purpose demonstrating properties feedback biasing offset method time interval available suppression smaller unbiased feedback original worst case delay maintained consequence number feedback messages higher biasing feedback timers however biasing early feedback messages thus also best feedback value received closer optimal11 feedback value feedback time offset normal figure 2 timevalue distribution optimize offset method truncating range r likely values normalizing resulting interval 01 implementation instead r use effect start biasing feedback ceivers rate less 90 senders rate doesnt significantly affect fairness saturate bias ceivers rate 50 senders rate since receivers even lower rates take several rounds loss measures change anyway 252 canceling feedback receiver sees echoed feedback another receiver must decide whether cancel feedback timer one possibility rely completely feedback timer bias cancel timer receipt first feedback round another possibility cancel timer echoed feedback indicates rate lower rate receiver wanted report latter guarantees receiver lowest rate always get send feedback former results significantly less feedback traffic worst case spectrum lies two extremes receivers calculated rate r calc rate echoed feedback r fb timer canceled r fb r calc r fb former method discussed corresponds latter change zero one reduce chance hearing absolute lowestrate ceiver also reduce increase number feedback messages shown 19 expected number feedback messages increases logarithmically n values 1 number becomes approximately constant limit large number responses number receivers n suppressed 10 lower suppressed higher suppressed figure 3 different feedback cancellation methods results corroborated simulations depicted figure 3 graph shows number feedback messages first round worstcase scenario n receivers except clr suddenly experience congestion effects 00 01 10 shown values around 01 result desired behavior marginally higher number feedback messages resulting transient transmission rate worse 10 higher improvement sent feedback values caused biasing combination feedback cancellation method results significant improvement characteristics feedback process normal exponential feedback timers 253 feedback low sending rates low sending rates high loss rates usually go together still possible get feedback implo sion feedback echo sender suppresses feedback sent next data packet thus delay next data packet sent close feedback delay arrive late suppression work problem prevented increasing feedback delay proportion time interval data packets sending rate r send low r send c number consecutive data packets lost without running risk implosion packet size recommend using values c 2 4 254 expected number feedback messages feedback delay feedback quality expected number duplicate feedback messages ef exponential feedback suppression given 7 n actual number receivers network delay unicast feedback channels 0 maximum feedback delay used suppression assuming worst case whilst primary concern avoid implosion low number responses say 1 2 also undesirable additional responses greatly increase probability lowrate lowestrate receiver respond also provide rtt measurements larger number receivers figure 4 shows plot ef different values 0 n range roughly 3 4 rtts result desired number feedback messages particularly common range n one two orders magnitude n reason values chosen tfmcc implementation 14 4 max rtt spectively given choices examine well feedback biasing methods achieve additional goal low response time close reported rate true lowestrate receiver figure 5 compares feedback delay unbiased exponential timers basic offset bias modified offset psfrag replacements35 6 1100100002060100number receivers rtts number responses figure 4 expected number feedback messages uses r 0 instead r three show logarithmic decrease response time number receivers typical feedback suppression based exponential timers difference methods great modified offset algorithm slight edge regular offset examining rates reported feedback messages advantage offset methods becomes ap parent figure 6 compares lowest reported rate feedback messages single feedback round actual lowest rate receiver set example value 01 indicates lowest reported rate average 10 higher one feedback round ideal case rates reported offset methods considerably closer real minimum reported unmodified exponential timers particularly r adjusted appropriately modified offset method feedback normally within percent minimum rate plain exponential feedback shows average deviations nearly 20 minimum rate135 response time number receivers n unbiased exponential basic offset modified offset figure 5 comparison methods bias feedback 26 slowstart tfmcc uses slowstart mechanism quickly approach fair bandwidth share start session quality reported rate number receivers n unbiased exponential basic offset modified offset figure comparison methods bias feedback slowstart sending rate increases exponentially whereas normal congestion control allows linear increase exponential increase easily lead heavy congestion great care taken design safe increase mecha nism simple measure end limit increase multiple minimum rate r min recv received receivers since receiver never receive rate higher link bandwidth effectively limits overshoot times bandwidth target sending rate calculated r target recv current sending rate gradually adjusted target rate course rtt implementation use value 2 slowstart terminated soon one receivers experiences first packet loss necessary use different feedback bias slowstart since receivers cannot calculate tcpfriendly rate reason use report receiver experiences first loss event suppressed reports also indicating packet loss reports receivers yet experience loss thus slowstart terminated later one feedback delay loss detected practice tfmcc seldomly reach theoretical maximum doubling sending rate per rtt two reasons target sending rate increased feedback new feedback round received thus doubling possible every rtt every feedback delay usually much larger rtt measuring receive rate several rtts gradually increasing r send r target send gives minimum receive rate end feedback interval lower sending rate interval thus setting r target send twice minimum receive rate double current sending rate desirable multicast protocol tfmcc slowstart behaves conservatively comparable unicast slowstart mechanisms 3 protocol behavior large receiver sets loss path multiplicity problem wellknown characteristic multicast congestion control mechanisms react single loss indications receivers different network paths prevents scaling mechanisms large receiver sets 3 authors propose possible solution tracking congested path taking loss indications path account since reports tfmcc receiver contain expected rate based loss event rate rtt single path sender ceiver protocol implicitly avoids loss path multiplicity problem yet tfmcc singlerate congestion control schemes may confined rate fair rate rather single congested path path changes time faster multicast congestion control protocol responds transient congestion pronounced effect tracking minimum stochastic variations calculated rate different receivers example loss several receivers independently varies fairly quickly 0 10 average 5 congestion control protocol may always track worst receiver giving loss estimate twice worstcase scenario respect high number receivers independent loss calculated rate range lowestrate receiver n receivers experience independent packet loss loss probability loss intervals exponential distribution expected value minimum n exponentially distributed random variables proportional 1n thus tfmcc based rate calculations single loss interval average sending rate would scale proportionally 1 n case moderate loss rates otherwise even worse rate calculation tfmcc based weighted average loss intervals since average exponentially distributed random variables gamma distributed expected loss rate tfmcc inversely proportional expected value minimum n gamma distributed random variables 5 effect shown figure 7 different numbers receivers n constant loss probability uncorrelated loss rate 10 rtt 50 ms fair rate tfmcc transmission around 300 kbits sending rate reached receiver set consists single 5 first order statistics gamma distribution simple closed form expressions exists details distribution minimum gamma distributed random variables found 8 throughput kbits number receivers constant distrib figure 7 scaling receiver quickly drops value fraction fair rate larger n example 10000 receivers 16 fair rate achieved fortunately loss distribution extremely unlikely real networks multicast data transmitted along paths distribution tree underlying multicast routing pro tocol lossy link high tree may affect large number receivers losses correlated effect occur receivers additional lossy links loss rates longer cor related rather values spread larger inter val thus decreasing number receivers similar loss rates demonstrate effect choose distribution loss rates closer actual loss distributions multicast trees limited number high loss receivers majority receivers moderate loss rates 6 small number receivers proportional logn constant high loss range 510 range 25 vast majority loss rates 05 2 network conditions throughput degradation 10000 receivers merely 30 thus throughput degradation plays significant role vast majority packet loss occurs last hop receivers losses amount loss rates impossible distinguish stochastic decrease sending rate real decrease caused increased congestion level otherwise would possible estimate effect adjust sending rate accordingly degradation effect alleviated increasing number loss intervals used loss history albeit expense less responsiveness 6 means claim chosen distribution exactly reflects network conditions multicast distribution trees simulations implemented tfmcc ns2 network simulator 2 investigate behavior controlled conditions paper report small fraction simulations carried simulations droptail queues used routers ensure acceptable behavior current internet generally fairness towards tcp intraprotocol fairness improve active queuing eg red used instead 41 fairness fairness towards competing tcp flows analyzed using wellknown singlebottleneck topology figure number sending nodes connected many receiving nodes common bottleneck figure 9 shows receivers senders bottleneck link figure 8 topology throughput tfmcc flow two sample tcp flows 15 typical example simulations average throughput tfmcc closely matches average tcp throughput tfmcc achieves smoother rate similar results obtained many combinations flows general higher level statistical multiplexing better fairness among competing flows scenarios number tfmcc flows greatly exceeds number tcp flows tfmcc aggressive tcp reason lies spacing data packets buffer requirements tfmcc spaces data packets sends backtoback send multiple packets making tcp sensitive nearlyfull queues typical droptail queue management instead one bottleneck topology separate bottlenecks last hops receivers observe throughput degradation predicted section 3 scenario modified tfmcc competes single tcp flows sixteen identical 1 mbits tail circuits tfmcc achieves 70 tcps throughput see figure 10 throughput kbits time figure 9 one tfmcc flow 15 tcp flows single throughput kbits time figure flow 16 tcp flows individual bottlenecks 42 responsiveness changes loss rate important concern design congestion control protocols responsiveness changes network condi tions furthermore receivers join leave session important tfmcc react sufficiently fast change clr required behavior investigated using star topology four links rtt 60 ms loss rates 01 05 25 125 respectively beginning simulation receiver set consists receiver lowest loss rate receivers join session 100 seconds 50 second intervals order loss rates lowerlossrate receivers join first 250 seconds receivers leave transmission reverse order 50 second intervals verify tfmcc throughput similar tcp throughput additional tcp connection receiver set duration whole experiment show figure 11 tfmcc matches closely tcp throughput four loss levels adaption sending rate new higherloss receiver joins fast receiver needs 5001000 ms join get enough packets compute meaningful loss rate major part delay caused exponential timer feedback sup pression increases overall delay new clr chosen roughly one three seconds 7 experiment 7 note high delay caused use initial rtt feedback suppression mechanism receivers valid rtt esti mate delay caused feedback suppression much shorter2610 throughput mbits time figure 11 responsiveness changes loss rate demonstrates tfmccs good reactivity changes congestion level delay tfmcc assumes ratelimiting receiver left group sending rate increased configurable currently absence feedback feedback delay used indication receiver left group case explicit leave messages used tfmcc protocol delay reduced one rtt simulation setting used investigate responsiveness changes rtt results shown similar since four receivers measured rtt time rtt changes oneway rtt adjustments immediately indicate change larger receiver sets amount time expires high rtt receiver found may greater effect investigated next section 43 initial rtt measurements responsiveness changes rtt number receivers measure rtt feedback round depends number feedback messages thus parameters used feedback suppression figure 12 shows number receivers valid rtt estimate evolves time large receiver set high initial rtt value link rtts 1000 receivers vary ms 140 ms initial rtt value set 500 ms single bottleneck used produce highly correlated loss receivers worst case since loss estimates receivers vary often unnecessary measure rtt lowloss receivers since calculated rate receivers still using initial rtt current sending rate least one receiver get first rtt measurement per feedback round receivers measured rtt beginning simulation number receivers obtaining initial rtt measurements close expected number receivers valid rtt time figure 12 rate initial rtt measurements number feedback messages per feedback round time receivers valid rtt number receivers want give feedback decreases rate initial rtt measurements gradually drops one new measurement per feedback round delay 200 seconds 700 1000 receivers measured rtt seems rather large one keep mind results congestion level receivers receivers experience higher loss rates receivers measure rtt first tfmcc adapt calculated rate real network conditions necessary measure rtt receivers scenarios 40 200 1000 receivers respectively investigate long takes high rtt receiver found among receivers low rtt receiver experience independent loss loss probability xaxis graph figure 13 denotes point time rtt increased experiment yaxis shows amount time change rtt reacted upon choosing correct clr later increase rtt greater number receivers already valid rtt estimates expected time highrtt receiver selected clr decreases2060100140 delay reaction time change receivers 200 receivers 1000 receivers figure 13 responsiveness changes rtt 44 slowstart highest sending rate achieved slowstart largely determined level statistical multiplexing otherwise empty link tfmcc reach roughly twice bottleneck bandwidth leaving slowstart depicted figure 14 tfmcc competes single tcp flow slowstart terminated rate fair rate 8 tfmcc flow rate relatively independent number tfmcc receivers already case two competing tcp flows even level statistical multiplexing higher slowstart rate decreases considerably number receivers increases increase fair rate takes place slowstart normal congestion control mode50015002500 max slowstart rate kbits number receivers fair rate tfmcc one competing tcp high stat mux figure 14 maximum slowstart rate include extra graph exact increase behavior tfmcc compared tcp since seen example figures 15 16 tfmcc tcp started time tcps increase fair rate rapid takes tfmcc roughly 20 seconds reach level bandwidth 45 latejoin lowrate receiver previous experiments investigated congestion control moderate loss rates expected prevalent applications domains tfmcc well suited circumstances loss rate receiver initially much higher consider example tfmcc operates fair rate several mbits receiver lowbandwidth connection joins immediately joining receiver may experience loss rates close 100 conditions difficult avoid tfmcc ensure exist limited amount time quickly choose new receiver clr initial setup simulation eightmember tfmcc session competing seven tcp connections 8 mbits link giving fair rate 1 mbits simulation 8 fair rate tfmcc three simulations 1 mbits new receiver joins session behind separate 200 kbits bottleneck sender time 50 100 seconds tfmcc problems coping sce nario choosing joining receiver clr within seconds although loss rate joining receiver initially high tfmcc rate drop zero soon buffer 200 kbits connection full receiver experiences first loss event loss history initialized details loss history initialization process found appendix b first loss occurs receiver gets data rate exactly bottleneck band width thus loss rate initialized value 80 value adapt appropriate loss event rate available bandwidth 200 kbits used additional tcp flow set using 200 kbits link duration experiment flow inevitably experiences timeout new receiver joins multicast group link flooded packets however shortly afterwards tfmcc adapts available capacity tcp recovers bandwidth shared fairly tfmcc tcp conclude tfmcc shows good performance fair ness even unfavorable network throughput kbits time aggregated tcp flows tfmcc flow figure 15 latejoin lowrate receiver200600100014000 20 40 throughput kbits time aggregated tcp flows tcp 200kbits link tfmcc flow figure additional tcp flow slow link 5 related work date number singlerate multicast congestion control schemes proposed prominent recent example pgmcc 17 selects receiver worst network conditions group representative called acker selection process acker mainly determines fairness protocol based simplified version tcp throughput model equation 4 similar tfmcc receiver tracks rtt smoothed loss rate feeds values model results communicated sender using normal randomized feedback timers avoid implosion available pgmcc also makes use network elements aggregate feedback acker selected tcpstyle windowbased congestion control algorithm run sender acker minor modifications compared tcp concern separation congestion control reliability able use pgmcc reliable well unreliable data transport handling order packets rtt changes different receiver selected acker evidenced simulations 17 pgmcc competes fairly tcp many different network conditions basic congestion control mechanism simple dynamics well understood analysis tcp congestion control close mimicking tcps window behavior produces rate variations resemble tcps sawtooth like rate makes pgmcc suited applications cope larger variations sending rate con trast rate produced tfmcc generally smoother predictable making tfmcc well suited applications constraints acceptable rate changes since acker selection process critical pgmccs perfor mance pgmcc might benefit using feedback mechanism similar tfmcc based biased exponentially weighted timers summarize believe pgmcc tfmcc present viable solutions singlerate multicast congestion control targeted somewhat different application domains pgmcc relies congestion window tcpemulation receivers tear 16 combination window ratebased congestion control features tcplike window emulation algorithm receivers window used directly control transmission instead average window size calculated transformed smoothed sending rate used sender space data packets far unicast version tear exists mechanism made multicastcapable implementing tfmcclike scalable feedback suppression scheme communicate calculated rate sender well scalable rtt measurements advantage tear lies fact require model tcp necessary assumptions compute rate however low levels statistical multiplexing tears emulation assumptions independence loss timing transmit rate timeout emulation mean shares many limitations tcp models use thus expect multicast variant tear behave significantly better worse tfmcc 6 conclusions described tfmcc singlerate multicast congestion control mechanism intended scale groups several thousand receivers performing multicast congestion control whilst remaining tcpfriendly difficult particular tcps transmission rate depends rtt measuring rtt scalable manner hard problem given limitations endtoend protocols believe tfmcc represents significant improvement previous work area extensively evaluated tfmcc analysis simulation believe good understanding behavior wide range network conditions summa rize believe sort conditions tfmcc experience realworld behave rather well however also examined certain pathological cases cases failure mode tfmcc achieve slower desired transmission rate given protocols bounds good behavior failure mode would desire ensures safety internet important part research identify limitations new design tfmccs main weakness startup phase take long time sufficiently many receivers measure rtt assuming cannot use ntp provide approximate default values addition large receiver sets tcpstyle slowstart really appropriate mechanism linear increase take time reach correct operating point however weaknesses specific tfmcc safe singlerate multicast congestion control mechanism limitations tcpcompatible implication therefore singlerate multicast congestion control mechanisms like tfmcc really wellsuited relatively longlived data streams fortunately also appears current multicast applications stockprice tickers video streaming involve longlived datastreams 61 future work plan pursue work several fronts largescale multicast experiments hard perform real world plan deploy tfmcc multicast filesystem synchronization application eg rdist gain smallscale experience real application reliable multicast protocols build applicationlevel tree acknowledgment aggregation devised hybrid ratewindowbased variant tfmcc uses implicit rtt measurement combined suppression within aggregation nodes variant need perform explicit rtt measurements endtoend feedback suppres sion whilst first glance would seem big improvement variant paper truth moves complex initialization problem rtt measurement scalable acktree construction shares many problems posed rtt measurement still seems promising additional line research finally basic equationbased rate controller tfmcc would also appear suitable use receiverdriven layered multicast especially combined dynamic layering 4 eliminate problems unpredictable multicast leave latency acknowledgements would like thank sally floyd luigi rizzo invaluable comments would also like acknowledge feedback suggestions received rmrg members earlier versions tfmcc r web servers view transport layer loss path multiplicity problem multicast congestion con trol reliable multicast framework lightweight sessions application level framing scaling feedback algorithms large multicast groups order statistics gamma distribu tion session directories scalable internet multicast address allocation rfc 2357 ietf criteria evaluating reliable multicast transport application protocols macroscopic behavior congestion avoidance al gorithm internet timekeeping around globe scalable feedback large groups modeling tcp reno performance simple model empirical validation tear tcp emulation receivers flow control multimedia stream ing extremum feedback large multicast groups extending equationbased congestion control multicast applications tr receiverdriven layered multicast reliable multicast framework lightweight sessions application level framing session directories scalable internet multicast address allocation scalable feedback large groups modeling tcp reno performance pgmcc equationbased congestion control unicast applications fliddl web servers view transport layer ctr jiang li murat yuksel shivkumar kalyanaraman explicit rate multicast congestion control computer networks international journal computer telecommunications networking v50 n15 p26142640 october 2006 hualiang chen zhongxin liu zengqiang chen zhuzhi yuan extending tcp congestion control multicast computer networks international journal computer telecommunications networking v51 n11 p30903109 august 2007 jiang li murat yuksel xingzhe fan shivkumar kalyanaraman generalized multicast congestion control computer networks international journal computer telecommunications networking v51 n6 p14211443 april 2007 alexandre brandwajn model periodic acknowledgement performance evaluation v52 n4 p221235 may jrg widmer catherine boutremans jeanyves le boudec endtoend congestion control tcpfriendly flows variable packet size acm sigcomm computer communication review v34 n2 april 2004 el khayat p geurts g leduc machinelearnt versus analytical models tcp throughput computer networks international journal computer telecommunications networking v51 n10 p26312644 july 2007 injong rhee lisong xu limitations equationbased congestion control acm sigcomm computer communication review v35 n4 october 2005 jrme viron thierry turletti kav salamatian christine guillemot source channel adaptive rate control multicast layered video transmission based clustering algorithm eurasip journal applied signal processing v2004 n1 p158175 1 january 2004 x brian zhang simon lam dongyoung lee group rekeying limited unicast recovery computer networks international journal computer telecommunications networking v44 n6 p855870 22 april 2004 john w byers guin kwon michael luby michael mitzenmacher finegrained layered multicast stair ieeeacm transactions networking ton v14 n1 p8193 february 2006 ch bouras gkamas sramt hybrid sender receiverbased adaptation scheme tcp friendly multicast transmission computer networks isdn systems v47 n4 p551575 15 march 2005 injong rhee lisong xu limitations equationbased congestion control ieeeacm transactions networking ton v15 n4 p852865 august 2007 puneet thapliyal sidhartha jiang li shivkumar kalyanaraman lesbcc lossevent oriented sourcebased multicast congestion control multimedia tools applications v17 n23 p257294 julyaugust 2002 yuan gao jennifer c hou sanjoy paul raccoom ratebased congestion control approach multicast ieee transactions computers v52 n12 p15211534 december christoph neumann vincent roca rod walsh large scale content distribution protocols acm sigcomm computer communication review v35 n5 october 2005 c bouras gkamas karaliotas k stamos architecture performance evaluation redundant multicast transmission supporting adaptive qos multimedia tools applications v25 n1 p85110 january 2005