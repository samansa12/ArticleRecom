differences effects rounding errors krylov solvers symmetric indefinite linear systems threeterm lanczos process symmetric matrix leads bases krylov subspaces increasing dimension lanczos basis together recurrence coefficients used solution symmetric indefinite linear systems solving reduced system one way another leads wellknown methods minres minimal residual gmres generalized minimal residual symmlq symmetric lq discuss way extent approaches differ sensitivity rounding errorsin analysis assume lanczos basis generated exactly way different methods consider errors lanczos process show method solution may lead certain circumstances large additional errors corrected continuing iteration processour findings supported illustrated numerical examples b introduction consider iterative methods construction approximate solutions starting linear system n n symmetric matrix kdimensional krylov subspace r standard 3term lanczos process generate orthonormal basis v 1 process recast matrix formulation v j defined n j matrix columns v 1 tridiagonal matrix mathematical institute utrecht university budapestlaan 6 utrecht netherlands institute mathematics medical university lubeck wallstrae 40 23560 lubeck germany email sleijpenmathuunl vorstmathuunl modersitzkiinformatikmuluebeckde assumption mean loss generality since case x0 6 0 reduced simple shift paige 11 shown finite precision arithmetic lanczos process implemented computed v k1 k satisfy mild conditions k u machine precision 1 denotes maximum number nonzeros row obtain convenient expression popular krylov subspace methods symmetric linear systems derived formula 1 starting point minres gmres 2 symmlq matrix k interpreted restriction respect krylov subspace main idea behind krylov solution methods given system replaced smaller system k krylov subspace reduced system solved implicitly explicitly convenient way solution transformed v k solution original ndimensional space main differences methods due different way solution reduced system differences backtransformation approximate solution original system describe differences relevant detail coming sections course methods derived assuming exact arithmetic instance generating formulas based exact orthogonal basis krylov subspace reality however compute basis well quantities methods importance know generating formulas behave finite precision arithmetic errors underlying lanczos process analysed paige 11 12 proven greenbaum strakos 8 rounding errors lanczos process may delaying effect convergence iterative solvers prevent eventual convergence general usually type error analysis worst case scenario consequence error bounds pessimistic particular error bounds cannot well used explain differences methods observe practical situations paper propose different way analysing methods different way attempt derive sharper upper bounds try derive upper bounds relevant differences processes finite precision arithmetic help us understand methods converges finite precision give us insight answering practical questions minres less accurate symmlq question already posed original publication 14 answer 14 p625 largely speculative minres suspect illconditioned systems minimal residual approach see 14 p619 although hints given reasons inaccuracies minres minres also stated 14 p 625 accurate symmlq reason 2 gmres designed combination arnoldis method unsymmetric systems symmetric systems arnoldis method lanczos method lead exact arithmetic relation 1 minimal residual method suspect 3 p 43 explicit relation suggested minres working 2 argued reason sensitivity rounding errors solution depends 2 2 even stated squared condition number 2 implying 2 seems mistake symmlq slower instance minres gmres minres sometimes lead rather large residuals whereas error approximation significantly smaller see instance observations made 14 p626 important understanding differences methods help us making choice briefly characterize different methods investigation 1 minres 14 determine x minimal minimization leads small system k tridiagonal structure k exploited get short recurrence relation x k advantage three vectors krylov subspace saved fact minres works transformed basis vectors explained section 23 implementation minres used see appendix 2 gmres 16 method also minimizes k 2 r k residual kb gamma ax k k 2 gmres designed unsymmetric matrices orthogonalisation krylov basis done arnoldis method leads small upper hessenberg system solved however symmetric exact arithmetic arnoldi method equivalent lanczos method see also 7 p41 although gmres commonly presented arnoldi basis various implementations differ finite precision instance modified gramschmidt classical gramschmidt householder variants view lanczos one way obtain orthogonal basis therefore stick name gmres rather introduce new possibly confusing acronym due way solution gmres basis vectors v j stored also symmetric 3 symmlq 14 determine x error x euclidean length may come surprise minimized without knowing x accomplished restricting choice x k ak k r 0 conjugate gradient approximations exist computed little effort symmlq information symmlq implementation suggested 14 used terminate iterations either symmlq iterate conjugate gradient iterate depending one best implementation symmlq used see appendix note methods carried exactly basis vectors v j tridiagonal matrix j bounds perturbations solutions kth iteration step expressed bounds corresponding perturbations residual kth step relative norm initial residual since iteration methods construct search spaces residual vector information start kr 0 k 2 since make least errors order u kbk 2 computation residuals may expect perturbations order less u 2 akbk 2 iteratively computed solutions bounds expected show computed residuals errors larger error induced computation residuals notations quantities associated n dimensional spaces represented bold face like v j vectors matrices low dimensional subspaces denoted normal mode constants denoted greek symbols exception use u denote relative machine precision absolute value matrix refers elementwise absolute values 2 differences roundoff error behaviour minres gmres 21 basic formulas gmres minres exact arithmetic first describe generic formulas iterative methods minres gmres assume exact arithmetic derivation formulas without loss generality may assume x aim minimize kb gamma axk 2 krylov subspace since see minimizing must linear least squares solution overdetermined system gmres system solved givens rotations leads upper triangular reduction r k k k upper triangular bandwidth 3 q k orthonormal columns using 6 k solved since x r r parentheses included order indicate order computation original publication 16 gmres proposed unsymmetric combination arnoldis method orthonormal basis krylov subspace however symmetric arnoldis method equivalent lanczos method 8 describes gmres symmetric wellknown disadvantage approach store columns v k computation x k minres follows essentially approach gmres minimization residual exploits banded structure r k order get short recurrences x k order save memory storage indeed computations generating formula 8 reordered z k computation w k easy see last column w k obtained last two columns w kgamma1 v k makes possible update x x k short recurrence since z k follows kth givens rotation applied vector z interpretation leads minres see minres gmres use v k r k k q k z k computation x k course dictated compute items exactly way two methods reason compute differently therefore compare implementations gmres minres based exactly items floating point finite arithmetic study way minres gmres differ finite precision arithmetic given exactly set v k r k k q k z k computed finite precision two different methods hence differences finite precision gmres minres caused different order computation namely ffl gmres x ffl minres x z k course could tried get upper bounds errors made process would likely reveal differences two methods want study differences two methods concentrate two generating formulas 22 error analysis gmres order understand difference gmres minres study computational errors v k indicate actual computation floating point finite precision arithmetic fl result denote b according 5 p 89 floating point arithmetic computed solution b r implies b k apart second order terms u exact value based computed r k z k make also errors computation x k compute b x k error bounds matrix vector product 10 p76 obtain hence error deltax attributed differences minres gmres two components error leads contribution deltar k residual deltar k part r k attributed differences minres gmres ignoring ou 2 deltar note finite precision av 3 leads contribution ou 2 deltar k also case forthcoming situations replace av k v k1 k derivation upper bounds error contributions using bound 10 bound delta 2 get skipping higher order terms u k 3 used k jr k 21 th 42 see lemma 51 details factor 2 denotes condition number respect euclidean norm 3 note could bound kv k1 k 2 local orthogonality v j crude overestimate according 15 p 267 bottom may realistic replace factor denotes number times ritz value k converged eigenvalue solving linear system value usually modest 2 3 say finally note r shown 6 matrix k obtained finite precision arithmetic may interpreted exact lanczos matrix obtained matrix e eigenvalues replaced multiplets multiplet contains eigenvalues differ ou 1 4 original eigenvalue 4 e k denote orthogonal matrix generates k exact arithmetic e hence e e e 3 also used computed q k orthogonal matrices errors order u ie ou ouerrors lead ou 2 errors 13 4 order difference pessimistic factors proportional u 1 2 even u likely proved 7 sect442 oe min r e oe r e implies 2 r k ignoring errors proportional mild orders u finally results upper bound error residual due difference gmres minres note even rounding errors matrixvector multiplication perturbation deltax gamma1 b would norm order u corresponds error kadeltaxk 2 u 2 akbk 2 residual therefore stability gmres cannot essentially improved 23 error analysis minres differences finite precision minres gmres reflected z k first analyze floating point errors introduced computation columns k jth row w j w k satisfies w means floating point finite precision arithmetic obtain solution b w j perturbed system note perturbation term delta r j depends j gives b w combine relations c may replace c k 18 leads ou 2 errors finally make errors computation x k finite precision errors multiplication c errors made c k error term errors held responsible difference minres gmres added together lead deltax k related minres leads following contribution minres residual deltar use bound 18 delta w use quantities bounds similar gmres obtain 3 3 also used fact kv k k f k expression bounded finally results following upper bound error contribution residual due differences implementation minres gmres 3k see different implementation minres leads relative error residual proportional squared condition number whereas gmres implementation difference led relative error proportional condition number means plot residuals minres gmres may expect see differences specifically difference computed residuals two methods may expected order square condition number soon computed residual gmres gets u difference may visible 24 discussion fig 1 plotted residuals obtained gmres minres analysis suggests may difference order square condition number times machine precision relative kbk 2 course computed residuals reflect errors made processes errors together lead perturbations order minres gmres see much difference however see errors gmres lead something proportional condition number effect square condition number clearly visible error residual minres analysis implies one careful minres solving linear systems illconditioned matrix specially eigenvector components solution corresponding small eigenvalues important residual norm reduction kr k k 2 kbk 2 exact unknown minres residual computed efficiently product ae k j js 1 sines k givens rotations minres well gmres value ae k used measure reduction residual norm practical computations residual norm often computed explicitly convergence history minres aqdiagdq q givens log10r solid line log10rho dotted line 2convergence history minres aqdiagdq q givens log10r solid line log10rho dotted line 2convergence history gmres aqdiagdq q givens log10r solid line log10rho dotted line 2convergence history gmres aqdiagdq q givens log10r solid line log10rho dotted line figure 1 minres top gmres bottom solid line dotted line estimated residual norm reduction ae k pictures show results positive definite system left pictures nondefinite system right pictures examples 2 specific left g givens rotation 1 30plane angle right diagonal g givens rotation left example examples others come b vector coordinates equal 1 relative machine precision kth floating point approximate therefore interest know much computed ae k may differ exact residual norm reduction errors made computation ae k order u neglected since computation ae k b x k based inexact lanczos process 22 implies situation gmres much better difference ae k true residual reduction gmres bounded quantity right hand side 14 fact observed end x22 except moderate constant 3 accurate computation expected 25 diagonal matrices numerical analysts often carry experiments unpreconditioned iterative solvers diagonal matrices least exact arithmetic convergence behaviour depends distribution eigenvalues structure matrix plays role krylov solvers however behaviour methods diagonal systems may quite different finite precision show particular minres experiments diagonal matrices may give optimistic view behaviour method rotating matrix diagonal nondiagonal ie diagonal q orthogonal instead hardly influence errors gmres residuals results shown case minres experimental results cf fig 2 indicate errors minres residuals diagonal matrices order gmres understood follows neglect ou 2 terms according 15 error due inversion r k jth coordinate minresx k given k diagonal j jentry j error jth coordinate minres residual equal use 1 6 k k therefore view 16 including error term multiplication c cf 19 minres applied diagonal matrix upper bound errors gmres residuals 14 perturbation matrix delta r j depends row index j since general delta r j different coordinate j 23 cannot expected correct nondiagonal matrices fact orthogonal matrix errors order jth coordinate x k transferred q mth coordinate may damped small value j j precisely gamma maximum size offdiagonal elements couple small diagonal elements large ones error minres residual order gamma r recover bound 22 26 errors approximations exact arithmetic assuming finite precision also gives right order magnitude errors related differences minres gmres approximate solutions 11 20 bounded essentially upper bound 3 convergence history minres adiagd log10r solid line log10rho dotted line 2convergence history minres adiagd log10r solid line log10rho dotted line figure 2 minres solid line dotted line delta delta delta log 10 estimated residual norm reduction ae k pictures show results positive definite diagonal system left picture nondefinite diagonal system right picture except givens rotation matrices examples equal matrices examples fig 1 may come surprise since bound error contribution residual minres proportional based upon observations numerical experiments think explained follows error gmres approximation mainly large components direction small singular vectors components relatively reduced multiplication less effect norm residual hand errors minres approximation less magnitude spectrum singular values multiplication make error components associated larger singular values dominating residual support viewpoint numerical example results fig 3 obtained positive definite matrix two tiny eigenvalues b took random perturbation ay order 001 example mimics situation righthand side vector affected errors measurements solution x equation huge components direction two singular vectors smallest singular value directions x equal plus perturbation less one percent coordinates vector example form parabola makes effects easier visible convergence history gmres minres shown example comparable ones left pictures fig 1 higher condition number final stagnation residual norm present example takes place higher level 3 fig 3 shows solution x k computed 80th step gmres top pictures minres bottom pictures right pictures show component x k orthogonal two singular vectors smallest singular value left pictures show complete x k note kx k k curve projected gmres solution topright picture slightly perturbed parabola indeed irregularities due perturbation p computational errors gmres process visible picture errors 005005015025xgmres proj spanv3n sing vectors v increasing sing values 050515xminres proj spanv3n sing vectors v increasing sing values figure 3 pictures show solution x computed 80 steps gmres top pictures minres bottom pictures ith coordinate xk along vertical axis plotted along horizontal axis sin 001 right pictures show component xk orthogonal two singular vectors smallest singular value left pictures show complete xk mainly direction two small singular vectors contrast irregularities minres curve bottomright almost purely effect rounding errors minres process symmlq minimize norm x means k solution normal equations system simplified exploiting lanczos relations 1 stable way solving set normal equations based l e q decomposition equivalent transpose q k r k decomposition k see 6 constructed gmres minres leads basic generating formula symmlq obtained k assume x actual implementation symmlq 14 based update procedure v k1 q k three term recurrence relation kr note symmlq carried exactly computed values v k1 q k r k r 0 gmres minres fact good reason using different values algorithms therefore differences roundoff three methods must attributed additional rounding errors made evaluation righthand side 25 largest factor upper bound additional rounding errors construction symmlq approximation x k caused inversion l k multiplication assembly x k leads factor k k upper bound similar minres gmres order simplify much complicated analysis symmlq chosen study effect errors introduced inversion l k resulting error deltax k written g k represents exact solution b g k value obtained finite precision arithmetic likewise coordinates bg k kr 0 k 2 denoted coordinates written manipulation leads 25 follows hence error symmlq residual r k written first term treated gmres define combining 29 27 definition k conclude orthogonality v k v k1 computed residual reduction k b k k 2 usually used monitoring convergence stopping criterion actual computations symmlq residual vectors computed expression 30 bounded realistically byp 3 used k jl k hence 3 straightforward estimate 3 much larger first term 33 experiments indicate k b towards 0 even value u 2 explain expected cf 49 fig 4 illustrates upper bound 33 k b accuracy 33 follows 3 symmlq residual respect computed symmlq approximate r k symmlq residual exact symmlq approximate finite precision lanczos apparently assuming kr k increases symmlq rather accurate since method errors order u expected anyway convergence history symmlq aqdiagdq q givens log10r solid line log10rho dotted line 55convergence history symmlq aqdiagdq q givens log10r solid line log10rho dotted line figure 4 symmlq solid line dotted line delta delta delta log 10 estimated residual norm reduction k b tk k2 pictures show results positive definite system left picture nondefinite system right picture fig 1 systems condition number convergence clear yet whether convergence symmlq insensitive rounding errors would follow 33 k b k would approach 0 unlikely much larger k b k k 2 unlikely inexact process converges faster process exact arithmetic therefore observed k b k k 2 small order u 2 may concluded speed convergence affected seriously rounding errors experiments see b k approaches zero k increases practical applications assuming kt k k 2 k b k k 2 useful know computable value k b k k 2 informs us accuracy computed approximate possible loss speed convergence however interest know advance whether computed residual reduction decrease 0 moreover would like know whether course impossible prove symmlq converge symmetric problem one easily construct examples kr k k 2 order 1 k n analyse next subsection interesting quantities bounded terms minres residual result used order show relatively unimportant soon minres converged degree 31 relation symmlq minres residual norms section assume exact arithmetic particular lanczos process assumed exact residuals r mr k r k denote residuals minres symmlq respectively norm residual b gamma ax b x b best approximate x k k bounded terms norm residual r mr kr mr follows observation r mr k x mr k subspace best approximate x b selected furthermore kb gamma ax b k 2 unfortunately symmlq selects approximation x k different subspace namely ak k r 0 makes comparison less straight forward following lemma used bounding symmlq error terms minres error proof uses fact r mr connects k k1 spanned r mr k ak k r 0 lemma 31 z 2 k k1 kr mr 2 36 proof simplicity assume x construction x z space ak k r 0 hence implies construction x consequence pythogoras theorem 37 conclude 36 follows combining result 38 unfortunately combination 36 k obvious estimate jff k j kr mr 37 lead useful result interesting result follows upper bound jff k j obtained relation two consecutive minres residuals lanczos basis vector result formulated next theorem theorem 32 proof use relation r mr r cg k kth conjugate gradient residual scalars c represent givens transformation used kth step minres relation special case slightly general relation gmres fom residuals formulated 2 22 symmetric gmres equivalent minres fom equivalent cg since r cg r mr kr mr kr mr moreover since r mr k therefore e kr mr r mr kr mr kr mr r mr kr mr kr mr hence kr mr kr mr combination 42 36 k1 leads kr mr kr mr using minimal residual property kr mr obtain following recursive upper bound 43 kr mr simple induction argument shows fi k k1 definition fi k implies kr mr completes proof analysis additional errors symmlq also need slightly general result formulated next theorem theorem 33 let best approximation k ak k r 0 mr best approximation c ak k r 0 k 39 kr mr ik kr mr proof proof comes along lines proof theorem 32 replace quantities x x mr k mr k since quantities fulfill orthogonality relations 36 valid also quantities also case upper bound jff k j kr mr hence e j kr mr kr mr define b find kr mr implies 45 relations symmlq minres assumed exact arithmetic assumed exact lanczos process well exact solve systems l k however exclude influence lanczos process applying theorem 32 right away system lanczos matrix tm initial residual kr 0 k 2 e 1 setting k 2 22 kr mr j sine jth givens rotation qr decomposition estimated reduction norms minres residuals relation 44 combination 31 conclude note inequality 47 correct symmetric tridiagonal extension e tm 47 holds e tm instead tm shown 6 extension e tm eigenvalue ou 1 4 neighborhood eigenvalue therefore fairly good precision leads upper bound x311 show upper bound 49 contains square condition number however interesting situation ae k decreases towards 0 effect condition number squared annihilated eventually remark 34 except constants k estimates 48 49 respectively appear sharp see fig 5 although maximal values ratio kt fig 5 exhibit slowly growing behavior growth order k 3 proof 49 cf x311 upper bounds 48 used consecutive number steps view irregular convergence symmlq upper bound 48 sharp steps exploiting observation one show growth order k 2 even less likely versus minres log10 quotient estimates residual norms symmlq minres 22ektldeltae1rhok deltaepsl eps2958e13 log10 perturbations symmlq figure 5 results nondefinite matrix condition number right pictures fig 1 fig 4 left picture shows log 10 ratio k b tk k2 ae k estimated residual norm reduction symmlq one minres right picture models k b tk gamma tk k2 ae k shows log 10 e 311 symmlq recurrences section derive upper bound 49 suppose jth recurrence fl perturbed relatively small ffi recurrence relation exact resulting perturbed quantities labeled e multiple symmlq residual tm system proof inequality 48 theorem 32 could applied estimating k e situation j 6 1 theorem 33 used precise notation theorem 33 ae k c j cosine jth givens rotation therefore theorem 33 ae k specific situation estimate fi k last paragraph proof theorem 32 improved shown fi j 1 fi k kgammaj therefore k1 54 replaced kgammaj combination 51 54 gives using definition j recurrence relations fl j express gamma jj therefore 48 hence cf 50 55 gives gives recurrences linear effect number perturbations cumulation effects single perturbations recurrence relation perturbed 50 estimate 49 appears cumulation bounds 57 vector b k 49 represents result successive perturbations due finite precision arithmetic finally explain effect rounding errors solving l described result successively perturbed recurrence relations 50 first note efl k resulting perturbation resulting perturbation means perturbation second term jth recurrence relation also interpreted similar perturbation first term j gamma 1st recurrence relation consider perturbations introduced recurrence relation due finite precision arithmetic errors let b actually computed rewritten different 0 since perturbation second term jth recurrence relation interpreted similar perturbation first term j gamma 1st recurrence relation already perturbed factor 1 3 computed b fl j interpreted result perturbing leading term factor 1 4 discussion conclusions krylov subspace methods two main effects floating point finite precision arithmetic errors one effect generated basis krylov subspace deviates exact one may lead loss orthogonality lanczos basis vectors main effect iterative solution process delay convergence rather misconvergence fact happens try find approximated solution subspace optimal respect dimension could effect determination approximation perturbed rounding errors view serious point concern main theme study study restricted symmetric indefinite linear systems b review main results noted expect upper bounds relative errors approximations x contain least condition number simply general compute ax k exactly studied effects perturbations computed solution effect residual residual norm often information get process residual information often obtained cheap way update procedure uncommon updated residual may take values far beyond machine precision relative initial residual analysis shows limits reduction true residual errors approximated solution view fact may expect least linear factor 2 working euclidean norms gmres x22 symmlq x3 lead acceptable approximate solutions methods converge relative error approximate solution apart modest factors bounded u 2 symmlq attractive since minimizes norm error respect times krylov subspace may lead delay convergence respect gmres minres number iterations necessary gain reduction residual see theorem 32 illconditioned systems may considerable pointed 14 conjugate gradient iterates constructed little effort symmlq information exist indefinite systems conjugate gradient iterates welldefined least every iteration step used terminate iteration advantageous however conjugate gradient process minimization property positive definite case matrix indefinite guarantee iterates sufficiently close desired solution symmlq converges indefinite symmetric systems see minres may lead large perturbation errors minres upper bound contains factor means condition number large methods choice gmres symmlq note symmetric case gmres based threeterm recurrence relation means drawback necessity store lanczos vectors storage premium symmlq method choice given system wellconditioned interested accurate solu tions minres may attractive choice course one may combine discussed methods variation iterative refinement stopping iteration approximation x k compute residual possible higher precision continue solve solution z j system used correct x procedure could repeated eventually leads approximations x relative error residual order machine precision details see 20 however would use minres restart carry least number iterations reduction factor equal condition number order arrive something quality gmres may make method much less effective gmres situations u minres may even incapable getting sufficient reduction iterative refinement procedure converge common practice among numerical analysts test convergence behavior krylov subspace solvers symmetric systems wellchosen diagonal matrices gives often quite good impression expect nondiagonal matrices spectrum however shown x25 minres may lead optimistic picture since floating point error perturbations minres lead errors residual approximated solution factor smaller nondiagonal matrices r templates solution linear sys temsbuilding blocks iterative methods theoretical comparison arnoldi gmres algorithms survey preconditioned iterative methods polynomial based iteration methods symmetric linear systems matrix computations behavior slightly perturbed lanczos conjugategradient recurrences iterative methods solving linear systems methods conjugate gradients solving linear systems accuracy stability numerical algorithms analysis lanczos algorithm tridiagonalizing symmetric matrix accuracy effectiveness lanczos algorithm symmetric eigenproblem approximate solutions eigenvalue bounds krylov subspaces solutions sparse indefinite systems linear equations symmetric eigenvalue problem gmres generalized minimal residual algorithm solving nonsymmetric linear systems reliable updated residuals hybrid bicg methods relaxiationsmethoden bester strategie zur losung linearer gleichungssysteme efficient high accuracy solutions gmresm superlinear convergence behaviour gmres tr