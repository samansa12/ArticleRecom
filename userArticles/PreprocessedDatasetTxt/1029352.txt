learning monotone dnf product distributions show class monotone 2supologinisupterm dnf formulae pac learned polynomial time uniform distribution random examples exponential improvement best previous polynomialtime algorithms model could learn monotone ologsup2sup initerm dnf also show various classes small constantdepth circuits compute monotone functions pac learnable polynomial time uniform distribution results extend learning constantbounded product distribution b introduction disjunctive normal form formula dnf disjunction conjunctions boolean literals size dnf number conjunctions also known terms contains seminal 1984 paper 26 valiant introduced distributionfree model probably approximately correct pac learning random examples posed question whether polynomialsize dnf pac learnable polynomial time past fifteen years dnf learning problem widely viewed one important challenging open questions computational learning theory paper substantially improves best previous results wellstudied restricted version dnf learning problem 11 previous work lack progress valiants original question polynomialsize dnf learnable random examples drawn arbitrary distribution polynomial time led many researchers study restricted versions dnf learning problem detailed restrictions considered include ffl allowing learner make membership queries value target function points selected learner ffl requiring learner succeed restricted distributions examples uniform distribution rather distributions ffl requiring learner succeed restricted subclasses dnf formulae dnf bounded number terms satk dnf dnf truth assignment satisfies k terms khardon 19 gave polynomial time membership query algorithm learning polynomialsize sat1 dnf uniform distribution result later strengthened blum et al 3 satk dnf constant k bellare 5 gave polynomial time membership query algorithm learning olog n term dnf uniform distribution somewhat general result given blum rudich 6 mansour 23 gave n olog log n time membership query algorithm learns arbitrary polynomialsize dnf uniform distribution celebrated result jackson 15 gave polynomialtime membership query algorithm learning polynomialsize dnf constantbounded product distributions algorithm efficiency subsequently improved several authors 8 20 known polynomial time algorithm learning unrestricted class polynomial size dnf nontrivial learning model standard pac model without membership queries positive results known various subclasses dnf restricted distributions readk dnf one variable appears k times kearns li pitt valiant 17 18 showed readonce dnf pac learnable uniform distribution polynomial time hancock 12 extended result readk dnf constant k verbeurgt 27 gave algorithm learning arbitrary polynomialsize dnf uniform distribution time n olog n linial et al 22 gave algorithm learning ac 0 circuit constant depth polynomial size unbounded fanin andor gates uniform distribution n polylog n time monotone dnf dnf negated variables hancock mansour 13 gave polynomial time algorithm learning monotone readk dnf constantbounded product distributions verbeurgt 28 gave polynomial time uniform distribution algorithm learning polydisjoint onereadonce monotone dnf readonce factorable monotone dnf kucera et al 21 gave polynomialtime algorithm learns monotone kterm dnf uniform distribution using hypotheses monotone kterm dnf improved sakai maruoka 25 gave polynomialtime algorithm learning monotone olog nterm dnf uniform distribution using hypotheses monotone olog nterm dnf subsequently bshouty tamon 9 gave polynomialtime algorithm learning class includes monotone olog 2 nlog log n 3 term dnf constantbounded product distributions 12 results give simple polynomial time algorithm learning monotone 2 log n term dnf uniform distribution first polynomial time algorithm uses random examples drawn nontrivial distribution successfully learns monotone dnf formulae polylogarithmic number terms also show essentially algorithm learns various classes small constantdepth circuits compute monotone functions variables results extend learning constantbounded product distribution algorithm combines ideas linial et als influential paper 22 learning ac 0 functions using fourier transform bshouty tamons paper 9 learning monotone functions using fourier transform analyzing fourier transform ac 0 functions linial et al showed almost fourier power spectrum ac 0 function contained low fourier coefficients ie coefficients correspond small subsets variables learning algorithm estimates low fourier coefficient sampling constructs approximation f using estimated fourier coefficients c size bound low fourier coefficients since c coefficients corresponding subsets c variables algorithm requires roughly n c time steps linial et al showed ac 0 circuits c essentially polylog n result later sharpened dnf formulae mansour 23 algorithm extends approach following way let c ae ac 0 class boolean functions would like learn suppose c following properties 1 every f 2 c set f important variables almost power spectrum f contained fourier coefficients corresponding subsets 2 efficient algorithm identifies set f random examples algorithm give section 31 implicit 9 requires f monotone learn unknown function f class c first identifying set estimating low fourier coefficients correspond small subsets f using estimates construct approximation f see works note since f almost power spectrum f low fourier coefficients moreover property 1 implies almost power spectrum f fourier coefficients correspond subsets consequently must case almost power spectrum f low fourier coefficients correspond subsets f thus setting need estimate roughly c fourier coefficients correspond small subsets variables much efficient estimating c low fourier coefficients section 2 formally define learning model give necessary facts fourier analysis boolean cube section 3 give learning algorithm uniform distribution section 4 describe algorithm modified work constantbounded product distribution preliminaries write n denote set ng use capital letters subsets n write jaj denote number elements barred lowercase letters denote bitstrings ie paper boolean circuits composed andornot gates gates unbounded fanin negations occur inputs view boolean functions n variables real valued functions map f0 1g n fgamma1 1g boolean function changing value input bit 0 1 never causes value f change 1 gamma1 distribution f boolean function f0 1g n 9 13 say influence x f respect probability fx differs fy x ith bit flipped x drawn ease notation let f i0 denote function obtained f fixing x 0 let f i1 defined similarly thus di monotone f simplified di use following version chernoff bounds sums independent random variables 11 theorem 1 let x independent identically distributed random variables ex implies pr 21 learning model learning model distributionspecific version valiants probably approximately correct model 26 studied many researchers eg 3 5 8 9 10 13 15 19 21 22 23 27 28 let c class boolean functions f0 1g n let probability distribution f0 1g n let f 2 c unknown target function learning algorithm c takes input accuracy parameter confidence parameter execution algorithm access example oracle exfd queried generates random labeled example hx fxi x drawn according learning algorithm outputs hypothesis h boolean function f0 1g n error hypothesis defined say learns c every f 2 c probability least outputs hypothesis h errorh f ffl 22 discrete fourier transform let u denote uniform distribution f0 1g n set real valued functions f0 1g n may viewed 2 n dimensional vector space inner product defined gamman norm defined hf fi given subset n fourier basis function defined subset n defined 2 x well known 2 n basis functions ranges subsets n form orthonormal basis vector space real valued functions f0 1g n refer basis basis particular function f uniquely expressed values known fourier coefficients f respect basis since functions form orthonormal basis value hf also linearity fx ga x another easy consequence orthonormality parsevals identity f boolean function value exactly 1 finally g real valued function f0 1g n 9 22 pr u signz takes value 1 z 0 takes value 0 3 learning uniform distributions 31 following lemma implicit 9 gives efficient algorithm identifying important variables monotone boolean function refer algorithm findvariables monotone boolean function algorithm access exfu runs polyn 1ffl log 1ffi time steps ffl ffi 0 probability least outputs set f n ai2a ai2a proof kahn et al 16 section shown u ai2a prove lemma thus suffices show u f estimated within accuracy ffl4 high probability equation 1 section 2 done estimating e u f i1 e u f i0 two applications chernoff bounds finish proof first verify high probability large sample drawn exfu contains many labeled examples many x second verify collection many labeled examples x high probability yields accurate estimate e u f ib 32 learning algorithm learning algorithm call learnmonotone given use findvariables identify set f important variables labeled examples hx exfu every f every jaj c 6 f set ff 0 ffl output hypothesis signgx ff aa x algorithm thus estimates sampling constructs hypothesis using approximate fourier coefficients values c parameter settings findvariables specified 33 learning monotone 2 log n term dnf monotone kterm dnf proof algorithm learnmonotone learns f uses dnf called f 1 show findvariables identifies small set variables f uses another dnf called f 2 show f approximated approximating fourier coefficients correspond small subsets dnf obtained f removing every term contains log 32kn ffl variables since k terms satisfied random example probability less ffl32kn pr u fx argument first used verbeurgt 27 let r n set variables f 1 depends clear jrj k log 32kn since u f 1 equation 3 section 31 implies since f f 1 boolean functions ffl8n parsevals identity ar a6r ar a6r thus a6r consequently ai2a set parameters findvariables high probability ai2a ai2a inequalities 4 5 imply f r js f j k log 32kn furthermore since 6 f implies following lemma due mansour 23 lemma 32 lemma 3 let f dnf terms size ffl 0 jaj20d log2ffl one approach point use mansours lemma approximate f approximating fourier coefficients subsets f smaller 20d log2ffl ffl maximum size term f 1 however approach give good bound large instead consider another dnf smaller terms f 1 closely approximates f using stronger bound term size mansours lemma get better final result precisely let f 2 dnf obtained f removing every term contains least log 32k ffl variables let log 8 implies moreover pr u f u let ff gx defined learnmonotone bound x observe ff 7 bound note ff inequalities 8 9 respectively remains bound z linial et al 22 sum made less ffl4 taking sufficiently large high probability estimate ff differs true value fa straightforward chernoff bound argument shows taking suffices thus recalling bounds js f j c proved theorem 4 uniform distribution ffl ffi 0 algorithm learnmonotone learns kterm monotone dnf time polynomial n k log kn ffl log1ffi taking log n obtain following corollary corollary 5 constant ffl algorithm learnmonotone learns 2 log n term monotone dnf polyn log1ffi time uniform distribution note bshouty tamons algorithm 9 learning monotone olog n 2 log log n 3 term dnf also requires ffl constant order achieve polyn runtime 34 learning small constantdepth monotone circuits variables let c class depth size circuits compute monotone functions r variables analysis similar last section simpler since need introduce auxiliary functions f 1 f 2 shows algorithm learnmonotone used learn c instead mansours lemma use main lemma linial et al 22 bounds fourier spectrum constantdepth circuits lemma 6 let f boolean function computed circuit depth size let c integer x taking learnmonotone obtain theorem 7 fix 1 let c class depth size circuits compute monotone functions r n variables uniform distribution ffl ffi 0 algorithm learnmonotone learns class c time polynomial n r logmffl log1ffi one interesting corollary following corollary 8 fix 1 let c class depth size 2 olog n 1d1 circuits compute monotone functions 2 olog n 1d1 variables constant ffl algorithm learnmonotone learns class c polyn log1ffi time class c rather limited perspective boolean circuit complexity learning theory perspective fairly rich note c strictly includes class depth size 2 olog n 1d1 circuits 2 olog n 1d1 variables contain unbounded fanin andor gates follows results okolnishnikova 24 ajtai gurevich 1 see also section 36 show monotone functions computed ac 0 circuits computable ac 0 circuits negations 4 learning product distributions product distribution f0 1g n characterized parameters distribution assigns values independently variable 2 f0 1g n uniform distribution product distribution 12 standard deviation x product distribution oe product distribution constantbounded constant c 2 0 1 independent n throughout rest paper denotes product distribution given product distribution define new inner product vector space real valued functions f0 1g n corresponding norm refer norm dnorm let z given n let oe defined oe noted bahadur 4 furst et al 10 2 n functions oe form orthonormal basis vector space real valued functions f0 1g n respect dnorm ie hoe oe b 1 otherwise refer basis oe basis following fact useful fact 9 4 10 oe basis basis would obtained gramschmidt orthonormalization respect dnorm basis performed order increasing jaj orthonormality oe basis real function f0 1g n uniquely expressed faoe x fourier coefficient respect oe basis note write oe basis fourier coefficient basis fourier coefficient also orthonormality parsevals identity 1 boolean f finally real valued function g 10 lemma 10 pr furst et al 10 analyzed oe basis fourier spectrum ac 0 functions gave product distribution analogues linial et als results learning ac 0 circuits uniform distribu tion section 41 sharpen extend results 10 section 42 use sharpened results together techniques 10 obtain product distribution analogues algorithms section 3 41 oe basis fourier lemmas random restriction ae pd mapping fx f0 1 lambdag x mapped probability 0 probability boolean function fdae represents function fae pd x whose variables x mapped whose x instantiated 0 1 according ae pd following variant hastads well known switching lemma 14 product distribution parameters fi defined let f clause literals let ae pd random restriction probability least 1 1 function fdae expressed dnf formula term literals 2 terms dnf accept disjoint sets inputs proof sketch proof minor modification arguments given section 4 2 following corollary product distribution analogue 22 corollary 1 corollary 11 conditions lemma 10 probability least 1 gamma 4fipd g proof linial et al 22 show fdae satisfies properties 1 2 lemma 10 hence fdae space spanned fa jaj sg fact 9 nature gramschmidt orthonormalization space spanned corollary follows corollary 11 sharpened version similar lemma implicit 10 states conditions probability least 1 gamma 5fipd2 g armed sharper corollary 11 using arguments 10 straightforward prove lemma 12 boolean function f integer jajt ae pd boolean duality implies lemma 10 corollary 11 also hold f dnf term length taking ffl dnf version corollary 11 ffl lemma 12 obtain following analogue mansours lemma lemma oe basis f dnf terms size ffl 0 jaj16fid log4ffl using arguments 10 corollary 11 also used prove following version main lemma 10 lemma 14 let f boolean function computed circuit depth size let c integer x version lemma given 10 1d 2 instead 1d exponent c new tighter bound enables us give stronger guarantees learning algorithms performance product distributions could obtain using lemma 10 42 learning product distributions 421 identifying relevant variables following analogue lemma 2 product distributions lemma monotone boolean function algorithm access exfd runs polyn fi 1ffl log 1ffi time steps ffl ffi 0 probability least outputs set f n ai2a ai2a proof uses fact 9 lemma 41 4oe 2 di ai2a boolean function f product distribution algorithm uses sampling approximate thus approximate di f call algorithm findvariables2 422 learning algorithm would like modify learnmonotone uses oe basis rather basis however 10 algorithm know exact values cannot use exactly oe basis instead approximates sample value 0 uses resulting basis call oe 0 basis detail algorithm follows use findvariables2 identify set f important variables labeled examples hx ffl every f jaj c set ff 0 every jaj c 6 f set ff 0 ffl output hypothesis signgx ff 0 x call algorithm learnmonotone2 10 note setting ff 0 sigma1 jff 0 bring estimated value closer true value 423 learning monotone 2 log n term dnf part minor changes analysis section 33 required since term size greater satisfied random example probability less new term size bound f 1 log fi ffl js f ffl similarly obtain term size bound thetafi log k ffl use oe basis parseval identity inequality 10 place basis identity inequality 2 respectively lemma 13 provides required analogue mansours lemma product distributions using new term size bound f 2 obtain one new ingredient analysis learnmonotone2 comes bounding quantity addition sampling error would present even 0 exactly must also deal error due fact ff 0 estimate oe 0 basis coefficient rather oe basis coefficient analysis entirely similar section 52 10 shows taking suffices thus theorem 16 product distribution ffl ffi 0 algorithm learnmonotone2 learns kterm monotone dnf time polynomial n fik log kn since constantbounded distribution corollary 17 constant ffl constantbounded product distribution algorithm learns 2 log n term monotone dnf polyn log1ffi time 424 learning small constantdepth monotone circuits variables using lemma 14 analysis similar obtain theorem let c class depth size circuits compute monotone functions r n variables product distribution ffl ffi 0 algorithm learnmonotone2 learns class c time polynomial n r fi log log1ffi corollary 19 fix 1 let c class depth size 2 olog n 1d1 circuits compute monotone functions 2 olog n 1d1 variables constant ffl constant bounded product distribution algorithm learnmonotone2 learns class c polyn log1ffi time 5 open questions major open problem area clearly find polynomial time algorithm learns arbitrary polynomial size dnf arbitrary distribution however seems difficult problem even discovery polynomial time algorithm tnterm dnf uniform distribution would substantial step forward another angle positive results reported paper provide hope polynomial time algorithm polynomial size monotone dnf uniform distribution 6 acknowledgements thank les valiant advice enouragement r monotone versus positive switching lemma primer proc 26th ann symp theory computing representation joint distribution responses n dichotomous items proc fifth ann workshop comp learning theory fast learning kterm dnf formulas queries complexity finite functions proc 12th ann conf comp learning theory fourier spectrum monotone functions proc fourth ann workshop comp learning theory guided tour chernoff bounds complexity learning formulas decision trees restricted reads proc 4th ann workshop comp learning theory computational limitations small depth circuits efficient membershipquery algorithm learning dnf respect uniform distribution proc 29th ann symp found comp sci proc 19th ann acm symp theory computing learning boolean formulas using fourier transform learn disjoint dnf proc 40th ann symp found comp sci learning monotone dnf formulae uniform distributions constant depth circuits log log n influence negations complexity realization monotone boolean functions formulas bounded depth proc seventh conf comp learning theory theory learnable proc 3rd ann workshop comp learning theory proc 9th conf algorithmic learning theory tr theory learnable computational limitations smalldepth circuits learnability boolean formulae monotone versus positive guided tour chernoff bounds learning dnf uniform distribution quasipolynomial time learning monotone italickuitalic dnf formulas product distributions improved learning acsupscrpt0supscrpt functions complexity finite functions technique upper bounding spectral norm applications learning constant depth circuits fourier transform learnability using fourier transform learn disjoint dnf learning monotone logterm dnf formulas weakly learning dnf characterizing statistical query learning using fourier analysis learning boolean formulas italiconsupscrptlog log nsupscrptitalic learning algorithm dnf uniform distribution exact learning boolean functions via monotone theory fourier spectrum monotone functions efficient membershipquery algorithm learning dnf respect uniform distribution efficient paclearning dnf membership queries uniform distribution queries concept learning boosting hardcore sets