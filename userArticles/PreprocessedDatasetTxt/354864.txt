design evaluation switch cache architecture ccnuma multiprocessors abstractcache coherent nonuniform memory access ccnuma multiprocessors provide scalable design shared memory continue suffer large remote memory access latencies due comparatively slow memory technology large data transfer latencies interconnection network paper propose novel hardware caching technique called switch cache improve remote memory access performance ccnuma multiprocessors main idea implement small fast caches crossbar switches interconnect medium capture store shared data flow memory module requesting processor stored data acts cache subsequent requests thus reducing need remote memory accesses tremendously implementation cache crossbar switch needs efficient robust yet flexible changes caching protocol design implementation details cache embedded switch architecture caesar using wormhole routing virtual channels presented explore design space switch caches modeling caesar detailed execution driven simulator analyze performance benefits results show caesar switch cache capable improving performance ccnuma multiprocessors 45 percent reduction remote memory accesses applications serving remote read requests various stages interconnect observe improvements execution time high 20 percent applications conclude switch caches provide costeffective solution designing high performance ccnuma multiprocessors b introduction alleviate problem high memory access latencies shared memory multiprocessors employ processors small fast onchip caches additionally larger offchip caches symmetric multiprocessor smp systems usually built using shared global bus however contention bus memory heavily constrains number processors connected bus build high performance systems scalable several current systems 1 10 12 13 employ cache coherent nonuniform memory access ccnuma architecture system shared memory distributed among nodes system provide closer local memory several remote memories local memory access latencies tolerated remote memory accesses generated execution bring performance applications drastically reduce impact remote memory access latencies researchers proposed improved caching strategies 14 18 27 within cluster multiprocessor caching techniques primarily based data sharing among multiple processors within cluster nayfeh et al 18 explore use shared l2 caches benefit shared working set processors within cluster another alternative use network caches remote data caches 14 27 network caches reduce remote access penalty serving capacity misses l2 caches providing additional layer shared cache processors within cluster hp exemplar 1 implements network cache configurable partition local memory sequents numaq 13 dedicates 32mb dram memory network cache dash multiprocessor 12 provision network cache called remote access cache recent proposal moga et al14 explores use small sram instead dram network caches integrated page cache use 32kb sram chips reduces access latency network caches tremendously goal reduce remote memory access latencies implementing global shared cache abstraction central processors ccnuma system observe network caches provide abstraction limited processors within cluster explore implementation issues performance benefits multilevel caching scheme incorporated current ccnuma systems embedding small fast sram cache within switch interconnection network called switch cache capture shared data flows interconnect provide future accesses processors reuse data scheme considered multilevel caching scheme without inclusion property studies application behavior indicate enough spatial temporal locality requests processors benefit small switch caches recently study mohapatra et al 15 used synthetic workloads showed increasing buffer size crossbar switch beyond certain value much impact network performance applicationbased study 4 confirms observation holds true ccnuma environment running several scientific applications thus think large amount buffers current switches spider 7 overkill better utilization buffers accomplished organizing switch cache several issues considered designing caching technique include cache design issues technology organization cache coherence issues switch design issues arbitration message flow control issues appropriate routing strategy message layout etc first issue design analyze cache organization large enough hold reusable data yet fast enough operate time request passes switch second issue involves modifying directorybased cache protocol handle additional caching layer switching elements cache blocks system properly invalidated write third issue design buffers arbitration switch guarantee certain cache actions within switch delay period example read request travels switch cache must incur additional delays even case switch cache hit request must pass home node update directory generate memory read operation final issue deals message header design enable request encoding network routing etc contribution paper detailed design performance evaluation switch cache interconnect employing caesar cache embedded switch architecture caesar switch cache made small sram cache operating speed wormhole routed crossbar switch virtual channels switch design optimized maintain crossbar bandwidth throughput time providing sufficient switch cache throughput improved remote access performance performance evaluation switch cache interconnect conducted using six scientific applications present several sensitivity studies cover entire design space identify important parameters experiments show switch caches offer great potential use future ccnuma interconnects applications rest paper organized follows section 2 provides background remote access characteristics several applications ccnuma environment builds motivation behind proposed global caching approach switch cache framework caching protocol presented section 3 section 4 covers design implementation crossbar switch cache architecture called caesar performance benefits switch cache framework evaluated analyzed great detail section 5 sensitivity studies various design parameters also presented section 5 finally section 6 summarizes concludes paper memory interconnection network memory network interfacerouter network interfacerouter remote memory node xs figure 1 ccnuma system memory hierarchy application characteristics motivation several current distributed shared memory multiprocessors adopted ccnuma architecture since provides transparent access data figure 1 shows disparities proximity access time ccnuma memory hierarchy systems load store issued processor x served cycles upon l1 l2 cache hits less hundred cycles local memory access incurs hundreds cycles due remote memory access latency stores memory write transactions hidden use weak consistency models stall time due loads read transactions memory severely degrade application performance 21 global cache benefits trace analysis reduce impact remote read transactions would like exploit locality sharing patterns processors figure 2 plots read sharing pattern six applications processors using cache line size 32 bytes six applications used evaluation floydwarshalls algorithm fwa gaussian elimination gauss gramschmidt gs matrix multiplication mm successive relaxation sor fast fourier transform fft xaxis represents number sharing processors x yaxis denotes number accesses blocks shared x number processors figure observe four six applications fwa gauss gs mm multiple processors read block two consecutive writes block shared reads form major portion 35 80 applications read misses take advantage readsharing patterns across processors fwa b sor c gauss gs e fft f mm figure 2 application read sharing characteristics introduce concept ideal global cache centrally accessible processors first request served memory data sent back reply message stored global cache since cache accessible processors subsequent requests data item satisfied global cache low latencies two questions arise ffl time lag two accesses different processors block define temporal read sharing locality processors somewhat equivalent temporal locality uniprocessor system question raised particularly relates size organization global cache general would translate longer time lag bigger size required global cache ffl given block held central location many requests satisfied cached block call term attainable read sharing estimate performance improvement employing global cache metric give indication required size global cache answer questions instrumented simulator generate execution trace information regarding cache miss fed traces trace analysis tool sharing identifier locality analyzer order evaluate potential global qwhuduulydo7lphsffohv fwa b gs c gauss sor e fft f mm figure 3 temporal locality shared accesses cache sila generates two different sets data temporal read shared locality figure 3 attainable sharing figure 4 data sets interpreted follows temporal read sharing locality figure 3 depicts temporal read sharing locality function different block sizes point fxyg data set indicates probability two read transactions different processors block occur within time distance x lower ie average interarrival time two consecutive read requests block seen figure applications inherent temporal reuse cached block processors interarrival time two consecutive shared read transactions different processors block found less 500 processor cycles pcycles 6080 shared read transactions applications ideally indicates potential atleast one extra request satisfied per globally cached block attainable read sharing figure 4 explores probability multiple requests satisfied global caching technique termed attainable sharing degree point fxyg data set indicates block held x cycles global cache average number subsequent requests per block served figure depicts attainable read sharing degree application based residence time block global cache residence time cache block defined amount time block held cache replaced invalidated invalidations cannot avoided note residence time directly relates several cache design issues cache size block size associativity qwhuduulydo7lphsffohv fwa b gs c gauss sor e fft f mm figure 4 attainable sharing degree figure 2 observed fwa gs gauss high read sharing degrees close number processors system case 16 processors however found attainable sharing degree varies according temporal locality application gauss attain sharing degree 10 global cache residence time 2000 processor cycles gs requires residence time 5000 fwa requires time 7000 mm application sharing degree approximately four five whereas attainable sharing degree much lower sor fft much interest since low percentage 12 shared block accesses see figure 2 22 interconnect global cache section 21 identified need global caching improve remote access performance ccnuma systems section explore possible use interconnect medium central caching location ffl makes interconnect medium suitable candidate central caching interconnect topology beneficial incorporating central caching scheme communication shared memory system occurs transactions consist requests replies requests replies traverse one node another network node node c interconnect overlapping path caching potential figure 5 caching potential interconnect medium interconnection network becomes global yet distributed medium entire system keep track transactions occur nodes global sharing framework efficiently employ network elements coherent caching data potential network caching strategy illustrated figure 5 path two read transactions memory module emerge different processors overlap point network common elements network act small caches replies memory later request recently accessed block x potentially find block cached common routing element illustrated shaded circle benefit scheme twofold processor point view read request gets serviced latency much lower remote memory access latency secondly memory relieved servicing requests hit global interconnect cache thus improving access times requests sent memory module example observe incorporating schemes interconnect requires significant amount path overlap processor memory requests replies follow path requests order provide intersection routing flow requests replies depends entirely upon topology interconnect ideal topology system global bus however global bus scalable medium bus contention severely affects performance number processors increases beyond certain threshold con sequently multiple bus hierarchies fattree structures considered effective solutions scalability problem tree structure provides next best alternative hierarchical caching schemes 3 switch cache framework section present new hardware realization ideal global caching solution improving remote access performance shared memory multiprocessors 31 switch cache interconnect network topology plays important role determining paths source destination system treebased networks like fat tree 11 heirarchical bus network 23 3 multistage interconnection network min 17 provide hierarchical topologies suitable global caching addition min highly scalable provides bisection bandwidth scales linearly number nodes system features min make attractive scalable high performance interconnects commercial systems existing systems butterfly 2 cm5 11 ibm sp2 21 employ bidirectional min illinois cedar multiprocessor 22 employs two separate unidirectional mins one requests one replies paper switch cache interconnect bidirectional min take advantage inherent tree structure note however logical trees also embedded popular direct networks like mesh hypercube 10 12 baseline topology 16node bidirectional min bmin shown figure 6a general nnode system using bmin comprises nk switching elements 2k theta 2k crossbar log k n stages connected bidirectional links chose wormhole routing virtual channels switching technique prevalent current systems sgi origin10 shared memory system communication nodes accomplished via readwrite transactions coherence requestsacknowledgments readwrite requests coherence replies processor memory use forward links traverse switches similarly readwrite replies data coherence requests memory processor traverse backward path shown bold figure 6a separating paths enables separate resources reduces possibility deadlocks network time routing switches provides identical paths requests replies processormemory pair essential develop switch cache hierarchy bmin tree structure enables hierarchy shown figure 6b basic idea caching strategy utilize tree structure bmin path overlap requests replies coherence messages provide coherent shared data intercon nect incorporating small fast caches switching elements bmin serve sharing processors switching elements use term switch cache differentiate caches processor caches example bmin employing switch caches serve invalidation response ack writeback request bidirectional min structure routing processorcache interface memory interface b multiple tree structures bmin processorcache interface memory interface c switch caching bmin serviced memory switch cache serviced switch caches different levels figure switch cache interconnect multiple processors shown figure 6c initial shared read request processor p block served remote memory j reply data flows interconnection network block captured saved switch cache switching element along path subsequent requests block sharing processors p j p k take advantage data blocks switch cache different stages thus incurring reduced read latencies 32 caching protocol incorporation processor caches multiprocessor introduces wellknown cache coherence problem many hardware cachecoherent systems employ fullmap directory scheme 6 scheme node maintains bit vector keep track sharers block local shared memory space every write ownership request sent home node invalidations sent sharers block ownership granted corresponding acknowledgments received processing node cache employs threestate msi protocol keep track state cache line incorporating switch caches comes requirement caches remain coherent data access remain consistent system consistency model cannot reach 2 states write initiated another processor expected state shared dirty uncached readreply readreply invtype invalid shared readreply readreply switch cache state diagram add sharer directory vector send invalidation requestor increment ackcounter wait additional ack protocol b change directory protocol switch hit read figure 7 switch cache protocol execution adopt sequential consistency model paper basic caching scheme represented state diagram figure 7 explained follows switch cache stores blocks shared state system block read processor cache dirty state cached switch effectively switch cache needs employ 2state protocol state block either shared valid transitions blocks one state another shown figure 7a illustrate difference block invalidations inv type block replacements readreply figure shows valid state conceptually separated two states invalid present respectively read requests read request message enters interconnect checks switch caches along path event switch cache hit switch cache responsible providing data sending reply requestor original message marked switch hit continues destination memory ignoring subsequent switch caches along path sole purpose informing home node another sharer read block message called marked read request request necessary maintain fullmap directory protocol note memory access needed marked requests reply generated destination memory marked read request find block two states shared transient see figure 7b directory state shared request updates sharing vector directory however possible write initiated cache line different processor invalidation write yet propagated switch cache present due false sharing application allows data race conditions exist occurs marked read request observes transient state physical address swc hit dest bits flits req age 4bits 2bits1 addr contd src 4bits 6bits 6bits 8bits figure 8 message header format directory event directory sends invalidation processor requested read waits additional acknowledgment committing write read replies read replies originate two different sources namely memory node owners cache dirty state read replies enter interconnect following backward path read reply originating memory node check switch cache along path line present switch cache data carried read reply written cache state cache line marked shared messages originating owners cache replies whose home node requester node identical allowed check switch caches along way replies find line absent cache enter data cache home node requester reply ignore switch cache reply allowed enter switch cache subsequent modification block owner visible interconnection network thus path owner requester coherent overlap write request invalidation request responsible coherence explained next summary read replies nonidentical home node requester node enter data switch cache already present writes writebacks coherence messages requests flow switches check switch cache invalidate cache line present cache switch cache coherence maintained somewhat transparently message format messages formatted network interface requests replies coherence messages prepared sent network wormholerouted network messages made flow control digits flits flit 8 bytes cavallino 5 message header contains routing information data flits follow path created header format message header shown figure 8 implement caching technique require header consists 3 additional bits information two bits reqtype encoded denote switch cache access type follows cache line switch cache present mark read header generate reply message 4w 4w 4w forward link inputs backward link inputs forward link outputs link outputs backward input block input block input block input block arbiter crossbar 4w routing tables 4w 4w 4w 4w figure 9 conventional crossbar switch line switch cache invalidate cache line present cache switch cache processing required note description caching protocol read requests encoded sc read requests read replies whose home node requestor id different encoded sc write coherence messages write ownership requests writeback requests encoded sc inv requests requests encoded sc ignore requests additional bit required mark sc read requests earlier switch cache hit request called marked read request used avoid multiple caches servicing request discussed marked request updates directory avoids memory access 4 switch cache design implementation crossbar switches provide excellent building block scalable high performance interconnects crossbar switches mainly differ two design issues switching technique buffer management use wormhole routing switching technique input buffering virtual channels since prevalent current commercial crossbar switches 5 7 begin presenting organization design alternatives 4 theta 4 crossbar switch cache later subsection extensions required incorporating switch cache module larger 8 theta 8 crossbar switch presented 4w 4w 4w forward link inputs backward link inputs forward link outputs link outputs backward input block input block input block input block routing tables crossbar 4w arbiter input block switch cache module 4w 4w 4w 4w 4w 4w 4w forward link inputs backward link inputs forward link outputs link outputs backward input block input block input block input block crossbar 4w routing tables 4w 4w 4w 4w switch cache module arbiter input block arbitrationindependent b arbitrationdependent figure 10 crossbar switch cache organization 41 switch cache organization base bidirectional crossbar switch four inputs four outputs shown figure 9 input link crossbar switch two virtual channels thus providing 8 possible input candidates arbitration arbitration process age technique similar employed sgi spider switch 7 arbitration cycle maximum 4 highest age flits selected 8 possible arbitration candidates flit size chosen 8 bytes 4w links similar intel cavallino 5 thus takes four link cycles transmit flit output one switch input next buffering crossbar switch provided input block link input block organized fixed size fifo buffer virtual channel stores flits belonging single message time virtual channels also partitioned based destination node avoids outoforder arrival messages originating source destination also provide bypass path incoming flits directly transmitted output input buffer empty organizing switch cache particularly interested maximizing performance serving flits within cycles required operation base crossbar switch thus organization depends highly delay experienced link transmission crossbar switch processing present two different alternatives organizing switch cache module within conventional crossbar switches arbitrationindependent organization based crossbar switch operation similar sgi spider 7 arbitrationdependent organization based crossbar switch operation similar intel cavallino 5 arbitrationindependent organization switch cache organization based link switch processing delays similar experienced sgi spider internal switch core runs 100 mhz link transmission operates 400mhz switch takes four 100mhz clocks move flits input link transmitter output link hand transmit 8byte flit single 100 mhz clock four 400 mhz clocks figure 10a shows arbitrationindependent organization switch cache organization arbitration independent switch cache performs critical operations parallel arbitration operation beginning arbitration cycle maximum four input flits stored input link registers transmitted switch cache module order maintain flow control required switch cache processing performed parallel arbitration transmission delay switch 4 cycles arbitrationdependent organization switch cache organization based link switch processing delays similar experienced intel cavallino 5 internal switch core link transmission operate 200mhz takes 4 cycles crossbar switch pass flit input output transmitter 4 cycles link transmit one flit one switch another figure 10b shows arbitrationdependent organization switch cache organization arbitration dependent performs critical operations end arbitration cycle parallel output link transmission end every arbitration cycle maximum four flits passed crossbar input buffers output link transmitters also transmitted switch cache since output transmission takes four 200mhz cycles switch cache needs process maximum four flits within 4 cycles organization advantagesdisadvantages arbitrationindependent organization cache operates switch core frequency remains independent link speed hand organization lacks arbitration information could useful completing operations orderly manner issue affect 4 theta 4 switches drawback evident study design larger crossbar switches arbitrationdependent organization benefits completion arbitration phase use resultant information timely completion switch cache processing however organization cache required run link transmission frequency order complete critical switch cache operations intel cavallino possible run switch core switch cache link transmission speed finally note cases reply messages switch cache module stored another input block shown figure 10 two virtual channels per input block crossbar switch size expands 4 also cases maximum number switch cache inputs 4 requests processing time limited 4 switch cache cycles cache size bytes cache access time direct cache size bytes cache access time direct 32byte cache lines b 64byte cache lines figure 11 cache access time fo4 42 cache design area access time issues access time area occupied sram cache depends several factors asso ciativity cache size number wordlines number bitlines25 16 section study impact cache size associativity access time area constraints aim find appropriate design parameters crossbar switch cache cache access time cacti model 25 quantifies relationship cache size associativity cache access time ran cacti model measure switch cache access time different cache sizes set associativity values order use measurements technology independent manner present results using delay measurement technique known fanoutoffour one fo4 delay signal passing inverter driving load capacitance four times larger input known 8 kbyte data cache cycle time 25 fo4 9 figure 11 shows results obtained fo4 units figure 11 xaxis denotes size cache curve represents access time particular set associativity find direct mapped caches lowest access time since direct indexing method used locate line however direct mapped cache may exhibit poor hit ratios due mapping conflicts switch cache setassociative caches provide improved cache hit ratios longer cycle time due higher data array decode delay current processors employ multiported set associative l1 caches operating within single processor cycle chosen 2way set associative design base crossbar switch cache maintain balance access time hit ratio however also study effect varied set associativity switch cache performance cache output width also important issue primarily affects data readwrite delay studied wilton et al 25 increase data array width increases number sense amplifiers required organization cache also make significant difference terms 32byte cache lines b 64byte cache lines figure 12 cache relative area chip area narrower caches provide data multiple cycles thus increasing cache access time average read request example cache 32byte blocks width 64 bits decreases cache throughput one read four cycles within range 64 256 bits data output width know 64 bits provide worst possible performance scenario designed switch cache using 64bit output width show overall performance affected parameter cache relative area order determine impact cache size set associativity area occupied onchip cache use area model proposed mulder et al16 area model incorporates overhead area drivers sense amplifiers tags control logic compare data buffers different organizations technology independent manner using register bit equivalent rbe one rbe equals area bit storage cell used area model obtained measurements different cache sizes associativities figure 12 shows results obtained relative area xaxis denotes size cache curve represents different set associativity values small cache sizes ranging 512 bytes 4kb find amount area occupied direct mapped cache much lower 8way set associative cache figure find increase associativity 1 2 lower impact cache area increase 2 4 observation think 2way set associative cache design would suitable organization terms cache area cache access time measured earlier 43 cache embedded switch architecture caesar section present hardware design crossbar switch cache called caesar cache embedded switch architecture block diagram caesar implementation shown figure 13 4 theta 4 switch maximum 4 flits latched switch cache registers 4w 4w f 4w f 4w 4w r u switch cache module cache access control switch cache module arbiter crossbar select source linkbuffer information process incoming flits flits transmitted crossbar header vector input block blocking info update read header cache data unit cache tag unit snoop registers ri buffer wr buffers figure 13 implementation caesar arbitration cycle operation caesar switch cache divided 1 process incoming flits 2 switch cache access 3 switch cache reply generation 4 switch cache feedback section cover design implementation issues operations detail process incoming flits incoming flits stored registers belong different request types request type flit identified based 2 bits r 1 r 0 stored header header flits request contain relevant information including memory address required processing reads invalidations subsequent flits belonging messages carry additional information essential switch cache processing write requests switch cache require header flit address information data flits written cache line finally ignore requests need discarded since require switch cache processing additional type request require processing marked read request read request swc hit bit set header inform switch caches served previous switch cache classified types flits entering cache switch cache processing broken two basic operations first operation performed flit processing unit propagating appropriate flits switch cache mentioned earlier flits need enter cache read headers invalidation headers write flits thus processing unit masks ignore flits marked read flits data flits invalidation read requests done reading r 1 r 0 bits header vector swc hit bit utilize header information subsequent data flits message switch cache maintains register stores bits flits requiring cache processing passed request queue one every cycle request queue organized two buffers ri buffer set wr buffers shown figure 13 ri buffer holds header flits read invalidation requests wr buffers store write flits organized num vc theta k2 different buffers multiple buffers required associate data flits corresponding headers data flits write request accumulated buffer request ready initiate cache line fill operation second operation complete processing incoming flits follows unmarked read header flits need snoop cache gather hitmiss information information needed within 4 cycles switch delay link transmission able mark header setting last bit swc hit perform snooping operation cache tag read headers also copied snoop registers shown figure 13 require two snoop registers maximum two read requests enter cache single arbitration cycle switch cache access figure 14 illustrates design cache subsystem cache module shown figure 2way set associative sram cache cache operates frequency internal switch core set associative cache organized using two subarrays tag data cache output width 64 bits thus requiring 4 cycles data transfer reading line tag array dual ported allow two independent requests access tag time describe switch cache access operations associated access delays requests switch cache broken two types requests snoop requests regular requests snoop requests read requests required snoop cache determine hit miss outgoing flit transmitted next switch arbitration independent switch cache organization figure 10a takes minimum four cycles moving flit switch input output thus need snoop operation within last cycle mark message link transmission similarly arbitration dependent organization figure 10b takes 4 cycles transmit 64bit header 16bit output link header loaded 64bit 4w output register message format figure 8 phit containing swc hit bit marked transmitted fourth cycle thus required cache access completed within maximum 3 cycles figure 13 copying first read snoop registers performed flit processing unit completed one cycle dedicating one ports tag array primarily snoop requests snoop cache takes additional cycle complete since maximum 2 read headers arrive switch cache single arbitration cycle complete snoop operation cache within 3 cycles note figure 14 snoop operation done parallel pending requests ri buffer qsize gen reply data buffer header bit directory 0 queue status send replyreply unit request address address write data data inout data inout hitmiss blocking info input block figure 14 design caesar cache module wr buffers snoop operation completes hitmiss information propagated output transmitter update read header output register snoop operation results switch cache miss request also dequeued ri buffer regular requests regular request request chosen either ri buffer wr buffers request processed maximum 4 cycles absence contention requests ri buffer handled fcfs basis avoids dependency violation read invalidation requests buffer however candidate cache operation ri buffer well one wr buffers absence address dependencies requests buffers progress order switch cache dependency exists two requests need make sure cache state correctness preserved identify two types dependencies request ri buffer request wr buffer ffl invalidation ri buffer cache line x write wr buffer cache line x preserve consistency simplest method discard write cache line thus avoiding incorrectness cache state thus invalidations enter switch cache write addresses pending write requests wr buffer compared invalidated parallel cache line invalidation ffl read ri buffer cache line x write wr buffer cache line map cache entry write occurs first cache line x replaced event read request cannot served since occurrence rare remedy send read request back home node destined satisfied typical remote memory read request switch cache reply generation invalidations writes cache generate replies switch cache read requests need serviced reading cache line cache sending reply message requesting processor read header contains information required send reply requester read header cache line data directly fed reply unit shown figure 14 reply unit gathers header beginning cache access modifies sourcedestination requestreply information header parallel cache access entire cache line read reply packet generated sent switch cache output block reply message switch cache acts message entering switch form flits gets arbitrated appropriate output link progresses using backward path requesting processor switch cache feedback another design issue caesar switch selection queue sizes section identify methods preserve crossbar switch throughput blocking requests violate correctness shown figure 13 14 finite size queues exist input switch cache ri buffer wr buffer reply unit virtual channel queues switch cache output block limited size buffer gets full two options processing readwrite requests first option block requests space available buffer second option probably wiser one allow request continue path memory performance switch cache dependent chosen scheme buffer sizes extremely limited finally invalidate messages processed switch cache since required maintain coherence messages need blocked ri buffer gets full modification required arbiter make possible quite simple implement blocking flits input switch cache needs inform arbiter status queues end cycle switch cache informs crossbar status queues form free space available queue modification arbiter perform required blocking minor depending free space queue appropriate requests based r 1 r 0 blocked others traverse switch normal fashion 44 design 8 theta 8 crossbar switch cache previous sections presented design implementation 4 theta 4 cache embedded crossbar switch current commercial switches sgi spider intel cavallino six hitmiss way bankbank select addrdata pair 1 way way way bankbankbankqsize gen reply reply unit queue status reply addrdata pair address 1 address figure 15 design caesar bidirectional inputs ibm sp2 switch 8 inputs 8 outputs section present extensions earlier design incorporate switch cache module 8 theta 8 switch maintain base parameters switch core frequency core delay link speed link width flit size main issue expanding switch number inputs switch cache module doubles 4 8 thus arbitration cycle maximum four read requests come switch require snoop operation switch cache within 4 cycles switch core delay link transmission depending switch cache organization shown figure 10 shown figure 13 takes one cycle move flits snoop registers thus require complete snoop operation 4 requests within 2 cycles mark header flit last cycle depending snoop result order perform four snoops two cycles propose use multiported caesar cache module multiporting implemented either duplicating caches interleaving two independent banks since duplicating cache consumes tremendous amount onchip area propose use 2way interleaved caesar caesar shown figure 15 interleaving splits cache organization two independent banks current processors mips r10000 26 use even odd addressed banked caches however problem remains four even addressed four odd addressed requests still require four cycles snooping due bank conflicts propose interleave banks based destination memory using outgoing link ids 8 theta 8 switch four outgoing links transmit flits switch towards destination memory viceversa cache bank serve requests flowing two links thus partitioning requests based destination memory arbitrationindependent organization figure 10a possible four incoming read requests directed memory module result bank conflicts however arbitration dependent organization figure 10b conflict gets resolved arbitration phase guarantees arbitrated flits flow different links 8 theta 8 switches would advantageous use arbitration dependent organization thus assuring maximum 2 requests per bank arbitration cycle result snoop operation four requests completed required two cycles finally note bits routing tag needed identify bank cache interleaved organization changes aspect ratio cache 25 may affect cycle time cache wilson et al24 showed increase cycle time measured using fanoutoffour banked interleaved caches single ported caches minimal 2way interleaved implementation also doubles cache throughput since two requests simultaneously access switch cache reply unit needs provide twice buffer space storing data cache similarly header flit two read requests also need stored shown figure 15 buffers connected outputs different banks gather cache line data performance evaluation section present detailed performance evaluation switch cache multiprocessor based executiondriven simulation 51 simulation methodology evaluate performance impact switch caches application performance ccnuma multiprocessors use modified version rice simulator ilp multiprocessors rsim 19 rsim execution driven simulator shared memory multiprocessors accurate models current processors exploit instructionlevel parallelism section present various system configurations corresponding modifications rsim conducting simulation runs base system configuration consists 16 nodes node consists 200mhz processor multiprocessor system processors processor memory speed 200mhz access time 40 issue 4way interleaving 4 cache network l1 cache 16kb switch size 8x8 line size 32bytes core delay 4 set size 2 core freq 200mhz access time 1 link width 16 bits l2 cache 128kb xfer freq 200mhz line 32bytes flit length 8bytes set size 4 virtual chs 2 access time 8 buf length 4 flits switchnetwork caches switch cache 128bytes8kb network cache 4kb application workload fwa 128x128 ge 128x128 gs 96x128 mm 128x128 table 1 simulation parameters capable issuing 4 instructions per cycle 16kb l1 cache 128kb l2 cache portion local memory directory storage local bus interconnecting components l1 cache 2way set associative access time single cycle l2 cache 4way set associative access time 8 cycles raw memory access time 40 cycles takes 50 cycles submit request memory subsystem read data memory bus system employs fullmap threestate directory protocol 6 msi cache protocol maintain cache coherence system uses release consistency model modified rsim employ wormhole routed bidirectional min using 8 theta 8 switches organized 2 stages shown earlier figure 6 virtual channels also added switching elements simulate behavior commercial switches like cavallino spider input link switch provided 2 virtual channel buffers capable storing maximum 4 flits single message crossbar switch operation similar description section 41 detailed list simulation parameters also shown table 1 figure percentage reduction memory reads evaluate switch caches modified simulator incorporate switch caches switching element switch cache system improves base system following respects switching element bidirectional min employs variable size cache models functionality caesar switch cache presented section 4 several parameters cache size set associativity varied evaluating design space switch cache selected numerical applications investigate potential performance benefits switch cache interconnect applications floydwarshalls allpairshortestpath algorithm gaussian elimination ge qr factorization using gramschmidt algorithm gs multiplication 2d matrices mm successive overrelaxation grid sor sixstep 1d fast fourier transform fft splash 20 input data sizes shown table 1 sharing characteristics discussed section 21 52 base simulation results subsection present analyze results obtained extensive simulation runs compare three systems base system base network cache nc switch cache sc base system employ caching technique beyond l1 l2 caches simulate system nc enabling 4kb switch caches switching elements stage 0 min note stage 0 stage close processor stage 1 stage close remote memory shown figure 6 sc system employs switch caches switching elements min main purpose switch caches interconnect serve read requests traverse memory enhances performance reducing number read misses served remote memory figure 16 presents reduction number read misses memory appl hit distribution sharing gs 6902 3098 194 237 ge 5955 4045 158 266 table 2 distribution switch cache accesses employing network caches nc switch caches sc base system base order perform fair comparison compare sc system 2kb switch caches stages overall 4kb cache space nc system 4kb network caches figure 16 shows network caches reduce remote read misses 620 applications except fft multiple layers switch caches capable reducing number memory reads requests upto 45 fwa gs ge applications table 2 shows distribution switch cache hits across two stages st0 st1 network table note high percentage requests get satisfied switch caches present lowest stage interconnect note however three six applications roughly 3040 requests switch cache hits stage close memory st1 also interesting note number requests satisfied storing block switch cache table 2 presents data sharing defined number different processor requests served block encached switch cache find sharing degree ranges 10 27 across applications applications high overall read sharing degrees fwa gs ge find degree sharing approximately 17 stage closer processor 4 16 processors connected switch many read requests find block first stage get satisfied subsequent stage thus find higher approximately 25 read sharing degree stage closer remote memory applications mm application overall sharing degree approximately 4 see figure 2 data typically shared four processors physically connected switch first stage network thus requests 882 get satisfied first stage attain read sharing degree 18 finally sor fft applications read shared requests satisfied first stage network figure 17 impact average read latency figure application execution time improvements figure 17 shows improvement average memory access latency reads application using switch caching interconnect application figure consists three bars corresponding base nc sc systems average read latency comprises processor cache delay bus delay network data transfer delay memory service time queueing delays network memory module shown figure employing network caches improve average read latency atmost 15 applications switch caches multiple stages interconnect find average read latency improved high 35 fwa gs ge applications read latency reduces 7 matmul application sor fft unaffected network caches switch caches due negligible read sharing ultimate parameter performance execution time figure 18 shows execution time improvement bar figure divided computation synchronization time read stall time write stall time release consistent system find write stall time negligible however read stall time base system comprises high 50 overall figure 19 impact cache size number memory reads figure 20 impact cache size execution time execution time using network caches find read stall time reduces maximum 20 fwa gs ge applications thus translates improvement execution time 10 using switch caches multiple stages interconnect observe execution time improvements high 20 three applications execution time mm application comparable network caches sor fft unaffected switch caches 53 sensitivity studies sensitivity cache size order determine effect cache size performance varied switch cache size mere 128 bytes large 8kb figures 19 20 show impact switch cache size number memory reads overall execution time cache size increased find switch cache size 512 bytes provides maximum performance improvement 45 reads 5hso qy figure 21 effect cache size eviction rate figure 22 effect cache size switch cache hits across stages 20 execution time three six applications mm sor applications require larger caches additional improvement mm application attains performance improvement 7 execution time switch cache size 2kb increasing cache size negligible impact performance sor found reduction number memory reads contrary negligible amount sharing application shown figure 2 upon investigation found switch cache hits come replacements l2 caches words blocks switch cache accessed highly processor whose initial request entered block switch cache switch cache acts victim cache application use switch caches affect performance fft application figure 21 investigates impact cache size eviction rate type switch cache fwa application xaxis figure represents size cache bytes block switch cache evicted either due replacement due invalidation bar figure divided two portions represent amount replacements versus invalidations switch cache figures normalized number evictions system 128 byte figure 23 effect line size number memory reads switch caches first observation figure reduction number evictions cache size increases note number evictions remains constant beyond cache size 1kb small caches also observe roughly 1020 blocks switch cache invalidated others replaced words blocks invalidations processed switch cache since already evicted replacements due small capacity cache size increases find fraction invalidations increase since fewer replacements occur larger caches 8kb switch cache find roughly 50 blocks invalidated cache next look impact cache size amount sharing across stages figure 22 shows amount hits obtained stage network fwa application bar divided two segments representing stage switch caches denoted stage number note stage0 stage closest processor interface figure interesting note small caches equal amount hits obtained stage network hand cache size increases find higher fraction hits due switch caches closer processor interface 6070 st0 beneficial fewer hops required network access data thereby reducing read latency considerably sensitivity cache line size earlier sections analyzed data lines section vary cache line size study impact switch cache performance figures 23 24 show impact larger cache line 64 bytes switch cache performance three applications ge vary cache size 256 bytes 16kb compare performance base system lines 64 byte cache lines note results normalized base system 64 byte cache lines found number memory reads figure 24 effect line size execution time figure 25 effect associativity switch cache hits reduced 37 45 increase cache line size base system however use switch caches still significant impact application performance 1kb switch caches reduce number read requests served remote memory high 45 execution time high 20 summary switch cache performance depend highly cache line size highly read shared applications good spatial locality sensitivity set associativity section study impact cache set associativity application performance figure 25 shows percentage switch cache hits cache size associativity varied find set associativity impact switch cache performance believe frequently accessed blocks need reside switch cache short amount time observed earlier trace analysis higher degree associativity tries prolong residence time reducing cache conflicts since require higher residence time switch cache performance neither improved hindered figure 26 effect application size execution time sensitivity application size another concern performance switch caches relatively small data set used faster simulation order verify switch cache performance change drastically larger data sets used fwa application increased number vertices 128 192 256 note data set size increases square number vertices base system execution time increases factor 23 46 respectively 512 byte switch caches execution time reduces 17 128 vertices 13 192 vertices 10 256 vertices summary believe switch caches require small cache capacity provide sufficient performance improvements large applications frequently accessed read shared data 6 conclusions paper presented novel hardware caching technique called switch cache improve remote memory access performance ccnuma multiprocessors detailed trace analysis several applications showed accesses shared blocks great deal temporal locality thus remote memory access performance greatly improved caching shared data global cache make global cache accessible processors system interconnect seems best location since ability monitor internode transactions system efficient yet distributed fashion incorporating small caches within switching element min shared data captured flowed memory processor designing switch caching framework several issues dealt main hindrance global caching techniques maintaining cache coherence organized caching technique hierarchical fashion utilizing inherent tree structure bmin caches kept coherent transparent fashion regular processor invalidations sent home node control infor mation maintain fullmap directory information read requests hit switch cache marked allowed continue path memory sole purpose updating directory caching technique also kept noninclusive thus devoid size problem multilevel inclusion property important issue designing switch caches incorporating cache within typical crossbar switches spider cavallino manner requests delayed switching elements detailed design cache embedded switch architecture caesar presented analyzed size organization cache depends heavily switch transmission latency presented dualported 2way set associative sram cache organization 4 theta 4 crossbar switch cache also proposed linkbased interleaved cache organization scale size caesar module 8 theta 8 crossbar switches simulation results indicate small cache size 1 kb bytes sufficient provide 45 reduction memory service thus 20 improvement execution time applications relates fact applications lot temporal locality shared accesses current switches spider maintain large buffers underutilized shared memory multiprocessors seems organizing buffers switch cache improvement performance realized paper studied use switch caches store recently accessed data shared state reused subsequent requests processor system addition requests applications also significant amount accesses blocks dirty state improve performance requests directories embedded within switching elements providing shared data switch caches ownership information switch directories performance ccnuma multiprocessor significantly improved latency hiding techniques data prefetching forwarding also utilize switch cache reduce risk processor cache pollution use switch caches along latency hiding techniques improve application performance ccnuma multiprocessors tremendously r overview hpconvex exemplar hardware butterfly parallel processor overview version 1 performance multistage bus networks distributed shared memory multiprocessor impact switch design application performance shared memory multiprocessors cavallino teraflops router nic new solution coherence problems multicache sys tems scalable pipelined interconnect distributed endpoint routing sgi spider chip tutorial recent trends processor design reclimbing complexity curve high frequency clock distribution sgi origin ccnuma highly scalable server network architecture connection machine cm5 stanford dash multiprocessor sting ccnuma computer system commercial mar ketplace effectiveness sram network caches clustered dsms performance model finitebuffered multistage interconnection networks area model onchip memories applica tion design analysis cache coherent multistage interconnection networks impact sharedcache clustering smallscale sharedmemory mul tiprocessors rsim reference manual version 10 splash stanford parallel applications shared memory sp2 high performance switch performance cedar multistage switching network hierarchical cachebus architecture shared memory multiprocessors designing high bandwidth onchip caches enhanced access cycle time model onchip caches mips r10000 superscalar microprocessor reducing remote conflict misses numa remote cache versus coma tr ctr takashi midorikawa daisuke shiraishi masayoshi shigeno yasuki tanabe toshihiro hanawa hideharu amano performance snail2 sssmin connected multiprocessor cache coherent mechanism parallel computing v31 n34 p352370 marchapril 2005