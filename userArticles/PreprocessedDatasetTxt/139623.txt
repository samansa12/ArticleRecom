convergence tdmyampersandlambda general myampersandlambda method temporal differences td one way making consistent predictions future paper uses analysis watkins 1989 extend convergence theorem due sutton 1988 case uses information adjacent time steps involving information arbitrary onesit also considers version td behaves face linearly dependent representations statesdemonstrating still converges different answer least mean squares algorithm finally adapts watkins theorem cal qlearning closely related prediction action learning method converges probability one demonstrate strong form convergence slightly modified version td b introduction many systems operate temporally extended circumstances whole sequences states rather individual ones important systems may frequently predict future outcome based potentially stochastic relationship current states furthermore often important able learn predictions based experience consider simple version problem task predict expected ultimate terminal values starting state absorbing markov process random processes generate terminating values absorbing states one way make predictions learn transition matrix chain expected values absorbing state solve simultaneous equation one fell swoop simpler alternative learn predictions directly without first learning transitions methods temporal differences td first defined sutton 16 17 fall simpler category given parametric way predicting expected values states alter parameters reduce inconsistency estimates one state estimates next state states learning happen incrementally system observes states terminal values sutton 17 proved results convergence particular case td method many control problems formalised terms controlled absorbing markov pro cesses policy ie mapping states actions defines absorbing markov chain engineering method dynamic programming dp 4 uses predictions expected terminal values way judging hence improving policies td methods also extended accomplish discussed extensively watkins 19 barto sutton watkins 3 td actually closely related dp ways significantly illuminate workings paper uses watkins insights extend suttons theorem special case td considers inconsistencies adjacent states general case arbitrary states important weighted exponentially less according temporal distance also considers td converges representation adopted states linearly dependent proves one version td prediction converges probability one casting form qlearning earliest work temporal difference methods due samuel 14 15 checkers draughts playing program tried learn consistent function evaluating board positions using discrepancy predicted values state based limited depth gamestree searches subsequently predicted values numbers moves elapsed many proposals along similar lines made sutton acknowledged influence klopf 9 10 17 discusses hollands bucket brigade method classifier systems 8 procedure witten 22 hampson 6 7 presented empirical results quite similar navigation task one described barto sutton watkins 3 barto sutton anderson 2 described early td system learns balance upended pole problem introduced related paper michie chambers 11 watkins 19 also gave references next section defines td shows use watkins analysis relationship dp extend suttons theorem makes comments unhelpful state representations section 3 looks qlearning uses version watkins convergence theorem demonstrate particular case strongest guarantee known behaviour td0 sutton 17 developed rationale behind td methods prediction proved td0 special case time horizon one step converges mean observations absorbing markov chain although theorem applies generally illustrated case point anexample simple random walkshown figure chain always starts state moves left right equal probabilities state reaches left absorbing barrier right absorbing barrier g problem facing td estimating probability absorbs right hand barrier rather left hand one given states current location insert figure 1 raw information available system collection sequences states terminal locations generated random walk initially knowledge transition probabilities sutton described supervised least mean squares lms 21 technique works making estimates probabilities place visited sequence closer 1 sequence ended right hand barrier closer 0 ended left hand one showed technique exactly td1 one special case td contrasted td particularly td0 tries make estimate probability one state closer estimate next without waiting see sequence might terminate discounting parameter td determines exponentially weights future states based temporal distance smoothly interpolating next state relevant lms case states equally weighted described introduction obeisance temporal order sequence marks td following subsections describe suttons result td0 separate algorithm vector representation states show watkins analysis provides wherewithal extend tdand finally reincorporate original representation 21 convergence theorem following sutton 17 consider case absorbing markov chain defined sets values terminal states nonterminal states vectors representing nonterminal states expected terminal value state j probabilities starting state p payoff structure chain shown figure degenerate sense values terminal states g deterministically 0 1 respectively makes expected value state probability absorbing g estimation system fed complete sequences x observation vectors together scalar terminal value z generate every nonterminal state prediction expected value ezji starting state transition matrix markov chain completely known predictions could computed following sutton let ab denote ab th entry matrix u denote th component vector u q denote square matrix components q ab denote vector whose components h 2 n equation 1 2 sutton showed existence limit equation follows fact q transition matrix nonterminal states absorbing markov chain probability one ultimately terminate learning phase linear td generates successive vectors w changing complete observation sequence prediction subsequently superscript used indicate tdbased estimator terminal value starting state stage n learning one sequence intermediate predictions terminal values abusing notation somewhat define also z observed terminal value note 17 sutton used p n according ff learning rate sutton showed td1 normal lms estimator 21 also proved following theorem theorem absorbing markov chain distribution starting probabilities inaccessible states outcome distributions finite expected values z j linearly independent set observation vectors fx ji 2 ng exists ffl 0 positive ff ffl initial weight vector predictions linear td weight updates sequence converge expected value ideal predictions 2 w n denotes weight vector n sequences experienced lim true case 0 paper proves theorem general 22 localist representation equation 3 conflates two issues underlying td algorithm representation prediction functions v n even though remain tangled ultimate proof convergence beneficial separate since makes operation algorithm clearer consider representing v n lookup table one entry state equivalent choosing set vectors x one component 1 others 0 state two states representation trivially satisfy conditions suttons theorem also make wn easy interpret component prediction one state using also prevents generalisation representation terms rwn v sum reduce counting number times chain visited state exponentially weighted recency case full linear case terms depend n states chain visits define characteristic function state j prediction function v n entry lookup table state stage n learning equation 3 reduced elemental pieces value state updated separately illustrate process consider punctate representation states b c e f figure 3 observed sequence c e f e f g sums step component sum clearly time 3 states g absorbing represented 23 contraction mappings watkins 19 showed fruitful way looking td estimators dynamic programming associated contraction mappings method starts current prediction function vn 8i 2 n shows define whole collection statistically better estimators vn1 8i 2 n based observed sequence uses td linear combination estimators except explicitly noted section follows watkins exactly equations developed exact analogues linear representation seen subsection 25 imagine chain starts state 0 runs forward states ultimately absorbing define rstep estimate 0 either estimate vn r state r chain absorbed r steps r 2 n observed terminal value z sequence chain absorbed time formally define random variables vn 1 z otherwise vn z otherwise vn r z otherwise 5 first state accessed markov chain one particular sequence starting second z observed terminal value chain gets absorbed time step r reached random variables since depend particular sequence states observed naturally also depend values vn point chain absorbs completing steps 0 v r r based terminal value provided world rather one derived bootstraps vn v r therefore average accurate vn used incrementally improve shown looking difference e ezji 0 ideal predictions tr2t vn r 6 whereas easily shown tr2t therefore watkins actually treated slightly different case target values predictors based discounted future values whose contribution diminishes exponentially time happen case easier see reduction error brought analogue equation discount factor since p q matrix markov chain watkins could guarantee provides weak guarantee error v r n less vn nondiscounted case different initial states nonzero probability chain absorb finishing r steps case value v r z unbiased provide error reduction even chain absorb value inaccurate component vn although error reduction due fl guaranteed inequality states possible absorb within r steps ensure since maximum could achieved pathologically state fromwhich impossible absorb r steps however estimates states within r steps absorption average improve average filter back states watkins demonstrated td based weighted average v consider also valid estimator terminal value starting 0 points choosing value trade bias caused error vn variance real terminal value z higher significant v r higher values r effect unbiased terminal values leads higher variance lower bias conversely lower less significant contributions higher values r less effect unbiased terminal values leads smaller variance greater bias remains shown td indeed based combination estimator expanding sum equation 10 vn 0 vn vn 1 vn 2 defining vn whole point defining v used make v accurate obvious incremental update rule achieve vn1 0 vn vn 0 equation 11 apparent changes vn 0 involve summing future values vn t1 vn weighted powers following watkins differences calculated activity trace based characteristic functions defined earlier way counting often recently chain entered particular states using index members observed sequence online version td rule problem sutton treated change vn applied offline complete sequence chain therefore states chain passes one sequence absorbs terminal value vn vn1 new estimator experiencing sequence vn1 0 vn ff vn t1 vn vn1 1 vn 1 ff vn t1 vn vn1 vn vn summing terms note expressions exactly td weight change formula equation 4 thus actual td algorithm based exponentially weighted sum defined equation 10 outcomes v r random variables themean contraction properties variables therefore determine mean contraction properties overall td estimator 24 linear representation previous subsection considered td algorithm isolated representation sutton used although number different representations might employed simplest linear one adopted identifying vectors x states represent vn wn x wn weight vector stage n learning basic algorithm concerned v predictor random variables rather values used change initial predictor vn new representation equation 12 longer makes sense since states cannot separated appropriate manner rather information error used update weights depends appropriate formula derived deltarule vn 0 rwn vn 0 weighting error due state 0 vector representation 0 equivalent equation 13 suttons main td equation 3 sophisticated representations kdtrees see 13 review cmacs may lead faster learning better generalisation requires separate convergence proof 5 compares qualities certain different representations barto sutton watkins grid task 3 25 proof theorem strategy proving theorem follow sutton 17 considering expected value new prediction weight vector given observation complete sequence follow watkins splitting change components due equivalents v r random variables summing mean error reduction iterations assured equivalent equation 9 linear representation define v r random variables equation 5 z otherwise x identified states observed sequence w r n current weight vector defining estimated terminal values z actual value observing whole sequence w r n updated visited vn rwn vn visited exact parallel suttons proof procedure turns apply w r define j ij number times sstep transition occurs intermediate states x kt 2 n sum equation 14 regrouped terms source destination states transitions ff z z j indicates terminal value generated distribution due state j extra terms generated possibility visiting x 2 n chain absorbs taking r steps taking expected values sequences 2 n ik q kj ik q kj expected number times markov chain state one sequence absorbing markov chain known dependency probabilities starting various states substituting equation 15 taking expectations sides noting dependence ew r n n linear using w denote expected values close relation equation 6 emerges linear representation z define x matrix whose columns x x ab x diagonal ab ffi ab kronecker delta remembering h converting matrix form since x covers possible options rgammastep moves state define correct predictions e also equation 2 e sum converges since chain absorbing another way writing equation 7 multiplying equation 17 left x subtracting e sides equation noting equation 18 gives update rule equivalent equation e e e watkins construction td developed equation 10 previous section reveals starting w r therefore since e io h e e w expected weights td procedure sum converges since truth theorem shown demonstrated 9ffl 0 e 1 estimates tend correct almost suttons proof 17 applies mutatis mutandis case 6 0 always provided crucial condition holds x full rank completeness entire proof given appendix overall implies expected values estimates converge desired values sequences observed conditions stated theorem 26 nonindependence x moving watkins representationfree proof suttons treatment linear case one assumption x vectors representing states indepen dent matrix x full rank proof breaks still positive however x xd longer full set eigenvalues positive real parts since null subspace empty nonzero member eigenvector eigenvalue 0 saying happen expected values weights turns easier understanding choose basis basis proof appendix applies exactly b exists lim also h definition writing e e xd e help understand result consider equivalent lms rule td1 xd e since symmetric e e e e equation 21 weights w square error state expected number visits one sequence therefore quadratic form e e loaded square error predictions state desired values loading factors expected frequencies markov chain hits states condition equation 24 implies expected values weights tend minimise error happen general 6 1 intuitively tradeoff bias variance returned case x full rank sutton shows harmless use inaccurate estimates next state x t1 w criticise estimates current state x w x full rank successive estimates become biased account might deemed shared representation amount extra bias related amount sharing frequency transitions happen one state next formalising leads second issue interaction two statistical processes calculating mean weight calculating expected number transitions comparing equations 20 21 one might expect lim e however key step proving equation 24 transition equations 22 23 relied symmetry since q general symmetric happen defining w e e e actually happen g although behaviour described equation 25 satisfactory described equation 26 revealing consider happens one attempts arrange hold achieved completing derivative ie learning rule whose effect e e q term effectively arranges backwards well forwards learning occur would state adjust estimate make like state t1 also state would adjust estimate make like state werbos 20 sutton personal communication discussed point context gradient descent td rather convergence nonindependent x werbos presented example based learning technique similar td0 completing derivative manner makes rule converge away true solution faulted procedure introducing unhelpful correlations learning rule random moves one state next mentioned pointed convergence terms functions g equation 26 w 0 weights fixed sutton presented example help explain result first sight augmenting td seems quite reasonable could quite easily happen random chance training sequences predictions one state accurate predictions next point therefore training second like first would helpful however sutton pointed time choices always move forward backwards imagine case shown figure numbers arrows represent transition probabilities numbers terminal nodes represent terminal absorbing values insert figure 2 value state reasonably 12 50 probability ending either z value state b though 1 chain certain end training forwards give training backwards make value b tend 34 werbos terms correlations weights possible transitions count augmented term incidentally result affect td1 training values terminal value sequence bear relation transitions number times state visited coming back case x full rank td 6 1 still converge away best value degree determined matrix 3 convergence probability one suttons proof proofs previous section accomplish nadir stochastic convergence viz convergence mean rather zenith viz convergence probability one watkins 19 proved convergence probability one form prediction action learning called qlearning section shows result applied almost directly discounted predictive version td0 albeit without linear representation provides first strong proof temporal difference method like dynamic programming dp qlearning combines prediction control consider controlled discounted nonabsorbing markovprocess ie one state finite set possible actions 2 taking one action leads immediate reward random variable r whose distribution depends stochastic transition according markov matrix p ij j 2 n agent policy 2 determines action would perform state defining value v satisfies fl discount factor define q value state action policy value taking action followed policy thereafter theory dp 4 implies policy least good take action state bg follow states fact lies utility q values discounted problems turns least one optimal policy define q qlearning method determining q hence optimal policy based exploring effects actions states consider sequence observations n process state n probed action taking state j n giving reward z n define recursively qn otherwise 27 starting values q 0 un j n bg ff n set learning rates obey standard stochastic convergence criteriax k th time watkins 19 proved addition rewards bounded probability one lim consider degenerate case controlled markov process one action possible every state case q v similarly defined u values exactly equal q equation 27 exactly online form td0 case nonabsorbing chain rewards ie terminal values discussed context absorbing markov chains arrive every state rather particular set absorbing states therefore conditions watkins theorem online version td0 converges correct predictions probability one although clearly td procedure various differences one described previous section learning online v q values changed every observation also learning need proceed along observed sequence requirement j uncoupled disembodied moves used 4 conditions equation 28 consequence every state must visited infinitely often also note suttons proof since confined showing convergence mean works fixed learning rate ff whereas watkins common stochastic convergence proofs requires ff n tend 0 also stated qlearning theorem applies discounted nonabsorbing markov chains rather absorbing ones previous section 4 one watkins main motivations allows system learn effects actions believes suboptimal important r ole watkins proof bounding effect early qn values fairly easy modify proof case absorbing markov chain ever increasing probability absorption achieves effect also conditions suttons theorem imply every nonabsorbing state visited infinitely often suffices one set ff satisfy conditions 28 apply sequentially visit state normal running chain conclusions paper used watkins analysis relationship temporal difference td estimation dynamic programming extend suttons theorem td0 prediction converges mean case theorem td general also demonstrated vectors representing states linearly independent td converges different solution least means squares algorithm applied special case watkins theorem qlearning method incremental dynamic programming converges probability one show td0 using localist state representation also converges probability one leaves open question whether td punctate distributed representations also converges manner appendix existence appropriate ff defining necessary show ffl case formula remains correct x full rank sutton proved pages 2628 17 showing successively di gamma q positive full set eigenvalues whose real parts positive finally ff thus chosen eigenvalues gamma ffx xdi gamma q less 1 modulus proof requires little alteration case 6 0 path followed exactly equivalent di gamma q positive definite according lemma varga 18 observation sutton shown strictly diagonally dominant positive diagonal entries part proof differs sutton even structure rather similar since q matrix absorbing markov chain q r diagonal elements 1 therefore r positive diagonal elements also 6 j since elements q hence also q r positive case r strictly diagonally dominant p strict inequality equation 29 follows equation 16 equation 30 holds since equation 31 holds since p 1 chain absorbing q also exists least one 0 inequality strict strictly diagonally dominant r 1 strictly diagonally dominant therefore positive definite next stage show x xd full set eigenvalues whose real parts positive x x non singular ensures set full let u eigenvalueeigenvector pair indicates conjugate transpose implies xv xv equivalently b since right side positive definiteness xv xv strictly positive real part must strictly positive furthermore u must also eigenvector since therefore eigenvalues gamma ffx xd positive ae take eigenvalues eigenvalues 1 gamma ff iteration matrix guaranteed modulus less one another theorem varga 18 lim 0 r new approach manipulator control cerebellar model articulation controller cmac neuronlike elements solve difficult learning problems learning sequential decision making applied dynamic programming reinforcing connectionism learning statistical way neural model adaptive behavior connectionistic problem solving computational aspects biological learning escaping brittleness possibilities generalpurpose learning algorithms applied parallel rulebased systems brain function adaptive systems heterostatic theory hedonistic neuron theory memory boxes experiment adaptive control efficient memorybased learning robot control efficient algorithms neural network behaviour studies machine learning using game checkers studies machine learning using game checkers ii recent progress temporal credit assignment reinforcement learning learning predict methods temporal difference matrix iterative analysis learning delayed rewards consistency hdp applied simple reinforcement learning problem adaptive signal processing adaptive optimal controller discretetime markov environments tr ctr claudenicolas fiechter efficient reinforcement learning proceedings seventh annual conference computational learning theory p8897 july 1215 1994 new brunswick new jersey united states fernando j pineda meanfield theory batched td lgr neural computation v9 n7 p14031419 oct 1 1997 vladislav b tadi asymptotic analysis temporaldifference learning algorithms constant stepsizes machine learning v63 n2 p107133 may 2006 satinder singh peter dayan analytical mean squared error curves temporal differencelearning machine learning v32 n1 p540 july 1998 vladislav tadi convergence analysis temporaldifference learning algorithms linear function approximation proceedings twelfth annual conference computational learning theory p193202 july 0709 1999 santa cruz california united states satinder singh tommi jaakkola michael l littman csaba szepesvri convergence results singlestep onpolicyreinforcementlearning algorithms machine learning v38 n3 p287308 march 2000 vladislav tadi convergence temporaldifference learning linear function approximation machine learning v42 n3 p241267 march 2001 peter auer philip long structural results online learning models without queries machine learning v36 n3 p147181 sept 1999 kazunori iwata kazushi ikeda hideaki sakai asymptotic equipartition property reinforcement learning relation return maximization neural networks v19 n1 p6275 january 2006 john w sheppard colearning differential games machine learning v33 n23 p201233 novdec 1998 david choi benjamin roy generalized kalman filter fixed point approximation efficient temporaldifference learning discrete event dynamic systems v16 n2 p207239 april 2006 craig boutilier planning learning coordination multiagent decision processes proceedings 6th conference theoretical aspects rationality knowledge march 1720 1996 netherlands florentin wrgtter bernd porr temporal sequence learning prediction control review different models relation biological mechanisms neural computation v17 n2 p245319 february 2005