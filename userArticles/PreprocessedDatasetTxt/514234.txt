nearoptimal adaptive control large grid application paper develops performance model used control adaptive execution atr code solving large stochastic optimization problems computational grids detailed analysis execution characteristics atr used construct performance model used specify nearoptimal dynamic values parameters govern distribution work b new task scheduling algorithm together new features minimize atr execution time collection compute nodes including varying collection heterogeneous nodes new adaptive code runs eightfold faster previously optimized code requires input parameters user guide distribution work furthermore modeling process led several changes condor runtime environment including new task scheduling algorithm produce significant performance improvements masterworker computations well possibly types grid applications b introduction paper develops model nearoptimal adaptive control stateoftheart stochastic optimization code atr 18 grid platforms condor 19 globus 10 legion 13 number capabilities distributed hosts execute atr varies course computation stochastic optimization uses large amounts computational resources solve key organizational economic financial planning decision problems involve uncertain data instance approximate solution cargo flight scheduling problem required hours computation four hundred processors find accurate solutions wider range scenarios considered verify quality approximate solutions may require vastly greater resources aim find decision optimizes expected performance system across possible scenarios uncertain demands since number scenarios may large typically 10 4 10 7 evaluation expected performance requires evaluation performance scenario quite expensive atr also representative class iterative algorithms 1 basic forkjoin synchronization structure 2 require unpredictable number iterations converge solution 3 adjust number sizes tasks forked per iteration computational grids running middleware condor currently provide one attractive environments running large computeintensive applications grids inexpensive widely accessible powerful time applications submitted using grid middleware given fair share computational resources used higher priority computations applications like atr obtain large quantities processing power easily inexpensively run efficiently grid environment application must able execute heterogeneous collection hosts whose size varies unpredictably execution moreover able adapt changes size composition collection hosts well changes computational demands algorithm executes may adapt instance changing distribution work among hosts unknown develop adaptive version stochastic optimization tool atr minimizes total execution time grid environment problem particularly complex parameters govern amount distribution work also affect intrinsic performance algorithm time initialize iteration number iterations reach convergence ways easily quantified furthermore runtime environment typically includes support functions add unpredictable delays task execution times previous work 18 developing atr algorithm condor platforms relied simple task scheduling extensive experimental measurements total application running time function 1 average number allocated compute nodes 2 fixed ie nonadaptive values atr parameters define number composition parallel tasks computation experiments resulted rules thumb selecting atr parameters function number compute nodes allocated example work partially supported national science foundation grants eia9975024 eia0127857 permission make digital hard copies part work personal classroom use granted without fee provided copies made distributed profit commercial advantage copies bear notice full citation first page copy otherwise republish post servers redistribute lists requires prior specific permission andor fee ics02 june 2226 2002 new york new york usa application runs x 100 distributed condor nodes rules provide parameters ensure 2x 5x parallel tasks available processing given time order keep processor efficiencies high presence high variability task execution times previous studies concerning adaptive control distributed applications eg 61420 proposed similar kinds experimentally determined rules thumb development adaptive codes minimize total execution time complex applications based precise modeling adaptation strategies knowledge previously investigated paper develops precise accurate performance model atr uses model develop adaptive version code minimizes total execution time large class planning problems collection compute nodes including varying number heterogeneous nodes new adaptive code require user input guide amount distribution work implements parameter settings quite different obtained rules thumb developed previously new adaptive code also uses 1 higher performance task scheduling strategy 2 reduced debug io level 3 proposed simple change condor runtime system support reduces needless overhead master node latter two changes motivated development atr performance model greatly improve task execution times well predictability task execution times three changes greatly improve grid applications well atr new adaptive code together runtime system change reduces total execution time compared previously optimized code factors six depending planning problem grid configuration remainder paper organized follows section 2 provides overview atr application condor runtime environment related work performance adaptive grid codes section 3 describes detailed measurementbased performance analysis atr needed develop model section 4 describes validates applies model select nearoptimal configurations atr sizable condor pool conclusions work stated section 5 2 background sections 21 22 briefly describe atr stochastic optimization application condor system mw runtime support library masterworker computations section 23 summarizes related work performance modeling development large distributed applications 21 atr atr iterative asynchronous trustregion algorithm solving fundamental stochastic optimization problem twostage stochastic linear programming recourse discrete probability space algorithm described detail 18 discuss aspects algorithm relevant selecting performancerelated algorithmic parameters particular parameters control number composition parallel tasks execution problem follows given set n scenarios subject ax x x x x q w optimal objective value secondstage linear program defined follows min x wy x evaluate function thus requires solution independent secondstage linear programs n large process computationally expensive function q piecewise linear convex function atr algorithm builds lower bounding function mx true objective function x using information gathered evaluation secondstage linear programs basic structure algorithm illustrated figure 1 master processor computes new candidate iterate x solving trustregion subproblem defined worker processors evaluate secondstage linear programs x produce information needed refine model function mx master processor updates model function asynchronously information arrives worker processor outdated information may also deleted mx occasion sufficient new information received ie processors working evaluation x returned results master computes new iterate x process repeats new iterates x generated solving subproblems form x x ax x subject min x incumbent best iterate identified algorithm date trustregion radius defines maximum distance move away incumbent current step general scenarios n included formulation realistic model although problem becomes larger harder solve n increases effect less marked good initial guess solution good strategy therefore start solving approximate problem modest value n 1000 say use resulting approximate solution starting point another approximate problem larger number scenarios 5000 say procedure repeated progressively larger n reduce time compute new iterate x n scenarios partitioned fixed set tasks denoted 2where j n denotes set scenario indices given task j worker computes following partial sum worker also computes one partial subgradient also known cut task information used master update partial model function corresponding task j complete model function c x plus sum tasks atr enables additional parallelism allowing one candidate iterate x evaluated time order generate additional candidate iterate master processor computes new iterate scenarios current candidate evaluated illustrated figure 2 atr thus creates maintains basket b candidates b 1 15 previous nonadaptive version atr tasks grouped g equalsize task groups containing tg tasks task group unit work sent worker example possible configuration task group contains two tasks 100 scenarios assume n fixed performance analysis affect amount distribution work varying parameters b g increasing b new iterates solved workers master processing results iterates slow worker slow evaluation one parallel iterates increasing cuts computed per value x making model function expensive compute master also better approximation true objective q generally reduces number iterations needed solve problem increasing g obtain task groups fewer tasks therefore smaller execution times group previous evaluations atr locally widely distributed condormw grids shown 50100 workers best execution times obtained three six concurrent candidates 100 tasks 25 50 task groups goal research create performance model select basket size b number tasks number task groups g statically program initiation time dynamically execution job minimize total atr execution time given planning problem interest 22 condorpvm mw condor system 19 manages heterogeneous collections computers including workstations pc clusters multiprocessor systems mechanisms glidein flocking used include processors separate sites resource pool user submits job system condor identifies suitable processors pool assigns processors job processor executing job becomes unavailable example reclaimed owner condor migrates job another node pool possibly restarting checkpoint saved earlier time size pool available particular user change unpredictably computation although users exercise control resources devoted application specifying number speed andor type workers mw 18 framework facilitates implementation masterworker applications variety computational grid platforms study use version mw implemented condorpvm 32 master runs submitting host worker tasks processors drawn condor pool condorpvm primitives used buffer pass messages master workers 23 related work parallel stochastic optimization algorithms investigated past 15 years 4 algorithm related atr without trust region asynchrony features applied multistage problems implemented cluster modest size earlier paper 5 describes interiorpoint approach linear algebra computations implemented parallel approach scale well large number processors small pc cluster used 11 approach use interiorpoint methods secondstage problems inexactly analytic center method master problem workers master master workers solve secondary stage linear programs master update mx compute new x update mx compute new x solve secondary stage linear programs figure 1 basic atr structure b1 workers master master solve secondary stage linear programs update mx andcompute new x j c solve secondary stage linear programs workers master update mx andcompute new x figure 2 asynchronous atr structure b2 table 1 atr configuration parameters symbol definition n number scenarios evaluated per iterate number tasks per iteration g number task groups scenarios partitioned x vector candidate planning decisions iterate b number iterates evaluated parallel three approaches adaptive software control explored previous work namely 1 provide interface user control application steering parameters eg 2 2 use measured userprovided estimates application execution time function system configuration heuristically adapt system resource management policies application configuration parameters automatically runtime 17614162027 3 use simple models compute execution time alternative configurations 3821232425 userprovided measured deterministic processing communication requirements per task possibly adjusted runtimemeasured node processing capacities available network bandwidth regarding historybased heuristic adaptive control algorithms ribler et al 20 describe autopilot system filters data instrumented client tasks eg characterize dominant file access pattern adapts system resource management policies eg file prefetch policy related grads project work 27 measures application signature ie processing io communication cycles function time uses fuzzy logic determine whether rates task within ranges defined userspecified performance contracts algorithms changing configuration contract violated addressed work heymann et al 14 measure condormw task execution times worker node efficiencies iteration results synthetic mw applications used estimate number worker nodes allocate achieve 80 efficiency 10 increase iteration execution time based relative processing times largest 20 tasks algorithm assumes task performs approximately work successive iterations homogeneous processing nodes application completes eachiteration starting next iteration lan et al use computation times measured previous iterations decide whether redistribute work next iteration astrophysics code 17 chang karamcheti 6 propose application structure containing tunability interface expression user preferences previous application execution measurements runtime resource application monitoring user preferences used automatically select certain parameters runtime example select image resolution available processing capacity userspecified image transmission time harmony system keleher et al 16 propose approach user specifies processing communication time provides model predict values runtime possible multidimensional configuration application system dynamically allocates resources achieve particular system objective maximizing throughput previous models adaptive runtime control compute node processing time communication time per iteration using known andor measured quantities per data point image pixel times number data points pixels assigned node example ripeanu et al 21 compute execution time loadbalanced finite difference application function data assigned node b redundant work computed nodes reduce communication nodes c communication time calculations used runtime select amount redundant work per node measured grid communication costs apples project used similar calculations determine many available compute data server nodes assigned simple adaptive iterative jacobi application 3 gene sequence comparison code 24 magnetohydrodynamics application 22 adaptive parallel tomography image reconstruction application 23 adaptive data server selection sara application 25 applications linear optimization model formulated compute work assigned node per candidate system configuration measured node processing communication capacities achieve objective minimizing total execution time maximizing image quality userspecified target refresh frequency approach paper similar modelbased adaptive runtime control 23 however targeting much complex atr application known model estimating execution time function system configuration ie parameters b g set workers furthermore develop model efficiently determines allocate work available compute nodes without solving optimization problem possible system configurations 3 atr execution time analysis section analyze atr task execution times communication latencies determine aspects computation communication need modeled model total atr execution time analysis guided task graphs figures 1 2 illustrate overall structure parallel atr execution basket size b equal one two respectively table 1 summarizes notation basic functions master process update function mx time worker processor returns results executing group tasks b compute new iterate x tasks associated particular iterate x completed existing nonadaptive version atr parameters b g specified inputs application fixed throughout run g specifies number task groups evaluated parallel workers task group contains tg tasks containing nt scenarios since individual task generates subgradient cut task group returns tg subgradients goals work determine whether values b g number tasks per group determined minimize total atr execution time collection workers whether nearoptimal adaptive values parameters computed runtime collection workers changes much performance improvement gained adaptive version code challenge modeling minimizing overall atr execution time previous measurements atr executions 18 revealed difficult quantify master execution times increase increases b number iterates need evaluated increases b increases decreases c high degree unpredictability execution times master workers possibly due runtime environment issues addressed section 31 analyzes worker execution times section 32 analyzes master execution times section 33 analyzes condor pvm communication costs pair local hosts well pair widely separated hosts message sizes transmitted master worker atr application measured computation communication times used section 4 develop performance model optimized adaptive parameter values atr code standard approach local condor pool submit condor job shared host becomes master processor shared host typically relatively high cpu load master processing times might highly variable andor large avoid problem unless otherwise noted atr measurements reported section submitted singleuser workstation free user processes run refer setup light load master processor initial analysis execution times based several planning problems several values n numbers scenarios range values g single worker node analyzing impact g investigates larger values b use single worker ensures measured master task execution times inflated unpredictable interrupts workers finished evaluating tasks basic task execution times understood interrupts modeled needed 31 worker execution times table 2 summarizes average minimum maximum coefficient variation cv execution time two principal tasks carried master namely updating mx computing new iterate table also shows average cv time worker evaluate task group iterations several different values g similar results also obtained planning problems values number scenarios n runs different times day many different worker processors measurements show worker execution times experiment low variability ie highly predictable furthermore results figure 3 show common case ng 25 average worker execution time approximately linear number scenarios evaluated ng well peak speed processor second third rows table 2 well results omitted conserve space demonstrate worker execution time independent results indicate execution time given atr task given worker predicted benchmark contains least 25 scenarios run worker first assigned atr application note deterministic worker execution times due fact condor job scheduler uses spacesharing rather time sharing grid nodes reasonably well utilized consequence interference jobs need modeled predicting atr execution times condor 32 master execution times table 2 shows master processing times updating mx computing new iterate highly variable might515253545 number scenarios evaluated ng worker execution time sec mips 780 mips 1100 mips 1700 minimum maximum figure 3 worker execution time ssn planning problem n10000 b1 tg1 table 2 example measured execution times master time update model function mx msec master time compute new iterate x sec worker execution time sec avg min max cv num avg min max cv avg cv appear variability time update mx important since average time required update milliseconds however since master needs perform update g times per iteration time worker returns results evaluating task group since g may 100 since largest update times order 12 seconds shown table 2 cumulative update time significant impact total atr execution time section 321 analyze causes execution time b propose two changes runtime system reduce c characterize task execution time precisely note experimentally observed variability worker execution times previous work may actually due variability master processing time updating mx noted section 21 increases average time compute new iterate increases number iterations decreases cuts added model function mx iteration larger causes model function become closer approximation true objective function fewer iterations making trustregion subproblem harder solve time compute new iterate varies one second three four times average value investigate variations detail section 322 goal understanding model variations function g 321 time update mx figure 4 provides example histograms master execution times updating model function mx three different measurement runs histogram lightly loaded 700 mips master default debug io corresponds data table 1 even greater variability shown histogram observed master one shared condor hosts commonly used submit condor jobs investigation revealed value condor debug io flag commonly used mw applications produces vast quantities debug output figure 4 provides second histogram lightly loaded master reduced debug io run reduced level debug io level 5 condormw still produces significant log mw execution events reduced debug io improves average time update mx variability execution time understand high variability observed execution times note due worker execution times see table 2 significant periods time order seconds atr master waiting results workers since master processor part condor pool surmised high variability time update model function shown figure 4 least partly due condor administrative functions condor jobs may run master processor periods third histogram figure 4 shows using isolated master node available running condor jobs administrative functions atr run greatly reduces variability well average execution time update mx thus proposed new feature condor allows userowned host serving lightly loaded mw master temporarily unavailable running condor jobs01101000 worker completion event count time update msec lightly loaded master default debug level lightly loaded master reduced debug level isolated master reduced debug level figure 4 example histogram times update mx ssn planning problem n10000 b1 t50 g5005152535 iteration number time compute x sec lightly loaded master isolated master impact isolated master ssn planning problem n10000 b1 t50 g50515250 200 400 600 800 iteration number time compute x sec typical profile isolated master 20term planning problem n5000 b1 t200 g50 figure 5 example histograms times compute new iterate x isolated master configurations average time update mx 150500 microseconds depending value tg case overall atr execution time dominated worker execution times time master compute new iterates see table 2 322 time compute new x figure 5 shows histogram time compute next iterate given planning problem given set input parameters figure 5a shows isolated master exhibits lower mean variability execution time computing new iterates master lightly loaded non isolated configuration lower variability makes easier optimize application parameters control parallelism except noted measurements atr obtained isolated master reduced still substantial debug io level figures 5b 6a show histograms execution times computing new iterates x two different planning problems 20term ssn figure 5b starting point blind far solution 6a chosen solution corresponding planning problem smaller number scenarios n figures see wide variability time required compute next iterate one iteration next trend similar specifically time required tends increase steadily drops sharply becomes minimal possibly long sequence iterates times increase last iterations similar trends also observed planning problems many starting points parameter settings ad hoc adaptive algorithm might estimate average time compute next iterates time compute current iterate fairly high accuracy iterations could use estimate adjust parameters governing parallelism evaluation current iterates namely b achieve desired tradeoff processor efficiency expected total time next iteration develop alternative approach section 4 based following observations figures 6ac illustrate dependence parameter number iterations time required compute new iterate ssn planning problem fixed starting point note increasing causes diminishing decrease number iterations b diminishing increase time compute new iterate overall effect illustrated figure 6d total master processing time computing new iterates grows10010000 number tasks totalmaster processing time sec cumulative time compute iteration number time compute x number tasks average time compute x sec c average time compute time compute new x20601000 100 200 300 400 500 600 700 800 900 number tasks number iterations average min b number iterations n vs figure impact time compute new x ssn planning problem n10000 slowly curves omitted clarity show time compute next iterate largely independent g observations hold three planning problems atr parameter values analyzed key insights needed optimize b g section 4 323 impact basket size b setting 2 workers evaluate scenarios corresponding one iterate x master computes new value x latest subgradient information since practical planning problems master execution times tend either one second least large worker execution times b set two order improve overall execution time however figure 7 shows total number iterates n required convergence atr nearly doubles b doubles similar behavior observed planning problems many values n thus total execution time improve b greater one fact verified experimentally 33 atr communication times figures 8a b provide measured condorpvm round trip time sending message given size another host receiving back small message process mimics roundtrip communication master worker atr figure 8a shows results hosts interconnected high speed local network figure 8b provides results host university wisconsin sending host bologna italy atr planning problems size message master worker typically ranges 250 bytes 12 kb furthermore sending message given worker overlapped execution time worker received previous message thus approximately one roundtrip time per iteration critical path atr computation comparison round trip time worker execution times master execution times suggests communication costs negligible practical values n g even wide area networks 4 nearoptimal adaptive atr measurements reported section 3 provided data needed create model total atr runtime summarize worker execution times deterministic approximately linear number scenarios per task group ng independent number tasks group tg communication costs master workers negligible high speed network even master workers widely distributed validate atr run widely distributed condor flock master processing time updating model function mx time worker returns results 100500 msec omitted firstcut model total atr run time time required master solve trustregion subproblem compute new iterate x significant component total atr execution time moreover total master execution time compute iterates increases slowly significant decrease number iterates n need considered increases t400 nt decreases tens observations motivate surprisingly simple firstcut model atr execution time used optimize execution atr various grid configurations because20601001400 1 number iterations maximum average minimum figure 7 impact b total number iterations n ssn planning problem n10000 t200 g50135790 4 8 12 28 ssage ki l obyte round trip time msec local nodes028084140 28 size message kilobytes round trip time sec experiment 1 experiment 2 b wisconsin bologna italy figure 8 roundtrip message time interrupts communication costs variability task execution times modeled model even simpler loggp type model used large complex applications 1 26 section 41 presents model validates sufficiently detailed estimate overall atr execution time widely distributed condor flocks well local condor pool sections 42 43 demonstrate model together improved task scheduling used minimize atr execution time varying set homogeneous grid nodes varying set heterogeneous grid nodes respectively 41 model validation fixed values n g homogeneous set g worker nodes firstcut model total atr running time simply w nt total master execution time computing new iterates n number iterations w time needed worker evaluate group tasks containing ng scenarios table 3 evaluates accuracy model ignores interrupts communication overhead small master execution times update mx table compares measured atr execution times execution time computed model using measured components w several different configurations planning problem ssn well two representative versions problems storm 20 term experiments table run local condor pool one two condor flocks master processor local isolated master homogeneous workers compute nodes albuquerque high performance computing center argonne national laboratory flock experiments communication master workers via internet communication costs similar graphed figure 9b rather figure 9a table 3 shows wide range total application execution times minutes hour runtime estimates obtained simple model within 10 measured execution times even workers geographically distant master employ fewer workers k number task groups g model nonadaptive atr running time modified follows nt g k last row table 3 similar experiments omitted conserve space validates simple extended model showing captures principal components total run time fixed n g set heterogeneous workers total execution time estimated max nt table validates model collections heterogeneous nodes local condor pool obtained results requesting g workers without restricting type worker nodes assigned case condor allocated wide variety processors ranging speed 186 mhz 17 ghz nonadaptive version atr total execution times estimated generally accurate homogeneous workers unless problem size fairly small ie execution time less 10 minutes number workers g large cases worker tasks table 3 simple model estimates total atr execution time homogeneous workers compute sec total execution time planning problem n g number workers num total benchmark avgerage measured note 20terms 5000 200 50 g 597 276294 235 6947 7054 wi pool ssn 40000 100 50 g 84 29736 3097 4883 5221 winm flock ssn 20000 50 50 g 108 18090 2091 4084 4470 wiargonne flock ssn 20000 100 50 g 84 24400 2089 3351 3638 wiargonne flock ssn 20000 200 50 g 61 29530 2088 2640 2932 wiargonne flock ssn 5000 200 50 g 131 107682 323 2505 2680 wi pool ssn 20000 400 50 g 44 44180 2096 2298 2498 wiargonne flock storm 10000 200 50 g 11 253 8244 1653 1848 wi pool ssn 10000 50 50 g 66 5670 644 814 923 wi pool ssn 10000 100 50 g 50 7179 648 670 823 wi pool ssn 10000 200 50 g 38 7946 644 551 662 wi pool ssn 10000 400 50 g 26 7045 643 407 492 wi pool ssn 10000 100 100 g 44 7032 331 365 471 wi pool ssn 10000 100 500 2g3 44 6481 632 1034 1210 wi pool small communication master workers ignored model secondary nonnegligible impact total running time since practical problems interest involve large planning problems model ignores communication costs used minimize total atr execution time 42 adaptive code homogeneous workers based results specify optimal adaptive configuration atr algorithm given number scenarios n number available homogeneous workers assuming objective minimize total atr execution time model could applied similar way achieve objective balance minimizing execution time maximizing processor efficiencies subject left future work nearly minimize total execution time homogeneous workers b set 1 number task groups g set equal number workers tasks distributed equally among workers could specified user set adaptive code 200 400 motivated fact planning problems range total master execution time increase greatly number iterations decreases significantly table 3 well many experiments omitted conserve space show various planning problems number scenarios number workers b1 g equal number workers running time decreases increase range 25400 due diminishing returns reducing number iterations values larger hundred representative planning problems studied paper improve total atr running time note atr modified allow g exceed change simple principle atr could still make productive use 400 workers still using nearoptimal value 200 400 optimal configuration fixed number homogeneous workers easily adapted case number workers changes execution program case worker currently available given approximately number tasks evaluate time master needs wait results workers minimized current version atr task evaluated single worker valuable large eg 400 work distributed evenly across workers number workers varies 43 heterogeneous grids table 4 shows unless user requests homogeneous worker pool processors allocated atr condor may diverse speeds typically differing factor seven ten also true grid environments equal partitioning work processors particularly good strategy case master processor need wait slowest worker complete leaving faster workers idle better approach would give worker fraction total scenarios n proportional speed instance evaluating n scenarios worker peak processing rate mips would receive nm scenarios evaluate total peak rate summed workers however algorithm computing next iterate master requires ie number subgradients computed per iteration fixed throughout computation work assigned worker must integral number tasks size nt table 4 predicted measured total atr execution time heterogenous workers worker time w sec non adaptive execution time adaptive execution time avg min max measured model number workers measured model number workers used estimated problem size 704 421 2862 3465 3423 50 1066 48 69 10000 703 419 2862 3507 3422 50 1122 48 68 10000 662 418 1382 1435 1396 50 881 48 39 10000 444 214 2152 1773 1637 100 397 72 78 10000 436 214 1533 1460 1319 100 449 68 69 10000 286 136 1388 1375 1073 150 395 75 71 10000 276 138 942 907 685 150 361 75 60 10000 286 137 978 963 665 150 330 75 66 10000 211 168 1021 967 715 200 282 100 71 10000 776 258 1946 2232 50 1143 986 26 49 10000 225 120 959 867 100 478 341 86 45 10000 284 083 960 952 100 678 324 67 29 10000 thus nearoptimal algorithm minimize total execution time given collection heterogeneous worker nodes follows number tasks chosen moderately large eg 400800 create smaller tasks balancing load across heterogeneous workers task contains nt scenarios using benchmark results worker tasks allocated one time workers task earliest expected completion time given task assignments made far way tasks assigned worker proportion execution time benchmark number assigned tasks multiplied time approximately worker assigned least one task workers high benchmark times might assigned tasks workers low benchmark times may assigned multiple tasks since task scheduling algorithm implemented current mw runtime library implemented inside atr application experiment effectiveness also collaborating mw developers implementing feature within mw since tasks assigned worker new iterate created schedule adaptive nature also advantage simplicity although number computational speeds workers may change dynamically run adaptive code yields minimum execution time without taking complex global view runtime environment table 4 shows predicted measured results applying nearoptimal approach first eleven rows show predicted execution time adaptive code workers allocated nonadaptive version atr highly heterogeneous allocations atr runtime reduced factor greater three scheduling strategy adapts worker speeds applied lower part table shows measured predicted execution times experimental implementation adaptive scheme along predicted total execution time runs performed nonadaptive code somewhat less heterogeneous processor pools factoroftwo speedups estimated adaptive code significant speedups anticipated number allocated workers changes greatly atr execution table 5 also shows compared atr execution times parameter settings recommended previous rules thumb new adaptive atr speedups factor four eight homogeneous workers factor three four heterogeneous workers 5 conclusion performed detailed analysis execution atr stochastic optimization code running condor grid environment initial measurements application work well previous work showed highly variable execution times key components algorithm particularly master processor previous work issue addressed creating parallel tasks number workers workers could productively evaluate scenarios long unpredictable master computations however detailed analysis revealed simple mechanisms reducing variability task execution times well complete understanding complex impact configuration parameters total atr execution time using analysis developed applied surprisingly simple performance models determine configurations atr minimize total execution time either static dynamic collections homogeneous heterogeneous workers experiments local condor pool well widely distributed condor flocks indicate total execution time reduced using simple modelbased adaptive execution factors four eight compared nonadaptive execution using previously recommended configuration parameters addition new adaptive atr uses taskscheduling algorithm improve performance parallel grid applications algorithm currently implemented condormw library temporarily isolated master also proposed improvement runtime environment could greatly benefit master worker grid applications ongoing research includes 1 applying atr model complex objectives take account utilization allocated processors well atr execution time 2 developing models control adaptive execution complex codes using approach emphasizes simplicity well accuracy weve used atr 3 improvements atr new heuristics updating model function mx assigning partial tasks workers achieve better load balancing andor higher degrees parallelism evaluating single iterate although development time simple high fidelity analytic model substantial still small fraction time design develop complex code atr potentially used solve many important problems b payoffs model optimizing adaptive execution significant also surmise logp class models 17 reasonable starting point developing modelbased adaptive codes since previous models simple adaptive applications reviewed section 23 well simple model developed paper atr viewed logp models since loggp model complex nonadaptive particle transport code 26 also highly accurate acknowledgements authors thank jeff linderoth many helpful discussions concerning atr solution algorithms help running experiments widely distributed condor flocks jichuan chang significant improvements condormw enabled work table 5 execution time comparisons previous atr ssn n40000 50 workers original atr recommended values b g execution time reduced debug default debug worker pool new adaptive atr execution time homogeneous 61 min 92 min 68 min 149 min r loggp incorporating long messages logp model cactus code problem solving environment grid application level scheduling distributed heterogeneous networks parallel implementation nested decomposition algorithm multistage stochastic linear programs computing blockangular karmarkar projections applications stochastic programming framework automatic adaptation tunable distributed applications logp towards realistic model parallel computation applicationaware scheduling magnetohydrodynamics application legion metasystem worldwide flock condors load sharing among workstation clusters globus project status report building solving largescale stochastic programs affordable distributed computing system enabling framework masterworker applications computational grid legion next logical step toward worldwide virtual computer adaptive scheduling masterworker applications computational grid predictive applicationperformance modeling computational grid environment exposing application alternatives dynamic load balancing samr applications distributed systems decomposition algorithms stochastic programming computational grid mechanisms highthroughput computing autopilot performancedirected adaptive control system cactus application performance predictions grid environment warren applying scheduling tuning online parallel tomography application level scheduling gene sequence comparison metacomputers using apples schedule simple sara computational grid predictive analysis wavefront application using loggp performance contracts predicting monitoring grid application behavior tr computing blockangular karmarkar projections applications stochastic programming logp towards realistic model parallel computation loggp worldwide flock condors parallel implementation nested decomposition algorithm multistage stochastic linear programs application level scheduling gene sequence comparison metacomputers predictive analysis wavefront application using loggp applicationlevel scheduling distributed heterogeneous networks iautopiloti performancedirected adaptive control system applying scheduling tuning online parallel tomography dynamic load balancing samr applications distributed systems framework automatic adaptation tunable distributed applications adaptive scheduling masterworker applications computational grid performance contracts cactus application globus project applicationaware scheduling magnetohydrodynamics application legion metasystem predictive applicationperformance modeling computational grid environment enabling framework masterworker applications computational grid cactus code exposing application alternatives ctr jeff linderoth steve wright coap computational optimization applications v29 n2 p123126 november 2004 e heymann senar e luque livny efficient resource management applied masterworker applications journal parallel distributed computing v64 n6 p767773 june 2004 jeff linderoth stephen wright decomposition algorithms stochastic programming computational grid computational optimization applications v24 n23 p207250 februarymarch