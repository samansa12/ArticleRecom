data relocation prefetching programs large data sets numerical applications frequently contain nested loop structures process large arrays data execution loop structures often produces memory reference patterns poorly utilize data caches limited associativity cache capacity result cache conflict misses also nonunit stride access patterns cause low utilization cache lines data copying proposed investigated order reduce cache conflict misses technique high execution overhead since performs copy operations entirely softwarewe propose combined hardware software technique called data relocation prefetching eliminates much overhead data copying use special hardware furthermore relocating data performing software prefetching overhead copying data reduced experimental results data relocation prefetching encouraging show large improvement cache performance b introduction numerical applications frequently contain nested loop structures process large arrays execution loop structures shown produce memory preference patterns poorly utilize data caches 34 first three problems involves insufficient capacity cache data accessed loop may exceed cache size resulting cache misses limited associativity cache presents second problem accesses different arrays even different elements single array may conflict final problem involves nonunit stride access patterns cause low utilization cache lines wasted bus memory cycles 5 potentially one could use larger cache size higher cache associativity eliminate cache capacity misses cache conflict misses brute force approach however scale well rapidly increasing amount data used sophisticated numerical applications moreover would result significant hardware cost increased cache access latency could avoided via costeffective approach proposed paper use blocking transformations could reduce workingset size data accessed loop nests 4 1 6 reordering execution iterations blocking transformations reduce amount data referenced two references data data accessed two references data reduced amount smaller cache size capacity misses eliminated practice however blocking transformations may reduce cache misses cache mapping conflicts additionally blocking alone reduce workingset size data accessed single loops since data accesses reordered data prefetching also proposed reduce cache misses fetching data cache referenced 7 8 used conjunction small cacheblock sizes one potentially eliminate problem low utilization cache blocks wasted bus cycles 5 however data prefetching may increase size working set introducing capacity misses also prefetched data may conflict current working set cache introducing conflict misses 9 10 order data prefetching improve performance reliable manner one must ensure current future working sets fit cache proposed approach achieves goal compressing current future working sets localized region virtual address space cache mapping conflicts exist among locations region paper presents approach however solve three cache performance problems arraybased applications rather solving problems singly first phase technique consists loop blocking inner loops reduce number array accesses working set second phase insertion special hardware instructions compress working set localized region virtual address space prefetch compressed working set cache compiler also modifies working set accesses references made compressed data cache since array data sequentialized localized region conflict misses eliminated also original data access pattern nonunit stride unused data brought cache compression prefetch resulting improved cacheline utilization computation completed additional instructions decompress modified data relocate back original program arrays order minimize overhead compressing decompressing data compression performed data prefetched memory cache also use compiler transformations compression prefetch next working set overlapped current computation order hide latency relocation using prototype compiler emulation tool simulation tool show extension cache architecture along requisite compiler support greatly improves data cache performance arraybased applications remainder paper organized six sections section 2 describes proposed method describes necessary architecture hardware support section 3 explains compiler transformations data relocation prefetching section 4 simulationbased experimental results provided demonstrate effectiveness proposed architecture related work discussed section 5 finally section 6 offers concluding remarks future directions 2 data relocation prefetching 21 method propose compilersupported hardwarebased technique called data relocation prefetching order improve data cache performance method array references inner loop nest sequentially mapped cache accessed relocation operations invoked explicit instructions compiler inserts compiler also inserts declaration original code relocation buffer allocates space relocated data memory special hardware attached cache unit maps compresses data virtual buffer space relocation performed prefetching data memory cache without stalling cpu order access relocated data instead original array data computation compiler replaces original array references corresponding relocation buffer references array data relocated prefetch binding computation newly assigned address relocation buffer space used access data rather original address conse quently relocation must completed computation data begins relocated cached data replaced original address accesses scalar accesses relocated data must written back relocation buffer memory since accesses computation use address relocated data insure writeback relocated data dirty bit set cache line data allocated computation uses relocated data finished modified relocated data written back relocation buffer original memory locations using explicit machine instruction data relocation prefetching improve locality array accesses loop nest figure 1 shows array data elements accessed first iteration outer loop copied sequential cache locations map relocation buffer memory array accessed stride two array layout memory layout cache relocation buffer figure 1 concept data relocation b accessed column order execution inner loop accesses array elements result poor performance 1 accesses may exhibit spatial locality nonunit access stride resulting wasted cache capacity may lower cache hit rate 2 sets accesses different arrays may conflict happen mapped locations cache 3 accesses single array may conflict large access stride accessed elements arrays relocated cache spatial locality improved packing elements arrays contiguous locations also since necessary elements brought cache extra memory requests time fill cache line due nonunit stride accesses reduced furthermore total size relocated array elements smaller cache size compression guarantees references relocated data conflict cache finally cache space conserved packing elements arrays order reduce instructionfetch overhead due inserted relocation instructions instruction contains enough information operate several elements array sequence also order accommodate latency relocation array data relocation computation phases separated time softwarepipelining outer loop details given section 3 22 architectural support implementing mechanism data relocation prefetching require extra instructions well extra hardware five instructions support data relocation prefetching added instruction set precollect preallocate distribute await finishup 221 precollect precollect instruction five operands collects array data referenced computation consecutive locations cache data needed computation first operand address first element array relocated whereas second operand address first element relocated array relocation buffer third fifth operands size array element bytes stride array accesses bytes number array elements collected information given third fifth operands necessary since scheme relocate entire cache lines rather relocates array elements accessed computation using operands precollect instruction fetches data original addresses stores data cache tag relocated addresses instruction nonblocking stall processor even causes cache misses therefore execution instruction overlapped execution instructions perform computations loop nest 222 preallocate preallocate instruction allocates necessary cache lines instead collecting array data consecutive locations within cache used array data read written computation case need collect data cache computation eliminating overhead fetching data preallocate instruction operands precollect instruction except first operand original starting address fourth operand stride original array accesses needed instruction also nonblocking 223 distribute distribute instruction writes relocated data updated computation back original array locations memory format distribute instruction precollect instruction first operand specifies starting address array elements relocation second operand specifies starting address array data relocation buffer like precollect preallocate instructions distribute instruction stall processor 224 await await instruction provides simple synchronization mechanism avoid accessing relocated array data precollect preallocate operation completed instruction single operand must match second operand associated precollect preallocate instruction precollect preallocate operation completed await operation causes data cache block operation finished 225 finishup loop nest finishes execution may unfinished distribute operations must completed array data accessed subsequent code finishup instruction provides synchronization mechanism insure distribute operations completed blocking data cache operands necessary 23 hardware support execution data relocation prefetch instructions handled special hardware called data relocation prefetch drp unit attached existing cache unit figure 2 drp unit shares mmu cache also shares cache cpu configuration processor cache higher priority drp unit accessing shared resources priority hierarchy helps ensure drp unit doesnt significantly slow execution application program executing precollect preallocate distribute instructions unless await finishup instruction executed highbandwidth memory system like splittransaction bus system used drp unit utilize bandwidth pipelining read write requests memory system data cache configuration necessary support drp unit writeback cache allocate write miss figure 3 illustrates components drp unit data path drp unit designed way cache need block due fetch generated drp instructions insured handling fetches drp unit since cache block writes serviced unless write buffer full writes misses generated drp instructions handled cache instead dedicated drp hardware furthermore drp unit block fetching memory location program execution proceeds precollect preallocate distribute instructions processor placed instruction queue instruction queue full drp unit stalls processor empty entry queue instruction head queue processed completely cpu cache mmu bus memory request data relocation cache unit precollect preallocate distribute await finishup figure 2 data relocation prefetch unit interfaces address generator next may proceed address generator calculates original address array element using starting array address stride information address generator also uses starting relocation buffer address element size calculate relocation buffer address array element pair addresses generated entry stored suboperation queue processing precollect preallocate instructions original addresses stored source address field buffer addresses stored destination field suboperation queue distribute instructions addresses reversed begin processing precollect suboperation source address field used send special read request cache data present cache first stored drp unit data buffer written cache using destination address also suboperation removed queue since finished data present cache cache send read request fetch data instead drp unit sends read request mmu using source address point next suboperation suboperation queue begins processing pending suboperation left queue processing suboperation queue enough entries queue block suboperation waiting memory access completed data returns memory read buffer source address fields suboperation queue searched associatively using address read buffer obtain destination address size data suboperation appropriate cache line allocated data read buffer written cache using destination address size blocks written relocated data cache marked dirty instruction queue suboperation queue data element counter cache unit id counter source addr dest addr size id array addr buffer addr size data address generator counter precollect preallocate distribute instructions await instructions bus mmu read buffer address data type type block cache buffer addr data addr size data addr size port read port data buffer figure 3 data relocation prefetch unit data path begin processing preallocate suboperation source address field used access cache tag store address misses cache cache line allocated relocated data array data fetched cache line allocated marked clean suboperation removed suboperation queue preallocate suboperation must finish relocated address written computation order insure correct results obtained program execution distribute suboperation relocated data read cache using source address field entry head distribute queue data present cache placed drp unit data buffer data present cache occur nonrelocated data conflicts relocated data cache cache fetch data instead drp unit sends read request mmu using source address point next suboperation queue begins processing pending suboperation left queue processing data returns memory address read buffer used associative search source address fields suboperation queue destination address size fields next write request sent cache using destination address data either read buffer data buffer depending whether read relocated data produced cache miss hit respectively suboperation removed queue data written original address cache write hit occurs memory write miss occurs instruction passed instruction queue address generator entry allocated data element counter total number data elements used initialize counter suboperation queue finishes processing data element element counter decremented one therefore preallocate precollect distribute instruction corresponding nonzero data counter entry execution await instruction causes data cache block data counter entry matching buffer address identifier reaches zero finishup instruction causes data cache block data counters contain zero interlock mechanism insures precollect preallocate instruction begin execution previously issued distribute instructions use relocation buffer completed execu tion identifier buffer address instruction head instruction queue used search data counter associatively instruction identifier still executed address generation delayed previous instruction completed 3 compiler support compiler support essential transforming source code proposed scheme improve cache performance impact research prototype compiler 11 supports highlevel transformations optimizations superscalar vliw optimizations scheduling 12 well classical machinespecific optimizations data relocation prefetching optimization applied directly loops nested least two deep figure 4a shows example loop nest illustrates different array access patterns invocation innermost loop array data accessed innermost loop first relocated prefetched computation proceeds inner loop finished transformation always applied two innermost loops loop nests nested deeply two case refer outer two innermost loops outer loop innermost loop inner loop highlevel code transformations employed data relocation prefetching loop unrolling insertion drp operations replacement array references relocation buffer references loop blocking 31 loop unrolling order overlap data relocation prefetching next outerloop iteration computation current iteration relocation prefetching phase softwarepipelined computation phase softwarepipelining scheme requires two relocation buffers inner loop duplicated unrolling outer loop shown figure 4b first outerloop body data relocation proceeds second relocation buffer computation performed using data already relocated first buffer second outerloop body method used first outerloop body except buffers switched data dependence information provided omega test 1314 used insure validity softwarepipelining data relocation computation phases 32 insertion operations new operations inserted sourcelevel code perform precollect preallocate await distribute finishup operations see figure 4c highlevel operations replaced corresponding machine instructions assembly code level one precollect andor preallocate operations inserted inner loop order relocate case precollect operation fetch data accessed next iteration original outer loop two operations nonblocking execution overlapped computation subsequent inner loop figure 4c 0 pointers relocation buffer starting addresses elements arrays b c respectively accessed first unrolled outerloop body 00 b 00 c 00 point relocation buffer starting addresses array elements accessed second unrolled outerloop body singledimensional datacachesized array divided two equal parts represent two relocation buffers declared entire loop nest pointer variables new locations initialized starting addresses relocated arrays one await operations inserted inner loop insure precollect preallocate operations completed innerloop array computation begins distribute operations inserted inner loop order restore updated data buffers original location memory finally finishup operation placed outer loop order insure distribute operations completed execution proceeds computations may involve array data 33 replacement array references buffer references relocation prefetching operations inserted array references computation within inner loop modified buffer locations accessed instead original array locations final version example loop transformations completed shown figure 4c original loop nest i0 j0 jn j b loop unrolling i0 i2 first outer loop body j0 jn j second outer loop body j0 jn j c relocation operation insertion first outer loop body preallocatec 8 n i0 i2 first outerloop body second outerloop body precollectb0i1 b 8 bd n preallocatec 8 n j0 jn j second outerloop body first outerloop body precollectb0i1 b 8 bd n preallocatec 8 n j0 jn j figure 4 example loop unrolling insertion relocation prefetching operations 34 loop blocking loop blocking optimization technique partitioning iteration space increase data reuse cache thereby reduce number cache misses data relocation technique blocking inner loop used reduce amount data relocated innerloop computation amount large fit cache consequence single loops often blocked even though access pattern data loop body affected blocking transformation data dependence information provided omega test used determine loop blocking valid transformation loop nest 4 experimental evaluation section effectiveness data relocation prefetching optimization technique evaluated simulations set arraybased benchmarks 41 methodology 411 benchmark programs benchmarks study consist six numeric programs adm ocean arc2d perfect 15 matrix300 spec89 tomcatv nasa7 spec92 benchmarks profiled looplevel obtain number invocations number iterations loops order apply transformations selectively effectively loop nests contain multiple inner loops return goto break statements excluded well loopnests contain subroutine calls possible sideeffects data dependence analysis also used exclude loops specific crossiteration dependences prevent necessary transformations 412 transformation correctness verification via emulation order provide realistic evaluation drp technique first optimize code using impact compiler classical optimizations applied ilp increasing optimizations loop unrolling superblock formation performed code scheduled register allocated optimized fourissue scoreboarded superscalar processor register renaming register file contains 64 integer registers 64 doubleprecision floatingpoint registers four functional units pipelined execute type instruction verify correctness code transformations emulation generated code performed hewlettpackard parisc 7100 workstation precollect distribute instructions emulated using machine language subroutines perform data relocation memory memory instead cache thus transformed code must relocate data reference using correct addresses emulation produce valid results timing simulation also needed verify correctness detailed operation drp unit since emulated drp instructions stall processor completed function latency function latency memory load 2 fp multiply 2 memory store 1 fp divide sgl 8 branch table 1 instruction latencies simulation experiments 413 simulation experiments emulator drives simulator models processor drp unit determine execution time cache performance bus utilization simulation latencies used hewlettpackard parisc 7100 microprocessor given table 1 processor model includes separate instruction data caches directmapped 8kbyte blocking caches 16byte block size data cache multiported writeback writeallocate cache satisfies four load store requests per cycle processor 8entry write buffer combines write requests cache line streaming data load misses minimizes load miss penalty instruction cache data cache share common splittransaction memory bus 64 bitscycle data bandwidth pipelined memory model used 10cycle latency directmapped branch target buffer 1024 entries used perform dynamic branch prediction using 2bit counter hardware speculation supported branch misprediction penalty approximately two cycles simulation model drp unit based description section 2 however infinitelysized queues buffers modeled blocking occurs drp unit due insufficient entries also data cache one read one write port dedicated service drp unit accesses since simulating entire benchmark programs level detail would impractical uniform sampling used reduce simulation time 16 samples 200000 instructions length spaced evenly every 20000000 instructions yielding 1 sampling ratio benchmarks used billion dynamic instructions least 50 samples thus 10000000 instructions simulated smaller benchmarks spacing reduced maintain least 50 samples 10000000 instructions experience emulationdriven simulator determined sampling least 50 samples introduces little error performance estimates typically statistics generated sampling within 1 generated without sampling 42 experimental results order show full performance benefit drp technique experimental results presented benchmark program loop nests modified drp transformation results entire transformed execution benchmark loop nest percentage filery35j 24 filery38j 23 arc2d tk1j 26 tkinv1j 27 rhsy20j 22 rhsy30j 21 rhsx200j 22 ocean in10 112 main501i 144 cholsky8l 10 copy100 31 table 2 original code loopnest execution time percentage entire program execution time drp transformed loopnests benchmarks also presented evaluate current state compiler techniques loop nest selection application drp technique 421 individual loop nest results performance statistics individual loop nests obtained marking drptransformed loop nests execution regions simulation consequently simulating execution entire program sufficient gather results transformed loop nests context entire program execution table 2 shows original code execution time loop nests selected drp technique percentage total original code execution time transformed loop nests identified function name fortran outer doloop number loop iteration variable necessary figure 5 illustrates speedups original loop nest execution time drptransformed loop nest execution time measured speedup loop nests relatively large demonstrating high performance improvement obtainable using data relocation prefetch technique loop nest arc2d ocean tomcatv matrix300 nasa7 figure 5 speedup drpoptimized code original code loop nests figure 6 shows data cache miss ratio original code drpoptimized code loop nests since writeallocate cache used experiments miss ratios original optimized code calculated dividing number cache read misses number cache read requests original code method calculating cache misses assures fair comparison number cache accesses transformed code different number original code ratio include requests initiated drp unit since cache reads stall cpu minimal impact performance note cache misses nearly eliminated loop nests since compiler relocates array data referenced loop nest possibilities cache misses occur scalar variables accessed loop nest prefetched may conflict relocated data causing cache misses several loop nests arc2d benchmark contain innerloop scalar accesses contribute nonzero cache miss rations stepfy430 tk1j tkinv1j rhsx200j rhsx200j eigval100j another cause cache misses additional memory accesses introduced register spill code illustrated loop nests eigval100j ypenta13 execution speedup obtained loop nests always highly correlated reduction cache miss ratio complexity scheduling multipleissue processor memory bus utilization transformed original loop nests displayed figure 7 drp unit utilizes unused bus cycles lower priority cache accessing bus cases however precollect preallocate instructions completed time await instruction issued aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa loop nest cache miss 100 200 300 400 500 700 800 900 1000 aaaa aaaa aaaa original code drp adm arc2d ocean tomcatv matrix300 nasa7 figure miss ratio drptransformed code original code loop nests loops insufficient computation overlap latency precollect operations even though operations moved back across one iteration outer loop software pipelining thus precollect always waiting use bus free resulting near 100 bus utilization loops figure 8 provides evidence effect displaying percentage execution cycles await instruction stalls processor transformed loop nests notice loop nests 95 bus utilization also large percentage cycles stalled awaits 95 bus utilization fewer stalled cycles reduce await cycles improve performance precollect preallocate operations must executed earlier one iteration corresponding computation 422 entire benchmark results speedups simulated execution drptransformed code original code six benchmarks given table 3 program execution speedup benchmarks except matrix300 tomcatv relatively small adm nasa7 arc2d small speedup attributable fact percentage execution time spent transformed loop nests relatively small since used overlyconservative loop nest transformation criterion guarantee correctness transformation aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa loop nest bus utilization 100 200 300 400 500 700 800 900 1000 aaaa aaaa aaaa aaaa drp original code adm arc2d ocean tomcatv matrix300 nasa7 figure 7 bus utilization drptransformed code original code loop nests loop nest await stall 100 200 300 400 500 700 800 900 1000 36 88 73 500 62 33 432 24 09 509 18 24 40 25 16 16 93 32 33 306 47 290 adm arc2d ocean tomcatv matrix300 nasa7 figure 8 await stall overhead drptransformed code original code loop nests transformed execution time benchmark loop nest billions cycles execution execution base drp speedup arc2d 319 9248 8300 111 ocean 198 11165 10512 106 adm 22 1732 1703 102 matrix300 963 3181 1586 201 table 3 speedup drptransformed code original code entire benchmarks aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa aaaa benchmark cache miss 100 200 300 400 500 arc2d ocean adm tomcatv matrix300 nasa7 63 215 373 62 54 155 263 aaaa aaaa aaaa original code drp figure 9 data cache miss ratio original code drptransformed benchmarks ocean processor stalls due await instructions severely restrict performance benefit drp technique discussed section 421 speedup measurements given table 3 match closely entire benchmark speedups calculated using loop speedups figure 5 loop execution percentages table 2 except tomcatv benchmark tomcatv whole program speedup exceeds weighted sum loop speedups nontransformed loops benefit cache effects transformed loops although transformed loops accesses enough array data replace data entire cache transformation substantially reduces space used cache array data since data replaced cache original code extensive reuse opportunities transformed code allowed data effectively reused reuse results much better cache performance important nontransformed loop nest due reduction interarray conflict misses data cache miss ratios show promising improvement almost benchmarks shown figure 9 bindingprefetching mechanism drp technique guarantees necessary data computation resides cache also compression data relocation buffer increases utilization cache data reside cache time 5 related work technique called data copying proposed investigated order reduce cache conflict misses 12 data copying however beneficial performance improvement outweighes overhead copying reusing data many times overhead copying data array array significant general proposed method data relocation prefetching benefits data copying reducing overhead copying furthermore unlike data copying data relocation prefetching relocates data cache prefetching minimizes overhead relocation precollect distribute operations conceptually similar gather scatter operations used cray1 17 cray1 array elements gathered memory vector registers performing vector operations scattered back memory vector operations complete however hardware necessary support data relocation prefetching would much easier add existing processor hardware support vectorization 6 conclusions architectural extension referred data relocation prefetching proposed perform data relocation compression prefetching data relocation used remove arraydata mapping conflicts compressing accesses loop nest sequential locations cache compression also improves utilization cache transforming nonunit stride array column accesses sequential accesses require fewer cache lines storage furthermore reduction cache space used hold data loop nest increase data reuse across transformed loop nests combining data relocation prefetch hardware supporting compiler transformations performance loop nests greatly improved set arraybased benchmarks also shown application data relocation prefetching optimization greatly improves cache performance performance measurements entire benchmarks motivate need future research tuning data relocation prefetch techniques precollect preallocate instructions need moved back across one iteration outer loop order reduce processor stalls caused await instruction addition compiler transformations expanded improved order transform loop nests effectively experiments warranted study performance optimization using various implementation parameters drp hardware r cache performance optimizations blocked algorithms copy copy compiletime technique assessing data copying used eliminate cache conflicts characteristics parallel programs impact hierarchical memory systems linear algebra design data prefetching multiprocessor vector cache memories compiletime partitioning iterative parallel loops reduce cache coherence traffic effective onchip preloading scheme reduce data access penalty design evaluation compiler algorithm prefetching data access microarchitectures superscalar processors compilerassisted data prefetching tolerating data access latency register preloading impact architectural framework multipleinstructionissue processors superblock effective technique vliw superscalar compilation practical algorithm exact array dependence analysis eliminating false data dependences using omega test perfect club benchmarks effective performance evaluation supercom puters simulate 100 billion references cheaply cray1 computer system tr cache performance optimizations blocked algorithms data prefetching multiprocessor vector cache memories impact data access microarchitectures superscalar processors compilerassisted data prefetching effective onchip preloading scheme reduce data access penalty practical algorithm exact array dependence analysis eliminating false data dependences using omega test tolerating data access latency register preloading design evaluation compiler algorithm prefetching copy copy superblock cray1 computer system compiletime partitioning iterative parallel loops reduce cache coherency traffic ctr zhong wang timothy w oneil edwin hm sha minimizing average schedule length memory constraints optimal partitioning prefetching journal vlsi signal processing systems v27 n3 p215233 march 1 2001 fei chen timothy w oneil edwin hm sha optimizing overall loop schedules using prefetching partitioning ieee transactions parallel distributed systems v11 n6 p604614 june 2000 rakesh kumar dean tullsen compiling instruction cache performance multithreaded architecture proceedings 35th annual acmieee international symposium microarchitecture november 1822 2002 istanbul turkey lixin zhang mike parker john carter efficient address remapping distributed sharedmemory systems acm transactions architecture code optimization taco v3 n2 p209229 june 2006 luddy harrison examination memory access classification scheme pointerintensive numeric programs proceedings 10th international conference supercomputing p133140 may 2528 1996 philadelphia pennsylvania united states weifeng zhang brad calder dean tullsen selfrepairing prefetcher eventdriven dynamic optimization framework proceedings international symposium code generation optimization p5064 march 2629 2006 timothy sherwood brad calder joel emer reducing cache misses using hardware software page placement proceedings 13th international conference supercomputing p155164 june 2025 1999 rhodes greece