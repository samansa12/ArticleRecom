data mining criteria treebased regression classification paper concerned construction regression classification trees adapted data mining applications conventional trees end propose new splitting criteria growing trees conventional splitting criteria attempt perform well sides split attempting compromise quality fit left right side contrast adopt data mining point view proposing criteria search interesting subsets data opposed modeling data equally well new criteria split based compromise left right bucket effectively pick interesting bucket ignore otheras expected result often simpler characterization interesting subsets data less expected new criteria often yield whole trees provide interpretable data descriptions surprisingly flaw works advantage new criteria increased tendency accept splits near boundaries predictor ranges socalled endcut problem leads repeated peeling small layers data results unbalanced highly expressive interpretable trees b introduction tree methods applied two kinds predictive problems regression trees used predict continuous response classification trees used predict class label goal tree methods partition data small buckets response value regression tree class label classification tree well predicted subset figure 1 shows partitions regression tree left panel classification tree right panel left panel data partitioned two subsets order explain response vertical axis best possible means subsets right panel data partitioned two subsets order explain class labels 123 best possible andreas buja technology consultant att labs research 180 park ave po box 971 florham park nj 079320971 andreasresearchattcom httpwwwresearchattcomandreas yungseop lee graduate student department statistics rutgers university hill center mathematical sciences busch campus piscataway nj 08855 regression tree x classification tree x class figure 1 partitions regression classification tree 11 structure trees data repeatedly partitioned even smaller subsets means binary splits thus tree described diagram figure 2 node represents subset data pair daughter nodes represents binary split subset corresponding parent node2 b figure 2 two predictors example standard trees partition data binary splits along predictor variables two daughter subsets parent subset obtained splitting fixed value fixed threshold example x goes left daughter x 1 1 goes right daughter figure 2 illustrates tree repeated splits x 1 respect 1 x 2 respect 2 x 1 respect 3 geometry resulting partitions illustrated figure 2 b 12 tree construction greedy optimization splits one repeatedly searches best possible split subset searching possible threshold values variables optimization according impurity measure discussed detail sections 3 4 regression trees impurity measured variants rss classification trees impurity measured example misclassification rate entropy gini index one stops splitting subset significant gain impurity obtained subset forms terminal node leaf tree since cart breiman et al 1984 sizing trees somewhat complex one tends grow first overly large tree prune back second stage reason greediness tree construction look ahead one step may hence miss successful splits line overgrowing pruning trees may therefore find optimal trees growing alone 13 splitting criteria data mining tree growing strategy specified defining measure impurity split done defining measure impurity left right bucket split combining two measures overall impurity measure split conventional splitting criteria left right combined weighted sum two sides effect one compromises left right buckets taking sizes account new data mining criteria however interested modeling data equally well rather content long find subsets data interesting therefore propose combining impurities left right buckets split way split occurs one bucket alone low impurity regardless bucket low impurity one bucket alone causes low value impurity measure split data mining criteria splits need developed regression classification trees separately section 3 deals regression section 4 classification 2 data trees 21 grow trees recently tree methods applied many areas data mining sometimes even data suitable tree methods general growing trees useful dependence predictors heterogeneous complex interactions present dependency heterogeneous complex multiple local structures handled tree methods models handle single prominent structure strength trees detecting interactions effectively advertized names first tree methods interaction detection morgan sonquist 1963 chisquarebased aid hartigan 1975 prediction accuracy trees suffer following situations ffl regression trees may fail dose response present term dose sponse borrowed biostatistics refers gradual monotone dependence opposite dependence threshold effects ffl classification trees may fail optimal decision boundaries aligned way variable axes hence splits variables able make full use information contained predictors interpretability trees hand may suffer presence highly correlated predictors correlated predictors substitute appear less random alternation fact one predictors would suffice effect added complexity interpreting data analyst even worse possibility misleading conclusions separate distinct effects attributed predictors even though share effects due correlations 22 example grow trees waveform data breiman et al 1984 p 49 134 example unsuitable data tree growing data generated artificially 21dimensional predictors three classes whose distribution predictor space analytically described follows numbers u ffl independently distributed according uniform distribution 0 1 gaussian zero mean unit variance respec tively three 21dimensional vectors h irrelevant except fact form obliquely placed triangle sides length roughly 115 115 165 shape resulting pointcloud three sausages ends linked together reflected projection onto first two principal components shown figure 3 reason tree methods successful data optimal classification boundaries aligned coordinate axes authors mention p 55 geometry data immediate linear discriminant analysis outperforms trees thus applying tree methods blindly one perform preliminary exploratory analysis determine type classification procedure make best use available information data figure 3 distribution class waveform data 23 uses trees two principles must balanced using tree growing algorithms accuracy prediction 2 interpretation prediction problems one wants grow trees produce accurate classifiers based training samples generalize well test samples often one also wants grow interpretable trees trees whose nodes represent potentially useful meaningful subsets interpretation top tree usually valuable part top nodes described using conditions general step tree adds new inequality description node exceptions see also helpful although essential interpretation top nodes tree tend statistically less variable lower level nodes 24 simplicity interpretability trees present work concerned construction methods favor interpretable trees attempt find methods search meaningful nodes close top tree possible emphasizing interpretability nodes near top deemphasize precise calibration tree depth prediction stopping criteria growing pruning trees low priority purposes interpreting data analyst always ignore lower nodes tree grown deeply harmless long analyst interpret statistical flukes lower nodes deemphasizing lower parts trees may sacrifice global accuracy prediction remark simplicity interpretability trees order although literature bias favor balanced trees balance interpretability different concepts exists type extremely unbalanced trees highly interpretable namely use variable repeatedly cascade splits see figure 4 example simplicity tree stems fact nodes described one two conditions regardless tree depth tree fragments similar example appear often trees fitted data exhibit dose response example figure 4 apparent mean values nodes response shows monotone increasing dependence predictor x kind tree structure found data analyst may want consider linear additive models describe monotone dependence direct way hand dose response may present locally case tree may still successful description data linear additive model illustrate boston housing data new data mining criteria uncover local dose response new criteria splitting sometimes generate trees less balanced yet interpretable conventional balanced trees contradiction preceding considerations show x 7x 4 figure 4 artificial example simple interpretable yet unbalanced tree 3 regression trees 31 conventional criteria regression trees mentioned section 11 trees grown recursively splitting data based minimization splitting criterion conventional splitting criteria compromises impurities left right buckets impurity measure choice buckets simply variance compromise splitting criterion weighted average variances function thereof left right buckets need notation means variances left right buckets denoted r nr r nr compromise split derived maximum likelihood estimation combined gaussian model two buckets iid nl oe 2 l left bucket iid nr oe 2 r right bucket cart equal variances assumed necessity assumption give criteria equal nonequal variances minimizing negative log likelihood models following splitting criteria result equal variance model cart crit lr nl nr n l oe 2 n l log l n r log oe 2 makes precise sense conventional criteria compromise left right bucket minimization negative log likelihood straightforward anyway equal variance case min l r oe gammalog likelihoodl oe l 2 pooled variance estimate thus minimizing yields splits minimizing negative log likelihood nonequal variance case get min l oe l r oe r gammalog likelihoodl r 1log constants dropped affect minimization splits 32 data mining criteria regression trees circumstances conventional criteria tree growing unable immediately identify single interesting buckets interesting could mean buckets high mean value response high purity terms small variance response kind situations mind depicted figure 5 left hand plot shows response subset small variance right right hand plot subset extreme mean right plain cart criterion deal properly left hand situation assumes equal variances right hand situation cart find split middle around x 300 whereas may interest finding small bucket extremely high mean right approach identify pure buckets oe 2 small extreme buckets ex treme quickly possible example ignore left bucket right side interesting r small r high low thus compromising anymore left right buckets order generate type tree use two new criteria splitting x 55x 226 figure 5 examples pure extreme buckets left around x200 find pure bucket small oe 2 right around x600 find extreme bucket large mean criterion searching one pure bucket early possible end rather using weighted average two variances criterion crit minimizing criterion possible splits finding split whose left right side single bucket smallest variance purity note subsequent split highpurity bucket ignored bucket candidates splitting thus ignored buckets get chance highpurity buckets split later typically highpurity bucket unlikely split result trees grown criterion tend purposely unbalanced ffl new criterion 2 onesided extremes high low value response criterion searching high mean response variable early possible dual criterion would searching low means criterion radical departure conventional approaches notion purity never questioned far means never thought splitting criteria although often greater interest variances point view minimizing variancebased criterion circuitous route searching interesting means mean criterion propose maximizes larger means left right bucket crit implicitly bucket smaller mean ignored maximizing criterion possible splits finding split whose left right side single bucket highest mean point natural criticism criteria may arise potentially excessive greediness trees built criteria may miss important splitting variables may instead capture spurious groups periphery variable ranges thus exacerbating socalled end cut problem criticism grain truth paints situation bleaker true important splitting variables generally missed depends data hand whether conventional newly proposed criteria successful criterion searches onesided extremes example successful extreme response values often found periphery variable ranges reason monotonicity many response surfaces end cut problem preference cuts near extremes variable ranges shared treebuilding strategies see breiman et al 1984 p313 nothing peculiar new criteria second criticism lack balance trees constructed criteria superficially one might expect balanced trees interpretable unbalanced ones defeating rationale criteria concern unfounded though show real data examples cases criteria succeed produce interpretable trees due simple rationale underlies finally recall present goal devise methods produce superior fit methods enhance interpretability therefore concerned usual problems tuning treedepth fit stopping pruning strategies course interpretation tree user simply ignores lower level nodes longer interpretable 4 classification trees 41 conventional criteria classification trees consider twoclass situation class labels denoted 0 1 given split left right buckets let p 0 probabilities 0 1 left right respectively conventional loss impurity notions buckets ffl misclassification rate left bucket minp 0 breiman et al 1984 p98 implicitly one assigns class bucket majority vote estimates misclassification rate proportion class ffl entropy information gammap 0 l probabilities estimated relative frequencies denoted abuse notation entropy also interpreted mingammalog likelihoodn l multinomial model left bucket nl size left bucket breiman et al 1984 p103 ffl gini index p 0 l terms estimates essentially mean square fitting mean n 2 f0 1g n 2 l short calculation shows l number 0s n 1 l number 1s left bucket020600 05 10 misclassification gini index figure impurity functions buckets left right misclassification rate entropy gini index function p 0 l example impurity criteria buckets conventionally blended impurity criteria splits forming weighted sums left right buckets thus compromising left right denoting pl marginal probabilities left right bucket compromise takes following form misclassification rate entropy r log r log gini index impurity functions buckets depicted figure 6 several desirable properties given node take maximum value classes equal proportions minimum value bucket members class also functions symmetric hence two classes treated equally assuming equal priors among three impurity functions misclassification rate problematic may lead many indistinguishable splits may intuitively preferable others problem illustrated example breiman et al 1984 p96 general reason linearity misclassification rate either half unit intervalsee left plot figure 6 linearity implies data shifted within limits left right buckets without changing combined misclassification rate split following consideration gives idea large number potential splits equivalent misclassification rate consider misclassification count nlminp 0 rate training data estimate probabilities relative frequencies denote n l l n l r counts minority class losers left right correspondingly n w l n w r counts majority class winners left right l r n w r 1 l 6 w 2 f0 1g note r 2 misclassification count l r n w r counts left right bucket respectively r n w r 3 count parent bucket fixed value misclassification count exist many combinations n l r satisfying conditions 1 2 3 example fixed exist large number combinations general exist combinations fixed n amounts 1281 possibilities fixed n maximum number combinations n attained misclassification count figure 7 shows number combinations functions missclassification count number combinations parent bucket size 100 figure 7 number combinations misclassification count fixed n100 considerations suggest misclassification rate used splitting criterion suffer considerable nonuniqueness minimizing splits considering two examples equivalent splits however one observes quickly one two splits usually preferable reasons misclassification rate example among two equivalent splits resulting combinations n l respectively latter clearly preferable provides one large bucket r completely pure root problem piecewise linearity misclassification rate minp 0 r right bucket therefore need impurity function accelerates toward zero decreases faster linearly proportion dominant class bucket moves closer 1 rationale using concave impurity functions entropy gini index cart breiman et al 1984 salford systems 1995 uses gini index c45 quinlan 1993 splus venables ripley 1994 use entropy twoclass case seem exist clear difference performance entropy gini index multiclass case however recent work breiman 1996 brought light difference gini entropy criterion gini index tends assign majority class one pure bucket exists rest classes bucket tends form unbalanced well distinguishable buckets entropy hand tries balance size two buckets according breiman results deficiencies entropy shared gini index 42 data mining criteria classification trees mentioned section 32 approach tries identify pure extreme buckets quickly possible criteria regression trees based variances means ones classification trees based probabilities class 0 1 goal therefore restated searching buckets one class probabilities p 0 either left right bucket necessarily another approach select one two classes 1 say look buckets purely class 1 example medical context one might want quickly find buckets show high rates mortality high rates treatment effect section 32 use two criteria splitting corresponding two approaches described criterion searches pure bucket regardless class early possible crit equivalent crit minp 0 example monotone transformations criteria also equivalent crit b one p 0 l maximum minimum latter criterion expresses idea pure buckets directly ffl new criterion 2 onesided extremes chosen class 1 class interest criterion searches pure class 1 bucket among l r crit identical crit p 0 l example note criteria direct analogs new data mining criteria gression shown following table regression trees classification trees onesided purity minoe 2 onesided extreme max 5 endcut problem high variability small buckets lead chance capitalization breiman et al p313 ff optimization splits take advantage randomly occurring purity small buckets implication splitting methods lead extremely unbalanced splits problem even greater data mining criteria look buckets individually without combining sizeweighted average cart usual cart criterion small buckets higher variability downweighted according size illustration endcut problem simulated simple case unstructured regression data case theoretically examined breiman et al 1984 theoretically cuts equal merit empirically endcut preference finite samples emerges known figures merit simulation necessary theoretical consideration cart criterion carry new data mining criteria thus generated set 200 observations gaussian distribution zero mean unit variance computed split buckets size 5 cart criterion rssl r new onesided purity criterion minoe l oe r computed optimal split locations criteria scheme repeated 10000 times optimality split location tallied histogram first histogram figure 8 shows frequency split location minimum cart criterion second histogram shows onesided purity criterion two figures show extent criteria prefer cut locations closer either extreme clearly effect pronounced onesided purity criterion criteria require measures counteract effect onesided purity criterion split location minimum total rss data mining criterion 1 split location minimum purity criterion figure 8 illustration endcut problem approach solving endcut problem penalization buckets small size criterion penalty terms make small buckets less viable penalization best understood criterion interpreted negative log likelihoods suitable model case literature offers large number additive penalty terms c p mallows c p statistic aic akaikes information criterion bic schwarz bayesian information criterion mdl minimum description length criterion among others present paper work aic bic criteria reasons popularity aic penalty adds effective number estimated parameters negative log likelihood whereas bic adds number estimated parameters multiplied logn2 applying penalties individual bucket obtain constant gaussian multinomial models underlie regression classification trees model log likelihood aic bic regression gaussian nlog classification multinomial n entropy stage raise important point intended use aic bic penalties conventionally penalties used model selection context one applies multiple models fixed dataset unconventional situation however apply one fixed model multiple datasets namely variablesized nr nl buckets data part larger dataset ensuing problem log likelihood comparable across different bucket sizes log likelihood unbiased estimate bucket size times divergence model respect actual distribution therefore comparability across bucket sizes gained one uses average log likelihood unbiased estimate divergence across bucket sizes log p z log p xdqx rest section n denotes generic sample size bucket size order avoid subscripts nl nr consequence penalized average log likelihood penalty term also divided bucket size model ave log likelihood 1 regression gaussian 1log classification multinomial entropy penalty terms 2 n log n n 1log n n monotone decreasing n 3 converge zero n 1 behaviors obvious requirements additive penalties penalized value associated bic bigger associated aic except small buckets illustrated figure 9 unfortunately even though approach produces intuitively pleasing penalties performance experiments somewhat disappointing expect however approach perform better recent results jianming ye 1998 taken account light yes insights plausible penalties number parameters model replaced yes generalized degrees freedom gdf take consideration fact extensive searching implicitly consumes degrees freedom gdfs tend considerably higher conventional dfs examples following sections counteract endcut problem imposing minimum bucket size roughly 5 overall sample size 6 example regression trees boston housing data following breiman et al 1984 demonstrate application new data mining criteria boston housing data wellknown data originally bucket size penalized value aic penalty bic penalty figure 9 plot 1 1log n except small size n created harrison rubinfeld 1978 also exploited belsley kuh welsch 1980 quinlan 1993 harrison rubinfelds main interest data investigate air pollution concentration nox affects value single family homes suburbs boston although nox turned minor factor data frequently used demonstrate new regression methods boston housing data available uc irvine machine learning repository httpwwwicsuciedumlearnmlrepositoryhtml data contain median housing values response 13 predictor variables 506 census tracts boston area predictors displayed table 1 61 comparison criteria regression trees constructed several trees based cart new data mining cri teria facilitate comparisons trees generated equal size namely terminal nodes minimum bucket size 23 chosen 5 overall sample size 506 pruning applied interest interpretation opposed prediction results displayed figures 10 variable description crim crime rate zn proportion residential land zoned lots 25000 sq ft indus proportion nonretail business acres chas charles river dummy variable 1 tract bounds river 0 otherwise nox nitric oxides concentration pphm rm average number rooms per dwelling age proportion owneroccupied units built prior 1940 dis weighted distances five boston employment centers rad index accessibility radial highways tax fullvalue property tax rate per 10000 ptratio pupil teacher ratio proportion blacks lstat percent lower status population response median value owner occupied homes 1000s table 1 predictor variables boston housing data 15 summarized table 2 figures mean response size sz given node perusing six trees summaries sequence worthwhile return cart tree figure 10 beginning apply lessons extreme means trees last two figures 14 15 recognize cart tree rm dominant variable particular rm 7 indicating monotone dependence rm large values rm rm 7 lstat takes learnt high means tree rm 7 exists monotone decreasing dependence lstat cart tree tries tell story prejudice favor balanced splits incapable successively layering data according ascending values lstat split lstat second level divides buckets size 51 34 left bucket divides lstat comparison high means criterion creates level 3 split lstat buckets sizes 9 77 clearly indicating left bucket first half dozen tree rings ascending order lstat descending order housing price summary appears us least interpretable trees first two corresponding cart criterion separatevariances criterion although 08 highest among six trees greater interpretability gained onesided purity criterion partly due fact successively peels many small buckets resulting less balanced yet telling tree greatest lstat536 b3804 crim748 57 89 m21 47 m24 87 67 49 65 m21 47 45 m17 87 m15 71 55 81 m35 45 45 m45 59 m23 sz1000 850 m23 510 m25 281 m24 134 m21 m15 340 m17 204 m16 158 136 m37 150 91 boston housing pooled variance model cart figure 10 boston housing data tree 1 cart criterion dis336 rm706 age932 47 m220 105 m260 85 57 m340 45 61 m230 81 m200 57 m210 71 59 m200 55 m170 45 81 m140 51 m120 49 99 47 sz1000 401 340 190 103 599 269 m200 130 330 101 148 m110 97 boston housing separate variance model figure 11 boston housing data tree 2 separate variance criterion age221 51 m27 146 53 m34 45 m46 53 61 51 61 55 m11 53 47 67 47 71 45 m17 89 m23 sz1000 m23 911 m24 866 m25 747 411 m31 m28 296 m27 200 285 m18 m17 156 m15 109 m16 boston housing raw onesided purity figure 12 boston housing data tree 3 onesided purity criterion m24 65 79 m23 61 55 m35 49 m28 49 m46 51 m21 67 89 49 85 m11 45 47 71 45 m17 89 m23 sz1000 m23 911 m24 866 m25 747 411 m24 206 m25 144 m35 206 154 m32 99 m18 269 138 m16 130 m16 boston housing data penalized onesided purity figure 13 boston housing data tree 4 penalized onesided purity criterion lstat622 m36 47 m27 87 m25 79 47 m21 57 59 m21 53 m21 55 85 m18 51 m18 57 m15 47 m16 45 m11 130 m33 45 m45 51 m23 sz1000 m21 949 m21 901 856 769 m18 690 m24 105 m17 585 m17 526 m16 m16 417 m15 281 m13 boston housing onesided extremes high figure 14 boston housing data tree 5 high means criterion lstat141 ptratio202 m13 47 m23 63 m25 57 m24 47 49 59 m34 47 m41 89 73 m21 87 53 67 m17 77 m15 85 45 51 m23 sz1000 m23 949 m24 901 m24 856 m25 771 694 m27 626 m27 573 486 413 m31 m24 105 m34 196 m39 136 boston housing onesided extremes low figure 15 boston housing data tree 6 low means criterion r cart pooled variance model somewhat balanced tree depth 6 major variables rm 3x lstat 6x minor variables appearing nox crim b ptratio dis indus splits mostly expected directions means terminal nodes vary 4510 1020 tree 2 nl log r separate variance model balanced tree cart tree depth 5 major variables rm 4x lstat 3x minor variables doesnt appear splits expected directions means terminal nodes vary 4510 994 tree 3 minoe 2 r data mining criterion 1 raw onesided purity unbalanced tree depth 9 ptratio 1x appears top cannot judged top importance splits small bucket size 9 apparently cluster school districts significantly worse pupiltoteacher ratios jority crimeinfested neighborhoods peeled next small bucket size 5 crim 1x nox makes surprisingly 3 appear ances would made harrison rubinfeld happy third split top nox breaks 12 highly polluted areas low bucket mean 16 compared 25 rest powerful variable lstat 3x creates next powerful split buckets size 41 34 means 19 respectively noticeable ambiguous role dis 3x correlates negatively housing values low values lstat 1015 highstatus positively high values lstat lowstatus high values nox 052 polluted rm 2x plays role highstatus neighborhoods crime neighborhoods peeled early singular areas extremely low housing values crim 1x splits age 1x zn 1x irrelevant due low meandifferences log oe 2 data mining criterion 2 penalized onesided purity aic qualitatively tree surprisingly similar previous one differences lower levels tree penalization seem affect splits till tree mining criterion 3 onesided ex tremes high mean search high response values creates unbalanced tree single powerful splits peeling splits small buckets one side repeated appearance two variables rm 2x levels 1 3 lstat 8x however tells powerful story highest housing prices bucket mean 45 average size homes measured rm 759 variable mat ters rm 708 persistent monotone decreasing dependence lstat takes median housing value 17 simple interplay rm lstat lends striking interpretability tree tells simple convincing story bottom crime crim 2x pollution nox 1x show remaining smaller effects expected directions smaller effects dis tax also seen halfway tree r data mining criterion 4 onesided ex tremes low mean tree tells similar story previous one greater precision achieved low housing values criterion looks first tree unbalanced first peeling split crim 1x sets aside 5 bucket crime infested neighborhoods lowest housing values around 10 second lowest mean bucket consists 5 census tracts low b 100 corresponding 63sigma32 africanamerican popula tion due quadratic transformation thereafter monotone decreasing dependence lstat takes form six peeling splits followed monotone increasing dependence rm form five peeling splits two successive dose response effects essentially previous tree found reverse order due peeling high low housing values table 2 result regression trees boston housing data interpretability achieved onesided extreme means criteria partly due extreme imbalance interpretability found examples two sides ffl buckets extremely high low means near top trees buckets desirable interpretation describe extreme response behavior terms simple clauses finding buckets exactly purpose extreme means criteria boston housing data highmean criterion example immediately finds bucket welloff areas large homes rm 759 lowmean criterion comparison immediately finds highcrime bucket crim 1579 cart also finds areas large homes second level find high crime bucket ffl dose response effects monotone dependencies iterative peeling behavior data mining criteria allows detection gradual increases decreases response function individual predictors iterative peeling predictor become apparent essential peeling layers form series small dangling terminal buckets hence form highly unbalanced trees cart comparison handicapped regard favors balanced splits data mining criteria last point ironic implies greater endcut problem data mining criteria compared cart works favor conversely carts endcut problem sufficiently strong allow clearly detect monotone dependencies highly unbalanced trees monotone dependencies detected plausible switch tree modeling additive even linear modeling include suitable interaction terms interaction terms may necessary localize monotone dependence example tree generated low means criterion might suggest linear model following form 62 graphical diagnostics regression trees although treebased methods sense flexible many conventional parametric methods still necessary guard artifacts best techniques diagnosing artifacts missfit graphical contrast linear regression basic diagnostics regression trees straightforward require additional computations first cut may sufficient graphically render effect split turn may achieved plotting response variable predictor split points parent bucket one graphically differentiate points two child buckets plotting different glyphs colors created series diagnostic plots tree generated low mean criterion figure 16 shows buckets ascending housing values split ffl high crime areas strongly africanamerican neighborhoods ffl segment decreasing fraction lower status people ffl communities unfavorable pupilteacher ratios schools ffl another segment decreasing fraction lower status people finally ffl segment increasing size homes matters plots confirm splits plausible high crime factor depresses housing values exist cluster neighborhoods whose pupilteacher ratio clearly worse well separated majority finally monotone dependencies clearly visible decreasing percentage lower status people increasing number rooms also visible outliers namely desirable areas beacon hill back bay near boston downtown top housing values yet limited size homes 14 figure graphical view major splits applied boston housing data using low means criterion 7 example classification trees pima indians diabetes demonstrate application new data mining criteria classification trees pima indians diabetes data pima data short data originally owned national institute diabetes digestive kidney diseases available uc irvine machine learning repository httpwwwicsuciedumlearnmlrepositoryhtml class labels pima data 1 diabetes 0 otherwise 8 predictor variables 768 patients females least 21 years age pima indian heritage near phoenix az among 768 patients 268 tested positive diabetes class 1 details data see documentation uc irvine repository predictor variables definitions shown table 3 variable description prgn number times pregnant plasma plasma glucose concentration two hours oral glucose tolerance test bp diastolic blood pressure mm hg thick triceps skin fold thickness mm insulin two hour serum insulin uml body body mass index weight kgheight 2 diabetes pedigree function age age years response class variable 1 diabetes 0 otherwise table 3 predictor variables pima indians diabetes data using pima data constructed four trees based cart new data mining criteria minimum bucket size 35 imposed 5 overall sample size 768 regression trees boston housing data since concerned interpretability use pruning resulting trees shown figures 20 23 node proportion p class size sz given tables 4 5 summarizes trees trees summary becomes clear plasma powerful predictor followed body particular third tree almost completely dominated two variables interleaved appearance tree suggests combined monotone dependence studied care 0 diabetes 50 100 150 200305070 plasma class 1 diabetes figure 17 distribution two classes pima diabetes data body plasma fully figure 17 shows distribution two classes plasmabody plane switched another fitting method natural describing monotone dependencies namely nearestneighbor fitting every point estimated conditional class 1 probability p 1 plasma body fraction class 1 samples among 20 nearest neighbors terms euclidean distance plasmabody coordinates standardizing variables unit variance figure shows sequence plots data plasma body plane rendition p 1 plasma body terms highlighted slices increasing sequence six values c plots make clear response behavior quite complex first plot shows slice best described cut low value body following four slices veer 90 clockwise last slice best described cut high value plasma one feature worth note though third plot notice hole center highlighted slice hole filled blob data fourth plot feature infer existence mild hump p 1 surface center data summary function p 1 plasma body shape clockwise ascending spiral rule surface hump middle obviously trees quite suboptimal fitting response charac teristics figure 19 shows third tree figure 22 tries approximate surface step function axesaligned rectangular tiles plasma plasma figure 18 pima indian diabetes data body plasma highlights represent slices nearconstant p 1 ffl values p 1 slices increase left right top bottom open squares diabetes class 0 figure 19 pima diabetes data body plasma plain tiled according buckets tree figure 22 open squares diabetes class 0 age28 plasma1051 pedigree026 insulin00000101p100000 132 p093007 70 p082018 49 p092008 49 p073027 66 p093007 60 p087013 51 p070030 48 p053047 66 p036064 72 p067033 86 p053047 59 p028072 79 p008092 48 p016084 64 p065035 sz1000 p079021 367 p097003 202 165 p087013 99 p066034 297 p059041 p069031 165 p079021 99 p037063 p027073 p039061 138 p013087 pima misclass error figure 20 pima indian diabetes data tree 1 cart criterion plasma993 body252 plasma1662 pedigree02 age28 p095005 56 p100000 49 p084016 49 p091009 46 p089011 81 p086014 77 p092008 51 p072028 74 p071029 64 p071029 49 p046054 146 p076024 48 p028072 61 p025075 52 p019081 48 p008092 48 p065035 sz1000 p063037 944 p092008 99 845 p058042 799 p055045 719 642 p058042 546 p080020 125 421 p047053 305 p057043 p052048 195 p014086 96 pima onesided purity misclass error figure 21 pima indian diabetes data tree 2 onesided purity plasma993 body252 thick28 plasma122 thick341 thick28 typical balanced tree depth 6 strongest variable plasma 5x creates successful split top body 3x next important variable much less followed pedigree 3x age 2x class ratios terminal buckets range 100000 left 016084 right splits right direction overall tree plausible simple interpretation r data mining criterion 1 onesided pu rity extremely unbalanced tree depth 12 spite depth tree overall structure simple tree moves right layers high class 0 diabetes shaved con versely tree steps left layers high class 1 diabetes shaved top tree dominated body age pedigree play role lower parts tree large rest bucket gets harder harder classify tree 3 maxp 0 r data mining criterion 2 onesided ex tremes high class 0 extremely unbalanced tree simple structure criterion searches layers high class 0 diabetes tree keeps stepping right order describe conditions class 0 prevalent appears body plasma mat ter tree shows sequence interleaved splits two variables indicating combined monotone dependence see investigation behavior interpretability tree successful one table 4 result classification trees pima indians diabetes data tree 4 maxp 1 r data mining criterion 2 onesided ex tremes high class 1 another extremely unbalanced tree simple structure criterion searches layers high class 1 diabetes causes tree step left order describe conditions class 1 prevalent plasma 6x matters far followed prgn 2x table 5 result classification trees pima indians diabetes data summary following messages investigations experiments ffl trees grown interpretation global measures goodness fit always desirable ffl hypergreedy data mining criteria give different insights ffl highly unbalanced trees reveal monotone dependence doseresponse effects endcut problem turns virtue ffl really understand data algorithms extensive visualization necessary following topics would merit research assess endcut problem hurts helps ffl extend new 2class criteria multiclass problem ffl develop sophisticated rules stopping pruning ffl increase accuracy limited 2step lookahead procedure new cri teria adopting suggestion breimans 1996 r new look statistical model regression diagnostics technical note properties splitting criteria classification regression trees treebased models statistical perspective knowledge discovery databases data mining knowledge discovery overview model selection principle minimum description length hedonic prices demand clean air clustering algorithms comments c p uci repository machine learning data bases httpwww problems analysis survey data proposal estimating dimension model cart supplementary module sys tat xgobi interactive data visualization x window system modern applied statistics splus measuring correcting effects data mining model selection tr c45 programs machine learning technical note simple fast effective rule learner ctr xiaoming huo seoung bum kim kwokleung tsui shuchun wang fbp frontierbased treepruning algorithm informs journal computing v18 n4 p494505 january 2006 soon tee teoh kwanliu paintingclass interactive construction visualization exploration decision trees proceedings ninth acm sigkdd international conference knowledge discovery data mining august 2427 2003 washington dc owen carmichael martial hebert shapebased recognition wiry objects ieee transactions pattern analysis machine intelligence v26 n12 p15371552 december 2004 vasilis aggelis panagiotis anagnostou ebanking prediction using data mining methods proceedings 4th wseas international conference artificial intelligence knowledge engineering data bases p16 february 1315 2005 salzburg austria