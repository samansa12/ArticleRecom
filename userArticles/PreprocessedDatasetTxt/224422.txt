hpf compiler ibm sp2 describe phpf research prototype hpf compiler ibm sp series parallel machines compiler accepts input fortran 90 fortran 77 programs augmented hpf directives sequential loops automatically parallelized compiler supports symbolic analysis expressions allows parameters number processors unknown compiletime without significantly affecting performance communication schedules computation guards generated parameterized form compiletime several novel optimizations improved versions wellknown optimizations implemented phpf exploit parallelism reduce communication costs optimizations include elimination redundant communication using dataavailability analysis using collective communication new techniques mapping scalar variables coarsegrain wavefronting communication reduction multidimensional shift communications present experimental results wellknown benchmark routines results show effectiveness compiler generating efficient code hpf programs b introduction fortran always synonymous fast execution high performance fortran hpf 13 8 defines set directive extensions fortran facilitate performance portability fortran programs compiling largescale multiprocessor architectures preserving sharedaddress space programming model unfortunately hpf compilers appeared rapidly originally hoped accepted high quality compilers hpf evolve time experience complexities compiling hpf result ffl ability perform communication optimizations essential high performance single innerloop communication result significant loss performance hpf performance approach handcoded performance sophisticated optimizations implemented ffl implementation every feature fortran affected distribution data across different address spaces fortran data primarily static hpf data must dynamically allocated since arrays redistributed even without redistribution number processors general known compiletime sizes local array partitions known statically thus addition loop parallelization hpf requires ibm tj watson research po box 704 yorktown heights ny 10598 authors reached email fmguptamidkiffschnbrgshieldskywchingtangogwatsonibmcom corresponding author reached schnbrgwatsonibmcom ibm software solutions division 1150 eglinton ave east north york ontario canada m3c 1v7 author reached email seshadrivnetibmcom enhanced runtime model new implementations features common blocks data statements block data array addressing ffl hpf extremely highlevel language amount generatedexecuted code per source line generally larger common programming languages poses challenge code generation runtime library design tool development ffl addition generating scalable code hpf compiler must generate code good uniprocessor node performance particular hpfspecific transformations must inhibit uniprocessor optimization spmd overhead must small paper describes phpf hpf compiler ibm sp2 architecture developed past two years 1 phpf consists extensions hpf subset 13 8 existing fortran 90 7 compiler paper focus novel aspects optimizations experience set benchmarks although phpf still development features described paper fully implemented several hpfrelated compiler efforts previously described 11 23 17 19 3 15 18 compiler unique following combination features ffl phpf exploits data parallelism fortran 90 array language also performs program analysis effectively parallelize loops fortran 77 code ffl phpf performs symbolic analysis generate efficient code presence statically unknown parameters like number processors array sizes rather leaving task determining communication schedule runtime phpf generates closedform parameterized expressions represent communication data sets communicating processors 2 ffl along wellknown communication optimizations like message vectorization 11 23 exploiting collective communication 16 9 phpf also performs aggressive optimizations like eliminating redundant communication reducing number messages multidimensional shift communication coarsegrain wavefronting phpf also uses novel techniques mapping scalar variables expose parallelism reduce communication costs rest paper organized follows section 2 presents overview architecture entire compiler spmdizer detail section 3 describes set optimizations performed spmdizer section 4 gives performance results set benchmarks section 5 describes compilers hpf similar languages discussed literature finally section 6 presents conclusions ideas future work compiler framework phpf implemented incorporating new spmdizer component existing fortran 90 compiler shown figure 1 additionally frontend extensions required process hpf directives impact hpf components fortran 90 compiler small scalarizer required significant modifications 1 prototype development project ibm researchphpf currently supports block cyclic blockcyclic distributions array language scalarizer hpf spmdizer locality optimizer optimizing backend fortran 90hpf frontend dependence analyzer loop transformer hpf runtime figure 1 architecture phpf compiler 21 architecture overview phpf native compiler preprocessor preprocessors portable easier build native compilers generally preferable obtaining highlytuned performance also easier write application debuggers native compilers preprocessors compilation process summarized considering output phases figure 1 ffl front end produces highlevel intermediate representation includes internal forms fortran 90 array language hpf directives ffl scalarizer compiles internal array language internal fortran 77 scalar form ffl spmdizer interprets hpf directives produces spmd singlenode program data computation partitioned communication code resolves nonlocal data references ffl locality optimizer performs loop reordering transformations uniprocessor program better utilize cache registers ffl back end performs traditional optimizations uniprocessor program scalarizer inlines fortran 90 intrinsic functions whenever possible inlining intrinsic functions possible eliminate extra array temporaries copying achieve performance fortran 90 programs comparable fortran 77 programs hpf extensions hpf spmdizer data partitioning preprocessing communication analysis dataflow dependence analyzer loop transformer computation partitioning communication code generation data partitioning postprocessing figure 2 architecture phpf spmdizer scalarizer ensure information parallelism implicit fortran 90 construct reduction operator lost inlining recording additional information needed later scalarizer also assigns suitable distribution array temporaries created scalar ization placement spmdizer scalarizer significant spmdizer processes fortran 77 programs scalarized fortran 90 programs uniform manner similarly locality optimizer processes spmdized uniprocessor input programs uniform way modularization functionality simplifies implementation without sacrificing performance spmdizer places communication outside loops legal locality optimizer subsequently able reorder inner communicationfree loops better cache utilization fur thermore spmdized form new loop bounds parallel loops facilitates easy identification conformable loops enable loop fusion transformations improving data locality distributed arrays spmdizer shrinks distributed dimensions addressspace holes data would otherwise reduce spatial locality compiler includes dataflow analysis data dependence analysis loop transformation modules enable variety optimizations performed different compilation stages example scalarizer uses data dependence information eliminate array temporaries otherwise needed preserve fortran 90 array language semantics 7 data dependence analysis loop transformations also critical locality communication optimizations structure spmdizer shown figure 2 illustrate compilation process particular different phases spmdizer help example integer a100 integer b100100 figure 3 fortran 90 example spread 511001 611001 figure 4 scalarized spread program 22 hpf compilation example figures 3 shows simple fortran 90 spread program hpf directives figure 4 shows result scalarizing program naive scalarization spread operation would result generation 2dimensional temporary array runtime call library spread routine scalarizer generates efficient inline code spread operation without creating new temporary variable figure 5 shows pseudocode output produced spmdizer scalarized program shown figure 4 use example illustrate spmdizer phases data partitioner distributed array may static commonblock array transformed fortran 90 pointer allocated dynamically library routine hpf allocate arrays shrunk according processors local block size adjusting bounds local array within global address space example 4 processors bounds second processor 2650 use fortran 90 dynamic pointer allocation allocate shrunk arrays spmdizer need perform globaltolocal address remapping unlike various hpf preprocessors 15 18 3 form output code general enough represent missing compiletime information example even though global array bounds b given number processors given processorlocal block sizes hence local array bounds known compiletime believe numberofprocessors loopbounds arraybounds information commonly unknown compiletime phpf designed important compilation parameters treated general expressions rather constants storage mapped arrays always dynamically allocated minimal observed performance loss communication analysis generation read reference program communication analysis phase determines whether requires interprocessor communication com 3 cyclic distributions require subscript modification exception integer pointer call hpfgetnumprocs2numprocspid call hpfallocateb globalbounds blocksize call hpfallocatea communication call hpfallocatecomputationbufferbuffercbsection pid2 le 99 blocksize2 pid1 le 99 blocksize1 call hpfbcastsectionasendsection buffer i8iownlbound2min0iownubound21001 i9iownlbound1min0iownubound11001 call deallocatebuffer figure 5 spmdized spread program munication needed analysis determines placement communication ii communication primitive carry data movement processor grid dimension phpf extends analysis described 9 handle scalars incorporate additional possibilities regarding distribution array dimensions namely replication mapping constant processor position code generator identifies processors data elements need participate communication generates calls runtime communication routines according communication requirements different grid dimensions figure 4 assigned row b since aligned first row b communication analysis selects broadcast along first dimension processor grid data processors participate communication specified array sections using globaladdressspace coordinates array section cb section specifies bounds computation buffer stores nonlocal data received another processor array send section specifies processorlocal section broadcast sending processor computation buffer bounds also specified global address space computation partitioning computation partitioning shrinks loop bounds introduces statement guards necessary distribute computation among different processors ownercomputes rule 11 generally used except reduction computations assignments scalars conditions described section 3 hpf runtime library runtime library allocates partitioned arrays performs local data movement communication provides high level interface compiler specify communication based global indices data indices corresponding arrays data distri bution processors data processors represented multidimensional sections possibly nonunit strides buffer allocation packing unpacking needed linearize data communication performed runtime routines runtime system also performs optimizations like overlapping unpack operations nonblocking receives waiting completion multiple receives runtime library portable across different basic communication systems currently supports ibm mpl mpi libraries runtime system also provides detailed performance statistics trace information debugging hand trace generation program visualization tools 12 optimizations spmdizer phpf performs several optimizations reduce communication costs overhead guards introduced computation partitioning optimizations wellknown discussed literature 11 23 17 19 3 many others unique phpf message vectorization moving communication outside loops amortize startup costs sending messages recognized extremely important optimization phpf uses dependence information determine outermost loop level communication placed applicability optimization improved ffl loop distribution preliminary analysis communication computation partitioning guards used guide selective loop distribution phpf uses data control dependence information first identify program structure maximal loop distribution align di ai 1 communication di communication di i1n figure enabling message vectorization loop distribution form strongly connected components sccs program dependence graph since unnecessary loop distribution hurt cache locality also redundant message elimination currently recognize redundant messages different loop nests phpf identifies sccs loop distribution expected improve performance sccs identified innerloop communication moved loop distribution mutually different local iteration sets obtained computation partitioning cases loop distribution reduces communication costs overhead computation partitioning example figure 6 communication reference di moved outside iloop result loop distribution ffl exploiting independent directive compiler assumes loopcarried dependence loops marked independent programmer often allows communication moved outside loops static analysis information imprecise collective communication phpf uses techniques 9 identify highlevel pattern collective data movement given reference information used recognize opportunities using collective communication primitives like broadcast reduction generating efficient sendreceive code special communication patterns like shift effectiveness analysis improved ffl idiomrecognition reductions currently phpf recognizes sum product min max reductions fortran 77 code since fortran 90 reduction operations handled inlining information reduction operations gathered idiomrecognition inlining represented uniformly communication analysis exploits information generate parallel code local reduction followed global reduction efficient communication ffl symbolic analysis since data distribution parameters block size often unknown eg number processors specified statically compiler uses symbolic analysis operations like checking equality expressions checking one expression integral multiple another expression enables phpf generate efficient code example detecting absence communication symbolic test strictly synchronous property 9 array references elimination redundant communication unique feature phpf analysis availability data processors owners 10 enables detection redundant align aij bij align di bi1 figure 7 example redundant communication program refs comm refs redundancy elim redundancy elim redundant comm grid block block 15 11 267 ncar block block 45 cmslow 44 21 523 table 1 results optimization eliminate redundant communication communication compiler infer data communicated already available intended receivers due prior communications example figure 7 communication reference ai1n statement 2 made redundant communication reference ai1n 1 implemented simplified version analysis presented 10 eliminate redundant communication finds redundant communication within single loop nests advantage analysis performed high level hence largely independent communication code generation table 1 summarizes results obtained phpf benchmark programs described section 4 table shows static counts number references requiring communication optimization eliminate redundant communication shows compiler quite successful identifying redundant communication optimizing nearestneighbor shift communication phpf compiler employs several techniques optimize shift communication occurs frequently many scientific applications scalarizer optimizes number temporary arrays introduced handle fortran 90 shift operations following optimizations performed reduce communication costs figure 8 example nearestneighbor shift communication figure 9 example wavefront computation ffl message coalescing consider program segment figure 8 communication two rhs references significant overlap neither completely covers augmenting data communicated bi1j1 extra data needed bi 1j separate communication bi1j avoided phpf communication analysis identifies data movement bi1j shift first dimension internalized data movement idm second dimension data movement bi1j1 shift dimensions recognizing communication bi1j1 dominant due interprocessor communication instead idm second dimension phpf drops communication bi1j augmenting dominant communication dropped communication data set case extends upper bound date communicated second dimension bi1j1 generalized implementation redundant communication elimination message coalescing well ffl multidimensional shift communication given array reference shift communication processor grid dimensions processor ignoring boundary cases sends data receives data 2 gamma 1 processors example communication bi1j1 figure 8 requires processor send data receive 3 processors phpf compiler uses optimization reduce number messages exchanged either direction 2 gamma 1 accomplished composing communication steps augmenting data communicated suitably step experiments show noticeable performance improvements optimization even shift communications two dimensions coarse grain wavefronting consider program segment shown figure 9 basic dependence based algorithm would place communication aij1 inside jloop communication ai1j inside iloop due true dependences references aij align ai bcd align ef privatize z without alignment figure 10 different alignments privatized scalars leads pipeline parallelism across grid dimensions regardless loop ordering one dimensions extremely finegrained parallelism high communication overhead phpf performs special analysis finegrained pipelined communication taking place inside loop nest identifies maximal fullypermutable inner loop nest 22 statements inside loop nest computation partitioning pipelined communication corresponding array dimension block distribution moved outside fullypermutable loop nest follows two observations first fullypermutable loop nest tiled arbitrarily 22 second loop blockdistributed array dimension pipelined communication traversed tiled onto outer loop traversing processors appear separate loop spmd code inner loop traversing local space processor communication moved loops thus example figure 9 phpf able move communication ai1j aij1 outside jloop leading wavefront parallelism low communication overhead associated coarsegrain pipeline future experiment loop stripmining control grainsize pipelining mapping scalars wellknown replicating scalar variable processors often leads inefficient code example replication variable x figure 10 would unnecessarily lead processor executing first statement loop every iteration values b1n c1n broadcast processors scalar variables privatized phpf chooses among alignment producer reference ii alignment consumer reference iii alignment first explain mappings chosen phpf scalar variables figure 10 describe general algorithm used determine mapping scalars privatizable variable aligned producer reference ie rhs reference ai statement computes scalar value avoids communication needed producer reference statement variable x aligned consumer reference ie lhs reference di2 uses scalar value computation x aligned instead producer reference bi ci communication x owner di2 would required communication inside iloop dependence definition x use x inside loop aligning x di2 communication needed two references bi ci communications moved outside iloop finally variable z uses value replicated array elements ei fi computation real a100100 iiown lbounda1i iown ubounda1i 1 end figure 11 guard optimization example explicitly aligned reference processor executes iteration iloop computation partitioning determined partitioning statements loop owns computes temporary value z loop iteration phpf uses static single assignment ssa representation 5 associate separate mapping decision assignment scalar chooses replication default mapping definition scalar loop privatized without copyout based defuse analysis reaching use identifies reaching definition phpf aligns scalar reference partitioned array rhs statement rhs reference statement available processors scalar variable explicitly marked alignment scalar value needs communicated owner consumer reference phpf determines separate pass desirability changing alignment communicated scalar scalars referenced computation consumer reference instead change done new alignment shows lower estimated cost resulting communication moved outside loop scalar computed reduction operation sum carried across processor grid dimension handled special manner additional privatized copy scalar created hold results local reduction computation initially performed processor global reduction operation combines values local operations optimizing statement guards statement guards needed enforce ownercomputes rule however innerloop guards inhibit parallelism significantly degrade performance several guard optimizations performed ffl statements within loop local iteration set loop loop bound shrinking used perform computation partitioning otherwise guards introduced individual statements ffl guards introduced computation partitioning hoisted loop nest far possible given statement local iteration sets different loops guards different processor grid dimensions handled independently increases ability float guards inner loops since type moved far possible example figure 11 guard needed first processor grid dimension iloop bounds reduced local iteration set guard second dimension based condition invariant inside iloop hence moved outside iloop mpl version 100 100 200 398 789 1578 3043 table 2 speedups grid program section discuss preliminary experimental results set benchmark programs utilization optimizations discussed previous section 41 experimental setup chose four programs hpf benchmark suite developed applied parallel research inc publicly available programs vary degrees challenge present compiler first benchmark program grid iterative 2d 9point stencil program features regular nearest neighbor communication followed global reduction operation grid program little computation benchmark version program takes logarithms 9point stencil compute exponential value average artificially boost computationcommunication ratio second program tomcatv originally spec benchmark mesh generator thompsons solver third program ncar shallow water atmospheric model nontrivial 2d stencil program contains nearest neighbor communication general communication last program x42 explicit modeling system using fourth order differencing using values 2 grids side center point benchmark routine used serial performance program compiled using ibm xlf compiler baseline performance compiling hpf programs number physical processors specified compile time performance results obtained running object code different number processors 1 2 4 8 16 processors speedup programs calculated dividing baseline time parallel time p experiments sequential parallel runs done 36processor ibm sp2 thinnodes 4 widenodes programs compiled optimization flag o3 grid ncar shallowwater x42 timing handoptimized version program using message passing library mpl also shown 42 results grid threedimensional arrays aligned template distributed onto twodimensional processor grid benchmark results obtained cycles compiler achieves linear speedup case noted previous section phpf successful eliminating redundant communication program column marked grid table 2 obtained specifying number processors compiletime shows phpf capable generating high quality code number processors unknown compile time table 3 speedups tomcatv program program speedups ncar block block 100 101 171 374 675 1220 1932 mpl version 100 114 228 453 882 1662 3110 table 4 speedups ncar shallow water program tomcatv arrays tomcatv distributed columnwise onto 1d processor grid arrays size 514x514 program first computes residuals requires nearest neighbor communication next maximum values residuals determined finally tridiagonal system solved parallel computation iterates maximum value residuals converges idiom recognition identified reduction operation computing maximum residuals communication onethird references initially identified needing communication recognized redundant eliminated optimizations useful included alignment scalars consumers replacement induction variables optimizations enabled bounds main computation loop nest shrunk guards needed statements inside loop nest although compiler achieves 55 ideal speedup 32 processors result quite good compared results seen hpf compilers ncar shallow water benchmark results table 4 computed based 512x512 arrays distributed block compiler rely independent directive loops could otherwise recognized parallel due statically unknown constant appearing subscripts small number processors speedup good scale well number processors increased arrays distributed block block number references need communication increases 25 45 performance 2d distribution better 1d distribution however cost extra messages less savings sending shorter messages redundant communication elimination also effective 2d distribution x42 benchmark version apr times part program computes wavefields arrays distributed using 1d block distribution redundant communication elimination removes 8 19 static communications summary compiled hpf programs many inherent performance overheads result performance less highlytuned handcoded programs benchmark performance x42 block block 100 118 203 381 699 1336 2196 mpl version 100 100 198 385 750 1377 2470 table 5 speedups x42 program results reported represent combined effects optimizations built compiler symbolic analysis ability compiler maintains level performance number processors known compile time 5 related work several groups looked problem compiling hpflike languages distributed memory machines 11 23 19 3 4 15 18 work also benefited early projects like kali 14 fortran compiler 11 performs several optimizations like message vectorization using collective communication exploiting pipeline parallelism also performs analysis eliminate partially redundant communication irregular computations 21 current version fortran compiler requires number processors known compile time supports partitioning single dimension array superb compiler 23 developed university vienna represents second generation compiler pioneered techniques like message vectorization use overlap regions shift communication compiler supports block distribution array dimensions puts special emphasis performance prediction guide optimizations datapartitioning decisions 6 fortran 90d compiler 3 exploits parallelism fortran 90 constructs generating spmd messagepassing program attempt parallelize sequential fortran 77 programs work focussed considerably supporting parallel io handling outofcore programs 20 paradigm compiler 19 2 targeted fortran 77 programs provides option automatic data partitioning regular computations also supports exploitation functional parallelism addition dataparallelism adaptor system 4 supports hpf subset performs optimizations handling fortran 90 array constructs improving cache locality addition reducing communication costs adaptor determines communication schedules runtime suif compiler 1 performs loop transformations increasing parallelism enhancing uniprocessor performance compiler also supports automatic data partitioning report results messagepassing machines 6 conclusions described hpf compiler ibm sp series parallel machines compiler phpf unique ability efficiently support fortran 90 array operations sequential fortran 77 loops hpf programs handles statically unknown parameters like number processors usually performance degradation uses symbolic analysis resort runtime determination communication schedules phpf makes several contributions optimizing communication eliminates redundant communication using dataavailability analysis deals problem mapping scalar variables comprehensive manner performs specialpurpose optimizations like coarsegrain wavefronting reducing number messages multidimensional shift communications presented experimental results indicate optimizations lead efficient code generation future plan apply optimizations communication across procedure boundaries interprocedural analysis plan support blockcyclic distribution array dimensions also provide efficient support irregular computations also investigating compilation strategies using remote memory copy operations like get put basic primitives transferring data across processors acknowledgements thank rick lawrence joefon jann providing performance results mpl version benchmark programs would also like thank alan adamson lee nackman support r overview compiler scalable parallel machines overview paradigm compiler distributedmemory multicomputers compilation approach fortran 90dhpf compilers distributed memory mimd computers adaptor compilation system dataparallel fortran programs efficiently computing static single assignment form control dependence graph static parameter based performance prediction tool parallel programs ansi fortran 90 standard committee high performance fortran forum methodology highlevel synthesis communication multicomputers unified dataflow framework optimizing communication compiling fortran mimd distributedmemory machines visualizing execution high performance fortran hpf programs compiling global namespace parallel loops distributed exe cution applied parallel researchs xhpf system compiling communicationefficient programs massively parallel chines process decomposition locality reference pghpf portland group automating parallelization regular computations distributed memory multicomputers paradigm compiler compiler runtime support outofcore hpf programs loop transformation theory algorithm maximize parallelism compiling distributedmemory systems tr process decomposition locality reference efficiently computing static single assignment form control dependence graph compiling fortran mimd distributedmemory machines methodology highlevel synthesis communication multicomputers high performance fortran handbook static parameter based performance prediction tool parallel programs giventakemyampersandmdasha balanced code placement framework paradigm compiler distributedmemory multicomputers compiling communicationefficient programs massively parallel machines compiling global namespace parallel loops distributed execution loop transformation theory algorithm maximize parallelism visualizing execution high performance fortran hpf programs overview compiler scalable parallel machines compilation approach fortran 90d hpf compilers unified dataflow framework optimizing communication ctr manish gupta edith schonberg static analysis reduce synchronization costs dataparallel programs proceedings 23rd acm sigplansigact symposium principles programming languages p322332 january 2124 1996 st petersburg beach florida united states combined compiletime runtimedriven proactive data movement software dsm systems proceedings 7th workshop workshop languages compilers runtime support scalable systems p16 october 2223 2004 houston texas shankar ramaswamy sachin sapatnekar prithviraj banerjee framework exploiting task data parallelism distributed memory multicomputers ieee transactions parallel distributed systems v8 n11 p10981116 november 1997 gerald roth ken kennedy loop fusion high performance fortran proceedings 12th international conference supercomputing p125132 july 1998 melbourne australia daniel j rosenkrantz lenore r mullin harry b hunt iii minimizing materializations arrayvalued temporaries acm transactions programming languages systems toplas v28 n6 p11451177 november 2006 jos e moreira samuel p midkiff fortran 90 cse case study ieee computational science engineering v5 n2 p3949 april 1998 vijay menon keshav pingali highlevel semantic optimization numerical codes proceedings 13th international conference supercomputing p434443 june 2025 1999 rhodes greece b di martino briguglio celino g fogaccia g vlad v rosato briscolini development large scale high performance applications parallelizing compiler practical parallel computing nova science publishers inc commack ny 2001 gerald roth john mellorcrummey ken kennedy r gregg brickner compiling stencils high performance fortran proceedings 1997 acmieee conference supercomputing cdrom p120 november 1521 1997 san jose ca shuo yang ali r butt charlie hu samuel p midkiff trust verify monitoring remotely executing programs progress correctness proceedings tenth acm sigplan symposium principles practice parallel programming june 1517 2005 chicago il usa soumen chakrabarti manish gupta jongdeok choi global communication analysis optimization acm sigplan notices v31 n5 p6878 may 1996 vikram adve john mellorcrummey using integer sets dataparallel program analysis optimization acm sigplan notices v33 n5 p186198 may 1998 steven j deitz bradford l chamberlain sungeun choi lawrence snyder design implementation parallel array operator arbitrary remapping data acm sigplan notices v38 n10 october daniel chavarramiranda john mellorcrummey effective communication coalescing dataparallel applications proceedings tenth acm sigplan symposium principles practice parallel programming june 1517 2005 chicago il usa manish gupta edith schonberg harini srinivasan unified framework optimizing communication dataparallel programs ieee transactions parallel distributed systems v7 n7 p689704 july 1996 ayon basumallik rudolf eigenmann optimizing irregular sharedmemory applications distributedmemory systems proceedings eleventh acm sigplan symposium principles practice parallel programming march 2931 2006 new york new york usa dhruva r chakrabarti nagaraj shenoy alok choudhary prithviraj banerjee efficient uniform runtime scheme mixed regularirregular applications proceedings 12th international conference supercomputing p6168 july 1998 melbourne australia john mellorcrummey vikram adve simplifying control flow compilergenerated parallel code international journal parallel programming v26 n5 p613638 october 1998 rudolf eigenmann jay hoeflinger david padua automatic parallelization perfect benchmarks174 ieee transactions parallel distributed systems v9 n1 p523 january 1998 christopher barton clin casaval george almsi yili zheng montse farreras siddhartha chatterje jos nelson amaral shared memory programming large scale machines acm sigplan notices v41 n6 june 2006 bradford l chamberlain steven j deitz lawrence snyder comparative study nas mg benchmark across parallel languages architectures proceedings 2000 acmieee conference supercomputing cdrom p46es november 0410 2000 dallas texas united states mahmut kandemir alok choudhary prithviraj banerjee j ramanujam nagaraj shenoy minimizing data synchronization costs oneway communication ieee transactions parallel distributed systems v11 n12 p12321251 december 2000 kandemir p banerjee choudhary j ramanujam n shenoy global communication optimization technique based dataflow analysis linear algebra acm transactions programming languages systems toplas v21 n6 p12511297 nov 1999 johan cockx kristof denolf bart vanhoof richard stahl sprint tool generate concurrent transactionlevel models sequential code eurasip journal applied signal processing v2007 n1 p213213 1 january 2007 ken kennedy charles koelbel hans zima rise fall high performance fortran historical object lesson proceedings third acm sigplan conference history programming languages p71722 june 0910 2007 san diego california jack dongarra ian foster geoffrey fox william gropp ken kennedy linda torczon andy white references sourcebook parallel computing morgan kaufmann publishers inc san francisco ca