efficient svm regression training smo sequential minimal optimization algorithm smo shown effective method training support vector machines svms classification tasks defined sparse data sets smo differs svm algorithms require quadratic programming solver work generalize smo handle regression problems however one problem smo rate convergence slows dramatically data nonsparse many support vectors solutionas often case regressionbecause kernel function evaluations tend dominate runtime case moreover caching kernel function outputs easily degrade smos performance even smo tends access kernel function outputs unstructured manner address problems several modifications enable caching effectively used smo regression problems modifications improve convergence time order magnitude b introduction support vector machine svm type model optimized prediction error model complexity simultaneously minimized 13 despite many admirable qualities research area svms hindered fact quadratic programming qp solvers provided known training algorithm years 1997 theorem 6 proved introduced whole new family svm training procedures nutshell osunas theorem showed global svm training problem broken sequence smaller subproblems optimizing subproblem minimizes original qp problem even recently sequential minimal optimization algorithm smo introduced 7 9 extreme example osunas theorem practice smo uses subproblem size two subproblem analytical solution thus rst time svms could optimized without qp solver addition smo new methods 5 2 proposed optimizing svms online without qp solver online methods hold great promise smo online svm optimizer explicitly exploits quadratic form objective function simultaneously uses analytical solution size two case e smo shown eective sparse data sets especially fast linear svms algorithm extremely slow nonsparse data sets problems many support vectors regression problems especially prone issues inputs usually nonsparse real numbers opposed binary inputs solutions many support vectors constraints reports smo successfully used regression problems work derive generalization smo handle regression problems address runtime issues smo modifying heuristics underlying algorithm kernel outputs eectively cached conservative results indicate high dimensional nonsparse data especially regression problems convergence rate smo improved order magnitude paper divided six additional sections section 2 contains basic overview svms provides minimal framework later sections build section 3 generalize smo handle regression problems simplest implementation smo regression optimize svms regression problems poor convergence rates section 4 introduce several modications smo allow kernel function outputs eciently cached section 5 contains numerical results show modications produce order magnitude improvement convergence speed finally section 6 summarizes work addresses future research area 2 introduction svms consider set data points input target output svm model calculated weighted sum kernel function outputs kernel function inner product gaussian basis function polynomial function obeys mercers condition thus output svm either linear function inputs linear function kernel outputs generality svms take forms identical nonlinear regression radial basis function networks multilayer perceptrons dierence svms methods lies objective functions optimized respect optimization procedures one uses minimize objective functions linear noisefree case classication 2 f1 1g output svm written fx optimization task dened subject intuitively objective function expresses notion one nd simplest model explains data basic svm framework generalized include slack variables missclassications nonlinear kernel functions regression well extensions problem domains beyond scope paper describe derivation extensions basic svm framework instead refer reader excellent tutorials 1 11 introductions svms classication regression respectively delve derivation specic objective functions far necessary set framework present work general one easily construct objective functions similar equation 1 include slack variables misclassication nonlinear kernels objective functions also modied special case performing regression ie 2 r instead 1g objective functions always component minimized linear constraints must obeyed optimize objective function one converts primal lagrangian form contains minimization terms minus linear constraints multiplied lagrange multipliers primal lagrangian converted dual lagrangian free parameters lagrange multipliers dual form objective function quadratic lagrange multipliers thus obvious way optimize model express quadratic programming problem linear constraints contribution paper uses variant platts sequential minimal optimization method generalized regression modied eciencies smo solves underlying qp problem breaking sequence smaller optimization subproblems two unknowns two unknowns parameters analytical solution thus avoiding use qp solver even though smo use qp solver still makes reference dual lagrangian objective functions thus dene output function nonlinear svms classication regression well dual lagrangian objective functions optimized respect case classication 2 f1 1g output svm dened kx x b underlying kernel function dual objective function minimized subject box constraint 0 c 8 linear constraint 0 c userdened constant represents balance model complexity approximation error regression svms minimize functionals formn jj insensitive error function dened jxj otherwise output svm takes form e intuitively positive negative lagrange multipliers ie single weight obey 0 dual form equation 4 written one minimize objective function respect subject constraints parameter c userdened constant represents balance model complexity approximation error later sections make extensive use two dual lagrangians equations 3 6 svm output functions equations 2 5 3 smo regression mentioned earlier smo new algorithm training svms smo repeatedly nds two lagrange multipliers optimized respect analytically computes optimal step two lagrange multipliers two lagrange multipliers optimized original qp problem solved smo actually consists two parts 1 set heuristics eciently choosing pairs lagrange multipliers work 2 analytical solution qp problem size two beyond scope paper give complete description smos heuristics information found platts papers 7 9 since smo originally designed like svms applicable classication problems analytical solution size two qp problem must generalized order smo work regression problems bulk section devoted deriving solution 31 step size derivation begin transforming equations 56 7 substituting thus new unknowns obey box constraint c c 8 also use shorthand model output objective function written e linear constraint goal analytically express minimum equation 9 function two parameters let two parameters indices b b two unknowns rewrite equation 9 k aa 2 2 b k ab v l c term strictly constant respect b v dened k ai f note superscript used explicitly indicate values computed old parameter values means portions expression function new parameters simplies derivation assume constraint true prior change b order constraint true step parameter space sum b must held xed mind let b rewrite equation 10 function single lagrange multiplier substituting solve equation 12 need compute partial derivative respect b however equation 12 strictly dierentiable absolute value function neverthe less take djxjdx sgnx resulting derivative algebraically consistent l setting equation 13 zero yields v f k aa k aa f equation 14 write recursive update rule b terms old value b f equation 15 recursive two sgn functions still single solution found quickly shown next subsection 0 b figure 1 derivative function b kernel function obeys mercers condition derivative equation 13 always strictly increasing 32 finding solutions figure partial derivative equation 13 dual lagrangian function respect b behaves kernel function svm obeys mercers condition common ones guaranteed always true strictly positive equation 13 always increasing moreover zero piecewise linear two discrete jumps illustrated figure 1 putting facts together means consider possible solutions equation 13 three possible solutions correspond using equation 15 sgn set 2 0 2 two candidates correspond setting b one transitions figure 1 also need consider linear boxed constraints relate one another particular need lower upper bounds b insure b within c range using l h lower upper bounds respectively guarantees parameters obey boxed constraints 33 kkt conditions step described section minimize global objective function one two parameters violates karushkuhntucker kkt condition kkt conditions regression kkt conditions also yield test convergence parameter violates kkt condition global minimum reached within machine precision 34 updating threshold update svm threshold calculate two candidate updates rst update used along new parameters forces svm f second forces neither update two parameters hits constraint two candidate updates threshold identical otherwise average candidate updates new old new b old new old new b old update rules nearly identical platts original derivation complete update rule smo work regression problems following steps performed pick two parameters b least one parameter violates kkt condition dened equation 18 compute try equation 15 sgn sgn b equal 2 0 2 new value zero equation 13 accept new value step failed try b equal 0 accept value property positive negative perturbations yield positive negative value equation 13 raw new b l set new l otherwise set new set new b new b set new specied equations 19 20 outer loop smothat nonnumerical parts make heuristics remain discussed section 5 modications made smo improve rate convergence regression problems much order magnitude e progress made 1 rst iteration previous iteration made progress let working set data points 2 otherwise let working set consist data points nonbounded lagrange multipliers 3 data points working set try optimize corresponding lagrange multiplier nd second lagrange multiplier 31 try best one found looping nonbounded multipliers according platts heuristic 32 try among working set 33 try nd one among entire set lagrange multipliers 4 progress made working set data points done figure 2 basic pseudocode smo 4 building better smo described section 2 smo repeatedly nds two lagrange multipliers optimized respect analytically computes optimal step two lagrange multipliers section 2 concerned analytical portion algorithm section concentrate remainder smo consists several heuristics used pick pairs lagrange multipliers optimize beyond scope paper give complete description smo figure 2 gives basic pseudocode algorithm information consult one platts papers 7 9 referring figure 2 notice rst lagrange multiplier work chosen line 3 counterpart chosen line 31 32 33 smo attempts concentrate eort needed maintaining working set nonbounded lagrange multipliers idea lagrange multipliers bounds either 0 c classication 0 c regression mostly irrelevant optimization problem tend keep bounded values best optimization step take time proportional number lagrange multipliers working set worst take time proportional entire data set however runtime actually much slower analysis implies candidate second lagrange multiplier requires three kernel functions evaluated input dimensionality large kernel evaluations may signicant factor time complexity told express runtime single smo step p w probability second lagrange multiplier working set w size working set input dimensionality goal section reduce runtime complexity single smo step e p 0 w additionally method reducing total number required smo steps also introduced also reduce cost outer loop smo well next subsections several improvements smo described fundamental change cache kernel function outputs however naive caching policy actually slows smo since original algorithm tends randomly access kernel outputs high frequency changes designed either improve probability cached kernel output used exploit fact kernel outputs precomputed 41 caching kernel outputs cache typically understood small portion memory faster normal memory work use cache refer table precomputed kernel outputs idea frequently accessed kernel outputs stored reused avoid cost recomputation cache data structure contains inverse index entries refers index main data set ith cached item maintain twodimensional mm array store cached values thus 1 either precomputed value k ab stored cache space allocated value ag set indicate kernel output needs computed saved cache following operations applied returns one three values indicate k ab either 1 cache 2 allocated cache present 3 cache ab force cache present already least recently used indices replaced b return k ab fastest method available mark indices b recently used elements use least recently used policy updating cache would expected following exceptions k ii maintained separate space since accessed frequently smos working set lagrange multipliers determined step 1 figure 2 accesses cache done without tickles without inserts working set proper subset requested indices part working set access done neither tickle insert without modication caching kernel outputs smo usually degrades runtime frequency cache misses extra overhead incurred modied caching policy makes caching benecial however next set heuristic improve eectiveness caching even 42 eliminating thrashing shown lines 31 32 33 figure 2 smo uses hierarchy selection methods order nd second multiplier optimize along rst rst tries nd good one heuristic fails settles anything working set fails smo starts searching entire training set line 33 causes problems smo two reasons first entails extreme amount work results two multipliers changing second caching used line 33 could interfere update policies cache avoid problems use heuristic entails modication smo line 33 executed working set entire data set must execute case sure convergence achieved platt 8 proposed modication similar goal mind example source code accessed via url given end paper heuristic corresponds using commandline option lazy short lazy loops 43 optimal steps next modication smo takes advantage fact cached kernel outputs accessed constant time line 31 figure 2 searches entire working set nds multiplier approximately yields largest step size however kernel outputs two multipliers cached computing change objective function results optimizing two multipliers takes constant time calculate thus exploiting cached kernel outputs greedily take step yields improvement let b rst multiplier selected line 3 figure 2 k ab cached calculate new values two multipliers analytically constant time let old values multipliers use superscripts b moreover let shorthand new old values svm output 1 change classication objective function equation 3 results accepting new multipliers k aa 0 f k aa ab b b b equation 21 derived substituting equation 3 rewriting equation terms trivially dependent independent andor b afterwards dierence two choices two multipliers calculated without summations independent terms cancel 1 note section refer lagrange multipliers maintain consistency earlier sections even though notation con icts equations 3 6 e change regression objective function equation similarly calculated f a2 k aa b b2 k ab b b thus modify smo replacing line 31 figure 2 code looks best second multiplier via equation 21 22 k ab cached example source code heuristic corresponds using commandline option best short best step 44 demand incremental svm outputs next modication smo method calculate svm outputs rapidly without loss generality assume svm used classication output svm determined equation 2 substituted least three dierent ways calculate svm outputs single lagrange multiplier use equation 2 extremely slow change equation 2 summation nonzero lagrange multipliers incrementally update new value f clearly last method fastest smo original form uses third method update outputs whose multipliers nonbounded needed often second method output needed incrementally updated improve method updating outputs needed computing second third method ecient need two queues maximum sizes equal number lagrange multipliers third array store time stamp particular output last updated whenever lagrange multiplier changes value store change multiplier change 0 queues overwriting oldest value particular output required number time steps elapsed since output last updated less number nonzero lagrange multipliers calculate output last known value changed values queues however fewer nonzero lagrange multipliers ecient update output using second method since outputs updated demand svm outputs accessed nonuniform manner update method exploit statistical irregularities example source code heuristic corresponds using commandline option clever short clever outputs 45 smo decomposition using smo caching along proposed heuristics yields signicant runtime improvement long cache size nearly large number support vectors solution cache size small kernel outputs support vector pair accesses cache fail runtime increased particular problem addressed combining osunas decomposition algorithm 6 smo basic idea iteratively build subproblem 2 n solve subproblem iterate new subproblem entire optimization problem solved however instead using qp solver solve subproblem use smo choose large cache benets combination twofold first much evidence indicates decomposition often faster using qp solver since combination smo decomposition functionally identical standard decomposition smo qp solver expect benet second using subproblem size cache guarantees kernel outputs required available every smo iteration except rst subproblem however note implementation decomposition naive way constructs subproblems since essentially works rst randomly selected data points violate kkt condition example source code heuristic corresponds using commandline option ssz short subset size 5 experimental results evaluate eectiveness modications smo chose mackeyglass system 4 test case highly chaotic making challenging regression problem wellstudied mackeyglass system described delaydierential equation dx dt experiments used parameter settings 1 numerical integration yields chaotic time series embedding dimension 4 perform forecasting use timedelay embedding 12 approximate map equal 4 6 8 thus predicting 85 time steps future svm 4 6 8 inputs purpose work evaluate prediction accuracy svms chaotic time series done 5 focus amount time required optimize support vector machine since objective function optimizing svms quadratic linear constraints svms either single global minimum collection minima identical objective function valuation hence excepting minor numerical dierences implementations svm optimization routines essentially 060811214 true predicted05070911130 50 100 150 200 250 300 350 400 true predicted06114 c true support vector06114 true support vector figure 3 mackeyglass system actual predicted time series twodimensional phase space plots show location support vectors c nd solution dier nd solution long takes get much memory required figure 3 shows four plots two training runs illustrate mackeyglass time series phasespace time series plots show predictions two values phasespace plots show location support vectors two dimensional slice timedelay embedding rst part experimental results summarized tables 1 2 experiments time series consisted 500 data points depending values yield number exemplars less 500 major blocks three table summarize specic problem instance unique set values within block performed combinations using smo without caching without decomposition without three heuristics block tables also contains results using royal holloway att gmd first sv machine code ragsvm 10 ragsvm work three dier ent optimization packages one optimizer freely available research used regression problems bottou implementation conjugate gradient method entries blocks labelled qp use ragsvm bottou without chunking option entries labelled qpchunk use sporty chunking uses decomposition method specied subset size qp solver subproblem general training runs congured similarly possible using gaussian kernels form kx congurations produces results nearly identical ragsvm respect value objective function found however run times dramatically dierent two implementations sets experiments smo caching heuristics consistently gave fastest run times often performing orders magnitude faster regular smo qp decomposition speed improvements smo ranged factor 3 much 25 interestingly experiments smo decomposition consistently yielded inferior run times compared smo without decomposition regardless runtime options motivation combining smo decomposition make caching effective problems many data points since rst set experiments used 500 data points used mackeyglass parameters generate time series 10000 data points experimentation table 3 summarizes second set experiments experiments chose vary whether smo used without decomposition seen table smo without decomposition gives nearly order magnitude improvement runtime compare ragsvm smo decomposition yields even faster run times however smo decomposition yields high standard deviation fastest slowest run times 391 1123 seconds respectively suspect high standard deviation result naive implementation decomposition nevertheless worst case smo decomposition nearly good best smo without decomposition moreover problem set smo decomposition nearly 25 times faster decomposition qp solver fact solutions found smo experiments table 3 superior ragsvm solutions nal objective function values signicantly larger magnitude smo runs e training subset cache options objective number cpu std method size size smo value svs time dev problem instance smo 100 100 none 159198 705 4272 753 smo 100 100 159247 679 764 104 qp 159002 63 8522 qpchunk 100 158809 59 2024 problem instance smo 100 100 none 54620 632 3570 625 smo 100 100 54636 627 606 093 qp 54698 59 6286 qpchunk 100 54619 problem instance smo 100 100 none 23005 554 1365 380 smo 100 100 23031 53 345 059 qp 22950 51 4086 qpchunk 100 22899 38 630 table 1 experimental results part 12 smo results averaged ten trials entries heuristics value indicate lazy loops section 42 best step section 43 clever outputs section 44 used entries subset size indicate size decomposition 0 meaning decomposition times cpu seconds 500 mhz pentium iii machine running linux ecient svm training flake lawrence training subset cache options objective number cpu std method size size smo value svs time dev problem instance smo 100 100 none 840284 1968 18412 2232 smo 100 100 838655 1953 4060 569 qp 840401 194 18663 qpchunk 100 840290 188 31654 problem instance smo 100 100 none 481120 1705 27876 3127 smo 100 100 481283 169 7509 1438 qp 481430 159 24567 qpchunk 100 481505 164 31021 problem instance smo 100 100 none 278421 1649 28926 2913 smo 100 100 278663 1598 7666 1269 qp 278958 149 25740 qpchunk 100 278941 144 32991 table 2 experimental results part 22 smo results averaged ten trials entries heuristics value indicate lazy loops section 42 best step section 43 clever outputs section 44 used entries subset size indicate size decomposition 0 meaning decomposition times cpu seconds 500 mhz pentium iii machine running linux e training subset cache options objective number cpu std method size size smo value svs time dev smo 500 500 938975 39325 62545 29585 qpchunk 500 872486 287 931489 table 3 experimental results problem instance data points time series smo statistics four trials times cpu seconds 500 mhz pentium iii machine running linux smo decomposition help large data sets caching policy eective cached elements must relatively high probability reused replaced large data sets goal far dicult achieve moreover smo must periodically loop exemplars order check convergence using smo decomposition makes caching much easier implement eectively make subset size decomposition size cache thus guaranteeing cached elements reused high probability 6 conclusions work shown smo generalized handle regression runtime smo greatly improved datasets dense support vectors main improvement smo implement caching along heuristics assist caching policy general heuristics designed either improve probability cached kernel outputs used exploit fact cached kernel outputs used ways infeasible noncached kernel outputs numerical results show modications smo yield dramatic runtime improvements moreover implementation smo outperform stateoftheart svm optimization packages use conjugate gradient qp solver decomposition kernel evaluations expensive higher input dimensionality believe shown modications smo even valuable larger datasets high input dimensionality preliminary results indicate changes greatly improve performance smo classication tasks involve large highdimensional nonsparse data sets future work concentrate incremental methods gradually increase numerical accuracy also believe improvement smo described 3 adapted regression problems well moreover altering decomposition scheme yield improvements acknowledgements thank tommy poggio john platt edgar osuna constantine papageorgious sayan mukherjee helpful discussions special thanks tommy poggio center biological computational learning mit hosting rst author research e source code availability source code used work part nodelib neural optimization development library nodelib freely available copyleft licensing agreement downloaded httpwwwnecinjneccomhomepagesflakenodelibtgz r tutorial support vector machines pattern recognition kerneladatron fast simple learning procedure support vector machines improvements platts smo algorithm svm classi oscillation chaos physiological control systems nonlinear prediction chaotic time series using support vector machines improved training algorithm support vector machines fast training support vector machines using sequential minimal optimiza tion private communication using sparseness analytic qp speed training support vector chines detecting strange attractors turbulence nature statistical learning theory tr ctr gary w flake eric j glover steve lawrence c lee giles extracting query modifications nonlinear svms proceedings 11th international conference world wide web may 0711 2002 honolulu hawaii usa adriano l oliveira letters estimation software project effort support vector regression neurocomputing v69 n1315 p17491753 august 2006 shuopeng liao hsuantien lin chihjen lin note decomposition methods support vector regression neural computation v14 n6 p12671281 june 2002 chihchung chang chihjen lin training vsupport vector regression theory algorithms neural computation v14 n8 p19591977 august 2002 vivek sehgal lise getoor peter viechnicki entity resolution geospatial data integration proceedings 14th annual acm international symposium advances geographic information systems november 1011 2006 arlington virginia usa quan yong yang jie yao lixiu ye chenzhou improved way tomake largescale svr learning practical eurasip journal applied signal processing v2004 n1 p11351141 1 january 2004 jianxiong dong adam krzyzak ching suen fast svm training algorithm decomposition large data sets ieee transactions pattern analysis machine intelligence v27 n4 p603618 april 2005