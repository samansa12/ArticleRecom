upper lower bounds learning curve gaussian processes paper introduce illustrate nontrivial upper lower bounds learning curves onedimensional guassian processes analysis carried emphasising effects induced bounds smoothness random process described modified bessel squared exponential covariance functions present explanation early linearlydecreasing behavior learning curves bounds well study asymptotic behavior curves effects noise level lengthscale tightness bounds also discussed b introduction fundamental problem systems learning examples estimate amount training samples needed guarantee satisfactory generalisation capabilities new data theoretical interest also vital practical importance example algorithms learn data used safetycritical systems reasonable understanding generalisation capabilities obtained recent years several authors carried analysis issue results presented depend theoretical formalisation learning problem approaches analysis generalisation include based asymptotic expansions around optimal parameter values eg aic akaike 1974 nic murata et al 1994 probably approximately correct convergence approaches eg vapnik 1995 bayesian methods pac uniform convergence methods concerned frequentist style confidence intervals derived randomness introduced respect distribution inputs noise target function central concern results identify flexibility hypothesis class f approximating functions belong example vapnikchervonenkis dimension f note bounds independent input noise densities assuming training test samples drawn distribution problem understanding generalisation capability systems also addressed bayesian framework fundamental assumption concerns kinds function system required model words bayesian perspective need put priors target functions context learning curves bounds analysed average probability distribution functions paper use gaussian priors functions advantage general simple linear regression priors analytically tractable priors functions obtained neural networks neal 1996 shown fixed hyperparameters large class neural network models converge gaussian process priors functions limit infinite number hidden units hyperparameters bayesian neural network define parameters corresponding gaussian process gp williams 1997 calculated covariance functions gps corresponding neural networks certain weight priors transfer functions investigation gp predictors motivated results rasmussen 1996 compared performances obtained gps obtained bayesian neural networks range tasks concluded gps least good neural networks although present study deals regression problems gps also applied classification problems eg barber williams 1997 paper mainly concerned analysis upper lower bounds learning curve gps plot expected generalisation error number training samples n known learning curve many results available concerning leaning curves different theoretical scenarios however many concerned asymptotic behaviour curves usually great practical importance unlikely enough data reach asymptotic regime main goal explain early behaviour learning curves gaussian processes structure paper follows gps regression problems introduced section 2 shown whole theory gps based choice prior covariance function c p x x 0 section 3 present covariance functions using study section 4 learning curve gp introduced present properties learning curve gps well problems may arise evaluating upper lower bounds learning curve gp nonasymptotic regime presented section 5 bounds derived two different approaches one makes use main properties generalisation error whereas derived eigenfunction decomposition covariance function asymptotic behaviour upper bounds also discussed set experiments run order assess upper lower bounds learning curve section 6 present results obtained investigate link tightness bounds smoothness stochastic process modelled gp summary results open questions presented last section gaussian processes collection random variables fy x jx 2 xg indexed set x defines stochastic process general domain x might r dimension although could even general joint distribution characterising statistics random variables gives complete description stochastic process gp stochastic process whose joint distribution gaussian fully defined giving gaussian prior distribution every finite subset variables following concentrate regression problem assuming value target function x generated underlying function x corrupted gaussian noise mean 0 variance oe 2 given collection n training data observed output value input point x would like determine posterior probability distribution p yjx n order set statistical model stochastic process set n random variables modelling function values respectively introduced similarly collection target values denote set training inputs also denote vector whose components test value point x distribution p yjx n inferred using bayes theorem order need specify prior functions well evaluate likelihood model evidence data choice prior distribution stochastic vector gaussian prior distribution gamma2 prior describes distribution true underlying values without reference target values covariance matrix sigma partitioned element k covariance ith jth training points ie k components vector k x covariances test point training data covariance test point gp fully specified mean e covariance function set valid assumption provided known offset trend data removed also deal x 6 0 introduces extra notational complexity discussion possible choices covariance function c p x x 0 given section 3 moment note covariance function assumed depend upon input variables x x 0 thus correlation function values depends upon spatial position input vectors usually chosen closer input vectors higher correlation function values likelihood relates underlying values function target data assuming gaussian noise corrupting data write likelihood likelihood refers stochastic variables representing data 2 r n andomega n theta n matrix given prior distribution values function p bayes rule specifies distribution p yjx n terms likelihood model p tjy evidence data p n given assumptions standard result eg whittle 1963 derive analytic form predictive distribution marginalising predictive distribution turns x n mean variance gaussian function probable value x regarded prediction gp test point x k covariance matrix targets estimate variance oe 2 x posterior distribution considered error bar x following always omit subscript taking understood since estimate 1 linear combination training targets gps regarded linear smoother hastie tibshirani 1990 3 covariance functions choice covariance function crucial one properties two gps differ choice covariance function remarkably diverse due role covariance function incorporate statistical model prior belief underlying function words covariance function analytical expression prior knowledge function modelled misspecified covariance function affects model inference influence evaluation equations 1 2 formally every function produces symmetric positive semidefinite covariance matrix k set input space x chosen covariance function applicative point view interested functions contain information structure underlying process modelled choice covariance function linked priori knowledge smoothness function x connection differentiability covariance function meansquare differentiability process relation smoothness process covariance function given following theorem see eg adler exists finite x x stochastic process x mean square differentiable ith cartesian direction x theorem relevant links differentiability properties covariance function smoothness random process justifies choice covariance function depending upon prior belief degree smoothness x work mainly concerned stationary covariance func tions stationary covariance function translation invariant ie c p x x depends upon distance two data points following covariance functions using presented order simplify notation consider case stationary covariance function squared exponential se defined lengthscale process parameter defines characteristic length process estimating distance input space function x expected vary significantly large value indicates function almost constant input space whereas small value lengthscale designates function varies rapidly graph covariance function shown continuous line figure 1 se function infinitely many derivatives gives rise smooth random processes x posses meansquare differentiability order 1 possible tune differentiability process introducing modified bessel covariance function order k mb k defined exp k delta modified bessel function order see eg equation 8468 gradshteyn ryzhik 1993 set constant c p 1 factors k constants depending order bessel function matern 1980 shows functions mb k define proper covariance stein 1989 also noted process covariance function mb k differentiable study deal modified bessel covariance function orders note mb 1 corresponds ornsteinuhlenbeck covariance function describes process mean square differentiable k 1 mb k behaves like se covariance function easily shown considering power spectra mb k se se exp since lim mb k behaves like se large k provided rescaled accordingly modified bessel covariance functions also interesting describe markov processes order k ihara 1991 defines x strict sense markov process order k differentiable every x 2 r p states gaussian process 1 note definition markov process discrete continuous time rather different discrete time markov process order k depends previous k times continuous time dependence derivatives last time however function values previous times clearly allow approximate computation markov process order k strict sense autoregressive model order k ark power spectrum fourier domain form power spectrum mb k form power spectrum ark model stochastic process whose covariance function mb k strict sense kple markov process characteristic mb k covariance functions important ultimately affects evaluation generalisation error shall see section 6 figure 2 shows graphs four discretised random functions generated using mb k covariance functions se func tion note smoothness random function specified dependent choice covariance function particular roughest function generated ornsteinuhlenbeck covariance function figure whereas smoothest one produced se figure 2d intermediate level regularity characterises functions figures 2b 2c corresponding mb 2 mb 3 respectively note number zerolevel upcrossings 0 1 denoted n u weakly dependent order process mb 2 mb 3 en u derivatives eg via finite differences thus one would expect continuoustime situation previous k process values contain information needed prediction next time note ornsteinuhlenbeck process depends previous observation respectively see papoulis 1991 eqn 167 details se process en u ornsteinuhlenbeck process nondifferentiable formula given en u cannot applied case learning curve gaussian processes learning curve model function relates generalisation error amount training data independent test points well locations training data depends upon amount data training set learning curve gp evaluated estimation generalisation error averaged distribution training test data regression problems measure generalisation capabilities gp squared difference e g target value test point x prediction made using equation 1 bayesian generalisation error point x defined expectation dn x actual distribution stochastic process assumption data set actually generated gp possible read equation 2 bayesian generalisation error x given training data n see let us consider n 1dimensional distribution target values x 1 x zeromean multivariate gaussian prediction test point x hence expected generalisation error x given theta theta theta tt used theta tt k equation 5 identical oe 2 x given equation 2 addition noise variance oe 2 since dealing noisy data variance also calculated vivarelli 1998 covariance matrix pertinent calculations true prior gp predictor different incorrect covariance function used expression generalisation error becomes c indices c denote correct incorrect covariance functions respectively shown vivarelli 1998 always larger equation 5 another property generalisation error derived following observation adding data points never increases size error bars prediction oe 2 n x proved using standard results conditioning multivariate gaussian see vivarelli 1998 also understood information theoretic argument conditioning additional variables never increases entropy random variable considering x random variable observe distribution gaussian variance independent although mean depend entropy gaussian is2 log log monotonic assertion proved argument extension qazaz et al 1997 inequality derived generalized linear regression dn x similar inequality applies also bayesian generalisation errors hence remark applied section 5 evaluating upper bounds learning curve equation 5 calculates generalisation error point x averaging dn x density distribution test points p x expected generalisation error e dn particular choices p x c p x computation expression reduced n theta n matrix computation e x theta theta k x k x also note equation 7 independent test point x still depends upon choice training data n order obtain proper learning curve gp e g dn needs averaged 2 possible choices training data n however difficult obtain analytical form e g gp function n presence k equation 5 matrix k vector k x depend location training points calculations averages respect data points seems hard motivates looking upper lower bounds learning curve gp 5 bounds learning curve noiseless case lower bound generalisation error n observations due michelli wahba 1981 let ordered eigenvalues covariance function domain input space x showed e g n bound learning curve noisy case since bound uses observations consisting projections random function onto first eigenfunctions expected tight observations consist function evaluations results aware pertain asymptotic properties n ritter 1996 shown optimal sampling input space asymptotics generalisation error hansen 1993 showed linear regression models possible average distribution training sets random process obeys sacksylvisaker 3 conditions order see ritter et al 1995 details sacksylvisaker conditions general sacksylvisaker order mb k covariance function 1 example mb 1 process hence generalisation error shows n gamma12 asymptotic decay case x ae r asymptotically optimal design input space uniform grid silverman 1985 proved similar result random designs haussler opper 1997 developed general asymptotic bounds expected loglikelihood test point seeing n training points following introduce upper lower bounds learning curve gp nonasymptotic regime upper bound particularly useful practice provides overestimate number examples needed give certain level performance lower bound similarly important contributes fix limit outperformed model bounds presented derived two different approaches first approach makes use particular form assumed generalisation error x e g x error bar generated one data point greater generated n data points former considered upper bound latter since observation holds variance due one data points envelope surfaces generated loosely speaking stochastic process possessing meansquare derivatives said satisfy sacksylvisaker conditions order variances due data point also upper bound oe 2 n x particular oe 2 dn x cf equation 5 envelope upper bound generalisation error gp following argument assert upper bound e g dn x one generated every gp trained subset n larger subset n tighter bound two upper bounds present differ number training points considered evaluation covariance derivation onepoint upper bound e u 1 n twopoint upper bound e u 2 n presented section 51 section 52 respectively section 53 reports asymptotic expansion e u 1 n terms oe 2 second approach based expansion stochastic process terms eigenfunctions covariance function within framework opper proposed bounds training generalisation error opper vivarelli 1999 terms eigenvalues c p x x 0 lower bound e l n obtained presented section 54 order tractable analytical expressions bounds derived introducing three assumptions input space x restricted interval 0 1 ii probability density distribution input points uniform iii prior covariance function c p x x 0 stationary 51 onepoint upper bound e u derivation onepoint upper bound let us consider error bar generated one data point x since c equation 2 becomes x far away training point x oe 2 confidence prediction test point lying far apart data point x quite low error bar large closer x x smaller error bar x irrespective value c p 0 r varies 0 1 normally c p 0 ae oe 2 thus oe 2 far used hypothesis concerning dimension variable x thus observation holds regardless dimension input space effect one data point helps introducing first upper bound interval 0 1 split n subintervals theta 2 b centred around ith data point x let us consider ith training point error bar oe 2 x x 2 theta 1 relation illustrated figure 3 envelope surfaces errors due datapoint denoted e g x upper bound overall generalisation error since dealing positive functions upper bound expected generalisation error interval theta written p x distribution test points summing contributions coming training datapoint sides equation 8 setting interval contribution variance due x contributes equation 8 also shown figure 3 assumption stationarity covariance function integrals right hand side equation 9 depend upon differences adjacent training points ie x right hand side equation 9 rewritten dx equation 11 derived changing variables two integrals equation equation 11 upper bound e g still depends upon choice training data n interval integration note arguments integrals delta equation 11 differences adjacent training points denoting differences model probability density distribution using theory order statistics david 1970 given uniform distribution n training data interval 0 1 density distribution differences adjacent points p since true differences omit superscript thus expectation integrals equation 11 p n integrals calculated following similar procedure let us consider second line obtained integrating parts last line follows fact able write upper bound learning curve calculations integrals expression straightforward though involve evaluation hypergeometric functions evaluation functions computationally intensive found preferable evaluate equation 14 numerically 52 twopoints upper bound e un second bound introduce natural extension previous idea using two data points rather one construction expect tighter one introduced section 51 let us consider two adjacent data points x x i1 interval 0 1 argument presented previous section following inequality holds 2 x variance prediction x generated data points x x i1 similarly equation 9 summing contributions sides equation 15 get upper bound generalisation error defined calculations see appendix obtain 1 calculation integrals respect e u 2 n complicated determinant delta denominator distribution n preferred evaluate numerically e u 53 asymptotics upper bounds equation 14 expansion e u 1 n terms oe 2 limit large amount training data obtained expansion depends upon covariance function dealing expanding covariance function around 0 asymptotic form e u 1 n mb 1 n whereas functions mb 2 mb 3 se asymptotic value e u depends neither lengthscale process order covariance function mb k k 1 function ratio r lim pointed section 51 minimum generalisation error achievable gp trained one datapoint n 1 scenario corresponds situation every test point close datapoints mentioned beginning section asymptotics learning curve mb k se covariance functions delta respectively although expansions decay asymptotically faster learning curves reach asymptotic plateau oe 2 also note asymptotic values get closer true noise level r 1 ie unrealistic case oe 2 smoothness process enters asymptotics factor factor affects rate approach asymptotic value oe 2 e u 1 n notice larger lengthscales noise levels increase rate decay e u 1 n asymptotic plateau asymptotic form e u 2 n mb 1 mb 2 mb 3 se covariance functions vivarelli 1998 value depends upon choice covariance function 0 similarly expansion e u 1 n decay rate 2 n faster asymptotic decay actual learning curves reaches asymptotic plateau lim straightforward verify asymptotic plateau e u 2 n lower one e u 1 n corresponds error bar estimated gp two observations located test point 54 lower bound e l n opper opper vivarelli 1999 proposed bound learning curve training error based decomposition stochastic process x terms eigenfunctions covariance c p x x 0 denoting k set functions satisfying integral equation z bayesian generalisation error e x true underlying stochastic function x gp predic tion written terms eigenvalues c p x x 0 particular average distribution input data e g n written e g n infinite dimension diagonal matrix eigenvalues v matrix depending training data ie v using jensens inequality possible show lower bound learning curve upper bound training error opper paper mean compare lower bound actual learning curve gp bounds rather must add oe 2 expression obtained equation 23 giving actual lower bound 6 results pointed section 4 analytic calculation learning curve gp infeasible since generalisation error complicated function training data inside elements k x k gamma1 problematic perform integration distribution training points comparing learning curve gp bounds found need evaluate expectation integral equation 25 distribution data e edn theta dn estimate e g n obtained using monte carlo approximation expectation used 50 generations training data sampling uniformly input space 0 1 generation expected generalisation error gp evaluated using 1000 datapoints using 50 generations training data obtain estimate learning curve e g n 95 confidence interval since study focused behaviour bounds learning curve gp assume true values parameters gp known chose value constant covariance functions equation 4 c p allowed lengthscale noise level oe 2 assume several values begin study smoothness process affects behaviour learning curve empirical learning curves figure 4 obtained processes whose covariance functions mb 1 01 notice learning curves exhibit initial linear decrease explained considering without training data generalisation error maximum allowable model c introduction training point x 1 creates hole error surface volume hole proportional value lengthscale depends covariance function addition new data point x 2 effect generating new hole surface data points likely two data lie far apart one giving rise two distinct holes thus effect small dataset exerts pull error surface proportional amount training points explains initial linear trend concerning asymptotic behaviour learning curves verified agree theoretical analysis carried ritter 1996 particular loglog plot learning curves mb k covariance function shows asymptotic behaviour similar remark applies se covariance function asymptotic decay rate opper 1997 also noted smoother process described covariance function smaller amount training data needed reach asymptotic regime behaviour learning curves affected also value lengthscale process noise level illustrated figure 7 learning curves shown figure 5a obtained mb 1 covariance function setting noise level oe 2 01 varying values parameters intuitively figure 5a suggests decreasing lengthscale stretches early behaviour learning curve approach asymptotic plateau lasts longer due effect induced different values lengthscale stretch compress input space verified rescaling amount data n ratio two lengthscales two curves figure 5a lay top variation noise level shifts learning curves prior value c p 0 offset equal noise level cf equation 5 order see significant effect noise learning curve figure 5b shows loglog graph e obtained stochastic process mb 3 covariance function setting notice two main effects noise variance affects actual values generalisation error since learning curve obtained high noise level always one obtained low noise level second effect concerns amount data necessary reach asymptotic regime learning curve characterised high noise level needs fewer datapoints attain asymptotic regime stochastic processes different covariance functions different values lengthscales noise variance behave similar way following discuss results two main subsections results bounds e u 2 n presented section 61 whereas lower bound section 54 shown section 62 results obtained experiments show common characteristics show bounds learning curve obtained setting 61 upper bounds e un e un graph figure 6 shows empirical learning curve confidence interval two upper bounds e u n curves shown mb 1 mb 2 mb 3 se covariance functions limited amount training data possible notice upper error bar associated edn e g n lies actual upper bounds effect due variability generalisation error small data sets suggests bounds quite tight small n effect disappears large n estimate generalisation error less sensitive composition training set expected twopoint upper bound e u 2 n tighter onepoint upper bound e u note tightness upper bound depends upon covariance function tighter rougher processes mb 1 getting worse smoother processes explained recalling covariance functions mb k correspond markov processes order k cf section 3 although markov process actually hidden presence noise e g n still dependent training data lying close test point x distant points since bounds e u calculated using local information namely closest datapoint test point closest datapoints left right respectively natural variance x depends local data points tighter bounds become instance let us consider mb 1 covariance function first order markov process noisefree process knowledge datapoints lying beyond left right neighbours x reduce generalisation error x 4 although noisy case distant datapoints 4 process values training points test point form markov chain knowledge process values left right test point blocks reduce generalisation error term oe 2 covariance matrix k likely local information still important bounds learning curves computed mb 2 mb 3 confirm remark looser mb 1 se covariance function effect still holds actually enlarged section 53 shown asymptotic behaviour bound depends covariance function plots upper bounds confirm analysis carried section 53 showed e u approach asymptotic plateaux particular e u tends oe 2 tends quality bounds processes characterised different length scales different noise levels comparable ones described far tightness e u still depend smoothness process explained beginning section variation lengthscale effect rescaling number training data observed explicitly asymptotic analysis equations 19 decay rate depends factor n fixed covariance function note bounds tighter lower noise variance due fact lower noise level better hidden markov process manifests smaller noise levels influence remote observations learning curve becomes closer bounds generalisation error relies local behaviour processes around test data contrary larger noise level hides underlying markov process thus loosening bounds 62 bound e l n also run experiments computing lower bound obtained equation 24 processes generated covariance priors mb 1 mb 2 mb 3 se equation 24 shows evaluation e l n involves computation infinite sum terms truncated series considering terms add significant contribution sums ie j k oe 2 machine precision since contribution series positive quantity computed still lower bound learning curve figure 7 shows results experiment set 01 graphs lower bound lies empirical learning curve tighter large amount data particular smoothest processes large amount data 95 confidence intervals lay actual lower bound lower bound tends noise level oe 2 empirical learning curve loglog plots e l n show asymptotic decay zero gamma2kgamma12k delta mb k se covariance functions respectively graphs figure 7 show also tightness bound depends smoothness stochastic process particular smooth processes characterised tight lower bound learning curve e g n explained observing e l n lower bound learning curve upper bound training error values smooth functions large variation training points thus model infer better test data reduces generalisation error pulling closer training error since two errors sandwich bound equation 24 e l n becomes tight smooth processes also notice tightness lower bound depends noise level becoming tight high noise level loose small noise level consistent general characteristic e l n monotonically decreasing function noise variance opper vivarelli 1999 paper presented nonasymptotic upper lower bounds learning curve gps theoretical analysis carried onedimensional gps characterised several covariance functions supported numerical simulations starting observation increasing amount training data never worsens bayesian generalisation error upper bound learning curve estimated generalisation error gp trained reduced dataset means given training set envelope generalisation errors generated one two datapoints upper bound actual learning curve gp since expectation generalisation error distribution training data analytically tractable introduced two upper bounds e u 1 n 2 n amenable average distribution test training points study evaluated expected value future directions research also deal evaluation variances order highlight behaviour bounds respect smoothness stochastic process investigated bounds modified bessel covariance function order k describing stochastic processes differentiable squared exponential function describing processes mean squaredifferentiable order 1 experimental results shown learning curves bounds characterised early linearly decreasing behaviour due effect exerted datapoint pulling surface prior generalisation error also noticed tightness bounds depends smoothness stochastic processes due facts bounds rely subsets training data ie one two datapoints modified bessel covariance functions describe markov processes order k although simulations markovian processes hidden noise learning curves depend mainly local information bounds become tighter rougher processes also investigated behaviour curves respect variation correlation lengthscale process variance noise corrupting stochastic process noticed lengthscale stretches behaviour curves effectively rescaling number training data noise level effect hiding underlying markov process upper bounds become tighter smaller noise variance expansion bounds limit large amount data highlights asymptotic behaviour depending upon covariance function approaches asymptotic plateau mb 1 covariance smoother processes rate decay plateau e u 2 n numerical simulations supported analysis one limitation analysis dimension input space bounds made analytically tractable using order statistics results splitting one dimensional input space gp higher dimensional spaces partition input space replaced voronoi tessellation depends data n averaging distribution appears difficult one suggest approximate evaluation upper bounds integration ball whose radius depends upon number examples volume input space bound holds case expect effect due larger input dimension loosen upper bounds note recent work sol lich 1999 derived good approximations learning curve methods apply one dimension 5 also ran experiments using lower bound proposed opper based knowledge eigenvalues covariance function process since bound e l n also upper bound training error observed bound tighter smooth processes learning curve becomes closer training error also noise vary tightness e l n low noise level loosens lower bound unlike upper bounds lower bound applied also multivariate problems easily extended high dimension input space however verified opper vivarelli 1999 bound becomes less tight input space higher dimension appendix twopoints upper bound e u appendix derive equation 17 starting equation 16 start calculating oe 2 x covariance matrix generated two data points 2 theta 2 matrix straightforward evaluate oe 2 considering two training data x x i1 covariance matrix 5 reference sollich 1999 added manuscript revised april 1999 gp evaluation determinant k covariance vector test point x k variance assumes form changing variables covariances c p turns upper bound generated oe 2 2 x interval theta 6 0 n 1 1 2 noticeable similarly equation 11 also integrals 1 delta 2 delta determinant delta depend upon length interval integration evaluate contributions upper bound intervals theta 0 x 1 x n 1 integrating variance oe 2 generated x 1 x n theta 0 x 1 x n 1 respectively hence right hand side equation 16 rewritten 1 delta defined equation 12 equation 26 still dependent distribution training data function distances adjacent training points similarly equation 11 obtain upper bound independent training data integrating equation 13 distribution differences gammac acknowledgments research forms part validation verification neural network systems project funded jointly epsrc grk 51792 british aerospace thank dr manfred opper dr andy wright bae helpful discussions also thank anonymous referees comments helped improve paper f v supported studentship british aerospace r geometry random fields new look statistical model identification gaussian processes bayesian classification via hybrid monte carlo order statistics table integrals stochastic linear learning exact test training error averages generalized additive models mutual information information theory design problems optimal surface interpolation network information criteriondetermining number hidden units artificial neural network models bayesian learning neural networks lecture notes statistics 118 regression gaussian processes average case per formance general bounds bayes errors regression gaussian processes upper bound bayesian error bars generalized linear regression evaluation gaussian processes methods nonlinear regression almost optimal differentiation using noisy data multivariate integration approximation random fields satisfying sacks ylvisaker conditions aspects spline smoothing approach nonparametric regression curve filtering learning curves gaussian processes theory learnable nature statistical learning theory studies generalisation gaussian processes bayesian neural networks prediction regulation linear least square methods computing infinite networks figures 6a figure 7 figures 7a tr ctr peter sollich anason halees learning curves gaussian process regression approximations bounds neural computation v14 n6 p13931428 june 2002