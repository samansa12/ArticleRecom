linear programming boosting via column generation examine linear program lp approaches boosting demonstrate efficient solution using lpboost column generation based simplex method formulate problem possible weak hypotheses already generated labels produced weak hypotheses become new feature space problem boosting task becomes construct learning function label space minimizes misclassification error maximizes soft margin prove classification minimizing 1norm soft margin error function directly optimizes generalization error bound equivalent linear program efficiently solved using column generation techniques developed largescale optimization problems resulting lpboost algorithm used solve lp boosting formulation iteratively optimizing dual misclassification costs restricted lp dynamically generating weak hypotheses make new lp columns provide algorithms soft margin classification confidencerated regression boosting problems unlike gradient boosting algorithms may converge limit lpboost converges finite number iterations global solution satisfying mathematically welldefined optimality conditions optimal solutions lpboost sparse contrast gradient based methods computationally lpboost competitive quality computational cost adaboost b introduction recent papers 16 shown boosting arcing related ensemble methods hereafter summarized boosting viewed margin maximization function space changing cost function dierent boosting methods adaboost viewed gradient descent minimize cost function authors noted possibility choosing cost functions formulated linear programs lp dismiss approach intractable using standard lp algorithms 14 6 paper show lp boosting computationally feasible using classic column generation simplex algorithm 11 method performs tractable boosting using cost function expressible lp specifically examine variations 1norm soft margin cost function used support vector machines 15 3 9 one advantage approaches immediately method analysis support vector machine problems becomes applicable boosting problem section 2 prove lpboost approach classification directly minimizes bound generalization error adapt lp formulations developed support vector machines section 3 discuss soft margin lp formulation adopting linear programming immediately tools mathematical programming disposal use duality theory optimality conditions gain insight lp boosting works mathematically section 4 examine column generation approaches solving large scale lps adapted boosting classification examine standard confidencerated boosting standard boosting algorithms use weak learners classifiers whose outputs set 1 1 schapire singer 17 considered boosting weak learners whose outputs reflected classification also associated confidence encoded value range 1 1 demonstrate socalled confidencerated boosting speed convergence composite classifier though accuracy long term found significantly aected section 5 discuss minor modifications needed lpboost perform confidencerated boosting methods develop readily extended boosting problem formulated lp demonstrate adapting approach regression section 6 computational results practical issues implementation method given section 7 motivation soft margin boosting begin analysis boosting problem using methodology developed support vector machines function classes considering form set weak learners assume closed complementation initially classification functions outputs set 1 1 though taken 1 1 confidencerated boosting begin however looking general function class quoting bound generalization error terms margin covering numbers first introduce notation distribution inputs targets x 1 1 define error err f function f f probability assume obtain classification function thresholding 0 f realvalued definition 21 let f class realvalued functions domain x cover f respect sequence inputs set functions f f exists size smallest cover denoted nf covering numbers f values nf remainder section assume training set realvalued function f f define margin example x yfx implicitly assume thresholding 0 margin training set defined note quantity positive function correctly classifies training examples following theorem given 8 implicit results 18 theorem 21 consider thresholding realvalued function space f fix r probability distribution x 1 1 probability 1 random examples hypothesis f f margin f error err f f log nf 2m describe construction originally proposed 19 applying result cases points attain margin let x hilbert space define following inner product space derived x definition 22 let lx set realvalued functions f x countable support suppf functions lx nonzero countably many points formally require countable define inner product two functions f g lx f g xsuppf fxgx implicitly defines norm 2 also introduce fx note sum defines inner product welldefined cauchyschwarz inequality clearly space closed addition multiplication scalars furthermore inner product linear arguments form product space x lx corresponding function class f lx acting via composition rule fixed 1 0 define embedding x product space x lx follows x lx defined x definition 23 consider using class f realvalued functions input space x classification thresholding 0 define margin slack variable example respect function f f target margin quantity x implies incorrect classification construction space xlx allows us obtain margin separation using auxiliary function defined terms margin slack variables function f target margin auxiliary function respect training set simple calculation check following two properties function f 1 f g f margin training set 2 f g f together facts imply generalization error f assessed applying large margin theorem f g f gives following theorem theorem 22 consider thresholding realvalued function space f domain x fix choose g f lx probability distribution x 1 1 probability random examples hypothesis f f f g f g generalization error err f f log ng 2m 2 discrete probability misclassified training points position apply results function class form described left open time class h learners might contain sets g theorem 22 chosen follows h h g hence condition function satisfies conditions theorem 22 simply 1 note quantity minimize boosting iterations described later sections use parameter c place 1 margin set 1 final piece puzzle require apply theorem 22 bound covering numbers gb terms class weak learners h bound b margin launching analysis observe input x 21 covering numbers convex hulls subsection analyze covering numbers ngb set h h g terms b class h scale assume first bcover g function class h respect set class binaryvalued functions take zero g set dichotomies realized class consider set v vectors positive real numbers indexed g 1 let vb function class suppose u cover vb claim set cover gb respect set prove assertion taking general function h g gb finding function within points x first h nonzero coecient h select h g hx form function lies set vb since hg h furthermore since u cover vb exists v u f follows f within f set hence forms cover class gb bound u using following theorem due 20 though slightly weaker version also found 1 theorem 23 20 class vb defined log nvb log hence see optimizing b directly optimizes relevant covering number bound hence generalization bound given theorem 22 note cases considered g growth function bh class h weak learners boosting lp classification discussion see soft margin cost function valuable boosting classification functions using techniques used support vector machines formulate problem linear program quantity b defined equation 1 optimized directly using lp lp formulated possible labelings training data weak learners known lp minimizes 1norm soft margin cost function used support vector machines added restrictions weights positive threshold assumed zero lp variants practically solved using column generation approach weak learners generated needed produce optimal support vector machine based output weak learners essence base learner become oracle generates necessary columns dual variables linear program provide misclassification costs needed learning machine column generation procedure searches best possible misclassification costs dual space optimality actual ensemble weak learners constructed 31 let matrix h n matrix possible labelings training data using functions label 1 1 given weak learner h j h training point x column h j matrix h constitutes output weak learner h j training data row h gives outputs weak learners example x may 2 distinct weak learners following linear program used minimize quantity equation 1 min n 2 c 0 tradeo parameter misclassification error margin maximization dual lp 2 alternative soft margin lp formulations exist one lp boosting 1 14 remove constraint 0 since 0 optimality complementation assumption dual lp 4 min u lp formulations exactly equivalent given appropriate choice parameters c proofs fact found 15 5 state theorem theorem 31 lp formulation equivalence lp 4 parameter primal solution dual solution u primal dual solutions lp 2 parameter similarly lp 2 parameter c primal solution dual solution primal dual solutions lp 4 parameter practically found lp 4 preferable interpretability parameter extensive discussion development characteristics svm classification found 15 maintain dual feasibility parameter must maintain 1 1 picking appropriately force minimum number support vectors know number support vectors number points misclassified plus points margin used heuristic choice reader consult 14 15 indepth analysis family cost functions 32 properties lp formulation examine characteristics lp 4 optimality conditions gain insight properties lp boosting useful understanding eects choice parameters model performance eventual algorithm optimality conditions 11 lp 4 primal feasibility dual feasibility complementarity stated equality primal dual objectives complementarity expressed using many equivalent formulations example complementarity property following equations hold svm optimality conditions tell us many things first characterize set base learners positively weighted optimal ensemble recall primal variables multiply base learner dual lp assigns misclassification costs u point u sum 1 dual constraint scores weak learner h j score weighted sum correctly classified points minus weighted sum incorrectly classified points weak learners lower scores greater weighted misclassification costs formulation pessimistic sense set best weak learners given u score dual objective minimizes optimal misclassification cost u pessimistic one ie minimizes maximum score learners complementary slackness condition weak learners scores equal positive weights j primal space resulting ensemble linear combination weak learners perform best pessimistic choice misclassification costs interpretation closely corresponds game strategy approach 6 also lp boosting formulation solvable lpboost notable dierence lp 5 additional upper bound misclassification costs u 0 produced introduction soft margin primal svm research know primal dual solutions sparse degree sparsity greatly influenced choice parameter size dual feasible region depends choice large forcing small dual problem infeasible large still feasible small still feasible problem degrades something close equalcost case u u forced nonzero practically means increases optimal solution frequently single weak learner best assuming equal costs decreases grows misclassification costs u increase hardtoclassify points points margin label space go 0 points easy classify thus misclassification costs u become sparser small large meaningless null solution 0 points classified one class becomes optimal good choice sparse solution primal ensemble weights optimal implies weak learners used also sparse dual u optimal means solution dependent smaller subset data support vectors data wellclassified sucient margin performance data critical lp sensitivity analysis know u exactly sensitivity optimal solution small perturbations margin sense sparseness u good weak learners constructed using smaller subsets data see section 7 sparseness misclassification costs lead problems practically implementing algorithms lpboost algorithms examine practical algorithms solving lp 4 since matrix h large number columns prior authors dismissed idea solving lp formulations boosting intractable using standard lp techniques column generation techniques solving lps existed since 1950s found lp text books see example 11 section 74 column generation frequently used largescale integer linear programming algorithms commercial codes cplex optimized perform column generation eciently 7 simplex method require matrix h explicitly available iteration subset columns used determine current solution called basic feasible solution simplex method needs means determining current solution optimal means generating column violates optimality conditions tasks verification optimality generating column performed learning algorithm simplexbased boosting method alternate solving lp reduced h corresponding weak learners generated far using weak learning algorithm generate bestscoring weak learner based dual misclassification cost provided lp continue welldefined exact approximate stopping criterion reached idea column generation cg restrict primal problem 2 considering subset possible labelings based weak learners generated far ie subset h columns h used lp solved using h typically referred restricted master problem solving restricted primal lp corresponds solving relaxation dual lp constraints weak learners generated yet missing one extreme case weak learners considered case optimal dual solution appropriate choice provide initialization algorithm consider unused columns feasible original primal lp u feasible original dual problem done since primal dual feasibility equal objectives optimal u infeasible dual lp full matrix h specifically violated least one weak learner equivalently j course want priori generate columns h h j use weak learner oracle either produces hj j guarantee h j exists speed convergence would like find one maximum deviation weak learning algorithm hs u must deliver function h satisfying thus becomes new misclassification cost example given weak learning machine guide choice next weak learner one big payos approach stopping criterion weak learner h current combined hypothesis optimal solution linear combinations weak learners also gauge cost early stopping since max hh 0 obtain feasible solution full dual problem taking u hence value v optimal solution bounded implies even potentially include nonzero coecient weak learners value objective increased assume existence weak learning algorithm hs u selects best weak learner set h closed complementation using criterion equation 10 following algorithm results algorithm 41 lpboost given input training set learners 0 coecients 0 1 corresponding optimal dual repeat find weak learner using equation hn hsu check optimal solution h hn solve restricted master new costs argmin st end lagrangian multipliers last lp return note assumption finding best weak learner essential good performance algorithm recall role learning algorithm generate columns weak learners corresponding dual infeasible row indicate optimality showing infeasible weak learners exist require base learner return column corresponding dual infeasible row need one maximum infeasibility merely done improve convergence speed fact choosing columns using steepest edge criteria look column leads biggest actual change objective may lead even faster convergence learning algorithm fails find dual infeasible learner one exists algorithm may prematurely stop nonoptimal solution small changes algorithm adapted perform lp boosting formulations simply changing restricted master lp solved costs given learning algorithm optimality conditions checked assuming base learner solves 10 exactly lpboost variant dual simplex algorithm 11 thus inherits benefits simplex algorithm benefits include 1 welldefined exact approximate stopping criteria typically ad hoc termination schemes eg fixed number iterations must used gradientbased boosting algorithms 2 finite termination globally optimal solution practice algorithm generates weak learners arrive optimal solution optimal solution sparse thus uses weak learners 4 algorithm performed dual space classification costs weights optimal ensemble generated fixed optimality 5 highperformance commercial lp algorithms optimized column generation exist suer numeric instability problems reported boosting 2 5 confidencerated boosting derivations algorithm last two sections rely assumption l ij 1 1 therefore apply reasoning implementing weak learning algorithm finite set confidencerated functions f whose outputs real numbers assume f closed complementation simply define apply algorithm assume existence weak learner f u finds function dierence associated algorithm weak learner optimizes equation algorithm 51 lpboostcrb given input training set learners 0 coecients 0 corresponding optimal dual repeat find weak learner using equation check optimal solution h f n solve restricted master new costs argmin st end lagrangian multipliers last lp return 6 lpboost regression lpboost algorithm extended optimize ensemble cost function formulated linear program solve alternate formulations need change lp restricted master problem solved iteration criteria given base learner assumptions current approach number weak learners finite improving weak learner exists base learner generate see simple example consider problem boosting regression functions use following adaptation svm regression formulations lp also adapted boosting using barrier algorithm 13 assume given training set data may take real value first reformulate problem slightly dierently st h introduce lagrangian multipliers u u construct dual convert minimization problem yield st restricted weak learners constructed far becomes new master problem base learner returns hypothesis h j dual feasible ie ensemble optimal weak learner added ensemble speed convergence would like weak learner maximum deviation ie perhaps odd first glance criteria actually explicitly involve dependent variables within lpboost algorithm u closely related error residuals current ensemble data point x overestimated current ensemble function complementarity u positive u next iteration weak learner attempt construct function negative sign point x point x falls within margin u next weak learner try construct function value 0 point data point x underestimated current ensemble function complementarity positive u next iteration weak learner attempt construct function positive sign point x sensitivity analysis magnitudes u proportional changes objective respect changes margin becomes even clearer using approach taken barrier boosting algorithm problem 13 equation 15 converted least squares problem objective optimized weak learner transformed follows constant term v 2 ignored eectively weak learner must construct regularized least squares approximation residual function final regression algorithm looks much like classification case variables u u initialized initial feasible point present one strategy assuming suciently large denotes plus function table 1 average accuracy standard deviations boosting using decision tree stumps stumps final ensemble lpboost n ab100 ab1000 cancer ionosphere algorithm 61 lpboostregression given input training set learners 0 coecients 0 corresponding feasible dual repeat find weak learner using equation check optimal solution h hn solve restricted master new costs st end lagrangian multipliers last lp return 7 computational experiments performed three sets experiments compare performance lpboost crb adaboost three classification tasks one boosting decision tree stumps smaller datasets two boosting c45 12 decision tree stumps six datasets used c45 experiments report results four large datasets without noise finally validate c45 experimented ten additional datasets rationale first evaluate lpboost base learner solves 10 exactly examine lpboost realistic environment using c45 base learner datasets obtained ucirvine data repository 10 c45 experiments performed traditional confidence rated boosting 71 boosting decision tree stumps used decision tree stumps base learner following six datasets cancer 9699 diagnostic 30569 heart 13297 ionosphere 34351 musk 166476 sonar 60208 number features number points dataset shown respectively parentheses report testing set accuracy dataset based 10fold cross validation cv generate decision tree stumps based midpoint two consecutive points given variable since limited confidence information stumps perform confidencerated boosting boosting methods search best weak learner returns least weighted misclassification error iteration lpboost take advantage fact weak learner need added ensemble thus stump added ensemble never evaluated learning algorithm weights weak learners adjusted dynamically lp advantage adaboost since adaboost adjust weights repeatedly adding weak learner ensemble parameter lpboost set using simple heuristic 01 added previouslyreported error rates dataset 4 except cancer dataset specifically values order datasets given 02 01 025 02 025 03 results adaboost reported maximum number iterations 100 1000 10fold average classification accuracies standard deviations reported table 1 lpboost performed well terms classification accuracy number weak learners training time little dierence accuracy lpboost best accuracy reported adaboost using either 100 1000 iterations variation adaboost 100 1000 iterations illustrates importance welldefined stopping criteria typically adaboost obtains solution limit thus stops maximum number iterations heuristic stopping criteria reached magic number iterations good datasets lpboost welldefined stopping criterion reached iterations uses weak learners 81 possible stumps breast cancer dataset nine attributes nine possible values clearly adaboost may require tree generated multiple times lpboost generates weak learner alter weight weak learner iteration run time lpboost proportional number weak learners generated since lp package used cplex 40 7 optimized column generation cost adding column reoptimizing lp iteration small iteration lpboost slightly expensive iteration adaboost time proportional number weak learners generated problems lpboost generates far fewer weak learners much less computationally costly next subsection test practicality methodology dierent datasets using c45 72 boosting c45 lpboost c45 base algorithm performed well operational challenges solved concept boosting using c45 straightforward since c45 algorithm accepts misclassification costs one problem c45 finds good solution guaranteed maximize 10 eect convergence speed algorithm may cause algorithm terminate suboptimal solution another challenge misclassification costs determined lpboost sparse ie points dual lp basic feasible solution corresponding vertex dual feasible region variables corresponding basic solution nonnegative face region corresponding many nonnegative weights may optimal vertex solution chosen practice found many lpboost converged slowly limited number iterations allowed 25 lpboost frequently failed find weak learners improved significantly initial equal cost solution weak learners generated using subsets variables necessarily good full data set thus search slow alternative optimization algorithms may alleviate problem example interior point strategy may lead significant performance improvements note authors reported problems underflow boosting 2 lpboost solved optimality decision tree stumps full evaluation weak learners problem occur boosting unpruned decision trees helped somewhat completely eliminate problem stability convergence speed greatly improved adding minimum misclassification costs table 2 large dataset results boosting c45 lpboost crb adaboost c45 original forest 07226 07259 07370 06638 original adult 08476 08461 08358 08289 original usps 09123 09103 09103 07833 original optdigits 09249 09355 09416 07958 dual min corresponding primal problem primal problem maximizes two measures soft margin corresponds minimum margin obtained points measures additional margin obtained point adaboost also minimizes margin cost function based margin obtained point one method boosting multiclass problems investigation multiclass approaches needed ran experiments larger datasets forest adult usps optdigits uci10 lpboost adopted multiclass problem defining h j instance x correctly classified weak learner h j 1 otherwise forest 54dimension dataset seven possible classes data divided 11340 training 3780 validation 565892 testing instances missing values 15dimensional adult dataset 32562 training 16283 testing instances one training point missing value class label removed use 8140 instances training set remaining 24421 instances validation set adult twoclass dataset missing values default handling c45 used missing values usps optdigits optical character recognition datasets usps 256 dimensions without missing value 7291 original training points use 1822 points training data rest 5469 validation data 2007 test points optdigits hand 64 dimensions without missing values original training set 3823 points use 955 training data remaining 2868 validation data parameter selection lpboost adaboost done based validation set results since initial experiments resulted parameter set lpboost crb set parameters equal crb lpboost expedite computational work order investigate performance boosted c45 noisy data introduced 15 label noise four datasets parameter used lpboost number iterations adaboost significantly aect performance thus accuracy validation set used pick parameter lpboost number iterations adaboost due excessive computational work limit maximum number iterations 25 boosting methods 2 varied parameter 003 011 initial experiments indicated small values lpboost results one classifier assigns training points one class extreme larger values lpboost returns one classifier parameter080084003 parameter080084 forest dataset b adult dataset parameter090094 parameter092096 c usps dataset optdigits dataset figure 1 validation set accuracy value triangles noise circles noise equal one found first iteration figure 1 shows validation set accuracy lpboost four datasets based validation set results use 2219 254 2225 2525 number iterations original 15noisy data respectively adaboost forest adult usps optdigits datasets testing set results using value best validation set accuracy given table 2 lpboost comparable adaboost terms cpu time seen table 2 lpboost also comparable adaboost terms classification accuracy validation set used pick best parameter settings adaboost performs better case noisy data crb least eective method terms classification accuracy among boosting methods boosting methods outperform c45 computational costs 25 iterations lpboost either variant adaboost similar provide sample cpu times timings considered rough estimates experiments performed cluster ibm rs6000s used batch mode since machines identical subject varying loads run times vary considerable run run dataset give seconds cpu time rs6000 forest adaboost 717 lpboost 930 adult adaboost 107 also conducted experiments boosting c45 small datasets strong evidence superiority boosting approaches addition six uci datasets used decision tree stumps experiments use four additional uci datasets house16435 hous datasets decision tree stumps experiments report results 10fold cv since best value lpboost varies 005 01 large datasets pick parameter small datasets results reported table 3 c45 performed best house dataset adaboost performed best four datasets ten lpboost crb best classification performance three two datasets respectively drop crb table 2 continuous response variable housing dataset categorized 215 table 3 small dataset results boosting c45 lpboost crb adaboost c45 cancer 09585 house 09586 housing ionosphere 09373 3 lpboost would case perform best five datasets although parameter tuned 8 discussion extensions shown lp formulations boosting attractive theoretically terms generalization error bound computationally via column generation lpboost algorithm applied boosting problem formulated lp examined algorithms based 1norm soft margin cost functions support vector machines generalization error bound found classificaiton case lp optimality conditions allowed us provide explanations methods work classification dual variables act misclassification costs optimal ensemble consists linear combination learners work best worst possible choice misclassification costs explanation closely related 6 regression discussed barrier boosting approach formulation 13 dual multipliers act like error residuals used regularized least square problem demonstrated ease adaptation boosting problems examining confidencerated regression cases extensive computational experiments found method performed well versus adaboost respect classification quality solution time found little clear benefit confidencerated boosting c45 decision trees optimization perspective lpboost many benefits gradientbased approaches finite termination numerical stability welldefined convergence criteria fast algorithms practice fewer weak learners optimal ensemble lpboost may sensitive inexactness base learning algorithm modification base lp able obtain good performance wide spectrum datasets even boosting decision trees assumptions learning algorithm violated questions best lp formulation boosting best method optimizing lp remain open interior point column generation algorithms may much ecient clearly lp formulations classification regression tractable using column generation subject research acknowledgements material based research supported microsoft research nsf grants 949427 iis9979860 european commission working group nr 27150 neurocolt2 r learning neural networks empirical comparison voting classification algorithms bagging combining support vector mathematical programming methods classification column generation approach boosting prediction games arcing algorithms introduction support vector machines generalized support vector machines uci repository machine learning databases new york barrier boosting robust ensemble learning boosting margin new explanation e improved boosting algorithms using confidencerated predictions structural risk minimization datadependent hierarchies margin distribution bounds generalization analysis regularised linear functions classification problems tr ctr cynthia rudin ingrid daubechies robert e schapire dynamics adaboost cyclic behavior convergence margins journal machine learning research 5 p15571595 1212004 jinbo bi tong zhang kristin p bennett columngeneration boosting methods mixture kernels proceedings tenth acm sigkdd international conference knowledge discovery data mining august 2225 2004 seattle wa usa robust loss functions boosting neural computation v19 n8 p21832244 august 2007 yi zhang samuel burer w nick street ensemble pruning via semidefinite programming journal machine learning research 7 p13151338 1212006 yijun sun sinisa todorovic jian li increasing robustness boosting algorithms within linearprogramming framework journal vlsi signal processing systems v48 n12 p520 august 2007 michael collins parameter estimation statistical parsing models theory practice distributionfree methods new developments parsing technology kluwer academic publishers norwell 2004 axel pinz object categorization foundations trends computer graphics vision v1 n4 p255353 december 2005 ron meir gunnar rtsch introduction boosting leveraging advanced lectures machine learning springerverlag new york inc new york ny