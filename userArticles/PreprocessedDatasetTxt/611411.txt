webconscious storage management web proxies many proxy servers limited file io needs even proxy configured sufficient io hardware file system software often fails provide available bandwidth proxy processes although specialized file systems may offer significant improvement overcome limitations believe userlevel disk management top industrystandard file systems offer similar performance advantages paper study overheads associated file io web proxies investigate underlying causes propose webconscious storage management set techniques exploit unique reference characteristics webpage accesses order allow web proxies overcome file io limitations using realistic tracedriven simulations show techniques improve proxys secondary storage io throughput factor 15 traditional opensource proxies enabling single disk serve 400 urlget operations per second demonstrate approach implementing foxy web proxy incorporates techniques experimental evaluation suggests foxy outperforms traditional proxies squid factor four throughput without sacrificing response latency b introduction world wide web proxies increasingly used provide internet access users behind firewall reduce widearea network traffic caching frequently used urls given web traffic still increases exponentially web proxies one major mechanisms reduce overall traffic core internet protect network servers traffic surges improve end user experience todays typical web proxies usually run unixlike operating systems associated file systems unixlike file systems widely available highly reliable often result poor performance web proxies instance rousskov soloviev 35 observed disk delays web proxies contribute 30 toward total hit response time mogul 28 observed disk io overhead caching turns much higher latency improvement cache hits web proxy digital palo alto firewall thus save disk io overhead proxy typically run noncaching mode observations surprising unixlike filesystems optimized generalpurpose workloads 27 30 21 web proxies exhibit distinctly different workload example read operations outnumber write operations traditional unixlike file systems 3 web accesses induce write dominated workload 25 moreover several common files frequently updated urls rarely ever updated short traditional unixlike file access patterns different web access patterns therefore file systems optimized unixlike workloads necessarily perform well webinduced workloads article study overheads associated disk io web proxies propose webconscious storage management webcosm set techniques designed specifically high performance show single important source overhead associated storing url separate file propose methods aggregate several urls per file reduce file management overhead show next largest source overhead associated disk head movements due file write requests widely scattered disk sectors improve write throughput propose file space allocation algorithm inspired logstructured file systems 34 write operations proceed maximum speed url read operations emerge a3 cache clients url response request web server url request lookup cache web server proxy server response 300111100110000011111 figure 1 typical web proxy action sequence next largest source overhead reduce disk read overhead identify exploit locality exists url access patterns propose algorithms cluster several read operations together reorganize layout urls file urls accessed together stored nearby file locations demonstrate applicability approach implementing foxy web proxy incorporates webcosm techniques successfully remove disk bottlenecks proxys critical path evaluate performance approach use combination tracedriven simulation experimental evaluation using simulations driven real traces show file io throughput improved factor 18 enabling single disk proxy serve around 500 urlget operations per second finally show implementation outperforms traditional proxies squid factor 7 heavy load begin investigating workload characteristics web proxies implications file system performance section 2 motivate describe webcosm techniques section 3 present comprehensive performance results show superior performance potential techniques verify validity webcosm approach section 4 outlining implementation proofofconcept proxy server evaluating performance experiments compare findings paper related research results section 5 discuss issues regarding webcosm section 6 summarize findings section 7 webconscious storage management techniques traditional web proxies frequently require significant number operations serve url request consider web proxy serves stream requests originating clients request receives proxy lookup cache particular url proxy local copy url cache miss requests url actual server saves copy data local storage delivers data requesting client hand proxy local copy url cache hit reads contents local storage delivers client action sequence illustrated figure 1 simplify storage management traditional proxies 42 8 store url separate file induces several file system operations order serve request indeed least three file operations needed serve url miss old file usually needs deleted order make space new url ii new file needs created order store contents url iii new file needs filled urls contents similarly url hits costly operations even though change files data metadata invoke one file read operation traditionally expensive due disk head movements thus url request operation regardless hit miss file system faced intensive stream file operations file operations usually time consuming easily become bottleneck busy web proxy instance takes 20 milliseconds create even empty file seen figure 2 1 make matters worse takes another 10 milliseconds delete empty file file creation deletion cost approaches milliseconds large files 10 kbytes given urlmiss proxy create delete file proxy able serve one url miss every 60 milliseconds equivalently less quantify performance limitations file system operations experimented hbenchos file system benchmark 6 hbenchos evaluates file creationdeletion cost creating 64 files given size directory deleting reverse ofcreation order run hbenchos ultrasparc1 running solaris 56 file sizes 0 10 kbytes results plotted figure 2 suggest time required create delete file high file size kbytes1030time per operation file create file delete figure 2 file management overhead figure plots cost file creation file deletion operations measured hbenchos latfs benchmark creates 64 files deletes order creation process repeated files varying sizes see time create file 20 msec time delete file 10 20 msec per second provide clients 100200 kbytes data throughput two orders magnitude smaller modern magnetic disks provide throughput even smaller internet connections consequently file system webproxy able keep proxys internet requests disparity file systems performance proxys needs due mismatch storage requirements needed web proxy storage guarantees provided file system address semantic mismatch two ways metadata overhead reduction datalocality exploitation 21 metadata overhead reduction metadata overhead cripples web proxy performance traced storage url separate file eliminate performance bottleneck propose novel urlgrouping method called buddy store urls small number files simplify space management within files use url size grouping criterion urls need one block disk space 512 bytes stored file urls need two blocks disk space 1024 bytes stored another file forth way file composed fixedsized slots large enough contain url new url stored first available slot appropriate file detailed behavior buddy follows initially buddy creates one file store urls smaller one block another file store urls larger block smaller two urls larger predefined threshold stored separate files one url per file ffl urlwrite request given size buddy finds first free slot appropriate file stores contents new url size contents url threshold 128 kbytes experiments buddy creates new file store url ffl url needs replaced buddy marks corresponding slot appropriate file free slot reused later time store another url ffl urlread request buddy finds slot appropriate file reads content requested url main advantage buddy practically eliminates overhead file creationdeletion operations storing potentially thousands urls per file urls occupy whole file large enough represent tiny percentage total number urls file creationdeletion overhead noticeable overall 22 datalocality exploitation although buddy reduces file management overhead makes special effort layout data intelligently disk way improves write read performance however experience suggests significant amount locality exists url reference streams identifying exploiting locality result large performance improvements 221 optimizing write throughput traditional proxies proposed buddy technique write new url data several different files scattered disk possibly requiring head movement operation url write number head movements significantly reduced write data disk logstructured fashion thus instead writing new data free space disk continually append data reach end disk case continue beginning approach widely used logstructured file systems 5 17 29 38 however unlike previous implementations logstructured file systems use userlevel logstructured file management achieves effectiveness logstructured file systems top commercial operating systems towards end develop filespace management algorithm called stream muchlike logstructured file systems streams write operations disk web proxy stores urls single file organized slots 512 bytes long url occupies integer number usually contiguous slots url read operations read appropriate portions file correspond given url urlwrite operations continue appending data file end file case new urlwrite operations continue beginning file writing free slots urldelete operations mark space currently occupied url free later reused future urlwrite operations 222 improving read requests stream improve performance write operations urlread operations still suffer disk seek rotational overhead disk head must move point writing data disk point read data make matters worse read operation completed head must move back point read operation continue writing data onto disk reason read operation within stream writes induces two head movements first move head reading position second restore head previous writing position resulting pingpong effect reduce overhead developed lazyreads technique extends stream batches read operations urlread operation issued serviced immediately instead sent intermediate buffer buffer fills read requests timeout period expires pending read requests forwarded file system sorted according position file data want read using lazyreads batch n urlread requests served n movements figure 3 illustrates movements heads lazyreads although lazyreads appear increase latency urlread operations subsecond timeout period guarantees unnoticeable latency increase read operations 223 preserving locality url stream url requests arrive web proxy exhibit significant amount spatial locality example consider case html page contains several embedded images time client requests html page probably request embedded images well page embedded images usually accessed together single object indication relationship requests sent proxy server sequentially within short time interval unfortunately requests arrive head position time read operations 6 head movements stream write operations without lazyreads head position time read operations 4 head movements stream write operations b lazyreads figure 3 disk head distance using stream technique disk receives stream write requests contiguous blocks interrupted read requests cause pingpong effect part three read requests responsible six long head movements part b lazyreads technique services three read requests together four disk head movements sequentially proxy server instead arrive interleaved requests several clients therefore web objects requested contiguously single client may serviced stored proxys disk subsystem interleaved web objects requested totally unrelated clients interleaved disk layout may result significant performance degradation future accesses one related web objects may require separate disk head movement make matters worse interleaving may result serious disk fragmentation related objects evicted disk leave behind set small noncontiguous portions recover lost locality augmented stream lazyreads techniques extra level buffers called locality buffers proxy server file system locality buffer associated web server accumulates objects originate web server instead immediately writing web object next available disk block proxy server places object locality buffer associated origin server locality buffer found proxy empties one locality buffers flushing disk creates new association newly freed locality buffer objects web server buffer fillsup evicted contents sent disk probably written contiguous disk locations figure 4 outlines operation proxy server augmented locality buffers figure shows three clients requesting different stream web objects a1a3 b1b2 c1c2 requests arrive interleaved proxy server forward internet appropriate web servers without locality buffers responses serviced stored disk interleaved fashion introduction locality buffers groups requested web objects according origin web server stores groups disk contiguously possible reducing fragmentation interleaving future read operations benefit reduced interleaving use prefetching techniques read multiple related urls single disk io even future write operations benefit reduced fragmentation since able write data disk contiguous fashion 3 simulationbased evaluation evaluate disk io performance web proxies using combination simulation experimental evaluation simulation study model web proxy storage system organized two level cache mainmemory disk cache using traces obtained busy web proxies drive twolevel cache simulator turn generates necessary secondary storage requests requests translated file system operations sent solaris ufs file system actual magnetic disk therefore methodology use simulation identify cache read write delete operations sent real storage system order accurately measure execution time thus methodology combines ease a3 a2 a1 a3 a2 c1 proxy server a3 client requests a3 locality buffers magnetic2 figure 4 streaming locality buffers sequences client requests arrive interleaved proxy server proxy server groups requested objects stream available locality buffers writes rearranged stream disk sake simplicity omitted forwarding client requests internet appropriate web server web servers responses urldelete file space simulator simulation disk cache simulation main memory traces disks magnetic urlwrite disk cache urlread main memory file access proxy traces file access calls mmapwrite lseek read simulated figure 5 evaluation methodology traces decs web proxy fed 512mbyte main memory lru cache simulator urls miss main memory cache fed 2gbyte disk lru cache simulator urls miss secondlevel cache assumed fetched internet misses generate urlwrite requests fetch url internet save disk secondlevel url hits generate urlread requests since read contents url disk make space newly arrived urls lru replacement policy deletes nonrecently accessed urls resulting urldelete requests urlwrite urlread urldelete requests fed file space simulator maps urls files blocks within files sends appropriate calls file system request type number requests percentage urlread 42085 4 urlwrite 678040 64 main memory hits 336081 32 total 1058206 100 table 1 trace statistics tracedriven simulation accuracy experimental evaluation 31 traces come busy web proxy located digital equipment corporation ftpftpdigitalcompubdectracesproxywebtraceshtml feed traces 512 mbytelarge firstlevel main memory cache simulator uses lru replacement policy 2 url requests hit main memory cache need access secondlevel disk cache remaining url requests fed secondlevel cache simulator whose purpose generate trace urllevel disk requests urlread urlwrite urldelete urlread requests generated result secondlevel cache hit misses secondlevel cache assumed contact appropriate web server internet save servers response disk generating urlwrite request finally urldelete requests generated secondary storage runs space lru replacement algorithm invoked delete unused urls generated trace urlread urldelete urlwrite requests sent filespace management simulator forwards solaris ufs file system reads deletes writes contents urls requested filespace management simulator implements several secondary storage management policies ranging simple one url per file squidlike stream lazyreads localitybuffers solaris ufs system runs top ultra1 workstation running solaris 56 equipped seagate st15150wc 4gbyte disk 9 millisecond average latency 7200 rotations per minute measured maximum write throughput 47 mbytes per second figure 5 summarizes methodology experiments feed simulator pipeline trace one million url read write delete requests generated 1058206 urlget requests table 1 summarizes trace statistics performance metric use total completion time needed serve one million requests time inversely proportional systems throughput operations per second thus direct measure example completion time reported 2000 seconds throughput system urlget requests per second another commonly used performance metric web proxies especially important end user service latency url request however latency misleading performance metric work significant relative differences latency unnoticeable end user amount small fraction second example proxy achieves request latency may appear twice good proxy achieves 60 millisecond average request latency end user perceive difference advocate long latency remains within unperceivable time ranges proxys throughput accurate measure systems performance 32 evaluation start experiments investigating performance cost previous approaches store one url per file comparing proposed buddy stores several urls per file grouped according size consider three approaches singledirectory squid multipledirs ffl singledirectory name implies uses single directory store url files ffl squid used squid proxy server uses twolevel directory structure first level contains directories named 0f contains 256 subdirectories named 00ff files stored second level directories round robin manner although sophisticated policies lru proposed influence results noticeably number url requests 10000 20000 30000 50000 completion time squid multipledirs figure file management overhead web proxies figure plots overhead performing 300000 urlreadurlwriteurldelete operations generated 398034 urlget requests 1gbyte large disk clear buddy improves performance considerably compared approaches ffl multipledirs creates one directory per server urls correspond server stored directory experimental results confirm buddy improves performance order magnitude compared previous approaches indeed figure 6 shows buddy takes forty minutes serve 300000 url requests approaches require six ten times time serve stream url requests buddy able achieve impressive performance improvement create delete files urls smaller predefined threshold choosing appropriate threshold value important performance buddy small threshold result frequent file create delete operations large threshold require large number buddy files may increase complexity management figure 7 plots completion time function threshold buddy management policy see threshold increases completion time buddy improves quickly increasing number urls stored file eliminating significant number file create delete operations threshold reaches 256 blocks ie 128 kbytes get almost best performance increases improve performance noticeably urls larger 128 kbytes given file urls rare large file creationdeletion overhead noticeable 33 optimizing write throughput although buddy improves performance order magnitude compared traditional squidlike approaches still suffers significant overhead writes data several different files requiring potentially long disk seek operations indeed snapshot disk head movements shown figure 8 taken taztool 9 reveals disk head traverses large distances serve proxys write requests easily see head moves frequently within region spans beginning disk upper portion figure end disk lower portion figure despite clustering seems appear lower quarter disk could possibly indicate locality accesses lower portion graph plots average maximum disk head distance indicates frequent long head movements eliminate long head movements incurred write operations distant locations stream stores urls single file writes data file contiguously possible much like logstructured file systems threshold kbytes50015002500completion time minutes figure 7 performance buddy function threshold figure plots completion time function buddys threshold parameter results suggest urls smaller 64128 kbytes buddied together urls larger limit given file one url per file without noticeable performance penalty indeed snapshot disk head movements figure shows stream accesses data disk mostly sequentially scattered accesses dots appear snapshot frequent enough undermine sequential nature accesses although stream obviously achieves minimal disk head movements usually comes cost extra disk space actually facilitate long sequential write operations logstructured file systems stream never operate nearly full disk surprising logstructured file systems operate disk utilization factor 60 even less 34 low disk utilization increases clustering free space allows efficient sequential write operations 3 fortunately experiments shown figure 10 suggest stream operate efficiently even 70 disk utilization outperforming buddy factor two expected disk utilization high 9095 performance buddy stream comparable however disk utilization decreases performance stream improves rapidly first evaluated performance stream noticed even always free disk space available even absence read operations stream write disk maximum throughput traced problem found experiencing smallwrite performance problem writing small amount data file system usually resulted diskread diskwrite operation reason peculiar behavior following process writes small amount data file operating system read corresponding page disk already main memory perform write main memory page later time write entire updated page disk reduce unnecessary read operations incurred small writes developed packetized version stream streampacketizer works like stream following difference urlwrite operations forwarded directly file system instead accumulated pageboundaryaligned onepagelong packetizer buffer long stored contiguously previous urlwrite request packetizer fills current request contiguous previous one packetizer sent file system written disk 3 fortunately recent measurements suggest file systems halffull average 11 thus logstructured approaches file management may attractive ever especially embarrassingly decreasing cost disk space 10 time distance disk head range figure 8 disk access pattern buddy snapshot taken taztool disk head position plotting time distance disk head range figure 9 disk access pattern stream snapshot taken taztool disk head position plotting figure 10 performance buddy stream function disk space utilization figure plots completion time serving 1000000 url operations function disk utilization expected performance buddy unaffected disk utilization performance stream improves disk utilization decreases disk utilization around 70 stream outperforms buddy factor two streampacketizer figure 11 performance stream streampacketizer function disk space utilization figure plots completion time serving 1000000 url operations function disk utilization stream consistently outperforms streampacketizer much 20 low disk utilizations way streampacketizer instead sending large number small sequential write operations file system like stream sends fewer larger page size long write operations file system figure 11 plots performance stream streampacketizer function disk utilization streampacketizer performs consistently better stream much 20 disk utilization low serving one million requests less three thousand seconds achieving service rate close 350 urlget operations per second 34 improving read requests stream improves performance urlwrite operations urlread operations still suffer seek rotational latency overhead first step towards improving performance read operations lazy reads reduce overhead clustering several read operations sending disk together grouping read requests reduces disk head pingpong effect also presents system better opportunities disk head scheduling figure 12 shows lazyreads consistently improve performance streampacketizer 10 4 although 10 performance improvement may seem impressive first glance believe importance lazyreads increase near future experimental environment read requests represent small percentage little 6 total disk operations therefore 4 careful reader notice however lazyreads may increase operation latency however advocate increase unnoticeable endusers trace measurements show streampacketizer augmented lazyreads able serve 1020 read requests per second addition write requests thus lazyreads delay average read operation fraction second given average web server latency may several seconds long 2 lazyreads impose unnoticeable overhead make sure user ever waits unbounded amount time read url disk even unloaded system lazyreads also augmented time period time elapses outstanding read operations sent disk streampacketizer lazyreads figure 12 performance lazyreads figure plots completion time serving 1000000 url operations function 2gbyte disk utilization lazyreads gathers reads requests tenatatime issues time disk reducing disk head movements write stream data read figure shows lazyreads improves performance streampacketizer 10 even significant improvements performance read requests necessarily yield significant overall performance gains increasing size web caches expected hit rates probably increase percentage disk read operations become comparable higher percentage disk write operations case optimizing disk read performance lazyreads similar techniques increasingly important 35 preserving locality url stream 351 effects locality buffers disk access patterns improve performance disk io even locality buffers policies lazyreadsloc stream improve disk layout grouping requests according origin web server storing disk without locality buffers available disk space tends fragmented spread disk locality buffers available disk space tends clustered one large empty portion disk indeed twodimensional disk block map figure 13b shows available free space long white stripe contrary absence locality buffers free space tends littered used blocks shown black dots figure 13a even magnify mostly allocated portion disk figure 13 b right small white flakes begin appear within mostly black areas corresponding small amounts free disk space within large portions allocated space see locality buffers able cluster white free space effectively sizable square white patches absence locality buffers free space clustered small narrow white bands figure 14 confirms locality buffers result better clustering free space plotting average size algorithm performance urlget operations per second table 2 performance traditional webconscious storage management techniques urlget operations per second chunks contiguous free space function time warmup period 300 thousand requests locality buffers manage sustain average free chunk size 145 kbytes contrary absence locality buffers stream exhibits fluctuating behavior average free chunk size kbytes locality buffers cluster free space effectively also populate allocated space clusters related documents gathering urls originating web server locality buffer probably contiguous disk blocks thus future read requests related web documents probably access nearby disk locations quantify effectiveness related object clustering measure distance file blocks successive disk read requests measurements suggest using locality buffers larger fraction read requests access nearby disk locations actually many l885 read requests refer immediately next disk block previous read request compared 611 read requests absence locality buffers seen figure 15 furthermore locality buffers improve clustering disk read requests significantly many 8400 17 total read requests fall within ten blocks previous read request compared 3200 6 total read requests fall within range streampacketizer expect improved clustering read requests observed eventually lead performance improvements possibly use prefetching 352 performance evaluation locality buffers given improved disk layout locality buffers expect performance lazyreadsloc superior lazyreads streampacketizer experiments vary number locality buffers 8 128 locality buffer 64kbytes large figure 16 shows eight locality buffers lazyreadsloc8 sufficient improve performance lazyreads 5 20 depending disk utilization however number locality buffers increases performance advantage lazyreadsloc increases even actually 76 disk utilization lazyreadsloc 128 locality buffers performs 25 times better lazyreads streampacketizer summarize performance results table 2 presenting best achieved performance measured urlget operations per second studied techniques see webconscious storage management techniques improve performance order magnitude serving close 500 urlget operations per second singledisk system actually experimental environment little room improvement lazyreadsloc128 transfers 76 gbytes data secondary storage 2020 seconds corresponds sustained throughput 37 mbytes per second given disk used experiments sustain maximum write throughput 47 mbytes per second see webcosm techniques achieve 78 maximum practically unreachable upper limit performance therefore additional sophisticated techniques expected result significant performance improvements least experimental environment b figure 13 disk fragmentation map figure plots twodimensional disk block allocation map end simulations stream locality buffers b plot allocated blocks black dots free blocks white dots beginning disk plotted lower left corner rest disk plotted following columnmajor order finally end disk plotted top right corner average size hole kbytes time thousands url requests locality buffers figure 14 average size free disk blocks figure plots average size chunks contiguous free space function time20060010001400180030 20 number read operations distance previous read request file blocks locality buffers figure 15 distribution distances read requests figure plots histogram block distances successive read operations streampacketizer streamloc using 128 locality buffers disk space utilization 1000300050007000 completion time streampacketizer lazyreads figure performance lazyreadsloc figure plots completion time serving 1000000 url operations function disk utilization lazyreadsloc attempts put urls server nearby disk locations clustering locality buffers sending disk implementation validate webconscious storage management approach implemented lightweight userlevel web proxy server called foxy goal developing foxy show webcosm management techniques easily implemented ii provide significant performance improvements iii require neither extensive tuning user involvement foxy consists 6000 lines c code implements basic functionality http web proxy simplicity rapid prototyping foxy implements http protocol notsofrequently used protocols like ftp icp etc foxy first foremost caching proxy uses two level caching approach proxys main memory holds frequently accessed documents rest reside local disk provide effective transparent main memory cache foxy capitalizes existing file buffer cache operating system secondarystorage system management foxy stores urls single file according stream packetizer policy urls contiguously appended disk disk utilization reaches high watermark cache replacement daemon invoked reduce utilization low watermark replacement policy used lruth 1 replaces least recently used documents cache order prevent large documents fillingup cache lruth cache documents larger prespecified threshold foxy developed top solaris 57 also tested top linux 22 41 design foxy web proxy design architecture foxy web cache follows sequence operations proxy must perform order serve user request mentioned also proxy request proxy connection user request decomposed sequence synchronous states form finite state machine fsm shown figure 17 based fsm diagram foxy functions pipelined scheduler infinite loop every step loop scheduler selects ready proxy request performs necessary operations advances request next state rectangles figure 17 represent various states comprise processing request clouds represent important asynchronous actions taking place two states finally lines represent transitions states annotated indicate operations must completed order transition incomplete transfer incomplete transfer new client connection object cache lookup object index connection failed retry receive www object parse http response object cache send object client read object cache established http response receiving connection close tcp connection connection dns store object cache http tcp finished transfer object read cache read final sent client object received message client exception request open tcp connection remote web server object client request figure 17 foxy processing finite state machine diagram number clients 100 number servers 4 server response time 1 second document hit rate 40 request distribution zipf06 document size distribution exp5 kbytes cacheable objects 100 table 3 webpolygraph parameters used experiments occur every request begins state 1 http request read parsed foxy searches index requested object index contains metadata objects stored foxys cache eg name size object storage location objects retrieval time etc search requested object successful foxy issues read request cache object read cache transition state 5 made object returned client successful transfer client tcpip connection closed state 6 processing http request completed search requested object unsuccessful foxy state 2 performs dns lookup ip address remote web server opens tcpip connection tcpip connection established foxy state 3 sends http request origin web server server response parsed state 4 foxy receives objects content possibly slow internet connection foxy passes object requesting client receives remote server objects contents transferred foxy uses cache admission policy lruth decide whether object cached objects content stored disk cache using streampacketizer algorithm corresponding metadata stored index finally tcp connection closed state 6 processing http request completed 42 experiments measure performance foxy compare squid used web polygraph proxy performance benchmark version 229 de facto industry standard benchmarking proxy servers 33 configured web polygraph 100 clients four web servers responding simulated response time one second table 3 summarizes web polygraph configuration used experiments run web polygraph sun ultra 4500 machine four ultrasparc2 processors 400mhz machine runs solaris 57 equipped seagate st318203fsun18g 18gbyte disk secondary storage size squid foxy use 8 gbytes used 22stable4 version squid configured according settings used second polygraph bakeoff 36 reduce effects possibly slow network run processes computer results experiments presented figures 18 19 20 figure plots throughput squid foxy function client demands input load ranges 40 350 requests per second see small input load less 80 requests per second throughput proxies increases linearly however throughput squid quickly levelsoff decreases 90 requests per second foxy sustains linear increase 340 requests per second giving factor four improvement squid deficiencies squid even pronounced figure 19 plots average response time achieved two proxies function input load easily see squids latency increases exponentially input load higher 50 requests per second thus although figure suggests squid achieve throughput 90 requests per second throughput comes steep increase request latency seen end user almost 8 seconds maximum throughput squid sustain without introducing noticeable increase latency around 50 requests per second contrary foxy manages serve 340 requests per second without noticeable increase latency fact foxy serve 340 requests per second user latency 07 seconds therefore acceptable subsecond latency ranges foxy achieves almost 7 times higher throughput squid make matters worse squid increases perceived enduser latency also increases network traffic required fact disk subsystem becomes overloaded squid effort offload disk may forward url requests origin server even local copy disk cache behavior observed throughput requestssecond input load requestssecond foxy squid figure 18 throughput squid foxy13579 avg response time per request sec input load requestssecond squid foxy figure 19 latency squid foxy network bandwidth kbytessec input load requestssecond squid foxy figure 20 network bandwidth requested squid foxy may increase network traffic significantly figure 20 shows input load less 50 requests per second squid foxy require network bandwidth input load increases beyond 50 requests per second network bandwidth required foxy increases linearly input load expected however network bandwidth required squid increases higher rate inducing 45 network traffic foxy 110 url requests per second 5 previous work caching extensively used web web browsers cache documents main memory local disk although widely used form web caching rarely results high hit rates since browser caches typically small accessed single user 1 improve effectiveness caching proxies widely used strategic places internet 8 42 behalf users caching proxies request urls web servers store local disk serve future requests cache whenever possible since even large caches may eventually fill cache replacement policies subject intensive research 1 7 23 26 32 37 43 44 proposed caching mechanisms improve user experience reduce overall traffic protect network servers traffic surges 15 improve hit rates even enhance overall user experience proxies may even employ intelligent prefetching methods 4 12 16 41 31 40 internet traffic grew larger realized internet servers bottlenecked disk subsystem fritchie found usenet news servers spend significant amount time storing news articles files one file per article 14 reduce overhead proposes store several articles per file manage file cyclic buffer implementation shows storing several news articles per file results significant performance improvement much like news servers web proxies also spend significant percentage time performing disk io rousskov soloviev observed disk delays contribute much 30 towards total hit response time 35 mogul suggests disk io overhead disk caching turns much higher latency improvement cache hits 28 thus save disk io overhead proxy typically run noncaching mode 28 reduce disk io overhead soloviev yahin suggest proxies several disks 39 order distribute load among disk several partitions order localize accesses related data unfortunately almeida cao 2 suggest adding several disks exiting traditional web proxies usually offers little performance improvement maltzahn richardson grunwald 24 measured performance web proxies found disk subsystem required perform large number requests url accessed thus easily become bottleneck subsequent work propose two methods reduce disk io web proxies 25 ffl store urls origin web server proxy directory squidl ffl use single file store urls less 8 kbytes size squidm although work 25 shares common goals approaches toward reducing disk metadata accesses web proxies work presents clear contribution towards improving data metadata access overhead ffl propose evaluate stream streampacketizer two filespace management algorithms much like logstructured file systems optimize write performance writing data contiguously disk ffl propose evaluate lazyreads lazyreadsloc two methods reduce disk seek overhead associated read operations performance results reported 25 paper verify metadata reduction methods improve performance significantly example maltzahn et al report digital alphastation 250 4266 512 mbytes ram three magnetic disks squid able serve around 50 urlget requests per second best performing squidmla approach able serve around 150 requests per second similarly sun ultra1 166 mhz 384 mbytes ram single magnetic disk squid able serve around 27 url get requests per second buddy simplest webcosm technique reduces metadata overhead achieves around 133 requests per second furthermore remaining webcosm techniques improve metadata also data accesses able achieve close 500 urlget requests per second web proxies implemented userlevel processes top commodity albeit stateofthe art filesystems web proxies built top custommade file systems operating systems netcache build top wafl file system improves performance write operations 18 inktomis traffic server uses unix raw devices 20 cacheflow developedcacheos specialpurpose operating system proxies 19 similarly novell developed specialpurpose system storing urls cache object store 22 unfortunately little information published details performance custommade web proxies thus direct quantitative comparison approach difficult although custommade operating systems enhanced filesystems offer significant performance improvements choose explore approach running web proxy userapplication top commodity unixlike operating system believe approach result lower software development maintenance costs easier deployment use therefore quicker wider acceptance web proxies main contributions article ffl study overheads associated file io web proxies investigate underlying causes propose webcosm set new techniques overcome file io limitations ffl identify locality patterns exist web accesses show web proxies destroy patterns propose novel mechanisms restore exploit ffl applicability approach shown foxy userlevel web proxy implementation top commercial unix operating system ffl comprehensiveperformancemeasurements using simulation experimental evaluation show approach easily achieve order magnitude performance improvement traditional approaches 6 discussion 61 reliability issues although squidlike policies store one url per file perform well appear robust case system crash webcosm approach capitalize expensive reliability provided file system example system crash squid scan directories files disk cache recreate complete index cached objects contrary webcosm methods store metadata associated url main memory potentially long periods time increasing caches vulnerability system crashes example stream stores urls single file contains indication beginning block url information stored main memory lost system crash fortunately seemingly lost reliability pose real threat systems operation ffl webcosm methods periodically ie every minutes write metadata information safe storage case crash lose work last minutes alternatively store along url name size disk blocks case crash system reboots disk scanned information urls exist disk recovered ffl even cached documents lost due crash easily retrieved web server permanently reside thus system crash lose information permanently loses local copy data ie minutes worth easily retrieved web 62 lessons learned course research called understand intricate behavior web proxy system interesting lessons learned include ffl user requests may invoke counterintuitive operating system actions example observed small write requests stream surprisingly invoked disk read operations case intuitive direct correspondence user requests operating system actually mismatch user requests operating system actions hurts performance also undermines users understanding system ffl system bottlenecks may appear places least expected popular belief proxies bottlenecked network subsystem contrary found secondary storage management system also major significant bottleneck designed operate efficiently web workloads example traditional file systems used serve 10 concurrent users requesting 50 kbytes per second 3 contrary busy web proxy especially countrywide proxy may required serve hundreds concurrent users requesting data totaling several mbytes per second 13 ffl optimizing performance read operations one major factors design secondary storage management systems web proxies write operations usually implemented efficiently proceed disk bandwidth contrary read operations even asynchronous ones involve expensive disk seek rotational latencies difficult impossible avoid disk bandwidth improves much faster disk latency read operations become increasing performance bottleneck ffl locality manifest even expected found clients exhibit significant amount spatial locality requesting sequences related urls traditional proxies tend destroy locality interleaving requests arriving different clients identifying exploiting existing locality url stream challenging tasks continuously pursued summary conclusions paper study disk io overhead worldwide web proxy servers using combination tracedriven simulation experimental evaluation show busy web proxies bottlenecked secondary storage management subsystem overcome limitation propose webcosm set techniques tailored management web proxy secondary storage based experience designing implementing evaluating webcosm conclude ffl single largest source overhead traditional web proxies file creation file deletion costs associated storing url separate file relaxing onetoone mapping urls files improves performance order magnitude ffl web clients exhibit locality reference accesses usually access urls clusters interleaving requests several clients web proxies tend conceal locality restoring locality reference stream results better layout urls disk reduces fragmentation improves performance least 30 ffl managing mapping urls files user level improves performance traditional web proxies factor 20 overall leaving little room improvement specialized kernellevel implementations believe results significant today even significant future disk bandwidth improves much higher rate disk latency two decades 10 methods like webcosm reduce disk head movements stream data disk result increasingly larger performance improvements furthermore webconscious storage management methods result better performance also help expose areas research discovering exploiting locality web acknowledgments work supported part institute computer science foundation research technology hellas part university crete project file systems web servers 1200 deeply appreciate financial support panos tsirigotis source inspiration many useful comments manolis marazakis george dramitinos gave us useful comments earlier versions paper katia obraczka provided useful comments earlier version paper p cao provided one simulators used thank r caching proxies limitations potentials measuring proxy performance wisconsin proxy benchmark measurements distributed file system speculative data dissemination service reduce server load heuristic cleaning algorithms logstructured file systems operating system benchmarking wake lmbench case study performance netbsd intel x86 architecture hierarchical internet object cache tracing revisited serverless network file systems largescale study file system contents prefetching hyperlinks measured access characteristics worldwideweb client proxy caches cyclic news filesystem getting inn less global internet project zebra striped network file system file system design nfs file server appliance cache flow inc inktomi inc workload requirements highcapacity proxy cache design replacement policies proxy cache performance issues enterprise level web proxies reducing disk io web proxy server caches main memory caching web documents fast file system unix speedier squid case study internet server performance problem caching sprite network file system tracedriven analysis unix 42 bsd file system using predictive prefetching improve world wide web latency simple web polygraph design implementation logstructured file system performance caching proxies second ircache web cache bakeoff case delayconscious caching web documents implementation logstructured file system unix file placement web cache server defining high speed protocols fast worldwide web browsing lowbandwidth links squid internet object cache removal policies network caches worldwide web documents proxy caching estimates page load delays tr design implementation logstructured file system measurements distributed file system zebra striped network file system diskdirected io mimd multiprocessors removal policies network caches worldwide web documents performance issues enterprise level web proxies operating system benchmarking wake italiclmbenchitalic performance caching proxies extended abstract proxy caching estimates page load delays measuring proxy performance wisconsin proxy benchmark lets put netapp cacheflow business cyclic news filesystem storage management web proxies efficient algorithms persistent storage allocation serverless network file systems ctr abdolreza abhari sivarama p dandamudi shikharesh majumdar web objectbased storage management proxy caches future generation computer systems v22 n1 p1631 january 2006