approximate inverse preconditioners via sparsesparse iterations standard incomplete lu ilu preconditioners often fail general sparse indefinite matrices give rise unstable factors l u cases may attractive approximate inverse matrix directly paper focuses approximate inverse preconditioners based minimizing iamf preconditioned matrix iterative descenttype method used approximate column inverse approach efficient iteration must done sparse mode ie sparsematrix sparsevector operations numerical dropping applied maintain sparsity compared previous methods natural way determine sparsity pattern approximate inverse paper describes newton global columnoriented algorithms discusses options initial guesses selfpreconditioning dropping strategies limited theoretical results properties convergence approximate inverses derived numerical tests problems harwellboeing collection fidap fluid dynamics analysis package show strengths limitations approximate inverses finally ideas experiments practical variations applications presented b introduction incomplete lu factorization preconditioners originally developed mmatrices arise discretization simple partial differential equations elliptic type usually one variable rather common situation matrix indefinite standard ilu factorizations may face several difficulties best known encounter zero pivot however problems serious consider incomplete factorization form e error preconditioned matrices associated different forms preconditioning similar sometimes missed fact error matrix e 11 important preconditioned error matrix l shown 12 matrix diagonally dominant l u typically well conditioned size l remains confined within reasonable limits typically clustering eigenvalues around origin hand original matrix diagonally dominant l gamma1 u gamma1 may large norms causing error large thus adding large perturbations identity matrix form instability studied elman 14 detailed analysis ilu milu preconditioners finite difference matrices observed experimentally work supported part national science foundation grant nsfccr 9214116 part nasa grant nag2904 department computer science minnesota supercomputer institute university minnesota 4192 eecsci bldg 200 union st se minneapolis minnesota 554550154 chowcsumnedu saadcsumnedu ilu preconditioners poor l gamma1 u gamma1 large situation often occurs indefinite problems problems large nonsymmetric parts one possible remedy proposed stabilized perturbed incomplete factorizations example 15 references 25 numerical comparison preconditioners given later paper consider trying find preconditioner require solving linear system example precondition original system sparse matrix direct approximation inverse sparse approximate inverses also necessary incomplete block factorizations large sparse blocks well several applications also described later focus methods finding approximate inverses based minimizing frobenius norm residual matrix gamma first suggested benson frederickson 5 6 consider minimization seek right approximate inverse important feature objective function decoupled sum squares 2norms individual columns residual matrix gamma e j j jth columns identity matrix matrix respectively thus minimizing 14 equivalent minimizing individual functions clearly useful parallel implementations also gives rise number different options minimization 15 often performed directly prescribing sparsity pattern solving resulting least squares problems grote simon choose banded matrix 2p emphasizing importance fast application preconditioner cm2 implementa tion choice structure particularly suitable banded matrices cosgrove diaz griewank 10 select initial structure diagonal use procedure improve minimum updating sparsity pattern new fillin elements chosen fillin contributes certain improvement minimizing number new rows least squares subproblem similar work grote huckle 18 reduction residual norm tested candidate fillin element fillin may introduced one time related work kolotilina yeremin 23 consider symmetric positive definite systems construct factorized sparse approximate inverse preconditioners also symmetric positive definite factor implicitly approximates inverse lower triangular cholesky factor structure factor chosen structure lower triangular part recent work 24 fillin elements may added locations chosen construction application approximate inverse much expensive model hypercube computer preconditioners general systems may constructed approximating left right factors separately paper organized follows x2 present several approximate inverse algorithms based iterative procedures well describe sparsesparse implementation various options derive simple theoretical results approximate inverses convergence behavior algorithms x3 x4 show strengths limitations approximate inverse preconditioners numerical tests problems harwellboeing collection fidap fluid dynamics analysis package finally x5 present ideas experiments practical variations applications approximate inverses 2 construction approximate inverse via iteration sparsity pattern approximate inverse general matrix prescribed since appropriate pattern usually known beforehand contrast previous work described locations values nonzero elements determined naturally sideeffect utilizing iterative procedure minimize 13 15 addition elements approximate inverse may removed numerical dropping strategy contribute little inverse features clearly necessary general sparse matrices xx21 22 briefly describe two approaches treated matrix entirety rather individual columns found however methods converge slowly columns treated separately remaining sections consider latter approach various options available 21 newton iteration alternative directly minimizing objective function 13 approximate inverse may also computed using iterative process known method hotelling bodewig 20 method modeled newtons method solving fx j 1x gamma 0 many similarities descent methods describe later iteration takes form convergence require spectral radius gamma 0 less one choose initial guess form convergence achieved practice follow pan reif 27 use right approximate inverse iterations progress becomes denser denser natural idea perform iteration sparse mode 26 ie drop elements else iterations become expensive case however convergence properties newton iteration lost show results numerical experiments x4 22 global iteration section describe global approach minimizing 13 use descenttype method treating unknown sparse matrix objective function 13 quadratic function space n theta n matrices viewed objects r n 2 actual inner product space matrices function 14 associated one possible descenttype method may use steepest descent describe later following call array representation n 2 vector x n theta n matrix whose column vectors successive nvectors x descent algorithms new iterate new defined taking step along selected direction g ie ff selected minimize objective function associated new achieved taking hag agi tr ag ag residual matrix note denominator may computed kagk 2 f descent steps taken resulting matrix tend become denser therefore essential apply kind numerical dropping either new search direction g taking descent step first case descent nature step lost ie longer guaranteed f mnew f second case fillin difficult control discuss alternatives x25 simplest choice descent direction g take residual new iterate corresponding descent algorithm referred minimal residual mr algorithm simpler case numerical dropping applied global minimal residual algorithm following form algorithm 21 global minimal residual descent algorithm 1 select initial 2 convergence 3 compute g 4 compute ff 22 5 compute 6 apply numerical dropping 7 end another popular choice take g direction steepest descent ie direction opposite gradient thinking terms n 2 vectors gradient f viewed n 2 vector g delta delta usual euclidean inner product represent vectors 2 dimensional n theta n arrays relation equivalent allows us determine gradient operator arrays rather n 2 vectors done next proposition proposition 22 array representation gradient f respect matrix r residual matrix proof matrix e theta r gamma ae r theta ae r r thus differential f applied e inner product gamma2a r e plus second order term gradient therefore simply gamma2a r steepest descent algorithm consists simply replacing g line 3 mr algorithm described algorithm slow cases since essentially steepest descenttype algorithm applied normal equations either global steepest descent minimal residual need form store g matrix explicitly scalars kagk 2 f trr ag computed successive columns ag generated used discarded therefore need store matrix ag show results numerical experiments global iteration compare methods x4 23 implementation sparse mode mr gmres describe columnoriented algorithms consist minimizing individual objective functions 15 perform minimization taking sparse initial guess solving approximately n linear subproblems steps nonsymmetric descenttype method mr untruncated gmres method efficient iterative method must work sparse mode ie j stored operated sparse vector arnoldi basis gmres kept sparse format following mr algorithm n iterations used solve 23 approximately column giving approximation jth column inverse initial j taken columns initial guess 0 assume numerical dropping applied gmres version algorithm never use restarting since since n typically small also variant called fgmres 31 allows arbitrary arnoldi basis actually used case algorithm 23 minimal residual iteration 1 2 column 3 4 6 e chow saad 5 r j e 7 8 apply numerical dropping j 9 end 10 end thus algorithm computes current residual r j minimizes residual norm e jnew set sparse implementation mr gmres matrixvector product saxpy dot product kernels entirely involve sparse vectors matrixvector product much efficient sparse matrix stored columns since entries need traversed efficient codes kernels may constructed utilize full nlength work vector 11 columns initial guess 0 approximate inverse used initial guesses iterative solution linear subproblems two obvious choices scale factor ff chosen minimize spectral radius aei gamma ffam denoting initial guess writing ff leads tram transpose initial guess expensive use denser identity initial guess however indefinite systems guess immediately produces symmetric positive definite preconditioned system corresponding normal error equations depending structure inverse denser initial guess often required involve matrix computation interest ingly cheaper computation uses local information less able may produce good approximate inverse choice initial guess also depends degree selfpreconditioning describe next additional comments choice initial guess presented 24 selfpreconditioning approximate solution linear subproblems using iterative method suffers problems solving original problem indefinite poorly conditioned however linear systems may preconditioned columns already computed precisely system 23 approximating column j may preconditioned 0 first 0 k already computed remaining columns initial guesses k j k n suggests possible define outer iterations sweep matrix well inner iterations compute column subsequent outer iteration initial guess column previous result column technique usually results much faster convergence approximate inverse unfortunately approach parallelism constructing columns approximate inverse simultaneously lost however another variant selfpreconditioning easier implement easily parallelizable quite simply inner iterations computed simultaneously results columns used selfpreconditioner next outer iteration thus preconditioner inner iterations changes outer iteration performance variant usually lies full selfpreconditioning self preconditioning reasonable compromise compute blocks columns parallel inner selfpreconditioning may used selfpreconditioning particularly valuable indefinite problems combined scaled transpose initial guess initial preconditioned system 0 positive definite subsequent preconditioned systems somewhat maintain property even presence numerical dropping selfpreconditioning transpose initial guess however may produce worse results matrix illconditioned case initial worsening conditioning system severe alternative scaled identity initial guess used instead also found cases selfpreconditioning produces worse results usually positive definite problems surprising since minimizations would progress well hindered selfpreconditioning poor approximate inverse early stages numerical evidence phenomena provided x4 algorithm 24 implements minimal residual iteration selfpreconditioning algorithm n iterations n inner iterations used initially also indicated numerical dropping might applied algorithm 24 selfpreconditioned minimal residual iteration 1 start 2 3 column 4 define 5 7 z mr 8 q az 9 ff rq 11 apply numerical dropping 12 end 13 update jth column 14 end 15 end fortran 77 implementation stored n sparse vectors holding lfil entries thus constructed place multiple outer iterations used constructing approximate inverse suggests use factorized updates factorized matrices express denser matrices sum numbers elements alone suppose one outer iteration produced approximate inverse 1 second outer iteration tries find 2 approximate inverse 1 general outer iterations looking update i1 minimizes min also possible construct factorized approximate inverses form min f alternate left right factors latter form reminiscent symmetric form kolotilina yeremin 23 since product never formed explicitly factorized approach effectively uses less memory preconditioner cost multiplying factor matrixvector multiplication approach may suitable large problems memory rather solution time limiting factor implementation however much complex since sequence matrices needs maintained 25 numerical dropping strategies many options numerical dropping far ease presentation discussed case dropping performed solution vectors matrices section 251 discusses case detail x252 discusses case dropping applied search directions latter case descent property algorithms maintained 251 dropping solution dropping performed solution options 1 dropping performed 2 elements dropped previous algorithms made first point precise however alternatives example dropping may performed column computed typically option expensive compromise dropping may performed end inner iterations updated namely step 13 algorithm 24 interestingly found experimentally option always better gmres krylov basis vectors kept sparse dropping elements selfpreconditioning step multiplication address elements dropped utilize dual threshold strategy based drop tolerance droptol maximum number elements per column lfil limiting maximum number elements per column maximum storage preconditioner known beforehand drop tolerance may applied directly elements dropped ie elements dropped magnitude smaller droptol however found strategy could cause spoiling minimization ie residual norm may increase several steps along deterioration quality preconditioner dropping small elements j suboptimal one may ask question whether dropping performed optimally simple perturbation analysis help understand issues denote j current column perturbed column formed adding sparse column process numerical dropping new column corresponding residual therefore square residual norm perturbed j given recall gamma2a r j gradient function 15 expected standard results optimization direction opposite gradient small enough achieve decrease residual norm spoiling occurs close zero practical sizes kdk 2 kadk 2 becomes dominant causing increase residual norm consider specifically situation one element dropped assume columns ae prescaled kae 1 case equation becomes strategy could therefore based attempting make function nonpositive condition easy verify suggests selecting elements drop j indices selection function 28 zero negative however note entirely rigorous since practice elements dropped time thus entirely perform dropping via numerical values alone twostage process first select number candidate elements dropped based numerical size determined certain tolerance among drop satisfy condition keep lfil elements largest ae ij another alternative based attempting achieve maximum reduction function 28 ideally wish since achieve optimal reduction 28 leads alternative strategy dropping elements positions j smallest found however strategy produces poorer results previous one neither strategies completely eliminate spoiling 252 dropping search direction dropping may performed search direction g algorithm 21 equivalently r j z algorithms 23 24 respectively cases descent property algorithms maintained problem spoiling avoided starting sparse initial guess allowed number fillins gradually increased iteration mrlike algorithm search direction derived dropping entries residual direction r sparsity pattern solution x controlled chosen sparsity pattern x plus one new entry largest entry absolute value drop tolerance used minimization performed choosing steplength ad ad thus residual norm new solution guaranteed previous residual norm contrast algorithm 23 residual may updated little cost iterations may continue long residual norm larger threshold set number iterations may used indefinite normal equations residual direction r may used search direction simply determine location new fillin interesting note largest entry r gives greatest residual norm reduction onedimensional minimization fillin allowed increase gradually using search direction technique becomes similar adaptive selection scheme 18 effect also similar selfpreconditioning transpose initial guess end iteration possible use second stage exchanges entries solution new entries causes reduction residual norm required sparsity pattern approximate inverse needs change approximations progress found necessary particularly unstructured matrices yet found strategy genuinely effective 7 result approximations using numerical dropping solution often better even though scheme described stronger theoretical justification similar 18 also shows adaptive scheme 18 may benefit exchange strategy algorithm 25 implements minimal residuallike algorithm numerical dropping strategy number inner iterations usually chosen lfil somewhat larger algorithm 25 selfpreconditioned mr algorithm dropping search direction 1 start 2 column 3 4 r j e 5 7 choose pattern one entry largest remaining entry absolute value 8 q ad 9 ff r j q 12 end 13 end dropping applied unpreconditioned residual economical use approximate inverse technique limited approximating solution linear systems sparse coefficient matrices sparse righthand sides approximation may found example factorized matrix dense operator may accessed matrixvector product need may arise instance preconditioning row projection systems approximations possible existing approximate inverse techniques must mention adaptive strategy one choosing sparsity pattern makes massive parallelization algorithm difficult instance processor task computing columns approximate inverse known beforehand columns must fetched processor 26 cost constructing approximate inverse cost computing approximate inverse relatively high let n dimension linear system number outer iterations n number inner iterations n algorithm 25 approximate cost number sparse matrixsparse vector multiplications sparse mode implementation mr gmres profiling problems shows operation accounts threequarters time selfpreconditioning used remaining time used primarily sparse dot product sparse saxpy operations case sparse mode gmres additional work within algorithm algorithm 24 used two sparse mode matrixvector products used first one computing residual three required selfpreconditioning used algorithm 25 residual may updated easily stored recomputed algorithm 24 additional product required selfpreconditioning cost simply nn times number sparse mode matrixvector multipli cations multiplication cheap depending sparseness columns dropping search directions however slightly expensive although vectors sparser beginning typically requires much inner iterations eg one fillin newton iteration two sparse matrixsparse matrix products required although convergence rate may doubled form chebyshev acceleration 28 global iterations without selfpreconditioning require three matrixmatrix products costs comparable columnoriented algorithms 3 theoretical considerations theoretical results regarding quality approximate inverse preconditioners difficult establish however prove rather simple results general approximate inverses convergence behavior algorithms 31 nonsingularity important question wish address whether approximate inverse obtained approximations described earlier singular cannot proved nonsingular unless approximation accurate enough typically level impractical attain difficulty approximate inverse preconditioners except triangular factorized forms described 23 drawback using possibly singular need check lution actual residual norm end linear iterations practice noticed premature terminations due singular preconditioned system likely rare event begin section easy proposition proposition 31 assume nonsingular residual approximate inverse satisfies relation consistent matrix norm nonsingular proof result follows immediately equality 3 wellknown fact knk 1 gamma n nonsingular note result true particular frobenius norm although induced matrix norm consistent may sometimes case poorly balanced result gamma large balancing yield smaller norm possibly less restrictive condition nonsingularity easy extend previous result follows corollary 32 assume nonsingular exist two nonsingular diagonal matrices 1 2 consistent matrix norm nonsingular proof applying previous result implies nonsingular result follows particular interest 1norm column obtained independently requiring condition residual norm form typically use 2norm since measure magnitude residual gamma using frobenius norm however using 1norm stopping criterion allows us prove number simple results assume following require condition form column prove following result proposition 33 assume condition 35 imposed computed column approximate inverse let 1 eigenvalue preconditioned matrix located disc 2 1 nonsingular 3 k columns k n linearly dependent least one residual associated one columns 1norm 1 proof prove first property invoke gershgorins theorem matrix column r residual vector r column version gershgorins theorem see eg 30 17 asserts eigenvalues matrix gamma r located union disks centered diagonal elements radius words eigenvalue must satisfy least one inequality form get therefore eigenvalue located disk center 1 radius second property restatement previous proposition follows also first property prove last point assume without loss generality first k columns linearly dependent k scalars ff zero assume also without loss generality 1norm vector ffs equal one achieved rescaling ffs multiplying 37 yields gives taking 1norms side get thus least one 1norms residuals r must 1 may ask question whether similar results shown norms since norms equivalent clearly adapt results easy way example however resulting statements would weak practical value exploit fact since computing sparse approximation number p nonzero elements column small thus replace scalar n inequalities p 18 point result tell us anything degree sparsity resulting approximate inverse may well case order guarantee nonsingularity must dense nearly dense fact particular case norm proposition 1norm proved cosgrove diaz griewank 10 approximate inverse may structurally dense always possible find sparse matrix dense ki gamma amk 1 1 14 e chow saad next examine sparsity prove simple result case assumption form 35 made proposition 34 let assume given element b ij b satisfies inequality element ij nonzero proof equality thus thus condition 39 satisfied must us r small enough nonzero elements located positions corresponding larger elements inverse following negative result immediate corollary corollary 35 let de defined proposition 33 nonzero elements equimodular nonzero sparsity pattern includes nonzero sparsity pattern gamma1 particular gamma1 dense elements equimodular also dense smaller value likely condition corollary satisfied another way stating corollary able compute accurate sparse approximate inverses elements actual inverse variations size unfortunately difficult verify advance 32 case nearly singular consider first singular matrix singularity rank one ie eigenvalue 0 single let z eigenvector associated eigenvalue subsystem 23 solved mr gmres provide approximation system except cannot resolve component initial residual associated eigenvector z words iteration may stagnate steps let us denote p spectral projector associated zero eigenvalue 0 initial guess system 23 r initial residual column j would end iteration approximate solution form whose residual term p r 0 cannot reduced iterations norm reduced selecting accurate ffi mr algorithm also break ar j vanishes causing division zero computation scalar ff j step 6 algorithm 23 although problem gmres interesting observation case singular well defined adding rankone matrix zv indeed yield residual assume nearly singular one eigenvalue ffl close zero associated eigenvector z note vector v z v norm one residual perturbed magnitude ffl viewed another angle say perturbation order ffl residual approximate inverse perturbed matrix norm close one 33 eigenvalue clustering around zero observed many experiments often matrix obtained selfpreconditioned iteration would admit cluster eigenvalues around origin precisely seems point eigenvalue moves close zero singularity tends persist later stages zero eigenvalue move away zero slowly eigenvalues seem slowdown even prevent convergence section attempt analyze phenomenon examine case given intermediate iteration matrix becomes exactly singular start assuming global mr iteration taken preconditioned matrix singular ie exists nonzero vector z algorithms initial guess next outer iteration current initial residual matrix 0 resulting next self preconditioned iteration either global mr gmres step residual form residual polynomial multiplying 310 right eigenvector z yields aeamz result showing z eigenvector 0 associated eigenvalue zero result extended columnoriented iterations first assume preconditioning used selfpreconditioning n inner iterations given outer loop fixed case need exploit left eigenvector w associated eigenvalue zero proceeding let 0 j new jth column approximate inverse residual polynomial associated mr gmres algorithm jth column form ae j ts j multiplying 311 left eigenvector w yields result w 0 rewritten w gives establishing result persistence zero eigenvalue global iteration finally consider general columnoriented mr gmres iterations selfpreconditioner updated one inner iteration next still write let 0 new approximate inverse resulting updating column j residual associated 0 columns residual associated except jth column given therefore w left eigenvector associated eigenvalue zero multiplying equality left w yields showing zero eigenvalue persist 34 convergence behavior selfpreconditioned mr next wish consider convergence behavior algorithms constructing approximate inverse particularly interested situation selfpreconditioning used numerical dropping applied 341 global mr iterations selfpreconditioning used global mr iteration matrix defines search direction z current residual therefore algorithm without dropping follows 1 r k gamma k 2 z k mr k 3 ff k hrk azk 4 k1 step new residual matrix r k1 satisfies relation first observation r k polynomial r 0 relation thus induction p j certain polynomial degree j throughout section use notation following recurrence easy infer 312 note b k1 also polynomial degree 2 k b 0 particular initial b 0 equivalently r 0 symmetric subsequent r k b k also symmetric achieved initial multiple ie ready prove number simple results proposition 36 selfpreconditioned mr iteration converges quadratically proof define ff recall ff k achieves minimum krffk f ffs particular proves quadratic convergence limit following proposition straightforward generalization matrix case wellknown result 13 concerning convergence vector minimal residual iteration proposition 37 assume given step k matrix b k positive definite following relation holds r cos 6 r br j min b smallest eigenvalue 1b b oe max b largest singular value b proof start construction new residual r k1 orthogonal az k sense hdelta deltai inner product result second term righthand side equation vanishes noting az f f result 316 follows immediately derive 317 note r ith column r similarly result follows substituting relations ratio 317 note 316 frobenius norm r k1 bounded k specifically kr k1 kf kr 0 kf k consequence largest singular value b also bounded specifically oe assume matrices b k symmetric addition b k positive definite smallest eigenvalue bounded positive converge identity matrix convergence quadratic limit 342 columnoriented mr iterations convergence result may extended case column updated individually exactly one step mr algorithm let current approximate inverse given sub step selfpreconditioned mr iteration computing jth column next approximate inverse obtained following sequence operations 1 r j e 2 3 4 note ff j written define preconditioned matrix given substep drop index j simplify notation new residual associated current column given use orthogonality new residual amr obtain kr new k 2 replacing ff value defined get kr new k 2 thus inner iteration residual norm jth column reduced according formula kr new 6 u v denotes acute angle vectors u v assuming column converges preconditioned matrix b converge identity result angle tend 6 therefore convergence ratio sin 6 r br also tend zero showing superlinear convergence consider equation 321 carefully order analyze explicitly convergence behavior denote r residual matrix observe sin 6 results following statement proposition 38 assume selfpreconditioned mr algorithm employed one inner step per iteration numerical dropping 2norm residual jth column reduced factor least ki gamma amk 2 approximate inverse current step ie kr new addition frobenius norm residual matrices r obtained outer iteration satisfies result algorithm converges quadratically proof inequality 322 proved prove quadratic convergence first transform inequality using fact kxk 2 kxkf obtain kr new k index corresponds outer iteration jindex column note frobenius norm reduced inner steps corresponding columns therefore yields kr new f kr j k 2which upon summation j gives completes proof also easy show similar result following variations 1 mr arbitrary number inner steps 2 gmresm arbitrary follow fact algorithms deliver approximate column smaller residual obtain one inner step mr emphasize quadratic convergence guaranteed limit theorem prove convergence presence numerical dropping proposition hold 4 numerical experiments observations experiments algorithms options described x2 performed matrices harwellboeing sparse matrix collection 12 matrices extracted example problems fidap fluid dynamics analysis package 16 matrices scaled 2norm column unity experiment report number gmres20 steps reduce initial residual rightpreconditioned linear system 10 gamma5 zero initial guess used righthandside constructed solution vector ones dagger tables indicates convergence 500 iterations tables also show value frobenius norm 13 even though function minimize see always reliable measure gmres convergence results shown outer iterations progress algorithm 24 dropping solution vectors one inner iteration used unless otherwise indicated algorithm 25 dropping residual vectors one additional fillin allowed per iteration various codes fortran 77 c matlab used run 64bit precision sun workstations cray c90 supercomputer begin comparison newton global columnoriented iterations early numerical experiments showed practice newton iteration converges slowly initially adversely affected numerical dropping global iterations also worse columnoriented iterations perhaps single ff defined 22 used opposed one column columnoriented case table 41 gives numerical results west0067 matrix harwellboeing collection number gmres iterations given number outer iterations increases mr iteration used selfpreconditioning scaled transpose initial guess dropping based numerical values intermediate solutions performed columnbycolumn basis although newton global iterations restriction necessary presence dropping find much larger matrices newton iteration gave convergent gmres iterations scaling iterate 1kam k 1 alleviate effects dropping superior behavior global iterations presence dropping table 41 typical table west0067 newton global column mr iterations dropping newton 414 158 100 41 global 228 102 25 mr newton 463 435 457 global mr 281 120 86 61 43 eigenvalues preconditioned west0067 matrix plotted fig 41 without dropping using columnoriented mr iterations iterations proceed eigenvalues preconditioned system become closer 1 numerical dropping effect spreading eigenvalues dropping severe spoiling occurs observed two phenomena either dropping causes 22 e chow saad eigenvalues become negative eigenvalues stay clustered around origin 03 02 010103 dropping 03 02 010103 b dropping 03 02 010103 c 03 02 010103 fig 41 eigenvalues preconditioned system west0067 next show results matrices arise solving fullycoupled navierstokes equations matrices extracted fidap package final nonlinear iteration problem examples collection matrices 2dimensional finite element discretizations using 9node quadrilateral elements velocity temperature linear discontinuous elements pressure table 42 lists statistics positive definite matrices collection combination illconditioning indefiniteness matrices difficult methods results shown matrices also symmetric except example 7 none matrices could solved ilu0 ilut 32 threshold incomplete lu factorization table fidap example matrices example n nnz flow past circular cylinder 7 1633 54543 natural convection square cavity 9 3363 99471 jet impingement narrow channel flow multiple steps channel 13 2568 75628 axisymmetric flow poppet valve liquid annulus radiation heat transfer cavity even large amounts fillin experience matrices produce unstable l u factors 12 table 43 shows results preconditioning approximate inverse using dropping residual search direction since problems illconditioned positive definite scaled identity initial guess selfpreconditioning used columns show results iterations fillin progress convergent gmres iterations could achieved even lfil small 10 showing approximate inverse preconditioner much sparser original matrix possible table number gmres iterations vs lfil 9 203 117 67 51 28 26 24 24 comparison solve problems using perturbed ilu factorizations perturbations added inverse diagonal elements avoid small pivots thus control size elements l u factors use twolevel block ilu strategy called bilu0svdff uses modified singular value decomposition invert blocks block needs inverted replaced perturbed inverse sigma sigma singular values thresholded ffoe 1 factor largest singular value table 44 shows results using block size 4 method successful set problems showing results comparable approximate inverse precon ditioning less work compute preconditioner none problems converged however 01 one ff gave best result problems show main results table 45 several standard matrices harwellboeing collection problems nonsymmetric indefinite except sherman1 symmetric negative definite addition saylr3 singular sherman2 reordered reverse cuthillmckee attempt change sparsity pattern inverse show number gmres iterations convergence number outer iterations used compute approximate inverse scaled transpose initial guess used columns initial guess contained lfil nonzeros dropping applied guess table preconditioner example ff 03 ff 10 9 28 72 numerical dropping applied intermediate vectors solution retaining lfil nonzeros using drop tolerance table number iterations vs matrix selfpreconditioned unselfpreconditioned problems sherman2 west0989 gre1107 nnc666 results become worse outer iterations progress spoiling effect due fact descent property maintained dropping applied intermediate solutions case dropping applied search direction seen table 43 except saylr3 problems could solved ilu0 also could solved bilu0svdff ilutp variant ilut suited indefinite problems since uses partial pivoting avoid small pivots 29 ilutp also substitutes 10 gamma4 times norm row forced take zero pivot ffi drop tolerance ilu factorization strategies simply apply cases shown best results trials different parameters method sensitive widely differing characteristics general matrices apart comments already made selecting initial guess whether use selfpreconditioning general set parameters works best constructing approximate inverse following two tables illustrate different behaviors seen three different matrices lapl0324 standard symmetric positive definite 2d laplacian matrix order 324 west0067 pores3 indefinite west0067 little structure pores3 symmetric pattern table 46 shows number gmres20 iterations table 47 shows frobenius norm residual matrix number outer iterations used compute approximate inverse table number iterations vs matrix lfil init west0067 none p 130 none u 484 481 472 none p 43 none u none p pores3 none p none u 274 174 116 table vs matrix lfil init west0067 none p 443 321 240 187 095 none u 607 607 607 607 607 none p 817 817 817 817 817 lapl0324 none p 791 569 425 312 223 none u 662 493 400 341 300 none p 534 421 353 308 275 pores3 none p 1078 930 825 766 716 none u 1295 1202 1148 1082 1020 5 practical variations applications approximate inverses expensive compute large difficult problems however best potential combinations techniques essence would like apply techniques problems either small start close good solution certain sense saw table 45 approximate inverses work well small matrices likely local nature next section show smaller approximate inverses may used effectively incomplete block tridiagonal factorizations 51 incomplete block tridiagonal factorizations incomplete factorization block tridiagonal matrices studied extensively past decade 1 2 3 4 9 21 22 numerical results reported general sparse systems banded polynomial approximations pivot blocks primarily used past systems arising finite difference discretizations partial differential equations currently options incomplete 26 e chow saad factorizations block matrices require approximate inversion general large sparse blocks inversefree form block tridiagonal factorization la strictly lower block tridiagonal part coefficient matrix ua corresponding upper part block diagonal matrix whose blocks defined recurrence starting factorization made incomplete using approximate inverses rather exact inverse 52 inversefree form requires matrixvector multiplications preconditioning operation illustrate use approximate inverses factorizations example 19 fidap largest nonsymmetric matrix collection 259879 problem axisymmetric 2d developing pipe flow using twoequation kffl model turbulence constant block size 161 used smallest block size would yield block tridiagonal system last block size 91 since matrix arises finite element problem careful selection partitioning may yield better results worse case pivot block may singular would cause difficulties several approximate inverse techniques 23 sparsity pattern augmented case minimal residual solution null space would returned since matrix contains different equations variables rows system scaled 2norms columns scaled similarly krylov subspace size gmres 50 used table 51 first illustrates solution bilu0svdff block size 5 comparison infinitynorm condition inverse block lu factors estimated klu gamma1 ek1 e vector ones condition estimate decreases dramatically perturbation increased table example 19 bilu0svdff condition gmres ff estimate steps 0500 129 87 1000 96 337 table 52 shows condition estimate number gmres steps convergence timings setting preconditioner iterations number nonzeros preconditioner method btif denotes inversefree factorization 51 may used several approximate inverse techniques mrslfil mrrlfil denote minimal residual algorithm using dropping solution residual vectors respectively ls least squares solution using sparsity pattern pivot block sparsity pattern approximate inverse mr methods used lfil 10 specifically 3 outer 1 inner iteration mrs lfil iterations mrr selfpreconditioning transpose initial guesses used ls used dgels routine lapack compute least squares solu tion experiments carried one processor sun sparcstation 10 code constructing incomplete block factorization somewhat inefficient two ways transposes data structure pivot block inverse use columnoriented algorithms counts number nonzeros sparse matrixmatrix multiplication performing actual multiplication table example 19 block tridiagonal incomplete factorization cond gmres cpu time est steps precon solve total precon timings show btifmrs10 comparable bilu0svd05 uses much less memory although actual number nonzeros matrix 259 879 39 355 block nonzeros required bilu0 therefore almost million entries needed stored bilu0 required time iterations preconditioner denser needed operate much smaller blocks mr methods produced approximate inverses sparser original pivot blocks ls method produces approximate inverses number nonzeros pivot blocks thus required greater storage computation time solution poor however possibly second third fourth pivot blocks poorly approximated cases least one local least squares problem linearly independent columns pivot blocks singular 52 improving preconditioner previous algorithms sought matrix make close identity matrix general seek instead approximation matrix b thus consider objective function f b matrix defined find matrix whose objective function 53 small enough preconditioner matrix defined implies b matrix easy invert rather solving systems b inexpensive one extreme best identity matrix solves b expensive extreme find standard situation corresponds characterized trivial bsolves expensive obtain matrices two extremes number appealing compromises perhaps simplest block diagonal 28 e chow saad another way viewing concept approximately minimizing 53 improving preconditioner b existing preconditioner example lu factorization factorization gives unsatisfactory convergence rate difficult improve attempting modify l u factors one solution would discard factorization attempt recompute fresh one possibly fillin clearly may wasteful especially case process must iterated times due persistent failures numerical example improving preconditioner use approximate inverses improve blockdiagonal preconditioners orsreg1 orsirr1 matrices experiments used dropping numerical values table 53 block size block size blockdiagonal preconditioner block precon number gmres iterations required convergence blockdiagonal preconditioner used alone number gmres iterations shown number outer iterations used improve preconditioner table improving preconditioner block block matrix besides applications used approximate inverse techniques several purposes like 53 generalize problem minimize f b righthand side x approximate sparse solution righthand side b need sparse dropping used search direction sparse approximate solutions linear systems may used forming preconditioners example form sparse approximation schur complement inverse see 8 details 6 conclusion paper described approach constructing approximate inverses via sparsesparse iterations sparse mode iterations designed economical however cost still competitive ilu factorizations approximate inverse techniques use adaptive sparsity selection schemes also suffer drawback however several examples show preconditioners may applied cases existing options perturbed ilu factorizations fail importantly conclusion greatest value sparse approximate inverses may use conjunction preconditioners demonstrated incomplete block factorizations improving block diagonal pre conditioners also used successfully computing sparse solutions constructing preconditioners one variant promise computing approximations operators may effectively dense two limitations approximate inverses general local nature question whether inverse approximated sparse matrix local nature suggests use effective small problems example pivot blocks incomplete factorizations else large amounts fillin must allowed current work tang 33 couples local inverses domain schur complement approach preliminary results consistently better approximate inverse applied directly matrix effect similarities 7 trying ensure enough variation entries inverse sparse approximation effective tried reordering reduce profile matrix different technique wan et al 34 compute approximate inverse wavelet space may greater variations entries inverse thus permit better sparse approximation acknowledgments authors grateful referees comments substantially improved quality paper authors also wish acknowledge support minnesota supercomputer institute provided computer facilities excellent environment conduct research r incomplete block matrix factorization preconditioning methods versions incomplete blockmatrix factorization iterative methods approximate factorization methods block matrices suitable vector parallel processors iterative solution large scale linear systems iterative solution large sparse linear systems arising certain multidimensional approximation problems approximate inverse techniques blockpartitioned matrices block preconditioning conjugate gradient method direct methods sparse matrices sparse matrix test problems variational iterative methods nonsymmetric systems linear equations stability analysis incomplete lu factorizations fidap examples manual matrix computations parallel preconditioning sparse approximate inverses parallel preconditioning approximate inverses connection machine theory matrices numerical analysis family twolevel preconditionings incomplete block factorization type modified blockapproximate factorization strategies private communication efficient parallel solution linear systems improved newton iteration generalized inverse matrix preconditioning techniques indefinite nonsymmetric linear systems effective sparse approximate inverse preconditioners fast waveletbased sparse approximate inverse preconditioners tr ctr davod khojasteh salkuyeh faezeh toutounian bilus block version ilus factorization korean journal computational applied mathematics v15 n12 p299312 may 2004 philippe guillaume yousef saad masha sosonkina rational approximation preconditioners sparse linear systems journal computational applied mathematics v158 n2 p419442 15 september prasanth b nair arindam choudhury andy j keane greedy learning algorithms sparse regression classification mercer kernels journal machine learning research 3 312003 kai wang jun zhang multigrid treatment robustness enhancement factored sparse approximate inverse preconditioning applied numerical mathematics v43 n4 p483500 december 2002 sosonkina saad x cai using parallel algebraic recursive multilevel solver modern physical applications future generation computer systems v20 n3 p489500 april 2004 n guessous souhar multilevel block ilu preconditioner sparse nonsymmetric mmatrices journal computational applied mathematics v162 n1 p231246 1 january 2004 edmond chow michael heroux objectoriented framework block preconditioning acm transactions mathematical software toms v24 n2 p159183 june 1998 tanaka nodera effectiveness approximate inverse preconditioning using mr algorithm origin 2400 proceedings third international conference engineering computational technology p115116 september 0406 2002 stirling scotland oliver brker marcus j grote sparse approximate inverse smoothers geometric algebraic multigrid applied numerical mathematics v41 n1 p6180 april 2002 edmond chow parallel implementation practical use sparse approximate inverse preconditioners priori sparsity patterns international journal high performance computing applications v15 n1 p5674 february 2001 j martnez g larrazbal waveletbased spai preconditioner using local dropping mathematics computers simulation v73 n1 p200214 6 november 2006 michele benzi preconditioning techniques large linear systems survey journal computational physics v182 n2 p418477 november 2002