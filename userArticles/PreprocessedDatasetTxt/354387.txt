block algorithm matrix 1norm estimation application 1norm pseudospectra matrix 1norm estimation algorithm used lapack various software libraries packages proved valuable tool however limitations offers user control accuracy reliability estimate based level 2 blas operations block generalization 1norm power method underlying estimator derived developed practical algorithm applicable real complex matrices algorithm works n matrices parameter t1 original algorithm recovered two improvements one real matrices one complex matrices accuracy reliability estimates generally increase computational kernels level 3 blas operations 1 last t1 columns starting matrix randomly chosen giving algorithm statistical flavor byproduct investigations identify matrix 1norm power method takes maximum number iterations application new estimator show used efficiently approximate 1norm pseudospectra b introduction research matrix condition number estimation began 1970s problem cheaply estimating condition number approximate null vector square matrix given factorization earliest algorithm one gragg stewart 10 improved cline moler stewart wilkinson 4 leading 1norm condition estimation algorithm used linpack 8 later included matlab function rcond 1980s attention drawn various componentwise condition numbers recognized condition estimation problems reduced estimation kak matrixvector products ax x cheaply computed 2 16 sec 141 hager 12 derived algorithm 1norm special case general pnorm power method proposed boyd 3 later investigated tao 18 hagers algorithm modified higham 14 incorporated lapack routine xlacon 1 matlab function condest linpack lapack estimators produce estimates practice almost always within factor 10 3 respectively quantities estimating 13 14 15 entirely adequate applications order magnitude estimate required evaluation error bounds however applications estimate one correct digits required see example pseudospectra application described section 4 linpack lapack estimators drawback offer user way control improve accuracy estimate accuracy refers average case behaviour also interest norm estimator worst case behaviour reliability work supported engineering physical sciences research council grants grl76532 grl94314 department mathematics university manchester manchester m13 9pl england highammamanacuk httpwwwmamanacukhigham z department mathematics university manchester manchester m13 9pl england ftisseurmamanacuk httpwwwmamanacukftisseur n j higham f tisseur table empirical probabilities minf e oe one form invrandn100 n0 1 vectors x j since estimating kak cheaply appears inevitably admit possibility arbitrarily poor estimates although proving open problem 6 one might look approach probabilistic statements made accuracy estimate definition subordinate matrix norm suggests estimate ae oe parameter x j independently chosen random vectors case 2norm kxk appropriate distribution x j explicit bounds available probability estimates within given factor kak 7 done 11 certain estimates frobenius norm scale estimates e oe oe constant chosen expected value e oe kak note e oe therefore greater less kak 1norm kxk interest investigate approach empirically fixed matrix form matlab notation invrandn100 table 11 shows observed probabilities minf e various ff based 1000 separate evaluations oe vectors x j normal n0 1 distribution determined empirically mean e oe table shows even 35 estimates within factor 09 true norm statistical sampling technique clearly crude useful obtaining estimates correct digits one way exploit information contained vectors ax j regard first iterates 1norm power method starting vectors x j continue iterate considerations motivate block generalization 1norm power method present paper block power method works matrix columns instead vector give feel new estimator compares sampling technique applied estimator algorithm 24 1000 random matrices form invrandn100 results shown table 12 estimates obtained approximately cost estimates e oe respectively superiority new estimator clear section 2 derive block 1norm power method develop practical algorithm real complex matrices section 3 present numerical experiments give insight behaviour algorithm application involving complex matrices given section 4 describe algorithm used approximate 1norm pseudospectra conclusions presented section 5 table empirical probabilities est est algorithm 24 form invrandn100 finally note although work specific 1norm 1norm estimated applying algorithm since 2 block 1norm power method 1norm power method special case boyds pnorm power method 3 derived independently hager 12 real matrix denote signa matrix according ij 0 ij 0 jth column identity matrix denoted e j algorithm 21 1norm power method given 2 r nthetan algorithm computes x repeat quit smallest algorithm 21 modified higham 14 alg 41 see also 15 16 alg 144 improve reliability efficiency modifications improve reliability first force least two iterations second take final estimate maximum produced algorithm vector b heuristic choice intended pick large elements cases elements fail revealed course algorithm efficiency improved terminating algorithm computing previous since shown convergence would otherwise declared subsequent computation z obtain accurate reliable estimate provided algorithm 21 could run algorithm times succession different starting vectors idea suggested 12 starting vector mean unit vectors e j already visited algorithm prohibited visiting unit vectors previously visited note estimates obtained way nondecreasing approach two weaknesses allows limited communication information different iterations highest level computational kernel remains matrixvector multiplication therefore develop block algorithm works n theta matrix whole instead separate 4 n j higham f tisseur nvectors block approach offers potential better estimates providing information base decisions allows use level 3 blas operations thus promising greater efficiency following algorithm estimates 1norm byproduct 1norms columns largest 1norms algorithm 22 block 1norm power method given 2 r nthetan positive integer algorithm computes vectors g ind g j lower bound 1norm column jth largest 1norm choose starting matrix x 2 r nthetat columns unit 1norm repeat sort g ind best reorder ind correspondingly like basic 1norm power method algorithm 22 attractive property generates increasing sequences estimates denote superscript k quantities kth iteration loop algorithm 22 let j denote jth column lemma 23 sorted vectors g k h k satisfy 2 proof first 1jt r r r r thus r r k1 kx k r k1 max furthermore h k r k prove 23 assume without loss generality x kgamma1 form z ff t1 ff t1 ff row general different ff ka n algorithm chooses ind values corresponding rows z k largest 1norm g k1 next stage least large 1norms since row j z k 1norm ka follows algorithm 22 three possible sources inefficiency first columns vectors sigma1s pair columns j may parallel case z computing z matrixvector multiplication redundant tgamma1 redundant matrixvector products per iteration maximum achieved second iteration matrix ones nonnegative elements second possible inefficiency column may parallel one previous iteration case formation redundant computation choose detect parallel columns replace random vectors randfgamma1 1g already previous denotes vector elements uniform distribution set fgamma1 1g detection done forming inner products columns looking elements magnitude n total cost computations ont 2 flops negligible compared 2n 2 flops required matrix product algorithm since n practice strategy could extended check parallel columns current previous return possibility section 3 columns parallel columns previous iteration easy see algorithm 22 converge therefore immediately terminate iteration without computing z finally step k x k computation ax k repeats earlier computation repeated vectors e j easily avoided keeping track indices previously used e j selecting ind ind among indices previously used indices repeats prematurely terminate iteration saving matrix product note first third inefficiencies possibly 1 second happen original 1norm power method strategy detecting redundant computations replacing ones provide new information three benefits first reduce amount computation premature detection convergence second lead better estimates columns depends random replacement vectors generated e j improvement deterministic future replacements 6 n j higham f tisseur columns third benefit dimensions matrix multiplications remain constant iteration opposed varying simply skip redundant computations helps us make efficient use computing resources next algorithm incorporates modifications algorithm forced take least 2 itmax iterations computes least columns also explicitly identifies approximate maximizing vector achieves norm estimate algorithm 24 practical block 1norm estimator given 2 r nthetan positive integers itmax 2 algorithm computes scalar est vectors v w est choose starting matrix x 2 r nthetat columns unit 1norm recording indices used unit vectors e j est est est est old ind best est est old est est old goto 6 end est 2 every column parallel column old goto 6 end ensure column parallel another column column old replacing columns randfgamma1 1g best goto 6 end reorder ind correspondingly 5 ind1 contained ind gamma hist goto 6 end replace ind1 first indices ind1 n ind gamma hist hist ind1 best note algorithm explicitly compute lower bounds 1norms largest columns information required aware applications needed obtained keeping track largest inequality 22 expressed est k h k est k1 still valid except h k est k1 possible last iteration original ind k 1 repeat event handled test 1 however 23 longer true avoidance repeated indices algorithms 24 22 compare ignoring itmax test algorithm 24 terminates nt1 iterations since vertices e j visited iteration first vertex visited cannot said algorithm 22 possibility repeated vertices algorithm 24 produce smaller estimate algorithm 22 redundant computation avoided new information computed lead apparently promising vertex based relative sizes h replacing one actually corresponds larger column however likely algorithms 24 algorithm 22 produce different results algorithm 24 produces better estimate following example example 25 certain starting obtained following results algorithm 22 1 0 810e001 0 613e001 2 10 177e000 4 176e000 underestimation ratio 199e001 first column denotes iteration number kth row gives sorted g k one preceded corresponding index ind since x 1 columns e j ind first iteration shown zero algorithm 24 1 0 810e001 0 613e001 2 10 177e000 4 176e000 parallel column sold 3 2 887e000 5 459e000 exact estimate algorithm 22 converges 2 iterations produces estimate small factor 5 however second iteration algorithm 24 detects column parallel one old replaces new column produces different z matrix causes convergence test 4 failed extra iteration visits unique column maximum 1norm exact estimate obtained 24 differs modified version algorithm 21 used lapack 30 14 alg 41 16 alg 144 two ways first algorithm 24 use extra estimate 21 second lapack algorithm checks algorithm 24 oversight recommend lapacks xlacon modified include extra test change affect estimates produced sometimes reduce number iterations explain choice starting matrix take first column x vector 1s starting vector used algorithm 21 advantage matrix nonnegative elements algorithm converges exact estimate second iteration matrices arise applications example stochastic matrix inverse mmatrix remaining columns chosen randfgamma1 1g check correction parallel columns exactly body algorithm choose random vectors difficult argue particular fixed vectors randomness lessens importance counterexamples see comments next section next consider complex matrices arise pseudospectrum application section 4 everything section remains valid complex matrices provided signa redefined matrix transposes replaced conjugate transposes matrix complex elements unit modulus much less likely find parallel columns 8 n j higham f tisseur one iteration next within current therefore complex matrices omit tests parallel columns however take real starting matrix one question complex case analogue algorithm 21 complex matrices 14 alg 51 z defined z rea based subgradient considerations block algorithm take former justified heuristic considerations preserves information return question next section motivation algorithm 24 enable accurate reliable estimates obtained provided 1norm power method question arises accuracy reliability estimates varies little said theoretically unlike approach 12 mentioned start section estimates monotonic run algorithm 24 1 using common set 1 starting vectors obtain smaller estimate 2 1 less promising choice unit vector e j turn better promising choice made available information nonmonotonicity unlikely however argue price worth paying advantages accrue next section investigate behaviour algorithm 24 empirically 3 numerical experiments aim section answer following questions algorithm 24 bearing mind algorithm implementation well understood 1norm power method 1 accuracy reliability norm estimates vary 2 good norm estimates general 3 number iterations behave 1 note searching counterexamples done previous condition estimators 4 5 14 know fixed starting matrix n must families matrices whose norm underestimated arbitrarily large factor since algorithm samples behaviour n theta n matrix fewer vectors since algorithm uses random starting matrix 1 counterexample valid particular starting matrices tests performed matlab first group tests deals random real matrices amongst matrices used 1 normal n0 1 distribution denoted randn inverse orthogonal qr factor upper triangular part inverse upper triangular part 2 uniform distribution set fgamma1 0 1g denoted rand101 gamma1 uniform distribution interval 0 1 3 gamma1 form u sigma v u v random orthogonal matrices singular values oe distributed exponentially arithmetically except smallest equal unity 2norm condition number ranging 1 10 16 note omit example matrices uniform 0 1 uniform fgamma1 1g distributions matrices algorithm 24 easily seen produce exact norm chose n range test matrix recorded variety statistics including underestimation ratio averaged minimized type fixed n relative error jest number iterations declared estimate exact relative error larger 10 gamma14 unit roundoff given matrix first generated starting columns max largest value used ran algorithm 24 using starting way could see effect increasing particular checked percentage estimates given least large estimates smaller denote improve set algorithm 24 first give general comments results 1 increasing usually gave larger average minimum underestimation ratios though exceptions quantity improve 100 half time never less 78 2 number iterations averaged 2 3 throughout maxima ranging 2 5 depending type matrix thus increasing 1 little effect number iterationsan important fact could predicted theory although specially constructed examples 1norm power method requires many iterations one described rare limit 5 iterations come effect 3 throughout tests also computed extra estimate 21 used lapack norm estimator 14 expected none tests random matrices extra estimate larger estimate provided algorithm 24 tables 31 32 show detailed results two particular types random matrix among described rand101 columns headed products show average maximum total number matrix products ax case 5000 matrices used matrices invrandn taking significantly improves worstcase average estimates proportion exact estimates estimate exact almost 98 percent time matrices rand101 improvements increases less dramatic still useful notice exactly four matrix products required every case well recording number products checked convergence achieved matrices rand101 convergence always achieved test 4 algorithm 24 invrandn convergence declared tests 4 2 approximately 96 4 percent cases respectively instances convergence 5 2 last two columns table 31 show average maximum number parallel columns detected small number repeated e j vectors detected replaced largest average 003 occurring maximum number 7 occurred columns repeated detected matrices table 32 strategy replacing parallel columns repeated e j vectors little effect overall performance algorithm 24 tests random matrices since particular examples found beneficial see example 25 cost negligible feel use worthwhile however see advantage extending strategy compare columns previous matrices higham 15 gives tridiagonal matrix 1norm power method algo rithm 21 requires n iterations converge constructed matrix n j higham f tisseur table results 5000 matrices invrandn dimension 100 underest ratio products parallel cols min average exact average max improve average max 9 0893 1000 9988 40 4 10000 122 15 table results 5000 matrices randf101g dimension 100 underest ratio products min average exact average max improve 9 0775 0951 2856 4 4 9074 maximum iterations required inverse bidiagonal matrix 1 ff3 1 gammaff3 minus sign front matrix necessary straightforward show algorithm 21 applied ff produces table results matrix 100 repetitions underest ratio products min average exact average max improve 6 1000 1000 10000 46 11 10000 7 1000 1000 10000 43 11 10000 8 1000 1000 10000 42 8 10000 9 1000 1000 10000 41 8 10000 thus every column ff computed order first last exact norm obtained algorithm terminated p iterations produces estimate 1 gamma ff behaves exactly way applied algorithm 24 1000 times 100 tests results shown table 33 6th column 11 denotes convergence declared iteration limit reached percentage occurrences varied 100 01 7 underestimation ratio agrees theory unacceptably small note randomness hence one estimate 1 2 average norm estimates satisfactory extra estimate 21 value 0561 thus significantly improves estimate worse average estimates greater complex matrices tried 3 algorithm 24 tables 34 35 compare two choices 5000 100 theta 100 random complex matrices form invrandirand rand matrix uniform distribution interval 0 1 test larger underestimation ratios obtained percentage exact estimates higher statistics number matrix products slightly better tests found complex choice z always perform least well overall real choice see example riffle shuffle example section 4 therefore keep z complex algorithm 24 version 30 lapack norm estimator modified version 20 keep vector z complex finally algorithm 24 compare suggestion hager mentioned beginning section 2 running 1norm power method times suc cession tests random matrices found hagers approach produce surprisingly good norm estimates generally inferior algorithm 24 since hagers approach based entirely level 2 blas operations algorithm 24 clearly preferred 4 computing 1norm pseudospectra section apply complex version algorithm 24 computation 1norm pseudospectra ffl 0 n j higham f tisseur table results 5000 matrices invrandirand dimension 100 underest ratio products min average exact average max improve 9 0859 1000 9986 40 4 10000 table results 5000 matrices invrandirand dimension 100 underest ratio products min average exact average max improve 9 0819 0999 9812 40 6 9974 subordinate matrix norm fflpseudospectrum 2 c nthetan defined 22 z eigenvalue e e withkek ffl g equivalently terms resolvent zi published work pseudospectra dealt 2norm utility 2 norm pseudospectra revealing effects nonnormality well appreciated 19 2norm pnorm n theta n matrix differ factor n small n pseudospectra therefore vary much different pnorms however jonsson trefethen shown 17 markov chain applications choice norm pseudospectra crucial markov chains representing random walk mdimensional hypercube riffle shuffle cards transition matrices dimension exponential factorial respectively known measured appropriate way random processes converge steady state gradually suddenly certain number steps processes involve powers matrices possibly huge dimension 1norm natural norm probability 1 1norm pseudospectra important tool explaining transient behaviour 17 one useful graphical representations pseudospectra plot level curves resolvent therefore consider standard approach evaluating equally spaced grid points z region interest complex plane sending results contour plotter variety methods carrying computations 2norm surveyed trefethen 21 ideas employed directly applicable 1norm explicitly forming zi grid point computationally expensive efficient approach factorize p lu point z lu factorization partial pivoting use algorithm 24 estimate kzi gamma matrix multiplications algorithm become triangular solves multiple righthand sides approach take advantage sparsity however method still requires 3 operations per grid point full consider instead efficient approach applicable compute schur factorization 9 chap 7 q unitary upper triangular given factorization forming matrix product zi conjugate transpose reduces solving multiple righthand side triangular system multiplying q given initial decomposition cost per grid point estimating resolvent norm using algorithm 24 2 flopsa substantial saving first approach order magnitude cost standard methods computing 2norm pseudospectra provided small place schur decomposition could use hessenberg decomposition computed either gauss transformations householder transformations 9 sec 74 decompositions less expensive cost per grid point larger multiple n 2 flops need factorize hessenberg matrix detail approach follows algorithm 41 1norm pseudospectra estimation given 2 c nthetan positive integer algorithm estimates kzi specified grid points complex plane using algorithm 24 parameter compute schur factorization grid point z apply complex version algorithm 24 zi parameter using representation 42 experience algorithm 41 frequently leads visually acceptable contour plots even 1 following example shows however larger value may needed take example markov chains gilbertshannonreeds model riffle shuffle deck n cards transition matrix p dimension n remarkably jonsson trefethen explain 17 dimension matrix 1 use row vectors markov chain literature actually 1norm relevant using k1 continue work 1norm 14 n j higham f tisseur reduced n certain transformations preserve 1norms powers resolvent experiment took 17 computed pseudospectra decay matrix working reduced form figure 41 shows approximations 1norm pseudospectra computed 100 theta 100 grid 24 contours plotted dashed line marks unit circle eigenvalues plotted dots exploit fact since real pseudospectra symmetric real axis contour plot clearly incorrect outer contour yields improvement plot visual accuracy table 41 summarizes key statistics computations showing average norm estimates correct significant digits 3 figure table confirm better keep z complex algorithm 24 give example spectral discretization integral operator landau 21 sec 21 like operator complex symmetric took dimension fresnel number 8 noted 21 2norm fine grid needed resolve details example used 200 theta 200 grid figure 42 shows computed pseudospectra summarizes statistics values 2 contours plotted dashed line marks unit circle eigenvalues plotted dots table 42 11 denotes convergence declared iteration limit reached percentage occurrences 012 0025 one contour lines misses eigenvalue northwest corner plot plot differs exact one tiny oscillations two outer contours algorithm 24 performs well even small measured underestimation ratio quite accurate values 1norm resolvent needed example order produce smooth contours 5 conclusions derived new matrix 1norm estimation algorithm algorithm 24 number key features importantly algorithm parameter used control accuracy reliability estimate actually lower bound guarantee increasing increases estimate leaving aside fact starting matrix partly random estimate typically increase leading quickly one correct significant digits estimate crucial property algorithm number iterations matrix products required convergence essentially independent random matrices 2 iterations required average corresponding 4 products n theta n n theta matrices algorithm avoids redundant computations keeps constant size matrix multiplications future work intend investigate choice affects efficiency algorithm high performance computing environment unlike statisticallybased norm estimation techniques 7 11 currently apply real matrices algorithm handles real complex matrices since algorithm uses partly random starting matrix 2 natural ask whether bounds valid obtained probability estimate within certain factor kak 1 feel features algorithm make effective make difficult impossible derive useful bounds type algorithm similar estimator lapack differences algorithm omits extra estimate 21 real matrices test parallel rather simply repeated sign vectors improves ef ficiency unlike estimator lapack 20 complex matrices take real part z vector improves quality estimates efficiency change incorporated lapack 30 new algorithm makes attractive replacement existing lapack estimator value would natural default choice extra estimate 21 included extra reliability backward compatibility user willing pay accurate reliable 1norm estimates would option choosing larger acknowledgements thank nick trefethen providing mfiles compute riffle shuffle landau matrices used section 4 n j higham f tisseur table results riffle shuffle example underest ratio products min average exact average max fig 41 1norm pseudospectra riffle shuffle clockwise top left table results landau matrix example underest ratio products min average exact average max fig 42 1norm pseudospectra landau matrix clockwise top left n j higham f tisseur r solving sparse linear systems sparse backward error power method estimate condition number matrix set counterexamples three condition number estimators open problems numerical linear algebra estimating extremal eigenvalues condition numbers matrices matrix computations stable variant secant method solving nonlinear equations condition estimates survey condition number estimation triangular matrices fortran codes estimating onenorm real complex matrix experience matrix norm estimator accuracy stability numerical algorithms numerical analyst looks convergence subgradient method computing bound norm matrices pseudospectra matrices pseudospectra linear operators computation pseudospectra spectra pseudospectra behavior nonnormal matrices operators tr ctr j r cash f mazzia n sumarti trigiante role conditioning mesh selection algorithms first order systems linear two point boundary value problems journal computational applied mathematics v185 n2 p212224 15 january 2006 j r cash f mazzia new mesh selection algorithm based conditioning twopoint boundary value codes journal computational applied mathematics v184 n2 p362381 15 december 2005