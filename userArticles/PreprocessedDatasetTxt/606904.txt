smoothing methods linear programs flexible update smoothing parameter consider smoothingtype method solution linear programs main idea reformulate corresponding central path conditions nonlinear system equations variant newtons method applied method shown globally locally quadratically convergent suitable assumptions contrast number recently proposed smoothingtype methods current work allows flexible updating smoothing parameter furthermore compared previous smoothingtype methods current implementation new method gives significantly better numerical results netlib test suite b introduction consider linear program x st given data assumed full rank classical method solution minimization problem dantzigs simplex algorithm see eg 11 1 last two decades however interiorpoint methods become quite popular viewed serious alternatives simplex method especially largescale problems recently socalled smoothingtype methods also investigated solution linear programs smoothingtype methods join properties interiorpoint methods explain detail consider optimality conditions 2 linear program 1 recall 1 solution 2 solution successful interiorpoint methods try solve optimality conditions 2 solving inexactly sequence perturbed problems also called central path conditions 0 denotes suitable parameter typically interiorpoint methods apply kind newton method equations within perturbed optimality conditions guarantee positivity primal dual variables appropriate line search many smoothingtype methods follow similar pattern also try solve inexactly sequence perturbed problems 3 end however rst reformulate system 3 nonlinear system equations apply newtons method reformulated system way smoothingtype methods avoid explicit inequality constraints therefore iterates generated methods necessarily belong positive orthant details smoothing methods given section 2 algorithm presented manuscript belongs class smoothingtype methods closely related methods recently proposed burke xu 2 3 investigated authors 13 14 contrast methods however allow exible choice parameter since precise way parameter updated within algorithm enormous uence entire behaviour algorithm feel highly important topic second motivation writing paper fact current code gives signicantly better numerical results previous implementations smoothingtype methods background smoothingtype methods interested reader referred 4 6 7 8 16 17 20 22 23 references therein paper organized follows develop algorithm section 2 give detailed statement show welldened section 3 discusses global local convergence properties algorithm particular shown method nice global convergence properties method suggested burke xu 3 section 4 indicates method works quite well whole netlib test suite close paper nal remarks section 5 words notation r n denotes ndimensional real vector space use subscript x order indicate ith component x whereas superscript like x k used indicate kth iterate sequence fx k g r n quite often consider triple form 2 r n course w vector r nmn order simplify notation however usually write instead using mathematically correct vector whose components nonnegative simply write x 0 expression like x 0 similar meaning finally symbol k k used euclidean vector norm 2 description algorithm section want derive predictorcorrector smoothing method solution optimality conditions 2 furthermore see method welldened since main idea method based suitable reformulation optimality conditions 2 begin simple way reformulate system end let denote socalled minimum function let dened since property 0 b 0 ab follows used order get characterization complementarity conditions consequently vector w solution optimality conditions 2 satises nonlinear system equations main disadvantage mapping dierentiable everywhere order overcome nonsmoothness several researchers see eg 7 5 18 21 proposed approximate minimum function continuously dierentiable mapping help socalled smoothing parameter particular function become quite popular typically called chenharkerkanzowsmale smoothing function literature 5 18 21 based function may dene mappings w obviously smooth approximation every 0 coincides limiting case furthermore observed 18 vector w solves nonlinear system equations vector solution central path conditions 3 solving system 4 say newtons method therefore closely related several primaldual pathfollowing methods become quite popular last 15 years cf 24 however due numerical experience 13 14 motivated stronger theoretical results obtained burke xu 3 prefer view independent variable rather parameter make clear notation write similarly since nonlinear system 4 contains equations add one equation dene mapping cf 3 also need following generalization function 2 0 1 denotes suitable centering parameter 0 1 r function following properties p1 continuously dierentiable 0 0 constant possibly depending 0 following functions satisfy properties fact quite easy see three examples satisfy properties p1 p2 p3 furthermore mapping independent 0 also mapping independent 0 hand simple calculation shows third example satisfy depends 0 note choice corresponds one used 2 3 whereas aim generalize approach 2 3 order allow exible procedure decrease since precise reduction signicant uence overall performance smoothingtype method feel generalization important computational point view give precise statement algorithm let us add comments properties function p1 obviously needed since want apply newtontype method system equations hence suciently smooth second property p2 implies strictly monotonically increasing together 0 property p1 means nonlinear system equations equivalent optimality conditions 2 central path conditions 3 since last row immediately gives third property p3 used order show algorithm presented welldened cf proof lemma 22 c furthermore properties p3 p4 together guarantee sequence monotonically decreasing converges zero see proof theorem 33 return description algorithm method presented predictorcorrector algorithm predictor step responsible local fast rate convergence corrector step guaranteeing global convergence precisely predictor step consists one newton iteration applied system x followed suitable update tries reduce much possible corrector step applies one newton iteration system usual righthand side replaced centering parameter 1 newton step followed armijotype line search computation iterates carried way belong neighbourhood central path 0 denotes suitable constant addition see later iterates automatically satisfy inequality x 0 important order establish result regarding boundedness iterates cf lemma 31 proposition 32 precise statement algorithm follows recall denote mappings 5 6 respectively algorithm 21 predictorcorrector smoothing method choose w 0 select kx 0 set k 0 termination criterion compute solution w linear system set else compute nonnegative integer corrector step choose r n r r n r linear system go step s1 algorithm 21 closely related methods recently investigated dierent authors example take algorithm almost identical method proposed burke xu 3 completely identical since use dierent update w k predictor step namely case necessary order prove global convergence results theorem 33 corollary 34 hand algorithm 21 similar method used authors 14 fact taking almost method 14 dierence remains use dierent righthand side predictor step namely w k k whereas 14 uses w k 0 latter choice seems give slightly better local properties however current version allows prove better global convergence properties always assume termination parameter algorithm 21 equal zero algorithm 21 generates innite sequence assume stopping criteria steps s1 s2 never satised restrictive since otherwise w k w k would solution optimality conditions 2 rst note algorithm 21 welldened lemma 22 following statements hold k 2 n linear systems 7 8 unique solution b unique k satisfying conditions step s2 c stepsize k s3 uniquely dened consequently algorithm 21 welldened proof taking account structure jacobians 0 w 0 using fact 0 0 property p2 part immediate consequence eg 12 proposition 31 second statement follows 13 proposition 32 essentially due burke xu 3 order verify third statement assume iteration index k 2 n since kx obtain property p3 taking inequality account proof completed using standard argument armijo line search rule 2 next state simple properties algorithm 21 refer couple times subsequent analysis lemma 23 sequences fw k generated algorithm 21 following properties b k 0 1 denotes constant property p4 c proof part holds choice starting point hence holds k 2 n since newtons method solves linear systems exactly order verify statement b rst note get fourth block row linear equation 8 since therefore follows property p4 updating rules steps s2 s3 algorithm 21 using simple induction argument see b holds finally statement c direct consequence updating rules algorithm 21 2 3 convergence properties section analyze global local convergence properties algorithm 21 since analysis local rate convergence essentially 3 recall predictor step identically one 3 focus global properties particular show iterates remain bounded strict feasibility assumption noted burke xu 3 particular member class methods namely choice true many smoothingtype methods like 5 6 7 8 13 14 22 23 central observation allows us prove boundedness iterates automatically satisfy inequality k 2 n provided inequality holds precisely statement rst result lemma 31 sequences fw k generated algorithm 21 following properties x b proof rst derive useful inequalities verify two statements simultaneously induction k begin preliminary discussions regarding statement end let xed moment assume take step s2 algorithm 21 since component function concave obtain third block row 7 hence get 11 claim righthand side 12 nonpositive prove statement rst note 0 hence remains show however obvious since last row linear system 7 implies next derive useful inequalities regarding statement b end still assume k 2 n xed using fact concave function component obtain 8 completes preliminary discussions verify statements b induction k 0 choice starting point w initial smoothing parameter step s0 algorithm 21 therefore set step s2 algorithm 21 also hand step s2 argument used beginning proof shows inequality x holds case suppose immediately implies consequently step s2 algorithm 21 obviously x erwise ie set step s2 argument used beginning part proof shows inequality holds completes formal proof induction 2 next show sequence fw k g generated algorithm 21 remains bounded provided strictly feasible point optimality conditions 2 ie vector x proposition 32 assume strictly feasible point optimality conditions 2 sequence fw k generated algorithm 21 bounded proof statement essentially due burke xu 3 include proof sake completeness assume sequence fw k generated algorithm 21 un bounded since f k g monotonically decreasing lemma 23 b follows lemma 23 c k 2 n denition smoothed minimum function therefore implies index ng x k 1 subsequence since otherwise would x k turn would imply kx subsequence contrast 14 therefore components two sequences fx k g bounded ie k r denotes suitable possibly negative constant hand sequence fw k unbounded assumption implies least one component ng x k subsequence since otherwise two sequences fx k g fs k g would bounded turn would imply boundedness sequence f k g well 23 assumed full rank strictly feasible point 2 whose existence guaranteed assumption particular since also k 2 n lemma 23 get subtracting equations premultiplying rst equation 16 x x k taking account second equation 16 gives reordering equation obtain k 2 n using 15 well view strict feasibility follows 17 fact x k subsequence least one index ng hence exists component ng independent k suitable subsequence using lemma 31 b k 2 n taking account denition looking jth component implies k 2 n using 18 15 see necessarily x k k belonging subsequence 18 holds therefore taking square 19 obtain simplications however since righthand side expression bounded 4 2 0 gives contradiction 18 2 next prove global convergence result algorithm 21 note result dierent one provided burke xu 3 spirit 22 13 14 burke xu 3 use stronger assumption order prove global linear rate convergence sequence f k g theorem 33 assume sequence fw k generated algorithm 21 least one accumulation point f k g converges zero proof since sequence f k g monotonically decreasing lemma 23 b bounded zero converges number 0 0 done assume 0 updating rules step s2 algorithm 21 immediately give k 2 n suciently large subsequencing necessary assume without loss generality 20 holds k 2 n lemma 23 b assumption follows 21 lim therefore stepsize satisfy line search criterion 9 k 2 n large enough hence k 2 n let w accumulation point sequence fw k g let fw k gk subsequence converging w since assume without loss generality subsequence f k g k converges number furthermore since 0 follows 20 lemma 22 corresponding subsequence converges vector unique solution linear equation cf 8 using f k g k 0 taking limit k 1 subset k obtain 20 22 hand get 22 10 property p3 20 lemma 23 c k 2 n suciently large using 20 implies continuously dierentiable function due 24 taking limit k 1 k 2 k gives x denotes solution linear system 23 using 23 gives contradiction 24 hence cannot 0 2 due proposition 32 assumed existence accumulation point theorem 33 automatically satised strictly feasible point optimality conditions 2 immediate consequence theorem 33 following result corollary 34 every accumulation point sequence fw k generated algorithm 21 solution optimality conditions 2 proof short proof essentially 14 example include sake completeness let w accumulation point sequence fw k k denote subsequence converging w k 0 view theorem 33 hence lemma 23 c implies ie x 0 0 x due denition lemma 23 also shows see indeed solution optimality conditions 2 2 nally state local rate convergence result since predictor step coincides one burke xu 3 proof result essentially 3 therefore omit details theorem 35 let parameter satisfy inequality 2 n assume optimality conditions 2 unique solution w suppose sequence generated algorithm 21 converges w f k g converges globally linearly locally quadratically zero central observation order prove theorem 35 sequence jacobian matrices 0 w k k converges nonsingular matrix assumption theorem 35 fact noted 3 12 convergence sequence nonsingular jacobian matrix equivalent unique solvability optimality conditions 2 implemented algorithm 21 c order simplify work took pcx code 10 9 modied appropriate way pcx predictorcorrector interiorpoint solver linear programs written c calling fortran subroutine order solve certain linear systems using sparse cholesky method ng peyton 19 since linear systems occuring algorithm 21 essentially structure arising interiorpoint methods possible use numerical linear algebra part pcx implementation algorithm 21 also apply preprocessor pcx starting method initial point w one used numerical experiments 14 constructed following way solve aa using sparse cholesky code order compute 0 b c solve aa using sparse cholesky code compute 0 note starting point feasible sense satises linear equations b furthermore initial smoothing parameter set ie 0 equal initial residual optimality conditions 2 recall starting vector satises linear equations 2 exactly least numerical inaccuracies order guarantee however sometimes enlarge value 0 satises inequalities ng x 0 note done 14 also took stopping criterion 14 ie terminate iteration one following conditions hold finally centering parameter k chosen follows let 01 start predictor step successful ie allowed take otherwise strategy guarantees centering parameters belong interval according experience larger value usually gives faster convergence entire behaviour method becomes unstable whereas smaller value centering parameter gives stable behaviour overall number iterations increases dynamic choice tries combine observations suitable way remaining parameters step s0 algorithm 21 chosen follows rst consider function less corresponds method test runs done sun enterprise 450 480 mhz table 1 contains corresponding results columns table 1 following meanings problem name test problem netlib collection number equality constraints preprocessing n number variables preprocessing k number iterations termination p number accepted predictor steps value k nal iterate value kw k k 1 nal iterate primal objective value primal objective function nal iterate moreover give number iterations needed related method 14 parantheses number iterations used new method table 1 numerical results algorithm 21 problem objective 1758e 04 550184589e03 adlittle aro agg 390 477 22 23 17 38e 02 6257e 04 359917673e07 agg2 514 750 22 25 agg3 514 750 21 30 beaconfd 86 171 21 18 5156e 04 335924858e04 blend 3652e 04 335213568e02 4166e 06 315018729e02 bore3d 81 138 14 28 11 59e 3980e brandy 133 238 3469e 04 151850990e03 9161e 04 269000997e03 cycle 1420 2773 5207e d2q06c 2132 5728 48 57 d6cube 403 5443 table results algorithm 21 problem objective degen2 2901e degen3 001 f800 322 826 28 36 17 12e 5876e 04 555679564e05 nnis 438 935 20 31 17 20e 7843e 04 172791066e05 8491e t2d 7494e 04 684642932e04 9397e forplan 121 447 26 28 17 22e 4722e 04 664218959e02 ganges 1113 1510 20 25 19 24e 1218e 04 109585736e05 greenbea 25 greenbeb 1932 4154 43 35 13 17e 9559e 04 430226026e06 israel 174 316 17 27 15 10e 02 4732e 04 896644822e05 kb2 43 68 1653e 06 174990013e03 lot 133 346 23 35 12 32e 7087e 04 252647043e01 maros 655 1437 22 37 14 24e 1738e 04 580637437e04 8053e 04 149718517e06 3330e 04 320619729e02 nesm 654 2922 46 52 9 47e 04 4718e 04 140760365e07 perold 593 1374 26 33 12 21e 6564e 04 938075527e03 pilot 1368 4543 71 81 9 90e pilotja 810 1804 pilotwe 701 2814 36 9981e 04 272010753e06 6888e 04 258113924e03 4059e 04 449727619e03 recipe 4205e 2793e sc50a 8546e sc50b 48 76 7714e 06 700000047e01 1049e 04 147534331e07 4563e 04 233138982e06 6230e 04 184167590e04 1834e 04 366602616e04 9098e 04 549012545e04 scorpion 340 412 19 21 14 24e 04 1815e 05 187812482e03 2169e 04 904293215e02 table results algorithm 21 problem objective 7203e 06 866666364e00 3131e 8910e 06 141224999e03 8233e 1051e seba 448 901 19 23 12 25e 1550e 06 157116000e04 share1b 112 248 29 43 14 22e 3762e 04 765893186e04 share2b 96 162 8099e shell 487 1451 19 22 ship04l 292 1905 22 20 7616e 04 179332454e06 ship04s 216 1281 1561e 04 179871470e06 ship08l 470 3121 25 21 15 21e 7592e 04 190905521e06 ship08s 276 1604 15 20 13 30e 02 7416e 04 192009821e06 ship12l 610 4171 21 21 13 70e 2670e 04 147018792e06 ship12s 340 1943 7548e 2548e stair 356 532 standata 314 796 standgub 314 796 standmps 422 1192 14 18 12 96e 4418e stocfor2 1980 2868 14 stocfor3 15362 22228 23 63 19 28e 04 5514e 05 399767839e04 stocfor3old 15362 22228 23 70 19 28e 04 5514e 05 399767839e04 truss 1000 8806 3621e 04 458815785e05 vtpbase table clearly indicates current implementation works much better previous code 14 fact almost examples able reduce number iterations considerably nally state results function giving another complete list however illustrate typical behaviour method presenting corresponding results test examples lie kb2 scagr7 list includes dicult pilot problems table 2 table 2 numerical results quadratic function problem objective kb2 43 68 15 9 20e 2458e lot 133 346 22 9 30e 6715e 04 252647449e01 maros 655 1437 20 11 30e 3805e 04 580637438e04 9450e 04 149718510e06 modszk1 665 1599 26 11 25e 3087e nesm 654 2922 perold 593 1374 55 11 57e 05 2585e 04 938075528e03 pilot 1368 4543 53 7 14e 04 2953e 04 557310815e02 pilotwe 701 2814 43 4 98e 04 9283e 04 272010754e06 5672e 04 258113925e03 3573e 04 449727619e03 recipe 1928e 1193e 04 522020686e01 sc50a 3224e sc50b 48 76 11 9 41e 4955e 9326e concluding remarks presented class smoothingtype methods solution linear programs class methods similar convergence properties one burke xu 3 example allows exible choice updating smoothing parameter numerical results presented implementation smoothingtype method encouraging particular signicantly better previous implementa tions results also indicate precise updating smoothing parameter plays important role overall behaviour methods however subject certainly needs investigated r introduction linear programming global local superlinear continuationsmoothing method p 0 r 0 ncp monotone ncp global linear local quadratic noninterior continuation method nonlinear complementarity problems based chenmangasarian smoothing functions class smoothing functions nonlinear mixed complementarity problems global superlinear convergence smoothing newton method application general box constrained variational inequalities pcx interiorpoint code linear programming pcx user guide linear programming extensions solution linear programs jacobian smoothing methods improved smoothingtype methods solution linear programs special newtontype optimization method complexity analysis smoothing method using chksfunctions monotone linear complementarity problems global convergence class noninterior point algorithms using chenharkerkanzowsmale functions nonlinear complementarity problems noninterior continuation methods linear complementarity prob lems block sparse cholesky algorithm advanced uniprocessor computers new look smoothing newton methods nonlinear complementarity problems box constrained variational inequalities algorithms solving equations analysis noninterior continuation method based chenmangasarian smoothing functions complementarity problems bounds superlinear convergence analysis newtontype methods optimization tr block sparse cholesky algorithms advanced uniprocessor computers noninteriorpoint continuation method linear complementarity problems class smoothing functions nonlinear mixed complementarity problems noninterior continuation methods linearcomplementarity problems primaldual interiorpoint methods global superlinear convergence smoothing newton method application general box constrained variational inequalities global linear local quadratic noninterior continuation method nonlinear complementarity problems based chenmangasarian smoothing functions global local superlinear continuationsmoothing method ipisubfont size10subfont irisubfont size10subfont ncp monotone ncp complexity analysis smoothing method using chksfunctions monotone linear complementarity problems complexity bound predictorcorrector smoothing method using chksfunctions monotone lcp