time optimal supernode shape abstractwith objective minimizing total execution time parallel program distributed memory parallel computer paper discusses selection optimal supernode shape supernode transformation also known tiling identify three parameters supernode transformation supernode size relative side lengths cutting hyperplane directions supernode transformations algorithms perfectly nested loops uniform dependencies prove optimality constant linear schedule vector give necessary sufficient condition optimal relative side lengths also prove total running time minimized cutting hyperplane direction matrix particular subset valid directions discuss cases subset unique results derived continuous space considered approximate model include cache effects assumes unbounded number available processors communication cost approximated constant uniform dependences loop bounds known compile time comprehensive example discussed application results jacobi algorithm b introduction supernode partitioning transformation technique groups number iterations nested loop order reduce communication startup cost paper addresses problem selecting optimal cutting hyperplane directions optimal supernode relative side lengths objective minimizing total running time sum communication time computation time assuming large number available processors execute multiple supernodes problem distributed memory parallel systems communication startup cost time takes message reach transmission media mo author supported att labs research supported part national science foundation grant ccr9502889 clare boothe luce professorship henry luce foundation ment initiation communication startup cost usually orders magnitude greater time transmit message across transmission media compute data message supernode transformation proposed 14 studied 1 2 3 4 15 18 20 25 26 27 others reduce communication startup cost informally supernode transformation several iterations loop grouped one supernode supernode assigned processor unit ex ecution data iterations su pernode need sent another processor grouped single message number communication startups reduced number iterations supernode one supernode transformation characterized supernode size relative lengths sides supernode directions hyperplanes slice iteration index space given algorithm supernodes three factors affect total running time larger supernode size reduces communication startup cost may delay computation processors waiting message therefore result longer total running time also square supernode may good rectangular supernode supernode size paper selection optimal cutting hyperplane directions optimal relative side lengths addressed rest paper organized follows section presents necessary definitions assumptions terminology section 3 discusses results detail section 4 briefly describes related work contribution work compared previous work section 5 concludes paper bibliography related work included end basic definitions models assumptions architecture consideration parallel computer distributed memory processor access local memory capable communicating processors passing mes sages model cost sending message represented message startup time computation speed single processor characterized time takes compute single iteration nested loop parameter denoted c algorithms consideration consist single nested loop uniform dependencies 22 algorithms described pair j j iteration index space n theta dependence matrix column dependence matrix represents dependence vector cone generated dependence vectors called dependence cone cone generated vectors orthogonal facets dependence cone called tiling cone assume n matrix full rank equal number loop nests n elements main diagonal smith normal form equal one discussed 23 assumptions satisfied iteration index space j contains independent components partitioned several independent subalgorithms assumptions satisfied supernode transformation iteration space sliced n independent families parallel equidistant hyperplanes hyperplanes partition iteration index space ndimensional parallelepiped supernodes tiles hyperplanes one family specified normal vector orthogonal hy perplanes square matrix consisting n normal vectors rows denoted h h full rank n hyperplanes assumed inde pendent parallelepiped supernodes also described n linearly independent vectors supernode sides described 20 column vectors matrix n side vec tors supernode template defined one full supernodes translated origin 0 ie 1g supernode index space j obtained supernode transformation h supernode dependence matrix 1 resulting supernode transformation h consists elements set use denote either matrix set consisting column vectors matrix whether matrix set clear context discussed 20 26 2 partitioning hyperplanes defined matrix h satisfy hd 0 ie entry product matrix greater equal zero order j computable implies cone formed column vectors e contain dependence vectors fore components vectors defined nonnegative numbers analysis throughout paper assume dependence vectors original algorithm properly contained supernode template consequently components 0 1 reasonable assumption real world problems 5 26 present analysis following additional notations introduced column vector l called supernode side length vector let l n theta n diagonal matrix vector l diagonal e u matrix unit determinant column vectors directions corresponding column vectors matrix e components vector l supernode side lengths units corresponding columns e u define cutting hyperplane direction matrix supernode size supernode volume denoted g defined number iterations one su pernode supernode volume g matrix extreme vectors e supernode side length vector l related relative supernode side length vector sg theta l clearly lengths supernodes relative supernode size example h identity matrix supernode square ever h u n supernode rectangle size square supernode ratio two sides 21 also use r denote diagonal n theta n matrix vector r diagonal ge u r transformation completely specified h u r g therefore denoted h g advantage factoring matrix h way allows us study three supernode transformation parameters separately implication 2 corollary 1 26 algorithm j linear schedule 22 defined oe j n oe linear schedule vector row vector n rational components minfd jg linear schedule assigns node j 2 j execution step dependence relations respected approximate length linear schedule note j 1 j 2 oe j 1 always extreme points iteration index space execution algorithm j follows apply supernode transformation h obtain j time optimal linear schedule found j execution based linear schedule alternates computation communication phases step assign oe available processors processor finishes computations supernode processors communicate passing messages order exchange partial results communication done go step 1 hence total running time algorithm depends following j h u g r c total running time sum total computation time total communication time multiples number phases execu tion linear schedule length corresponds number communication phases execution approximate number computation phases number communication phases linear schedule length 2 total running time sum computation time comp communication time tcomm one phase multiplied number phases p computation time number iterations one supernode multiplied time takes compute one iteration cost communicating data computed one supernode dependent supernodes denoted tcomm c number processors data needs sent ct model communication greatly simplifies analysis acceptable message transmission time overlapped operations computations communication startup next message communication startup time dominates communication operation thus total runningv1v2 denotes vector dot product vectors v1 v2 time ct 3 optimal supernode shape section present results pertaining time optimal supernode shape ie supernode relative side length matrix r cutting hyperplane direction matrix h u derived model assumptions set previous section model constant communication cost linear schedule vector linear schedule length expression 3 depend supernode shape therefore order minimize total running time need choose supernode shape minimizes linear schedule length transformed algorithm problem nonlinear programming problem diagonal matrix deth h scalar n 1g constant computed independent h u q without loss generality exclude objective func tion studied selection supernode size 9 floor operator 1 droped objective function simplify model shown error linear schedule length bounded insignificant components close 1 large iteration index spaces theorem 1 gives closed form optimal linear schedule vector transformed algorithm theorem 1 optimal supernode transformation optimal linear schedule proof defined section 2 min since order feasible linear schedule ie 0 must 1 extreme projection vectors nonnegative components linear schedule length minimized linear schedule vector smallest components ie extreme projection vectors negative components initial optimal linear schedule vector may different 1 still must order satisfy definition linear schedule vector ie min 1 let ith component greater 1 set 0 modify supernode shape setting diagonal matrix linear schedule vectors transformed algorithm remains ie linear schedule vector divide 0 min 0 shorten linear schedule points therefore got shorter linear schedule algorithm got one component linear schedule vector equal 1 continuing process eventually get linear schedule ones 2 theorem 2 gives necessary sufficient condition optimal relative side length matrix r consequently inverse matrix q assuming optimal linear schedule vector 1 theorem 2 let g h u fixed let linear schedule vector 1 let set maximal projection vectors transformed space relative side lengths vector r optimal vector equal components v belongs cone generated maximal projection vectors transformed algorithm proof let linear schedule length without loss generality let v vector equal components sufficient condition let vector v included conem let corresponding relative supernode side length matrix r consider another supernode transformation close original slightly different r suppose image v transformation r 0 schedule length v 0 1v 0 based relation geometric arithmetic new maximal projection vectors linear schedule length greater equal 1v 0 greater therefore supernode relative side length matrix r optimal necessary condition prove contradiction let r optimal assume v conem exists separating hyperplane z x 2 z za 0 2 1 select normal vector z arbitrarily close orthogonal vector 1 arbitrary length order ensure 4 convenience abuse notation write p v mean sv choice clear context0000000000000000000000000000000000000000000000001111111111111111111111111111111111111111111111111111 z z conem figure 1 construction vector illustration proof theorem 2 1 former ensured selecting sufficiently small length vector z latter ensured selecting appropriate angle z 1 sinks curve based relation arithmetic geometric mean must s1 latter case construction z thus construction feasible figure 1 illustrates construction vector two dimensional space scaling supernode index space diags ie choosing improve linear schedule length vectors 2 choosing vector z arbitrarily close orthogonal vector 1 ensure extreme projection vector becomes new maximal projection vector improving r r 0 contradict hypothesis r optimal 2 theorem 2 also implies relation q h u enables analysis h u independent q stated following corollary corollary 1 given h u vector u convex hull original iteration index space maps vector equal components maximal linear schedule length convex hull supernode index space optimal q components optimal selection q objective function 4 1qh u u 6 reduces expression proof relation 5 readily derived const1 expression 7 follows substituting expression 5 q 6 2 special case single maximal projection vector optimal q easily computed based 5 example iteration index space hyperrectangle h optimal q one turns supernode index space hypercube makes supernode similar original iteration index space based 7 objective function optimal h u u vector convex hull original iteration index space defined corollary 1 following shows positively combining rows matrix ie taking matrix rows inside cone generated rows original trix obtain better cutting hyperplane direction matrix u1 cutting hyperplane direction matrix let u square matrix u 0 matrix h u 2 give cutting hyperplane matrix shorter schedule length proof enough show f h u 1 vector u 8 vector defined corollary 1 corresponding h u 1 let w square nonnegative matrix unit determinant h u1 u let 1q 1 h u 1 used inequality sum nonnegative numbers root sum squares hadamard inequality deductive sequence abovebased lemma 1 state following regarding optimality choice h u general theorem 3 optimal hyperplane direction matrix h u assumes row vectors surface tiling cone proof row vectors h u interior tiling cone another hyperplane direction matrix h 0 u row vectors surface h ie cone generated rows h u included cone generated h 0 takes row vectors surface tiling cone based lemma u better choice h u 2 case algorithm exactly n extreme dependence vectors state stronger result provided following theorem theorem 4 optimal hyperplane direction matrix h u algorithm n extreme dependence directions uniquely defined uniform rescaling n extreme directions tiling cone proof dependence cone exactly n extreme directions implies exactly n extreme directions corresponding tiling cone row hyperplane direction matrix nonnegative linear combination n extreme directions tiling cone sure hyperplane direction matrix extreme directions tiling cone rows best choice based lemma 1 2 equivalent statement two dimensional algorithms gives even stronger result theorem 5 optimal extreme direction matrix e u two dimensional algorithms column vectors directions two extreme dependence vectors proof two dimensional algorithms always exactly two extreme dependence vectors since theorem 4 applies always 2 following example confirms optimal hyperplane directions matrix h u take row vectors surface tiling cone general case set extreme directions tiling cone example 1 example apply supernode transformation jacobi algorithm 19 select optimal q discuss selection h u assuming selection g discussed 9 core jacobi algorithm constant number iterations written follows iteration index space three dimensional rectan gle eight extreme points iteration index space shown column vectors matrix x 500 500 500 500c four dependencies caused access elements array code represented matrix column vectors tiling cone generating vectors ie extreme vectors tiling cone example include following four row vectors construct four different hyperplane direction matrices h ui 4 four row vectors matrix b however none four matrices constructed extreme directions tiling cone necessarily optimal see rest example set extreme points x construct set extreme projection vectors 28 vector differences different pairs column vectors x applying iterative nonlinear optimization procedure obtain optimal q h ui applying supernode transformations obtained way set extreme projection vectors finding maximum get linear schedule length 6062445 transfor mation thus h ui equally good due regularity iteration index space dependence vectors let us consider another hyperplane direction matrix 05 gamma05 gamma05 constructed two row vectors b sum two row vectors b properly normalized corresponding optimal supernode relative side lengths given corresponding tiling matrix 0125 gamma0125 0125 0125 gamma0125 gamma0125 matrix 5 gives shape supernode applying h 5 set extreme index points get extreme index points supernode index space 625 gamma3125 gamma1875 similarly get transformed extreme projection vectors dont show 28 show maximal projection vectors nine nine maximal projection vectors sink single two dimensional plane vector equal components linear schedule length belongs cone formed maximal projection vectors sinks plane ensuring selected optimal relative side lengths based theorem 2 easily verified computing normal vectors faces cone generated maximal projection vectors showing vector vs projection onto normals indicates vector v inside cone 4 related work irigoin triolet 14 proposed supernode partitioning technique multiprocessors 1988 new restructuring method ramanujam sadayappan 20 studied tiling multidimensional iteration spaces multiprocessors showed equivalence problem finding partitioning hyperplane matrix h problem finding containing cone given set dependence vectors 4 choice supernode shape discussed goal minimizing new objective function key feature new objective function scal ability defined way independent supernode size 24 authors discussed problem finding optimum wavefront optimal linear schedule vector terminol ogy minimizes total execution time two dimensional data arrays executed one two dimensional processor arrays 18 optimal tile size studied different model assumptions 26 extended definition supernode transformation given extension definition originally given 14 27 choice matrix h criterion minimizing communication volume studied similar 4 optimization criterion include iteration index space thus model include linear schedule effects execution time 3 choice optimal tile size minimizes total running time studied authors approach two steps first formulate abstract optimization problem include architectural program characteristics partially solve optimization problem second step include architectural program details model solve problem optimal tile size yielding closed form solution recently international seminar 6 held topic tiling twenty five lectures pre sented lectures covered many issues related tiling transformation selection optimal supernode size studied previous work within similar model 9 studied choice cutting hyperplane directions two dimensional algorithms 12 selection supernode shape case dependence cone extreme directions 13 results presented paper extenssion results 13 compared related work optimization criterion minimize total running time rather communication volume ratio communication computation volume addition use different approach specify supernode transformation supernode size relative side length vector r cutting hyperplane direction three variables become independent studied separately 5 conclusion build model total running time based algorithm characteristics architecture parameters parameters supernode transformation supernode transformation specified three independent parameters supernode size supernode relative side lengths cutting hyperplane direc tions independence parameters allows us study selection separately paper two three parameters studied give necessary sufficient condition optimal relative side lengths show optimal cutting hyperplane directions surface tiling cone show linear schedule optimal linear schedule transformed algorithm final supernode transformation violates assumption results hold results derived continuous space reason considered approximate r scanning polyhedra loops optimal orthogonal tiling optimal orthogonal tiling 2d iterations pen ultimate tiling practical dependence testing tiling optimal resource utiliza tion linear scheduling nearly optimal eval uating compiler optimizations fortran supernode transformations minimized total running time supernode partitioning hyperplanes two dimensional algorithms time optimal supernode shape algorithms n extreme dependence di rections supernode partitioning cache performance optimizations blocked al gorithms new jersey modeling optimal granularity adapting systolic algorithms transputer based supercomput ers op timal tile size adjustment compiling general doacross loop nests parallel computing theory practice tiling multidimensional iteration spaces multicomputers automatic blocking nested loops time optimal linear schedules algorithms uniform dependen cies independent partitioning algorithms uniform dependencies finding optimum wavefront parallel computation iteration space tiling tiling loop transformation communicationminimal tiling uniform dependence loops tr ctr georgios goumas nikolaos drosinos maria athanasaki nectarios koziris automatic parallel code generation tiled nested loops proceedings 2004 acm symposium applied computing march 1417 2004 nicosia cyprus sriram krishnamoorthy muthu baskaran uday bondhugula j ramanujam atanas rountev p sadayappan effective automatic parallelization stencil computations acm sigplan notices v42 n6 june 2007 georgios goumas nikolaos drosinos maria athanasaki nectarios koziris messagepassing code generation nonrectangular tiling transformations parallel computing v32 n10 p711732 november 2006 maria athanasaki aristidis sotiropoulos georgios tsoukalas nectarios koziris panayiotis tsanakas hyperplane grouping pipelined schedules execute tiled loops fast clusters smps journal supercomputing v33 n3 p197226 september 2005 saeed parsa shahriar lotfi new genetic algorithm loop tiling journal supercomputing v37 n3 p249269 september 2006