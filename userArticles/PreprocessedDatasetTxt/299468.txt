axiomatic approach feature subset selection based relevance abstractrelevance traditionally linked feature subset selection formalization link attempted paper propose two axioms feature subset selectionsufficiency axiom necessity axiombased link formalized expected feature subset one maximizes relevance finding expected feature subset turns nphard devise heuristic algorithm find expected subset polynomial time complexity experimental results show algorithm finds good enough subset features presented c45 results better prediction accuracy b introduction problem feature subset selection fss hereafter long active research topic within statistics pattern recognition eg 9 work area dealt linear regression past years researchers machine learning realised see example 18 16 practical algorithms supervised machine learning degrade performance prediction accuracy faced many features necessary predicting desired output therefore fss since received considerable attention machine learning researchers interested improving performance algorithms common machine learning algorithms including topdown induction decision trees cart id3 c45 nearestneighbour algorithms instancebased learning known suffer irrelevant features 18 19 good choice features may help improve performance accuracy also aid finding smaller models data resulting better understanding interpretation data broadly speaking fss select subset features feature space good enough regarding ability describe training dataset predict future cases wealth algorithms fss see example 2 15 1 17 14 24 regard evaluate goodness subset features fss methods fall two broad categories filter approach wrapper approach illustrated figures 1 2 filter approach good feature set selected result preprocessing based properties data feature set feature selection algorithm learning algorithm figure 1 filter model feature set feature subset evaluation feature subset search learning algorithm learning algorithm figure 2 wrapper model independent induction algorithm section 51 presents review empirical use notion goodness category special type approach feature weighting 15 slightly different mainstream filter approach way search good feature set conducted basically mainstream approach evaluates subset features finds optimal weighting approach weighs individual feature selects quasioptimal set features typically whose weights exceed given threshold 15 17 wrapper approach feature selection done help induction algorithms feature selection algorithm conducts search good feature set using induction algorithm part evaluation function typically feature subset performs best induction algorithm selected types approach fss closely related notion relevance example focus 2 relief 15 schlimmers model 22 use relevance estimate goodness feature subset one way another section 52 presents review respect although wrapper approach use relevance measure directly shown 16 optimal feature subset obtained way must relevant feature set strongly relevant weakly relevant features however mathematical foundation fss still lacking 26 25 unified framework relevance proposed framework relevance quantified related mutual information furthermore shown quantification satisfies axiomatic characterisations relevance laid leading researchers area renders notion relevance solid mathematical foundation light attempt characterise fss terms relevance framework order give fss solid foundation theoretical study present algorithm fss based relevance characterisation also present experimental results applying algorithm real world datasets 2 characterisation feature subset selection section characterise fss realm machine learning confined following sense input supervised learning algorithm training set labelled instances target concept 1 typically assumed drawn independently identically distributed iid unknown distribution labelled instance space unlabelled instance x element n dimensional space ith feature variable feature space labelled instances tuples x label output let l learning algorithm hypothesis space h l maps h 2 h h maps unlabelled instance label task learning algorithm choose hypothesis best explains given data paper training set represented relation table rx set features output target variable follows use rx denote learning task training set problem feature selection search subset pi x performs well training dataset also predicts well unseen new cases good enough objective section characterise best feature subset first principles well known principles 21 preservation learning information given dataset rx learning task characterise relationship x relationship used predict future cases either one dataset new case therefore selected feature subset expected work well given dataset preserve existing relationship x hidden dataset natural measure relationship mutual information 7 call relationship learning information specifically given learning task rx learning information mutual information suppose sigma pi two subsets x sigma say sigma pi contribution learning task sufficient feature set simply sfs learning task subset sigma x sigma contribute learning task restated following axiom axiom 21 preservation learning information given learning task rx best feature subset pi preserve learning information contained training dataset following two lemmas follow directly chain rule mutual information nonnegativity mutual information lemma 21 given rx pi x lemma additivity mutual information 7 know given sfs pi removing remaining features sigma lose learning information contained original dataset words conditionally independent sigma given pi 1 target target concept usually defined subset instance space 2 interpreted bipartition instance space use general sense target concept arbitrary partition instance space regarded variable 2 paper use x refer variable domain variable identified context 3 6 12 use notation 12 relation scheme r set variables features relation table r indicator function set tuples written tuple relation otherwise purpose paper extend indicator function frequency tuple appearing relation extension talk distribution tuples easily obtained lemma 22 pi sfs learning task rx superset sigma pi also sfs lemma helps determining sfss without calculate learning information property exploited design fss algorithm later 22 simplest description occams razor given learning task may number sfss however may perform prediction best feature subset perform best respect however easy determine subset features predicts better since full knowledge future although dataset assumed drawn iid labelled instance space according unknown distribution assumption doesnt help individual cases focus training dataset apply empirical principles number empirical principles occams razor one occams razor known principle parsimony tool application many areas science incorporated methodology experimental science principle becoming influential machine learning principle formulated given two hypotheses consistent training set examples given task simpler one guess better future examples task 4 27 3 shown see example 4 general assumptions occams razor produces hypotheses high probability predictive future cases one basic question concerned meaning simplicity namely occam simplicity typically occam simplicity associated difficulty implementing given task namely complexity implementation example number hidden neurons neural networks 3 number leaf nodes decision tree 10 11 minimum description length mdl 21 20 encoding length 23 however wolpert 27 noticed complexity implementation directly related issue prediction generalisation therefore direct reason believe minimisation complexity measure result improvement general isation wolpert 27 derived uniform simplicity measure concerned exclusively learning generalises wolpert showed 27 expressed terms uniform simplicity measure occams razor indeed way set good generaliser main disadvantage uniform simplicity measure calculation needs learning sets questions well guessing distribution simplicity distribution 27 impossible practice seems uniform simplicity measures theoretical significance fortunately many conventional simplicity measures shown rough approximations uniform simplicity measure 27 practice rely approxima tions like mentioned back problem practical simplicity measures approximations uniform simplicity measure modeldependent however looking fss independently learning model modelindependent simplicity measure required entropy seems ideal candidate measures average number bits encoding length describe source eg random variable using entropy occam simplicity measure context given learning task rx occams razor dictates selection sfs pi minimises h pi h shannons entropy function make formal restate conjunction information preservation axiom following axiom axiom 22 minimum encoding length given learning task rx set sufficient feature subsets one pi minimises joint favoured respect predictive ability set characterise pi minimises joint entropy lemma 23 given learning task rx consider two sfss pi sigma x h proof since pi sigma sfss definition pi therefore hy furthermore according lemma favourable feature subset would sufficient one least marginal entropy 23 characterisation feature subset selection terms relevance previous two sections derived two axiomatic characterisations fss preservation learning information minimum encoding length section going show two axioms restated terms relevance even concise form given two variables x definition see appendix relevance x therefore sfs pi x ie pi preserving learning information amounts preserving relevance rela general due fact pi pi preserves learning information fact maximises relevance rx consider two sfss pi sigma since definition pi therefore conjunction previous requirement favourable feature subset would sufficient one maximises relevance ry x summarising discussion following theorem theorem 21 given learning task rx favourable feature subset pi sufficient preserving learning information pi minimises joint entropy putting concisely one maximum rpi maximum ry pi theorem formalises less intuitively justified connection relevance fss 3 relevancebased algorithm feature selection section present heuristic fss algorithm based characterisation previous section straightforward algorithm systematically examine feature subsets find one satisfies two axioms unfortunately shown 8 class algorithms turns npcomplete branch bound based characteristics relevance attempted 25 shown also exponential general attempted heuristic approaches present preferred heuristic fss algorithm objective find sufficient subset features close optimal axiomatic sense heuristic used feature attribute highly relevant likely feature optimal feature set since features examined individually need take account correlation among individual features consider example two features target suppose rx 1 x 1 selected x 2 needed since rx according lemma 61 words x 2 becomes irrelevant given x 1 algorithm select end design algorithm takes advantage conditional relevance algorithm 31 cr feature selection based conditional relevance given learning ffl calculate every x 2 x relevance rx find feature x 0 largest relevance ffl main procedure 1 2 repeat add x bsfs x bsfs rx largest among possible relevance values 3 rbsfs ffl return bsfs clearly time complexity calculating relevance finding largest analyse complexity main procedure loop k k features left inspection need compute conditional relevance rx features hence complexity ok find feature largest conditional relevance value need comparisons hence complexity ok gamma 1 worst case need loop hence complexity p 1 therefore overall complexity algorithm 2 algorithm highly dependent choice initial set features individual feature relevant bsfs selected cr guaranteed sfs guaranteed necessary conjectured x 0 optimal sfs bsfs found cr indeed optimal 4 experiment evaluation evaluate performance feature selection algorithm presented previous section using real world datasets choose three datasets u c irvine machine learning repository australian diabetes heart general information datasets shown table 1 evaluate performance feature selection algorithm chose use c45 module clementine package experiment feed selected feature subsets c45 compare results without feature subset selections test accuracies c45 without feature selection shown table 2 evaluation method used cross validation implemented clementine experiment results see applying feature selection algorithm indeed improve test accuracies three datasets corresponding decision trees smaller sizes however success limited sense accuracy improvements great case reason probably c45 builtin feature selection facility based mutual information reasonable believe feature selection algorithm described used learning algorithms without builtin feature selection facilities eg nearest neighbour accuracy improvement could higher reported dataset features examples classes class distribution australian 14 690 2 445 diabetes 8 768 2 651 australian c c c c c c diabetes c c c c c c c c heart c c c c c c c table 1 general information datasets refers discrete categorical c refers continuous size trees test accuracy selected features size trees test accuracy australian diabetes 54 729 25678 42 742 heart table 2 decision tree sizes test accuracies decision trees generated c45 without feature selection together selected feature sets evaluation method used cross validation implemented clementine datasets u c irvine machine learning repository australian credit diabetes heart also carry experiment inspect change accuracies gradually adding features order relevance values first rank features according individual relevance values start evaluation one highest relevance value results shown figure 3 figure see features gradually added order accuracy average go first reach peak go diagram justifies extent algorithm although algorithm may always find feature subsets corresponding exactly peak points another observation experiment performance c45 three datasets descending order australian heart diabetes table 2 terms average test accuracy percentage continuous features descending order diabetes 88 heart 713 australian 614 indicates c45 doesnt work well continuous features discrete features feature selection algorithm didnt change situation c45 continuous features treated discrete features way values divided two groups discrete cluster used classification granularity point view 13 granularity continuous features made simply coarse feature selection algorithm continuous features treated discrete features way continuous value taken discrete value used individually classification granularity seems fine points direction future studies proper granularity continuous feature use classification australian diabetes heart figure 3 accuracy vs first k features used relevance ranking k starts 1 relevancebased rankings three datasets follows australian diabetes 76258413 heart 58101331214911276 5 comparison related work section take closer look related work relevance point view compare 51 best feature subset characterised literature 15 best feature subset characterised sufficient necessary describe target ideally sufficiency necessity requirement quantified measure j pi evaluates feature subset pi target concept given data best feature subset best value j pi however nature sufficiency necessity requirement made clear 15 context learning examples seems reasonable sufficiency concerns ability feature subset describe given dataset called qualified later necessity concerns optimality among qualified feature subsets regarding predictive ability say two axiomatic characterisations possible interpretations sufficiency necessity requirement proposed 15 practice best feature subsets measured pragmatic ways example focus 2 good feature subset minimal subset consistent training dataset consistency understood sufficiency requirement since feature subset consistent given dataset qualify describe dataset without losing learning information minimality feature subset understood necessity requirement used bias learning regarding subset predict better future cases relief 15 good subset one whose elements relevance level greater given threshold relevancy threshold together determine whether given feature subset sufficient qualified describe given dataset direct justification feature subset determined way would perform better predicting future cases ie necessary 22 good subset one minimal determinations nothing mentioned one best minimal determinations sufficient necessary left open 52 remodelling using relevance framework many fss algorithms use relevance estimate feature usefulness one way another focus 2 algorithm starts empty feature set carries breadthfirst search finds minimal combination pi features consistent training dataset features pi relevant target concept c terms relevance framework 25 requirement amounts rpi minimum relief feature relevance estimation algorithm meaning relevance different theoretically justified associates feature weight indicating relative relevance feature concept class c returns set features whose weights exceed threshold amounts firstly calculate feature x rx c select set features x set rx c threshold compared focus method computationally efficient furthermore allows features ranked relevance schlimmer 22 described related approach carries systematic search space feature sets one minimal cardinality minimal determinations consistent training dataset algorithm attractive polynomial complexity due spacefortime technique caching search path avoid revisiting states determination fact sfs minimal determination sfs removing element render sfs anymore therefore algorithm amounts finding sfss within given length pi rpi recent research feature selection differs early methods relying wrapper strategies rather filtering schemes general argument wrapper approaches induction method use feature subset provide better estimate accuracy separate measure may entirely different inductive bias john kohavi pfleger 14 first present wrapper idea general framework feature selection generic wrapper technique must still use measure select among alternative features one natural scheme involves running induction algorithm entire training data using given set features measuring accuracy learned structure training data however john et al argue crossvalidation method use implementation provides better measure expected accuracy novel test cases major disadvantage wrapper methods filter methods formers computational cost results calling induction algorithm feature set considered cost led researchers invent ingenious techniques speeding evaluation process wrapper scheme 16 use relevance measure directly rather uses accuracy obtained applying induction algorithm measure goodness feature sets however kohavi sommerfield show optimal feature set x obtained way must relevant feature set strongly relevant weakly relevant features shown 25 strong relevance weak relevance characterised relevance formalism wrapper scheme also modelled relevance rx however caruana freitag 5 observe features relevant necessarily useful induction tested focus relief calendar scheduling problem fed feature sets obtained two algorithms id3c45 found direct feature selection procedure hillclimbing feature space finds superior feature sets didnt explain reason possible explanation based relevance follows given concept class c many sfss sfs x 1 one many sfss satisfies criteria optimal general optimal feature set may minimal one general starting occams razor argue optimal one rc x maximised conclusion discussion relief schlimmers algorithm wrapper take account sufficiency condition evidenced addressing rx c focus takes account sufficiency necessity conditions necessity measured cardinality feature subset minimal relationship measurement occams razor characterisation clear yet 6 conclusion paper derived first principles occams razor principle two axiomatic requirements feature subset qualify good preservation learning information minimum encoding length since fss traditionally linked relevance showed identified variable relevance unified framework relevance relevance direct relationship fss maximising relevance ways ie result favourable feature subset based axiomatic characterisation fss one heuristic fss algorithm designed presented algorithm weights ranks features using conditional relevance rx jz stepwise way starts feature highest unconditional relevance value keeps selecting features highest conditional relevance values respect current selected subset algorithm get rid highly correlated features shown complexity 2 also presented evaluation results using three real world problems australian credit diabetes diagnosis heart diagnosis uci machine learning repository purpose evaluation two fold firstly evaluated performance algorithm results quite encouraging average test accuracies three datasets improved resultant decision trees smaller tree sizes since c45 builtin feature selection process based gain ratio defined mutual information conjecture algorithm used learning algorithms without builtin feature selection process eg nearest neighbour accuracy improvement could higher secondly evaluated relationship relevance learning accuracy results show strong connection relevance learning accuracy features ranked according conditional relevance values adding features one one feature set would lead clear pattern accuracy first ascending peak descending gradually therefore conclude highly relevant features improve learning accuracies highly irrelevant features degrade learning accuracies aside observed c45 based learning accuracy whether feature selection used related proportion continuous features higher proportion continuous features lower accuracy argued one possible reason c45 continuous features bipartitioned could coarse future studies direction focus developing algorithms find proper granularities continuous features r feature selection casebased classification cloud types learning many irrelevant features size network good generalization specific task interest occams razor useful relevance relational model data large shared data banks elements information theory pattern recognition statistical approach minimized decision tree attribute selection problem decision tree generation relational databases tutorial statisticians irrelevant features subset selection problem feature selection problem traditional methods new algorithm feature subset selection using wrapper method overfitting dynamic search space topology estimating attributes analysis extensions relief selection relevant features machine learning machine learning inferring decision trees using minimum description length principle stochastic complexity modeling efficiently inducing determinations complete systematic search algorithm uses optimal pruning occam algorithms computing visual motion prototype feature selection sampling random mutation hillclimbing algorithms towards unified framework relevance computer systems learn classification predication methods statistics relationship occams razor convergent guessing tr ctr xianghong zhou gareth chelvanayagam michael hallett identifying significant pairwise correlations residues different positions helices subset selection problem using least squares optimization proceedings 2001 acm symposium applied computing p5155 march 2001 las vegas nevada united states jongmin park convergence application online active sampling using orthogonal pillar vectors ieee transactions pattern analysis machine intelligence v26 n9 p11971207 september 2004 arno j knobbe eric k ho maximally informative kitemsets efficient discovery proceedings 12th acm sigkdd international conference knowledge discovery data mining august 2023 2006 philadelphia pa usa