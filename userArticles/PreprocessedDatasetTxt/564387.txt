twostage language models information retrieval optimal settings retrieval parameters often depend document collection query usually found empirical tuning paper propose family twostage language models information retrieval explicitly captures different influences query document collection optimal settings retrieval parameters special case present twostage smoothing method allows us estimate smoothing parameters completely automatically first stage document language model smoothed using dirichlet prior collection language model reference model second stage smoothed document language model interpolated query background language model propose leaveoneout method estimating dirichlet parameter first stage use document mixture models estimating interpolation parameter second stage evaluation five different databases four types queries indicates twostage smoothing method proposed parameter estimation methods consistently gives retrieval performance close toor better thanthe best results achieved using single smoothing method exhaustive parameter search test data b introduction wellknown optimal settings retrieval parameters generally depend document collection query permission make digital hard copies part work personal classroom use granted without fee provided copies made distributed profit commercial advantage copies bear notice full citation first page copy otherwise republish post servers redistribute lists requires prior specific permission andor fee sigir02 august 1115 2002 tampere finland example specialized term weighting short queries studied 3 salton buckley studied many different term weighting methods used vectorspace retrieval model recommended methods strongly depend type query characteristics document collection 13 great challenge find optimal settings retrieval parameters automatically adaptively accordingly characteristics collection queries empirical parameter tuning seems inevitable order achieve good retrieval performance evident large number parametertuning experiments reported virtually every paper published trec proceedings 15 need empirical parameter tuning due part fact existing retrieval models based certain pre assumed representation queries documents rather direct modeling queries documents result adaptability model restricted particular representation assumed reserving free parameters tuning becomes way accommodate difference among queries documents captured well representation order able set parameters automatically necessary model queries documents directly goal explored recently language modeling approach information retrieval attracted significant attention since first proposed 9 first uses language modeling approach focused empirical effectiveness using simple models 9 7 2 1 recent work begun develop sophisticated models systematic framework new family retrieval methods 4 risk minimization retrieval framework proposed incorporates language modeling natural components unifies several existing retrieval models framework based bayesian decision theory one important advantage risk minimization retrieval framework traditional models capability modeling queries documents directly statistical language models provides basis exploiting statistical estimation methods set retrieval parameters automatically several special language models explored 6 4 16 uses language modeling ir smoothing plays crucial role empirical study 17 reveals retrieval performance generally sensitive setting smoothing parameters also sensitivity depends type queries input system paper propose family language models information retrieval refer twostage models first stage involves estimation document language model independent query second stage involves computation likelihood query according query language model based estimated document language model thus twostage strategy explicitly captures different influences query document collection optimal settings retrieval parameters derive twostage models within general risk minimization retrieval framework present special case leads twostage smoothing method first stage smoothing document language model smoothed using dirichlet prior collection language model reference model second stage smoothed document language model interpolated query background language model propose leaveoneout method estimating firststage dirichlet parameter make use mixture model estimating secondstage interpolation parameter evaluation five different databases four types queries indicates twostage smoothing method proposed parameter estimation method fully automaticconsistently gives retrieval performance close better result using single smoothing method exhaustive parameter search test data proposed twostage smoothing method represents step toward goal setting databasespecific queryspecific retrieval parameters fully automatically without need tedious experimentation effectiveness robustness approach along fact ad hoc parameter tuning volved make useful solid baseline method evaluation retrieval models rest paper organized follows first derive twostage language models section 2 present twostage smoothing method special case section 3 describe section 4 methods estimating two parameters involved twostage smoothing method report experimental results section 5 section 6 presents conclusions suggestions future work 2 twostage language models 21 risk minimization framework risk minimization retrieval framework general probabilistic retrieval framework based bayesian decision theory 4 framework queries documents modeled using statistical language models user preferences modeled loss functions retrieval cast risk minimization problem framework unifies several existing retrieval models within one general probabilistic framework facilitates development new principled approaches text retrieval traditional retrieval models vectorspace model 12 bm25 retrieval model 11 retrieval parameters almost always introduced heuristically lack direct modeling queries documents makes hard models incorporate principled way parameters adequately address special characteristics queries documents exam ple vectorspace model assumes query document represented term vector however mapping query document vector somehow arbitrary thus model sees document vector rep resentation principled way model length doc ument result heuristic parameters must used see eg pivot length normalization method 14 similarly bm25 retrieval formula direct modeling queries making necessary introduce heuristic parameters incorporate query term frequencies 11 one important advantage risk minimization retrieval framework 4 traditional models capability modeling queries documents directly statistical language modeling although query document similar sense text important differences example queries much shorter often contain keywords thus viewpoint language modeling query document require different language models practically separating query model document model important advantage able introduce different retrieval parameters queries documents appropriate general using statistical language models allows us introduce parameters probabilistic way also makes possible set parameters automatically statistical estimation methods 22 derivation twostage language models original language modeling approach proposed 9 involves twostep scoring procedure 1 estimate document language model document 2 compute query likelihood using estimated document language model directly twostage language modeling approach generalization twostep procedure query language model introduced query likelihood computed using query model based estimated document model instead using estimated document model directly use explicit separate query model makes possible factor influence queries smoothing parameters document language models derive family twostage language models information retrieval formally using risk minimization framework risk minimization framework presented 4 documents ranked based following risk function z z let us consider following special loss function indexed small constant c otherwise q r model distance function c constant positive cost thus loss zero query model document model close c otherwise using loss function obtain following risk z z sphere radius centered parameter space assuming pd concentrated estimated value approximate value integral integrands value note constant c ignored purpose ranking thus using b mean b effect ranking z pq q u dq q belong parameter space ie small value integral approximated value function times constant volume constant ignored purpose ranking q u therefore using risk actually ranking documents according p q u ie posterior probability user used estimated document model query model applying bayes formula rewrite u 1 equation 1 basic twostage language model retrieval mula similar model discussed 1 formula following u captures well estimated document model explains query whereas p u encodes prior belief user would use query model prior could exploited model different document sources document characteristics paper assume uniform prior generic twostage language model refined specifying concrete model pd generating documents concrete model pq q u generating queries different specifications lead different retrieval formulas query generation model simplest unigram language model scoring procedure original language modeling approach proposed 9 first estimate document language model compute query likelihood using estimated model next section present generative models lead twostage smoothing method suggested 17 3 twostage smoothing query denote words vocabulary consider case q parameters unigram language models ie multinomial distributions words v simplest generative model document unigram language model multinomial document would generated sampling words independently according p pd document assumed generated potentially different model assumed general risk minimization frame work given particular document want estimate use dirichlet prior parameters 1 2 v given parameters chosen parameter p collection language model estimated based set documents source posterior distribution given pd pw cwdpw s1 also dirichlet parameters using fact dirichlet mean j k k pw z pw pd sdd wv cw length dirichlet prior smoothing method described 17 consider query generation model simplest model unigram language model q result retrieval model dirichlet prior single smoothing method however observed 17 model able explain interactions smoothing type queries order capture common nondiscriminative words query assume query generated sampling words twocomponent mixture multinomials one component q query background language model p u pq q parameter roughly indicating amount noise q combining estimate query model following retrieval scoring formula document query q pq formula document language model effectively smoothed two steps first smoothed dirichlet prior second interpolated query background model thus refer twostage smoothing model empirically motivated observation smoothing plays two different roles query likelihood retrieval method one role improve maximum likelihood estimate document language model least assigning nonzero probabilities words observed document role explain away commonnon discriminative words query documents discriminated primarily based predictions topical words query twostage smoothing method explicitly decouples two roles first stage uses dirichlet prior smoothing method improve estimate document language model method normalizes documents different lengths appropriately prior sample size parameter performs well empirically 17 second stage intended bring query background language model explicitly accommodate generation common words queries query background model p u general different collection language model p insufficient data estimate p u however assume p would reasonable approximation p u form twostage smoothing method essentially combination dirichlet prior smoothing jelinekmercer smoothing 17 indeed easy verify dirichlet prior smoothing whereas mercer smoothing since combined smoothing formula still follows general smoothing scheme discussed 17 implemented efficiently next section present methods estimating data collection avg doc length max doc length vocab size table 1 estimated values along database characteristics 4 parameter estimation 41 estimating purpose dirichlet prior smoothing first stage address estimation bias due fact document extremely small amount data estimate unigram language model specifically discount maximum likelihood estimate appropriately assign nonzero probabilities words observed document usual role language model smoothing useful objective function estimating smoothing parameters leaveoneout likelihood sum loglikelihoods word observed data computed terms model constructed based data target word excluded left criterion essentially based crossvalidation used derive several wellknown smoothing methods including goodturing method 8 formally let collection docu ments using dirichlet smoothing formula leaveoneout loglikelihood written 1 thus estimate easily computed using newtons method update formula first second derivatives 1 given since g 0 long g 0 solution global max imum experiments starting value 10 algorithm always converges estimated values three databases shown table 1 clear correlation database characteristics shown table estimated value 42 estimating query model hidden query likelihood pq z order estimate approximate query model space set n estimated document language models col lection approximate integral sum possible document language models estimated collec tion pq smoothed unigram language model estimated based document using dirichlet prior approach thus assume query generated mixture n document models unknown mixing weights n leaving really want maximize likelihood generating query every document collection instead want find maximize likelihood query given relevant documents estimate would indeed allocate higher weights documents predict query well likelihood function presumably documents also likely relevant likelihood function parameters n estimated using em algorithm update formulas 5 experiments section first present experimental results confirm dualrole smoothing provides empirical justification using twostage smoothing method retrieval present results twostage smoothing method using estimated parameters comparing optimal performance using single smoothing methods exhaustive parameter search 51 influence query length verbosity smoothing 17 strong interactions smoothing type queries observed however unclear whether high sensitivity observed long queries due higher density common words queries length twostage smoothing method assumes former order clarify design experiments examine two query factorslength verbosity specifically consider four different types queries ie short keyword long keyword short ver bose long verbose queries compare behave respect smoothing show high sensitivity indeed caused presence common words query provides empirical justification twostage smoothing method generate four types queries trec topics 1150 150 topics special unlike trec topics concept field contains list keywords related topic keywords serve well long key word version queries figure 1 shows example topic topic 52 title south african sanctions description document discusses sanctions south africa narrative relevant document discuss aspect south african sanctions sanctions declaredproposed country south african government response apartheid policy response pressure individual organization another country international sanctions pretoria imposed united nations effects sanctions africa opposition sanctions compliance sanctions company document identify sanctions instituted considered eg corporate disinvestment trade ban academic boycott arms embargo concepts 1 sanctions international sanctions economic sanctions 2 corporate exodus corporate disinvestment stock divestiture ban new investment trade ban import ban south african diamonds un arms embargo curtailment defense contracts cutoff nonmilitary goods academic boycott reduction cultural ties 3 apartheid white domination racism 4 antiapartheid black majority rule 5 pretoria figure 1 example topic number 52 keywords used long keyword version queries use 150 topics generate four versions queries following way 1 short keyword using title topic description usually noun phrase 1 2 short verbose using description field usually one sentence 3 long keyword using concept field 28 keywords average 4 long verbose using title description narrative field 50 words average occasionally function words manually excluded order make queries purely keywordbased relevance judgments available 150 topics mostly documents trec disk 1 disk 2 order observe possible difference smoothing caused types docu ments partition documents disks 1 2 use three largest subsets documents accounting majority relevant documents queries three databases ap8889 wsj8792 ziff12 400mb500mb size queries without relevance judgments particular database ignored experiments database four queries judgments ap8889 49 queries judgments ziff12 preprocessing documents minimized porter stemmer used stop words removed combining four types queries three databases gives us total 12 different testing collections understand interaction different query factors smoothing examine sensitivity retrieval performance smoothing four different types queries jelinekmercer dirichlet smoothing 12 testing collections vary value smoothing parameter record retrieval performance parameter value results plotted figure 2 case show average precision varies according different values smoothing parameter figures see two types keyword queries behave similarly two types verbose queries retrieval performance generally much less sensitive smoothing case keyword queries verbose queries whether long short therefore sensitivity much correlated verbosity query length query indeed short verbose queries clearly sensitive long keyword queries cases insufficient smoothing much harmful verbose queries keyword queries confirms smoothing indeed responsible explaining common words query provides empirical justification twostage smoothing approach also see consistent order performance among four types queries expected long keyword queries best short verbose queries worst long verbose queries worse long keyword queries better short keyword queries better short verbose queries appears suggest queries presumably good keywords tend perform better verbose queries also longer queries generally better short queries 52 effectiveness twostage smoothing method evaluate twostage smoothing method first test 12 testing collections described earlier collections represent good diversity types queries databases homogeneous relatively small order test robustness twostage smoothing method test three much bigger heterogeneous trec collections official ad hoc retrieval collections used trec7 trec8 trec8 small web track official trec7 trec8 ad hoc tasks used document database ie trec disk4 disk5 excluding congressional record data different topics topics 351400 trec7 401450 trec8 trec8 web track trec8 official ad hoc task share 50 topics since topics concept field three types queries short shortverbose longverbose size large collections 2gb original source perform minimum preprocessing porter stemmer used precision lambda sensitivity precision jelinekmercer ap8889 shortkeyword longkeyword shortverbose longverbose010305 precision lambda sensitivity precision jelinekmercer wsj8792 shortkeyword longkeyword shortverbose longverbose010305 precision lambda sensitivity precision jelinekmercer zf12 shortkeyword longkeyword shortverbose longverbose010305 precision prior mu sensitivity precision dirichlet ap8889 shortkeyword longkeyword shortverbose longverbose010305 precision prior mu sensitivity precision dirichlet wsj8792 shortkeyword longkeyword shortverbose longverbose010305 precision prior mu sensitivity precision dirichlet zf12 shortkeyword longkeyword shortverbose longverbose figure 2 sensitivity precision jelinekmercer smoothing top dirichlet prior smoothing bottom ap8889 left wsj8792 center ziff12 right stop words removed testing collection compare retrieval performance estimated twostage smoothing parameters best results achievable using single smoothing method best results single smoothing method obtained exhaustive search parameter space ideal performance smoothing method experiments use collection language model approximate query background model results shown table 2 four types queries abbreviated two initial letters eg sk shortkeyword standard trec evaluation procedure ad hoc retrieval followed considered three performance measures noninterpolated average precision initial precision ie precision recall precision five documents results see performance twostage smoothing estimated parameter values consistently close better best performance single method three measures cases difference statistically significant indi cated star quantify sensitivity retrieval performance smoothing parameter single smoothing methods also show parentheses median average precision parameter values tried 2 see jelinekmercer sensitivity clearly higher verbose queries keyword queries median usually much lower best performance verbose queries means much harder tune jelinekmercer verbose queries keyword queries interestingly dirichlet prior median often slightly best even queries verbose worst cases significantly lower though sensitivity curves figure 2 see long set relatively large value 2 jelinekmercer tried 13 values 001 005 01 02 03 04 05 06070809095 099 dirichlet prior tried 10 values 100 500 800 1000 2000 3000 4000 5000 8000 10000 dirichlet prior performance much worse best performance great chance median large value immediately suggests expect perform reasonably well simply set safe large value however clear results table 2 simple approach would perform well parameter estimation methods indeed twostage performance always better median except three cases shortkeyword queries slightly worse since dirichlet prior smoothing dominates twostage smoothing effect shortkeyword queries due little noise somehow suggests leaveoneout method might underestimated note general jelinekmercer performed well dirichlet prior experiments two cases verbose queries trec8sv trec8lv trec78 database outperform dirichlet prior two cases twostage smoothing method performs either well better jelinekmercer thus twostage smoothing performance appears always track best performing single method optimal parameter setting performance twostage smoothing reflect performance fullfledged language modeling approach would involve sophisticated feedback models 4 6 16 thus really comparable performance trec sys tems yet performance figures shown actually competitive compared performance official trec submissions eg performance trec8 ad hoc task trec8 web track results twostage smoothing method en couraging especially ad hoc parameter tuning involved retrieval process approach automatically estimated based specific database query completely determined given database deter database query best jelinekmercer best dirichlet twostage avgpr med initpr pr5d avgpr med initpr pr5d avgpr initpr pr5d database query best jelinekmercer best dirichlet twostage avgpr med initpr pr5d avgpr med initpr pr5d avgpr initpr pr5d web trec8sv 0203 0191 0611 0392 0267 0249 0699 0492 0253 0680 0436 table 2 comparison estimated twostage smoothing best single stage smoothing methods small collections top large collections bottom best number measure shown boldface asterisk indicates difference twostage smoothing performance best single smoothing performance statistically significant according signed rank test level 005 mined database query together method appears quite robust according experiments different types queries different databases 6 conclusions paper derive general twostage language models information retrieval using risk minimization retrieval frame work present concrete twostage smoothing method special case twostage smoothing strategy explicitly captures different influences query document collection optimal settings smoothing parameters first stage document language model smoothed using dirichlet prior collection model reference model second stage smoothed document language model interpolated query background model propose leaveoneout method estimating firststage dirichlet prior parameter mixture model estimating secondstage interpolation parameter methods allow us set retrieval parameters automatically yet adaptively according different databases queries evaluation five different databases four types queries indicates twostage smoothing method proposed parameter estimation scheme consistently gives retrieval performance close better best results attainable using single smoothing method achievable exhaustive parameter search effectiveness robustness twostage smoothing approach along fact ad hoc parameter tuning volved make solid baseline approach evaluating retrieval models shown automatic twostage smoothing gives retrieval performance close best results attainable using single smoothing method yet analyzed optimality estimated parameter values twostage parameter space example would important see relative optimality estimated fixing one would also interesting explore estimation methods example might regarded hyperparameter hierarchical bayesian approach estimation query model parameter would interesting try different query background models one possibility estimate background model based resources past queries addition collection documents another interesting future direction exploit query background model address issue redundancy retrieval results specifically biased query background model may used rep resentexplain subtopics user already encountered eg reading previously retrieved results order focus ranking new subtopics relevant set documents acknowledgements thank rong jin jamie callan anonymous reviewers helpful comments work research sponsored full advanced research development activity information technology arda statistical language modeling information retrieval research program contract mda904 00c2106 r information retrieval statistical translation hidden markov model information retrieval system estimation small probabilities leavingoneout language modeling approach information retrieval relevance weighting search terms pivoted document length normalization tr termweighting approaches automatic text retrieval pivoted document length normalization improving twostage adhoc retrieval short queries language modeling approach information retrieval hidden markov model information retrieval system information retrieval statistical translation vector space model automatic indexing document language models query models risk minimization information retrieval relevance based language models study smoothing methods language models applied ad hoc information retrieval estimation small probabilities leavingoneout ctr mark smucker james allan lightening load document smoothing better language modeling retrieval proceedings 29th annual international acm sigir conference research development information retrieval august 0611 2006 seattle washington usa donald metzler estimation sensitivity generalization parameterized retrieval models proceedings 15th acm international conference information knowledge management november 0611 2006 arlington virginia usa l azzopardi girolami c j van rijsbergen user biased document language modelling proceedings 27th annual international acm sigir conference research development information retrieval july 2529 2004 sheffield united kingdom james allan courtney wade alvaro bolivar retrieval novelty detection sentence level proceedings 26th annual international acm sigir conference research development informaion retrieval july 28august 01 2003 toronto canada hugo zaragoza djoerd hiemstra michael tipping bayesian extension language model ad hoc information retrieval proceedings 26th annual international acm sigir conference research development informaion retrieval july 28august 01 2003 toronto canada winslett k chang doan j han c zhai zhou database research university illinois urbanachampaign acm sigmod record v31 n3 september 2002 rong jin joyce chai luo si learn weight terms information retrieval using category information proceedings 22nd international conference machine learning p353360 august 0711 2005 bonn germany donald metzler w bruce croft combining language model inference network approaches retrieval information processing management international journal v40 n5 p735750 september 2004 xiaohua zhou xiaohua hu xiaodan zhang xia lin ilyeol song contextsensitive semantic smoothing language modeling approach genomic ir proceedings 29th annual international acm sigir conference research development information retrieval august 0611 2006 seattle washington usa guihong cao jianyun nie jing bai integrating word relationships language models proceedings 28th annual international acm sigir conference research development information retrieval august 1519 2005 salvador brazil tao tao chengxiang zhai mining comparable bilingual text corpora crosslanguage information integration proceeding eleventh acm sigkdd international conference knowledge discovery data mining august 2124 2005 chicago illinois usa seunghoon na insu kang jieun roh jonghyeok lee empirical study query expansion clusterbased retrieval language modeling approach information processing management international journal v43 n2 p302314 march 2007 paul ogilvie jamie callan combining document representations knownitem search proceedings 26th annual international acm sigir conference research development informaion retrieval july 28august 01 2003 toronto canada jaime teevan david r karger empirical development exponential probabilistic model text retrieval using textual analysis build better model proceedings 26th annual international acm sigir conference research development informaion retrieval july 28august 01 2003 toronto canada xiaoyong liu w bruce croft clusterbased retrieval using language models proceedings 27th annual international acm sigir conference research development information retrieval july 2529 2004 sheffield united kingdom jianfeng gao haoliang qi xinsong xia jianyun nie linear discriminant model information retrieval proceedings 28th annual international acm sigir conference research development information retrieval august 1519 2005 salvador brazil thomas r lynam chris buckley charles l clarke gordon v cormack multisystem analysis document term selection blind feedback proceedings thirteenth acm international conference information knowledge management november 0813 2004 washington dc usa jianfeng gao jianyun nie guangyuan wu guihong cao dependence language model information retrieval proceedings 27th annual international acm sigir conference research development information retrieval july 2529 2004 sheffield united kingdom ying zhao justin zobel searching style authorship attribution classic literature proceedings thirtieth australasian conference computer science p5968 january 30february 02 2007 ballarat victoria australia jianyun nie guihong cao jing bai inferential language models information retrieval acm transactions asian language information processing talip v5 n4 p296322 december 2006 jianfeng gao chinyew lin introduction special issue statistical language modeling acm transactions asian language information processing talip v3 n2 p8793 june 2004 joyce chai chen zhang rong jin empirical investigation user term feedback textbased targeted image search acm transactions information systems tois v25 n1 p3es february 2007 chengxiang zhai john lafferty risk minimization framework information retrieval information processing management international journal v42 n1 p3155 january 2006 wessel kraaij jianyun nie michel simard embedding webbased statistical translation models crosslanguage information retrieval computational linguistics v29 n3 p381419 september