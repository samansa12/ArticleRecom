predictive analysis wavefront application using loggp paper develops highly accurate loggp model complex wavefront application uses mpi communication ibm sp2 key features model include 1 elucidation principal wavefront synchronization structure 2 explicit highfidelity models mpisend mpireceive primitives mpisendreceive models used derive l g simple twonode microbenchmarks model parameters obtained measuring small application problem sizes four sp nodes results show loggp model predicts seconds high degree accuracy measured application execution time large problems running 128 nodes detailed performance projections provided large future processor configurations expected available application developers results indicate scaling beyond one two thousand nodes yields greatly diminished improvements execution time synchronization delays principal factor limiting scalability application b introduction paper investigates use parallel machine model called loggp analyze performance large complex application stateoftheart commercial parallel platform application known sweep3d interest threedimensional particle transport problem identified asci benchmark evaluating high performance parallel architectures application also interest fairly complex synchronization structure synchronization structure must captured analytic model order model accurately predict application execution times thus provide accurate performance projections larger systems new architectures modifications application one question addressed research variants logp model 4 best suited analyzing performance sweep3d ibm sp system since version sweep3d uses mpi communication primitives loggp model 2 includes additional parameter g accurately model communication cost large pipelined messages turned provide requisite accuracy possibly due blocking nature mpi primitives contention message processing resources negligible thus recent extensions logp capturing impact contention 712 needed previous work 467 logp models applied important fairly simple kernel algorithms fft lu sorting algorithms sparse matrix multiply two experimental studies applied model complex full applications splash benchmarks 9 11 however studies effects synchronization application performance scalability measured empirically rather estimated model many previous analytic models analyzing application performance restricted simpler synchronization structures sweep3d eg 8 one exception deterministic task graph analysis model 1 shown accurately predict performance applications complex synchronization structures loggp model represents synchronization structures abstractly task graph key question addressed research whether abstract representation sufficient analyzing full complex application sweep3d construct loggp model captures synchronization structure also elucidates basic synchronization structure sweep3d similar approach 2 use communication microbenchmarks derive input parameters l g however show section 3 deriving parameters somewhat complex mpi communication sp2 meiko cs2 thus explicit models mpisend mpireceive primitives developed although loggp input parameters derived fourprocessor runs sweep3d loggp model projects performance quite accurately 128 processors several fixed total problem sizes several cases fixed problem size per processor model also quickly easily projects performance large future processor configurations expected available application developers research supported part darpaito contract n6600197c8533 computer sciences technical report 1392 university wisconsinmadison february 1999 appear proc 7 th acm sigplan symp principles practice parallel programming ppopp 99 atlanta ga may 1999 show several interesting results derived analysis section 2 provides brief overview sweep3d application section 3 derives models mpisend mpireceive parameter values characterize communication cost section 4 presents loggp equations sweep3d well modifications needed application utilizes multiprocessor smp nodes sp2 latter case two types communication costs intracluster inter cluster section 5 provides model validation results well performance projections future systems section 6 provides conclusions work 2 sweep3d sweep3d described 10 detailed task graph showing complex synchronization among tasks version code analyzed paper given 5 give simple overview version sweep3d including aspects relevant loggp model structure algorithm apparent loggp model presented section 4 name implies sweep3d transport calculations implemented series pipelined sweeps three dimensional grid let dimensions denoted ijk 3d grid mapped onto twodimensional array processors size mn processor performs calculations partition j dimensions size itjtk shown figure 1 note due problem mapping figure 1 processors processor grid figure 2 numbered p ij varies 1 n indicates horizontal position processor single iteration consists series pipelined sweeps 3d grid starting 8 corners octants grid mapping sweeps two dimensional processor grid illustrated figure 2 mo denotes number angles considered problem processor performs itjtkmo calculations sweeps octant create finer granularity pipeline thus increasing parallelism computation block data computed given processor partitioned angle blocking factor mmi kplane blocking factor mk parameters specify number angles number planes kdimension respectively computed boundary data forwarded next processor pipeline processor interior processor grid receives boundary data two neighbor processors computes block based values sends results calculations two neighbor destination processors determined direction sweep optimized version sweep3d analyze blocks given processor calculated sweeps given pair octants processor free start calculating blocks sweeps next pair octants example lower left corner processor start compute first block sweep octant 7 computed last block sweep originating octant 6 shown greater detail loggp model sweep3d section 4 pipelined sweep octant 8 completes one iteration algorithm one energy group code analyze twelve iterations executed one time step target problems interest asci program involve order groups 10000 time steps grid sizes order 10 9 twenty million eg 280280255 scale model projections problem sizes shown section 5 3 communication parameters l g present loggp model sweep3d sp2 derive models mpisend mpireceive communication primitives used application mpisendreceive models needed loggp model sweep3d also needed derive two communication parameters values namely network latency l processing overhead send receive message communication structure sweep3d ignore gap g parameter time consecutive message transmissions greater minimum allowed value intermessage transmission time give roundtrip communication times mpi communication ibm sp measured using simple communication microbenchmarks value g gap per byte derived directly measurements discuss modeled sp2 mpisend mpireceive primitives using l g parameters followed description values l derived significant result derive values l g different values fortran c microbenchmark measurements greatly increases confidence validity mpi communication models figure 1 partitioning 3d grid j dimensions figure 2 sweeps octant jt processor grid 31 measured communication times roundtrip communication time function message size simple fortran communication microbenchmark given figures 3 b data point message given size sent processor processor b received process processor b immediately sent back roundtrip time measured subtracting time calls mpisend time mpireceive operation completes figure also includes results model roundtrip communication used derive l discussed seen figures measured communication time increases significantly message size hence g parameter required accurately model communication cost two points worth noting figure communication cost changes abruptly message size equal 4kb due handshake mechanism implemented messages larger 4kb handshake modeled slope curve g changes message size equal 1kb message processing overhead also different messages larger 1 kb messages smaller 1kb due maximum ip packet size thus derive separate values g g l messages 32 models mpisend mpireceive models developed reflect fairly detailed understanding mpisend mpireceive primitives implemented sp2 able obtain author mpi software might necessary modify models future versions mpi library sweep3d run different messagepassing architecture modified use nonblocking mpi primitives models illustrate general approach capturing impact system modifications since sp2 system uses polling receive messages assume overhead send message approximately overhead receive message messages smaller 4kb handshake required total endtoend cost sending receiving message modeled simply values g depend whether message size larger smaller 1kb messages larger 4kb endtoend communication requires handshake header initially sent destination processor destination processor must reply short acknowledgment corresponding receive posted receive posted header message sent endtoend cost modeled follows note processing overhead receiving ack modeled subsumed processing overhead sending data corresponding receive yet posted additional synchronization delay incurred delay modeled next section addition total cost communication given loggp model sweep3d requires separate costs sending receiving messages message size less 4kb value depends message size message size greater equal 4kb receive cost includes time inform sending processor receive posted delay message arrive 33 communication parameter values using equations totalcomm measured roundtrip communication times derive values l given table 1 values g g computed directly slope curve figure respective range message sizes derive l solve three equations totalcomm message sizes less 1kb 14kb greater 4kb respectively three unknowns l applying method roundtrip time measurements obtained c microbenchmarks yields values l g measurements figure 3 mpi round trip communication message size time usec1003005000 2000 4000 6000 8000 10000 message size time usec measured modeled obtained fortran benchmarks although value different shown table 1 greatly increases confidence validity models mpi communication primitives using parameter values derived way measured modeled communication costs differ less 4 messages 64256kb shown figure 3 note although measured modeled values seem diverge message size equal 8kb figure 2a figure 2b shows values message sizes 8kb good agreement 4 loggp model sweep3d section develop loggp model sweep3d using models mpi communication costs developed section 3 first present model assumes processor mn processor grid mapped different smp node sp2 case network latency communication give modified equations case 22 regions processor grid mapped single fourprocessor smp node sp2 roundtrip times parameter values computed section 3 communication processors different smp nodes equations used compute intranode communication parameters 41 basic model loggp model takes advantage symmetry sweeps performed execution thus calculates estimated execution time sweeps one octant pair uses execution time obtain total execution time sweeps explained sweep described section 2 processor waits input two neighbor processors computes values portion grid size mmi mk jt processor sends boundary values two neighbor processors waits receive new input using costs associated activities develop loggp model summarized table 2 directly expresses precedence sendreceive synchronization constraints implemented algorithm time compute one block data modeled equation 5 table 2 equation w g measured time compute one grid point mmi mk jt input parameters defined section 2 specify number angles grid points per block per processor consider octant pair 56 sweeps begin processor upperleft corner processor grid shown figure 2 recall upperleft processor numbered p 11 account pipelining wavefronts sweeps use recursive formula equation 6 table 2 compute time processor p ij begins calculations sweeps denotes horizontal position processor grid first term equation 6 corresponds case message west last arrive processor p ij case message north already sent cannot received message west processed due blocking nature mpi communications second term equation 6 models case message north last arrive note startp appropriate one two terms equation 6 deleted processors east north edges processor grid sweep3d application makes sweeps across processors direction octant pair critical path time two rightdownward sweeps computed equation 7 table 2 time lowerleft corner processor p 1m finished communicating results last block sweep octant 6 point sweeps octants 7 8 upper right start processor p 1m proceed toward note subscripts send receive terms equation 7 included indicate direction communication event make easier understand term included equation send receive costs derived section 32 critical path sweeps octants 7 8 time processors grid complete calculations sweeps since sweeps octants 1 2 next iteration wont begin processor p n1 finished due symmetry sweep3d algorithm mentioned time sweeps northeast total time sweeps octants 5 6 start processor p 00 move southeast processor p nm thus compute critical path time octants 7 8 shown equation 8 table 2 equation 8 represents time processor p nm finished last calculation second octant pair processor table loggp model sweep3d message size 1024 1024 table 1 sp2 mpi communication parameters directly east p n1m must start computing calculate communicate needed results blocks octants wait processor p nm receive results last block calculations compute results based block due symmetry sweeps octants 1 4 sweeps octants 5 8 total execution time one iteration computed equation 9 table 2 equation 56 contains one term m1l equation 78 contains two terms m1l n2l account synchronization costs synchronization terms motivated observation measured communication times within sweep3d greater measured mpi communication cost discussed section 3 m1l term 56 78 captures delay caused send blocked destination processor posts corresponding receive delay accumulates j direction thus total delay 1m depends number processors north m1 furthermore synchronization cost zero problems message sizes smaller 4kb since case processor sends message whether corresponding receive posted second synchronization delay 78 n2l represents difference receive posted message actually received sending processor since processor receives north west southeast sweep likely wait message west since delay cumulative processors dimension processor p n1m model delay n2l notice receive synchronization term 0 processors west edge processor grid since processors west receive message included 56 expression 42 model clustered smp nodes modifications model needed 22 region processor grid mapped single fourprocessor smp cluster ibm sp2 rather mapping processor grid separate smp node changes outlined anticipation next generation mpi software sp support full use cluster processors let l local denote network latency intracluster message remote denote latency intercluster message l local l remote 2 following discussion assumed intracluster intercluster messages equations easily modified case let l r subscripts denote model variable eg totalcomm send receive computed using l local l remote respectively using notation modified equations compute execution time sweep3d given table 3 described recall processor numbering starts 1 j dimensions also recall processor p ij denotes horizonal position processor grid j even incoming messages intracluster outgoing messages intercluster vice versa true j odd means startp ij computed totalcomm l receive l send l incoming messages former case totalcomm r receive r send r latter case odd j even variables first term startp ij intercluster communication communication variables second term intracluster communication vice versa true even j odd send receive variables equations 56 78 intracluster variables assuming number processors j dimensions even mapping 22 processor regions smp clusters synchronization terms 56 78 computed using l avg changes required model modified model validated detailed simulation 3 however since cannot yet validate system measurements efficient mpi software intracluster communication doesnt yet exist results case processor mapped separate smp node given paper nevertheless changes model full cluster use simple illustrate models versatility furthermore equations used project system performance next generation mpi software 43 measuring work w value work per grid point w g obtained measuring value 2x2 grid processors fact obtain accuracy results paper measured w g perprocessor grid size account differences 20 arise cache miss effects since sweep3d program contains extra calculations fixups five twelve iterations measure w g values iteration types although detailed creators logploggp may intended increased accuracy substantial needed large scale projections section 5 furthermore recursive model sweep3d represents table 3 modified loggp equations intracluster communication sp2 sweeps sweep3d code addition measure computation time main body code ie iterations time step computation times denoted w w measured single processor run specific problem size model parameters thus measured using simple code instrumentation relatively short one two fourprocessor runs next section investigate accurately model predicts measured execution time sweep3d application 5 experimental results section present results obtained loggp model validate loggp projections sweep3d running time measured running time 128 processors use loggp model predict evaluate scalability sweep3d thousands processors two different problem sizes interest application developers unless otherwise stated reported execution times one energy group one time step twelve iterations time step figure 4 compare execution time predicted loggp model measured execution time fortran version sweep3d 128 sp2 processors fixed total problems sizes 150150150 505050 kblocking mk equal 10 number processors increases message size computation time per processor decrease overhead synchronization increases problem sizes processor configurations message sizes vary 16kb 1kb remarkably high agreement model estimates measured system performance across entire range figure 5 shows larger problem size achieves reasonably good speedup ie low communication synchronization overhead 128 processors smaller problem size note model highly accurate cases figure 6 show predicted measured application execution time function number processors sp2 two different cases fixed problem size per processor figure 6a processor partition threedimensional grid size 20201000 figure 6b processor partition size 45x45x1000 experiments total problem size increases number processors increases agreement model estimates problem size 150150150 b problem size 505050 figure 4 validation loggp model fixed total problem size 128 processors b 2500 processors figure 5 sweep3d speedups fixed total problem sizes figure 4 code mk10 mmi3501500 50 100 150 processors time sec model processors time processors processors measured execution time generally excellent level abstraction model however results show model less quantitatively accurate mk1 verified many configurations loggp model qualitatively accurate determining whether execution time mk1 higher lower execution time mk10 also verified model quantitatively accurate values mk larger 10 results 45x45x1000 also illustrate c version code created fortran version using f2c somewhat slower fortran code although absolute performance c differs performance trends report paper fortran code also observed c code model projections figure 7 shows projected execution time sweep3d fixed problem size per processor system scaled thousands processors expected available asci sites near future two fixed perprocessor problem sizes considered 661000 1414255 cases model predictions validated 2500 processors using simulation shown measured execution times 661000 case illustrate unexplained system anomaly measured execution time suddenly increases given small increase number processors anomaly occurred couple fixed perprocessor grid sizes examined note anomaly occurs even though problem size per processor fixed thus seems unlikely explained cache behavior message size one hazards modeling analytic simulation anomalous system behavior cannot predicted however model estimates show jump execution time due expected communication synchronization costs detailed examination system implementation required discover hopefully correct cause anomaly figure 6 figures 7a b predict excellent scaling case memory usage per processor kept constant nevertheless solving 10 9 problem size 661000 grid points per processor requires 27000 processors results figure 7a suggest execution time scaled groups 10000 time steps prohibitive problem configuration 20201000 b 45451000 figure validation loggp model fixed problem size per processor 661000 figure 7 projected sweep3d execution time fixed problem size per processor mmi3 mk1040012000 100 200 300 400 500 number processors time sec c measured mk10 c loggp mk10 c measured mk1 c loggp mk1 measured mk10 loggp mk10 measured mk1 loggp mk150150250 number processors time number processors time number processors time sec measured loggp figure 8 gives projected execution time sweep3d system scaled 20000 processors two different total problem sizes interest application developers case projected execution times single time step involving 12 iterations scaled factor 30 reflect fact computation interest scientists involves energy groups rather one note problem size per processor decreases number processors increases thus sweep3d configurations larger mk higher performance loggp model used determine values sweep3d configuration parameters ie mmi yield lowest execution time given processor configurations problem sizes one key observation results figure 8 point greatly diminishing improvement execution time number processors increased beyond one two thousand second key observation figure 8b even optimal values sweep3d configuration parameters unlimited number processors solving billion grid point problem time steps appears require prohibitive execution time using current algorithm investigate causes limited scalability figures 7 8 figure 9 shows breakdown execution time problem sizes figure 8 breakdown shows much critical path execution time due computation nonoverlapped synchronization nonoverlapped communication key observation system scaled synchronization delays become significant dominant factor execution time synchronization delays modeled m1l n1l terms equations 7 8 table 2 modifications reduce synchronization costs would highly desirable solving large problems interest example simple modification might explored use nonblocking form mpisend however fundamental algorithmic changes reduce synchronization delays may needed figure 10 shows could yield greater benefit improved processor technology due difficulty speeding communication latencies 20 million grid points b 1 billion grid points figure 8 projected sweep3d execution times fixed total problem size one time step 20 million grid points b 1 billion grid points figure 9 projected sweep3d execution time communication synchronization costs one time step number processors time loggp mmi1 mk1 loggp mmi3 mk1 loggp mmi6 mk1 loggp mmi1 mk10 loggp mmi3 mk10 loggp mmi6 mk105001500250035000 5000 10000 15000 20000 25000 number processors number processors total comp comm synch10003000500070000 5000 10000 15000 20000 25000 number processors 6 conclusions principal contribution research loggp model analyzing projecting performance important application complex synchronization structure wavefront application loggp equations capture principal synchronization costs also elucidate basic pipelined synchronization structure illustrating abstraction capability domain comparable simplicity communication parameters l g research provides case study model validates extremely well measured application performance illustrates potential loggp model analyzing wide variety interesting applications including important class wavefront applications significant results obtained sweep3d application studied paper follows first scaling beyond one two thousand processors yields greatly diminished returns terms improving execution time even large problem sizes second solving problem sizes order 10 grid points groups 10000 time steps appears impractical current algorithm finally synchronization overhead principal factor limiting scalability application future work includes generalizing model presented research create reusable analytic model wavefront applications executing production parallel architectures developing model sharedmemory version sweep3d developing loggp models applications complex synchronization structures r analyzing behavior performance parallel programs loggp incorporating long messages logp model logp towards realistic model parallel computation poems endtoend performance design large parallel adaptive computational systems fast parallel sorting logp experience cm5 lopc modeling contention parallel algorithms effects latency occupancy bandwidth distributed shared memory multiprocessors solution firstorder form fo 3d discrete orginates equation massively parallel processor effects communication latency overhead bandwidth cluster architecture logpc modeling network contention message passing programs tr logp towards realistic model parallel computation analyzing behavior performance parallel programs loggp predicting application behavior large scale sharedmemory multiprocessors fast parallel sorting logp effects communication latency overhead bandwidth cluster architecture poems effects latency occupancy bandwidth distributed shared memory multiprocessors ctr ewa deelman gurmeet singh meihui su james blythe yolanda gil carl kesselman gaurang mehta karan vahi g bruce berriman john good anastasia laity joseph c jacob daniel katz pegasus framework mapping complex scientific workflows onto distributed systems scientific programming v13 n3 p219237 july 2005 fumihiko ino noriyuki fujimoto kenichi hagihara loggps parallel computational model synchronization analysis acm sigplan notices v36 n7 p133142 july 2001 gabriel marin john mellorcrummey crossarchitecture performance predictions scientific applications using parameterized models acm sigmetrics performance evaluation review v32 n1 june 2004 daniel nurmi anirban mandal john brevik chuck koelbel rich wolski ken kennedy grid scheduling protocolsevaluation workflow scheduler using integrated performance modelling batch queue wait time prediction proceedings 2006 acmieee conference supercomputing november 1117 2006 tampa florida kirk w cameron rong ge predicting evaluating distributed communication performance proceedings 2004 acmieee conference supercomputing p43 november 0612 2004 ruoming jin gagan agrawal performance prediction random write reductions case study modeling shared memory programs acm sigmetrics performance evaluation review v30 n1 june 2002 vikram adve rizos sakellariou application representations multiparadigm performance modeling largescale parallel scientific codes international journal high performance computing applications v14 n4 p304316 november 2000 rajive bagrodia ewa deelman thomas phan parallel simulation largescale parallel applications international journal high performance computing applications v15 n1 p312 february 2001 david k lowenthal accurately selecting block size runtime pipelined parallel programs international journal parallel programming v28 n3 p245274 june 2000 det buaklee gregory f tracy mary k vernon stephen j wright nearoptimal adaptive control large grid application proceedings 16th international conference supercomputing june 2226 2002 new york new york usa vikram adve rajive bagrodia james c browne ewa deelman aditya dube elias n houstis john r rice rizos sakellariou david j sundaramstukel patricia j teller mary k vernon poems endtoend performance design large parallel adaptive computational systems ieee transactions software engineering v26 n11 p10271048 november 2000 ruoming jin gagan agrawal methodology detailed performance modeling reduction computations smp machines performance evaluation v60 n14 p73105 may 2005 vikram adve mary k vernon parallel program performance prediction using deterministic task graph analysis acm transactions computer systems tocs v22 n1 p94136 february 2004