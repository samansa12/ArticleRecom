approximation determinant large sparse symmetric positive definite matrices paper concerned problem approximating deta1n large sparse symmetric positive definite matrix order n shown efficient solution problem obtained using sparse approximate inverse method explained theoretical properties discussed method ideal implementation parallel computer numerical experiments described illustrate performance new method provide comparison monte carlotype methods literature b introduction throughout paper denotes real symmetric positive definite matrix order n eigenvalues number applications example lattice quantum chromodynamics 12 certain functions determinant deta 1n lndeta interest wellknown cf also x2 large n function deta poor scaling properties illconditioned certain matrices paper consider function basic properties function discussed x2 paper present new method approximating da large sparse matrices method based replacing matrix certain sense close gamma1 determinant computed low computational costs one popular method approximating based construction incomplete cholesky factorization incomplete factorization often used preconditioner solving linear systems matrix paper use another preconditioning technique namely sparse approximate inverses cf 1 7 9 11 remark 310 comment advantages use sparse approximate inverse preconditoning approximating da let cholesky decomposition using techniques known literature sparse approximate inverse ge l ie lower triangular matrix ge prescribed sparsity structure e approximation l gamma1 constructed use detge ii approximation da x3 explain construction ge discuss theoretical properties sparse approximate inverse example sparse approximate inverse shown exist symmetric positive definite interesting optimality property related da direct consequence optimality property one obtains holds approximation da detge gamma2n becomes better larger sparsity pattern e used institut fur geometrie und praktische mathematik rwth aachen templergraben 55 d52056 germany reusken x4 consider topic error estimation paper 2 bounds determinant symmetric positive definite matrices derived bounds frobenius norm estimate extreme eigenvalues matrix involved used often yield rather poor estimates determinant cf experiments 2 x41 apply technique preconditioned matrix geag e thus obtain reliable rather pessimistic error bounds turns error estimation technique rather costly x42 introduce simple cheap monte carlo technique error estimation x5 apply new method examples large sparse symmetric positive definite matrices 2 preliminaries section discuss elementary properties function give comparision conditioning function fuction use notation k delta k 2 euclidean norm denotes spectral condition number trace matrix denoted tra lemma 21 let ffia symmetric positive definite matrices order n following inequalities hold proof result 21a follows result 21b follows inequality geometric arithmetic mean courantfischer characterization eigenvalues follows hence holds note approximation determinants 3 thus result 21c proved result 21c shows function da wellconditioned matrices large condition number briefly discuss difference conditioning functions deta symmetric positive definite matrix b order n courantfischer eigenvalue characterization obtain hence b spd b spd equality thus condition number function n note diagonal matrix inequality 22 one obtains equality n 1 equality second inequality 21c condition number given ie n times larger condition number 22 condition numbers give indication sensitivity perturbation kffiak 2 sufficiently small note bound 21c valid arbitrary symmetric positive definite perturbations ffia bound shows even larger perturbations function wellconditioned large function effect relatively large perturbations much worse asymptotic case ffia 0 characterized condition number 23 consider example 2 perturbation large example results section show numerical approximation function much easier task numerical approximation deta 3 sparse approximate inverse section explain analyze construction sparse approximate inverse matrix let cholesky factorization ie l lower triangular l gamma1 al note ii construct sparse lower triangular approximation g l gamma1 approximate da dg ii construction sparse approximate inverse use paper introduced 9 10 11 also found 1 results derived section presented 1 4 reusken first introduce notation let e ae fi ng given sparsity pattern e denote number elements e let se set n theta n matrices entries set zero corresponding index e use representation define projection note matrix symmetric positive definite typical choices sparsity pattern e cf x5 n small number compared n eg case projected matrix p ap small dimension facilitate analysis first discuss construction approximate sparse inverse 2 se general framework use representation note n given b 2 r nthetan symmetric positive definite consider following problem determine 33 e equations determine e entries first give two basic lemmas play important role analysis sparse approximate inverse defined 39 lemma 31 problem 33 unique solution ith row given ith row b proof equations 33 represented b ith row consider unknown entries obtain system equations approximation determinants 5 equivalent matrix p ap symmetric positive definite thus must satisfy using obtain result 34 construction proof shows solution unique use frobenius norm denoted k lemma 32 let cholesky factorization let 2 se unique solution 33 unique minimizer functional proof let e ith basis vector r n take 2 se ith rows b denoted minimum functional 36 obtained 37 minimize functionals 38 rewritten unique minimum functional obtained using lemma 31 follows unique minimizer functional 36 sparse approximate inverse introduce sparse approximate inverse used approximation l gamma1 choose lower triangular ng assume 2 e l sparse approximate inverse constructed two steps l 2 6 reusken construction g e l 39 first introduced 9 theoretical background factorized sparse inverse given 11 approximate inverse 39a form 33 lemma 31 follows 39a unique solution note e l lower triangular 2 e l hence follows lemma 31 ith row denoted g given ith entry g ie e strictly positive symmetric positive definite hence diag contains strictly positive entries second step 39b welldefined sparse 39a computed solving low dimensional symmetric positive definite systems derive interesting properties sparse approximate inverse 39 start minimization property theorem 33 let cholesky factorization l 39a unique minimizer functional proof construction 39a 33 hence lemma 32 applicable follows l unique minimizer decompose l gammat l gammat strictly upper triangular r lower strictly upper triangular respectively obtain f hence minimizers 313 312 remark 34 result theorem 33 see scaled frobenius norm scaling l optimal approximation set e l sense l closest identity seemingly natural minimization problem min ie directly approximate l gamma1 instead use scaling minimization problem 314 form lemma 32 approximation determinants 7 hence unique minimizer 314 denoted must satisfy 33 e l contains indices must satisfy similar system equations 39a characterizes l however 316 one needs values l ii general available hence opposite minimization problem related functional 312 minimization problem 314 general solvable acceptable computational costs 2 following lemma used proof theorem 37 lemma 35 let l 39a decompose l diagonal strictly lower triangular define e l ng l unique minimizer functional also functional furthermore proof construction 39a follows form 33 lemma 32 obtain l unique minimizer functional ie functional 317 proof lemma 32 minimization problem min decouples seperate minimization problems cf 38 rows l min l al g 320 8 reusken ith rows l respectively minimization problem corresponding 318 min al decouples minimization problems 320 hence functionals 317 318 minimizer using construction 39a obtain ii j ik2e l hence ii holds ie 319 holds corollary 36 319 follows diag thus using 39b obtain sparse approximate inverse g e l 2 following theorem gives main result theory approximate inverses first derived 11 proof found 1 theorem 37 let g e l approximate inverse 39 g e l unique minimizer functional proof g 2 e l use decomposition diagonal l furthermore l inequality 323 follows inequality arithmetic geometric 39a use decomposition l approximate l lemma 35 follows detj l detj furthermore lemma 35 obtain g l thus equality conclude g e l unique minimizer functional approximation determinants 9 322 remark 38 quantity seen nonstandard condition number cf 1 9 properties quantity given 1 theorem 135 one elementary property corollary 39 approximate inverse g e l 39 cf321 ie let l lower triangular sparsity pattern larger e l ie e l ae ng optimality result theorem 37 follows motivated theoretical results corollary 39 propose use sparse approximate l 39 approximating estimate da properties method discussed following remark remark 310 consider method approximating da dg practical realization method boils chosing sparsity solving small systems 311 list properties 1 sparse approximate inverse exists every symmetric positive definite note existence result hold incomplete cholesky factorization furthermore factorization obtained solving low dimensional symmetric positive definite systems form p ap cf 311 realized stable way 2 systems p ap solved parallel 3 computation dg need diagonal entries cf 324 systems p ap e compute last entry systems solved using cholesky lower triangular need l since g 4 sparse approximate inverse optimality property related determinant functional g inequality 324 monotonicity result 325 follow reusken 5 from324 obtain upper bound 0 relative error 1 x4 derive useful lower bounds relative error posteriori error bounds use matrix g e l 2 4 posteriori error estimation previous section explained estimate dg e l gamma2 da computed 324 error bound section discuss posteriori estimators error dadg x41 apply analysis 2 derive posteriori lower bound quantity 41 approach results safe often rather pessimistic bounds error x42 propose simple stochastic method error estimation method although yield guaranteed bounds error turns useful practice 41 estimation based bounds 2 section show analysis 2 used obtain error estimator first recall main result 2 theorem 2 let symmetric positive matrix order n f exp l exp 2 result applied obtain computable bounds da often bounds yield rather poor estimates da present paper approximate use result 42 error estimation upper bound turns satisfactory numerical experiments cf x5 therefore restrict derivation lower bound dadg e l gamma2 based left inequality 42 theorem 41 let g e l approximate inverse 39 0 ff 1 following holds ff 1 exp proof right inequality 43 already given 41 introduce eigenvalues g e l ag l 321 obtainn follows ff 1 1 holds furthermore approximation determinants 11 yields use left inequality 42 applied matrix g e l ag l note simple computation yieldsn l substitution 45 44 results inn l using left inequality 43 follows left inequality 42 note lower bound 43 computable need f strictly positive lower bound ff smallest eigenvalue g e l ag l discuss methods computing ff methods used numerical experiments x5 first discuss two methods computing ff first method applied mmatrix based following lemma use lemma 42 let symmetric positive definite matrix order n ij 0 6 j g e l sparse approximate inverse 39 furthermore let z holds proof assumptions follows mmatrix 11 theorem 41 proved g e l ag l mmatrix let z nonnegative entries follows reusken hence using min g e l ag obtain result 46 based lemma obtain following method computing ff choose apply conjugate gradient method system g e l ag results approximations z z one iterates stopping criterion 1 view efficiency one take small tolerance j experiments x5 use 1 note cg method applied system preconditioned matrix g e l ag l situations preconditioning effective one may expect relatively cg iterations needed compute z j kg e l ag numerical experiments presented x5 second method determining ff applicable symmetric positive definite propose lanczos method approximating eigenvalues applied matrix g e l ag l method yields decreasing sequence 1 l approximations j 1 min g e l ag holds used theorem 41 however practice usually known obtain reasonable values 47 therefore experiments use simple heuristic error estimation instead rigorous bound 47 based observed convergence behaviour j known lanczos method convergence extreme eigenvalues relatively fast moreover often occurs small eigenvalues preconditioned wellseparated rest spectrum positive effect convergence speed j numerical experiments indeed observe often already lanczos iterations approximation min g e l ag estimated relative error percent however ff computed second method rigorous analysis guarantees numerical experiments see method satisfactory partly explained relatively fast convergence lanczos method towards smallest eigenvalue explanation follows form lower bound 43 ff typically case experiments x5 lower bound essentially behaves like expffi ln ff gff note holds hence sensitivity lower bound respect perturbations ff mild discuss computation quantity f needed 43 clearly computing one needs matrices g e l avoid unnecessary storage requirements one compute matrix x g e l ag determine f respect storage efficient approach based e ith basis vector r n computation kg e l ag approximation determinants 13 done parallel one needs sparse matrixvector multiplications matrices g e l furthermore computation ag use dg e l follows 310 holds remark 43 note error estimators discussed section must available thus stored whereas computation approximation dg e l gamma2 da store matrix g e l cf remark 310 item 3 furthermore see x5 computation error estimators relatively expensive 2 42 estimation based monte carlo approach section discuss simple error estimation method turns useful practice opposite treated previous section method yield approximation bounds error exact error given l sparse symmetric positive definite matrix fore ease presentation assume pattern e l sufficiently large holds 11 proved mmatrix block hmatrix 48 satisfied every lower triangular pattern e l numerical experiments cf matrices mmatrices block hmatrices 48 turns satisfied standard choices e l note 48 hold technique discussed still applied one replaces suitable damping factor aei gamma e exact error obtain using taylor expansion lni gamma b b 2 r nthetan gamman hence error estimation based estimates partial sums sm construction g e l diage e l thus tre 14 reusken 3 obtain note 2 3 quantity tre 2 f occurs also used error estimator x41 section use monte carlo method approximate trace quantities sm method use based following proposition 8 3 proposition 44 let h symmetric matrix order n trh 6 0 let v discrete random variable takes values 1 gamma1 probability 05 let z vector n independent samples v z hz unbiased estimator trh ez approximating trace quantity 2 use following monte carlo algorithm 1 generate z j 2 r n entries uniformly distributed 0 1 2 z j 3 j based proposition 44 410 use approximation 2 corresponding error estimator approximation 3 replace step 3 algorithm 3 j use estimate 3 corresponding error estimator expgamman approximation determinants 15 clearly technique extended partial sums sm 3 however applications use 3 error estimation turns least experiments two leading terms expansion 49 sufficient reasonable error estimation note due truncation taylor ex pansion estimators e 2 e 3 biased shown 3 based socalled hoeffding inequality cf 13 probabilistic bounds derived z independent random variables proposition 44 paper use bounds based numerical experiments take fixed small value parameter monte carlo algorithm experiments x5 remark 45 setting paper proposition 44 applied known polynomial degree 2 3 monte carlo technique approximating proposition 44 applied lna quantity z lnaz considered riemannstieltjes integral approximated using suitable quadrature rules 3 quadrature based gausschristoffel technique unknown nodes weights quadrature rule determined using lanczos method detailed explanation method refer 3 alternative could considered error estimation use method 3 setting method could used compute rough approximation detg e l ag investigate possibility results 2 3 give indication alternative probably much expensive method presented section 2 5 numerical experiments section present results numerical experiments methods introduced x3 x4 experiments done using matlab implementation use matlab notation nnzb number nonzero entries matrix b experiment 1 discrete 2d laplacian consider standard 5point discrete laplacian uniform square grid mesh points directions ie symmetric positive definite matrix eigenvalues known sin choice sparsity pattern e l use simple approach based nonzero structure powers matrix first describe features methods case vary k let denote discrete laplacian case la lower triangular part nnzla 2640 sparse approximate inverse obtain nnzg e l 2 6002 systems p ap solved determine g e l 2 cf 311 dimensions 1 7 mean dimensions 67 approximation obtain reusken hence 0965 computation approximation along lines described remark 310 item 3 compute cholesky factorizations n approximately needed matlab implementation compare costs one matrixvector multiplication x 8760 flops denoted matvec follows computing approximation da error 35 percent need work comparable 5 matvec see arithmetic costs error estimation significantly higher first consider methods x41 arithmetic costs measured terms matvec computation ff indicated lemma 42 using cg method starting vector need 8 iterations cg iteration compute matrixvector multiplication g e l ag costs approximately 37 matvec obtain ff 00155 method based lanczos method approximating min g e l ag use heuristic stopping criterion need 7 lanczos iterations resulting ff direct computation results min g e l ag computation f first computed lower triangular part l computed kxkf making use symmetry total costs approximately matvec application lemma 41 ff cg ff lanczos yields two intervals contain exact error 0965 cases total costs error estimation 4045 matvec approximately 10 times costs computing approximation dg e l 2 gamma2 consider method x42 use estimators e 2 e 3 413 415 6 results 0973 note order magnitude exact error 35 percent approximated well step 3 monte carlo algorithm computing need one matrixvector multiplication g e l ag matvec total arithmetic costs e 2 approximately 20 matvec 3 need two matrixvector multiplications l third step monte carlo algorithm total costs e 3 approximately 40 matvec table 51 give results discrete 2d laplacian use sparsity pattern e l 2 third column table give computed approximation da corresponding relative error fourth column give total arithmetic costs cholesky factorization matrices p ap item 3 columns 58 give results corresponding arithmetic costs error estimators discussed x4 fifth column corresponds method discussed x41 ff determined using cg method applied g e l ag starting vector 1 stopping criterion take computed used input lower bound 43 resulting bound relative error arithmetic costs computing error bound shown column 5 column 6 one finds computed error bounds ff determined using lanczos method stopping criterion 52 last two approximation determinants 17 table results 2d discrete laplacian costs thm 41 thm 41 mc mc table results 2d discrete laplacian costs thm 41 thm 41 mc mc columns results monte carlo estimators given table 52 show results corresponding arithmetic costs method sparsity pattern concerning numerical results note following third fourth column table 51 see using method obtain approximation da relative error percent arithmetic costs matvec moreover efficiency hardly depends dimension n comparison third fourth columns tables 51 52 shows approximation significantly improves enlarge pattern e l 2 e l 4 corresponding arithmetic costs increase factor 9 caused fact mean dimensions systems p ap approximately 7 e l 2 approximately 20 n values similar ratios number nonzeros matrices la note matrix g e l stored error estimation computation approximation dg e l gamma2 error bounds fifth sixth column tables 51 52 rather conservative expensive furthermore deterioration quality quite strong increase costs dimension n grows strong increase costs mainly due fact cg lanczos method need significantly iterations n increases wellknown phenomenom matrix g e l ag e l condition number proportional n also note costs error estimators high compared costs computation dg e l gamma2 results last two columns indicate monte carlo error estimators although less reliable favourable figure 51 show eigenvalues matrix g e l ag l case computed matlab function eig eigenvalues reusken interval 0025 14 mean eigenvalues 1 see relatively many eigenvalues close 1 eigenvalues close zero fig 51 eigenvalues matrix g e l ag l experiment 1 experiment 2 matlab random sparse matrix sparsity structure matrices considered experiment 1 regular experiment consider matrices pattern nonzero entries irregular used matlab generator sprandn n 2n generate matrix b order n approximately 2n nonzero entries uniformly distributed random entries 0 1 matrix b b sparse symmetric positive semidefinite generic case matrix many eigenvalues zero obtain positive definite matrix generated random vector entries chosen uniform distribution interval 0 1 randn 1 testmatrix used b bdiagd performed numerical experiments similar experiment 1 consider case sparsity pattern 2 error estimator based cg method applicable sign condition lemma 42 fulfilled case 900 eigenvalues g e l ag shown figure 52 smallest largest eigenvalues 00099 570 respectively picture right figure 52 shows matrix sparse approximate inverse preconditioning results wellconditioned matrix related one see table 53 random matrix approximation da based sparse approximate inverse much better discrete laplacian experiment 1 respectively mean dimensions systems p ap spectively three cases costs matrixvector multiplication g e l ag e l x approximately 43 mv furthermore three cases matrix g e l ag l wellconditioned number lanczos iterations needed satisfy stopping criterion 52 hardly depends n due increasing n growth costs error estimator based theorem 41 column 5 much slower experiment 1 tables 51 52 table 53 error quantities columns 3 567 bounds estimates relative error approximation determinants 19 fig 52 eigenvalues matrices g e l ag l experiment 2 table results matlab random sparse matrices costs thm 41 mc mc values da given column 2 fact matrices irregular sparsity patterns cholesky factorization suffers much fillin matrices experiments 1 3 matrix experiment 10000 run storage problems try compute cholesky factorization using matlab function chol experiment 3 qcd type matrix experiment consider complex hermitean positive definite matrix sparsity structure experiment 1 matrix motivated applications qcd field qcd simulations determinant socalled wilson fermion matrix interest matrices properties discussed 4 5 nonzero entries wilson fermion matrix induced nearest neighbour coupling regular 4dimensional grid couplings consist 12 theta 12 complex matrices xy tensor product structure xyomega u xy p xy 2 r 4theta4 projector u xy 2 c 3theta3 su 3 x denote nearest neighbours grid coupling matrices xy strongly fluctuate function x consider toy problem matrix similarities wilson fermion matrices start 2dimensional regular grid experiment 1 n grid points couplings nearest neighbours use complex numbers length 1 numbers chosen follows couplings south west neighbours grid point x exp2iff x exp2iff w x respectively ff x ff w x chosen uniform distribution interval 0 1 couplings north east neighbours taken matrix hermitean make comparison experiment 1 easier matrix scaled factor n reusken ie couplings nearest neighbours length n diagonal take fli fl chosen smallest eigenvalue resulting matrix approximately 1 realized using matlab function eigs estimating smallest eigenvalue performed numerical experiments experiment 1 2 number nonzero entries la g e l experiment 1 900 eigenvalues matrices g e l ag shown figure 53 spectra intervals respectively results numerical experiments presented table 54 note error fig 53 eigenvalues matrices g e l ag l experiment 3 estimator x41 cg method used computing ff used matrix assumptions lemma 42 satisfied consider case application eig function computing smallest eigenvalue led memory problems comparison results table 54 table 51 shows table results costs thm 41 mc mc method applied qcd type problem instead discrete laplacian performance method change much finally note measurements arithmetic costs take account costs determining sparsity pattern e l k building matrices r cambridge university press bounds trace inverse determinant symmetric positive definite matrices large scale matrix computation problems progress lattice qcd algorithms exploiting structure krylov subspace methods wilson fermion matrix matrix computations parallel preconditioning sparse approximate inverses stochastic estimator trace influence matrix laplacian smoothing splines alternative approach estimating convergence rate cg method family twolevel preconditionings incomplete block factorization type factorized sparse approximate inverse precondi tionings theory quantum fields lattice convergence stochastic processes tr