second order methods optimal control timedependent fluid flow second order methods open loop optimal control problems governed twodimensional instationary navierstokes equations investigated optimality systems based lagrangian formulation adjoint equations derived newton quasinewton methods well various variants sqp methods developed applications optimal flow control complexity terms system solves discussed local convergence rate convergence proved numerical example illustrates feasibility solving optimal control problems twodimensional instationary navierstokes equations second order numerical methods standard workstation environment b introduction research devoted analysis second methods solving optimal control problems involving time dependent navier stokes equations thus consider min jy u u subject bounded domain r 2 suciently smooth boundary nal time 0 initial condition 0 xed vector valued variable scalar valued variable p represent velocity pressure uid u denotes control variable b control operator precise functional analytic setting problem 11 12 given section 2 moment suces say typical cost functionals include tracking type functionals involving vorticity uid jcurl yt j 2 0 z given following discussion convenient formally represent equality constraints involved 12 ey 11 12 expressed form min jy u u subject form solving 11 12 appears rst standard task see second order methods flow control 3 references given however formidable size 11 12 goal analyzing second order methods necessitate independent analysis second order methods applied optimal control problems two classes distinguished depending whether p 11 12 considered independent variables functions control variable u former case represents explicit constraint optimization problem whereas latter case serves purpose describing evaluation p function u fact p expressed reduced problem yu implicitly dened via obtain second order method case p considered independent variables one derive optimality system p apply newton algorithm optimality system referred sequential quadratic programming sqpmethod alternatively p considered functions u newtons method applied directly relative merits two approaches discussed section 4 anticipate discussion let us point dierence numerical eort two methods rather small fact proper rearrangements dierence computational cost per iteration sqpmethod p newton method solving either linearized equation 12 full nonlinear equation view timedependence either two equations iterative procedure used solution dierence solving linearized nonlinear equation per sweep signi cant second consideration may uence choice sqpmethod newtonmethod applied u 0 p u clearly used independently sqp method states decoupled controls sometimes hinted decoupling important initialization also iteration consequence sqpmethod may require fewer iterations newtons method shall see variables p initialized independently u 0 also newton method specically available necessary abandon choice initial guess rely one suboptimal strategies developed recent past obtain approximate solutions 11 12 mention reduced order techniques ir podbased methods hk kv lt instantaneous control method ctmk bmt chk another possibility one carry gradient steps one switches newton iteration let us brie comment related contributions optimality systems derived problems type 11 12 gradient technique proposed gm solution 11 12 similarly b gradient techniques analyzed boundary control problem related 11 12 fgh authors analyze optimality systems exterior boundary control problems one contributions focusing second order methods optimal control uids given gb h works restricted stationary problems however paper hand focuses second order methods time dependent problems show despite diculties due size 11 12 fact optimality systems contains two point boundary value problem forward time primal backwards time adjoint vari ables second order methods computationally feasible establish initial approximation reduced hessian compact perturbation hessian minimizer addition give conditions second order sucient optimality conditions tracking type problems results imply superlinear convergence quasinewton well sqpmethods present paper focuses distributed control problems future paper plan address case velocity control along boundary paper organized follows section 2 contains necessary analytic prerequisites first second order derivatives cost functional respect control computed section 3 fourth section contains comparison second order methods solve 11 12 section 5 convergence quasi newton method sqpmethods applied p analyzed numerical results newtonmethod comparisons gradient method contained section 6 2 optimal control problem section consider optimal control problem 11 12 abstract subject ey dene spaces operators arising 21 assume bounded domain r 2 lipschitz boundary introduce solenoidal spaces superscripts denoting closures respective norms dene w endowed norm h equipped norm h denoting dual space v abbreviation l 2 0 set measure zero 0 elements identied elements c0 identied elements c0 v 21 u denotes hilbert space controls r cost functional assumed bounded weakly lower semicontinuous twice frechet dierentiable locally lipschitzean second derivative radially unbounded u ie furthermore control space u identied second order methods flow control 5 dual u simplify notation second derivative also assume functional j decomposed nonlinear mapping dened comparing 11 12 21 note conservation mass well boundary condition realized choice space w dynamics described condition ey variational form constraints 21 equivalently expressed given u 2 u nd 2 w following existence result navierstokes equations dimension two well known cf l chapter iii proposition 21 exists constant c every u 2 u exists unique element u proposition 21 conclude respect existence 21 equivalent subject u 2 u theorem 21 problem 21 admits solution proof formalism proof quite standard give brief outline since j bounded exists minimizing sequence un g w u due radial unboundedness property j u proposition 21 sequence fy n un g bounded w u hence exists subsequence weak limit u 2 w u weak lower semicontinuity u jy u implies remains show achieved passing limit 23 u replaced yu n un shall also require following result concerning strong solutions navierstokes equation theorem iii 310 proposition 22 every u 2 u solution moreover every bounded set u u bounded h 21 q 6 michael hinze karl kunisch shall frequently refer linearized navierstokes system adjoint equations given next ae 0 ae 0 proposition 23 let 3 25 admits unique variational solution v 2 w 26 unique variational solution w rst equation 26 holding l v w moreover following estimates hold iii jwj l 2 addition iv jwj l 2 solutions v 25 w 26 elements h 21 q satisfy apriori estimates v vi proof proof sketched appendix 3 derivatives section representations rst second derivatives j appropriate treatment 24 newton quasinewton method derived shall utilize notation proposition 31 operator continuously differentiable lipschitz continuous second derivative action rst two derivatives e 1 given 1 v r 2 x second order methods flow control 7 proof since e 2 linear restrict attention e 1 let dened recall due assumption u v argue local lipschitz continuity e let pz tjy c denotes constant independent x x due continuous embedding w l 1 h jx r using holders inequality implies estimate jx consequently redening c one last time estimate establishes local lipschitz continuity e verify formula e x given represents frechet derivative e estimate r sup r cjy r frechet dierentiability e follows show lipschitz continuity rst derivative let x x v r x estimate r r cjy expression second derivative veried estimate analogous one rst derivative second derivative independent point taken thus necessarily lipschitz continuous 32 follows 2 l 2 v w 2 w mapping element w section 4 shall use fact also identied element l 4 lemma 31 2 l 2 v w 2 w functional identied element w l 43 v proof argue 2 l 43 using 32 k embedding constant v h gives claim proposition 32 let homeo morphism moreover inverse adjoint e applied element setting w w w w variational solution 26 proof due proposition 31 e x bounded linear operator closed range theorem claim follows argued 25 unique solution direct consequence proposition 23 ii assertion concerning adjoint follows proposition iii proof consequence propositions 31 32 implicit function theorem rst derivative mapping u yu u direction u given xe u xu u chain rule thus obtain introducing variable obtain utilizing proposition 23 iii representation rst derivative variational solution rst equation holds l 43 v w computation second derivative j involved let u v 2 u u note second derivative u yu u w expressed second order methods flow control 9 chain rule since w introduce lagrangian matrix operator xe u x observe second derivative l respect x expressed computation j 00 u together 34 imply 4 second order methods section contains description comparison second order methods solve 21 throughout u denotes local solution 21 41 newtonand quasinewton algorithm sake reference let us specify newton algorithm algorithm 41 newton algorithm 1 choose u 2 convergence ii update u let us consider linear system 2 dimension control space u characterization hessian conclude evaluation requires many solutions linearized navierstokes equation 34 appropriate right hand sides dimension u u innite dimensional appropriate discretization must carried let us assume dimension u large direct evaluation feasible case 2 must solved iteratively e g conjugate gradient technique shall refer 2 inner loop opposed doloop 2 outer loop newton algorithm inner loop iteration level k outer loop requires ii iteratively evaluate action j jth iterate inner loop kth level outer loop iterate j evaluated successively applying steps solve l 2 b evaluate j yy xv c solve w w nally set q j uu u b w recall 1 2 l 2 v 2 w 1 z tz moreover lemma 31 functional appearing b element w hence proposition 23 adjoint equation c equivalently rewritten rst equation holds w l 43 v summarizing outer iteration newton method one navierstokes solve yu k one linearized navierstokes solve required inner loop one forward time well one backwards linearized navierstokes solve per iteration necessary concerning initialization observe initial guesses available 0 necessarily yu 0 alternatively initialization algorithm 41 information used advantageously compute adjoint variable 1 required initial guess right hand side linear system well carry steps c evaluation hessian necessity recompute yu 0 u 0 avoid diculties evaluating action exact hessian algorithm 41 one resort quasinewton algorithms specify one prominent candidates bfgsmethod w z u dene rankone operator z 2 lu action given w bfgsmethod hessian j 00 u approximated sequence operators algorithm 42 bfgsalgorithm 1 choose u second order methods flow control 11 2 convergence ii update u note bfgsalgorithm requires system solves gradient algorithm applied 21 one forward solution nonlinear equation obtain yu k one backward solve linearized equation 37 obtain adjoint variable u k order compare newtons method sqp method derived next section rewrite update step 2 algorithm 41 begin observe right hand side update step written help adjoint variable operator x dened 310 j j dropped iteration indices consequence 33 update written j j holds since e x closed sequence identities x thus exists 2 z e j using equation together denition newtons update may rewritten 2y 42 basic sqpmethod regard 21 minimization problem functional j space x subject explicit constraint sqpalgorithm consists applying newtons method rst order optimality system lagrangian l dened 39 x denoting solution problem p e x surjective proposition 32 hence exists lagrange multiplier 2 z even unique 44 holds sqpmethod well dened locally second order convergent addition surjectivity e x following second order optimality condition holds exists 0 h1 holds due regularity properties e exists neighborhood uniformly positive denite kere x x every algorithm 43 sqpalgorithm 1 choose 2 convergence solve ii update newtons method step 2 dicult one contrast newtons method neither navierstokes equation linearization needs solved dimension system matrix twice dimension state plus dimension control space formidable applications uid mechanics addition experience algorithm 43 optimal control problems see ka v example well known preconditioning techniques must applied solve 45 eciently preconditioner one might consider action operator h inverse discretized instationary stokes operator discretized linearization navierstokes equation state k either one homogenous boundary conditions one iteration preconditioned version algorithm 43 therefore requires two linear parabolic solves one forward one backwards time con sequence even application preconditioning techniques numerical expense counted number parabolic system solves less sqpmethod newtons method however number iterations iterative methods applied solve system equations algorithms 41 43 strongly depends system dimension much larger algorithm 43 algorithm 41 compare structure newton sqpmethods let us assume instance x k feasible primal equation e ex k feasible adjoint equation 35 e e second order methods flow control 13 right hand side 45 formj u comparing computation end section 41 observe linear systems describing newton sqpmethods coincide general nonlinear primal linearized adjoint equation satised iterates sqpmethod therefore refer sqpmethod outer unfeasible method newton method feasible one 43 reduced sqpmethod idea reduced sqpmethod replace 45 equation ker e x x reduced system smaller dimension original one develop reduced system follow lines ks recall denition note rightinverse e x x fact xe u xv proposition 32 due b 2 lu l 2 v operator x isomorphism u ker e x x hence second equality 45 given expressed using rst equality 45 nd x applying x last equation ii implies u solution coordinate 45 also satises u computed 48 obtained 47 requires one forward linear parabolic solve rst equation 45 another backwards linear parabolic solve let us note x feasible rst term right hand side 48 zero 48 identical step 2 newtons algorithm 41 ects fact newtons method viewed sqp method obeys feasibility constraint also points fact amount work measured equation solves inner loop coincides newton reduced sqpmethods signicant dierence two methods lies outer iteration make evident next specify reduced sqpalgorithm 14 michael hinze karl kunisch algorithm 44 reduced sqpalgorithm 1 choose x 2 convergence lagrange multiplier update solve e ii solve iii update note algorithm specied follow procedure outlined update lagrange variable fact reduced sqpmethods optimal update strategy two choices described natural frequently used implement algorithm 44 two linear parabolic systems solved steps 2 2 ii addition two linear parabolic systems necessary evaluate term involving operator right hand side 2 ii applications term often neglected since vanishes x reduced sqpmethod newtons method turn similar let us discuss points dier signicantly velocity eld updated means nonlinear equation newtons method via linearized equation reduced sqpmethod ii right hand sides linear systems dier due appearance term involving operator mentioned term frequently implemented iii formally dierence initialization procedure 0 chosen independently u 0 reduced sqpmethod newtons method however explained section 41 good initial guess 0 independent yu 0 available utilized newtons method well 5 convergence analysis present local convergence results algorithms introduced section 4 cost functionals separable type 22 purpose essential derive conditions ensure positive deniteness h1 key conditions apriori estimates proposition 23 shall also prove dierence compact property required rate convergence analysis quasinewton methods rst result assert positive deniteness hessian provided j x suciently small condition applicable trackingtype problems second order methods flow control 15 lemma 51 positive deniteness hessian let u 2 u assume j yy positive semidenite j uu x 2 lu positive denite hessian u positive denite provided jj xj l 2 v suciently small proof recall 311 solution 37 follows e note u 2 u functional element w since j yy x assumed positive denite j uu x positive denite result follow provided operator norm r e bounded jj xj l 2 v straightforward estimation gives proposition 23 conclude estimate yy x 1 xik lww recall 1 zz using 32 continuity embedding w l 1 h may estimate constant c independent g h therefore applied iii proposition 23 37 lemma 52 let x 2 x denote function dened 35 assumptions lemma 51 j condition h1 satised replaced x proof let v u 2 n e x x v solves 25 proposition 23 v 2 w satises chosen j uu xu u juj 2 u u 2 u nd pt c denotes generic constant independent v u due 35 proposition 23 estimates imply combined 54 claim follows lemma 53 b 2 lu l 2 h dierence compact every u 2 u proof utilizing 52 may rewrite shown summands dene compact operators u purpose let u bounded subset u utilizing proposition 23 conclude xe bounded subset w hence l 2 v since assumption j twice continuously frechet dierentiable respect l 2 v r follows j yy bounded subset l 2 v proposition 23 iii implies consequently e j yy bounded w 2 g since w 2 43 compactly embedded l 2 h cf b 2 lu l 2 h follows fact e precompact u let us turn second addend 55 due lemma 31 proof set bounded subset w l 43 v follows utilizing proposition 23 bounded subset w 2 43 h assumption b 2 lu l 2 h implies second order methods flow control 17 precompact u lemma veried following lemma concerning operators x ax dened 310 46 required analysis reduced sqpmethod lemma 54 mappings let x 7 ax x lz x x 7 x x lu x frechet dierentiable lipschitz continuous derivatives proof immediate consequence ii proposition 23 identities ii iii section 43 together dierentiability properties mapping x 7 e x x position prove local convergence algorithms discussed section 4 throughout assume u local solution 21 set addition general conditions j b e require positive semidenite j uu positive denite jj suciently small h2 holding h1 satised due lemma 51 particular second order sucient optimality condition holds u strict local solution 21 following theorem follows well known results newtons algorithm theorem 51 h2 holds exist neighbourhood uu every u 0 2 uu iterates fu n gn2n newtons algorithm 41 converge quadratically u theorem 52 h2 holds exist neighbourhood uu 0 u positive denite operators h 0 2 lu bfgs method algorithm 42 converges linearly u addition b 2 convergence superlinear proof rst part theorem refer gr section4 example second claim observe dierence lemma 53 claim follows gr theorem 51 see also ks1 theorem 53 assume h2 holds let lagrange multiplier associated x exist neighbourhood ux x z 43 well dened iterates converge quadratically proof since j e twice continuously dierentiable lipschitz continuous second derivative e x surjective proposition 32 h1 satised second order convergence sqpmethod follows standard results see instance ik turn reduced sqpmethod theorem 54 assume h1 holds let denote lagrange multiplier associated x exist neighbourhood ux x reduced sqpalgorithm 44 well dened iterates fx n gn2n converge twostep quadratically x ie positive constant c independent k 2 n proof first note h1 implies positive deniteness neighbourhood ux x lemma 54 mappings x 7 x x 7 ax frechet dierentiable lipschitz continuous derivatives fur thermore utilizing proposition 23 iii lemma a2 shown mapping x 7 x locally lipschitz continuous dened 35 particular implies lagrange multiplier updates k estimate constant c positive depends x supfjj yy xj 2 v l 2 v ux g altogether assumptions corollary 36 k met exists neighbourhood claim follows 6 numerical results present numerical examples rst demonstrate feasibility utilizing newtons method optimal control twodimensional instationary navierstokes equations workstation environment despite formidable size optimization problem total number unknowns primal adjoint control variables example 1 instance order 2210 6 time horizon could still increased mesh size decreased utilizing reduced storage techniques expense additional cputime shall pursue aspect control problem given 11 12 cost function j dened qo qc c subsets denoting control observation volumes respectively examples 400 b indicator function q c results newtons method compared gradient algorithm recall sake convenience algorithm 61 gradient algorithm 1 choose u 0 2 set 3 set 4 2 second order methods flow control 19 given control u evaluation gradient j point u amounts solving 12 state 37 adjoint variable implementing stepsize rule determine approximation numerically expensive every evaluation functional j control u requires solving instationary navierstokes equations right hand side bu compare two possibilities computing approximations optimal step size purpose let us consider search direction 2 u solutions systems associated adjoint variable 1 given search direction 2 u interpolate function quadratic polynomial using values i0 0 0 00 0 ie use unique zero equation 0 approximation w given 63 2 use linearization mapping 7 yu cost functional j results quadratic approximation 2 jyu functional use unique root equation 0 approximation v given 62 denominator ud di u 51 u replaced u follows proof theorem 51 positive provided state yu suciently close z l 2 h let us note computation 1 requires solution linearized navierstokes equations forward backward time whereas requires one solve linearized navierstokes equations addition numerical comparison shows stepsize guess performs better respect number iterations gradient method respect computational time numerical results presented therefore use step size proposal thus every iteration gradient algorithm amounts solving nonlinear navierstokes equations forward time associated adjoint equations backward time computation gradient solving linearized navierstokes equations forward time step size proposal inner iteration newtons method performed conjugate gradient method choice justied neighbourhood local solution u optimal control problem positive deniteness desired state z suciently close optimal state yu numerical tests target ow z given stokes ow boundary condition z tangential direction see fig 1 termination criterion jth iterate u k j conjugate gradient method chosen min initialization newtons method u 0 0 figure 1 control target stokes ow cavity discretization navierstokes equations linearization adjoint carried using parts code developed bansch ba based taylorhood nite elements spatial discretization time step size took resulted 160 grid points time grid 545 pressure 2113 velocity nodes spatial discretization computations performed decalpha tm station 500 iteration cgsteps 6 19 4686819e6 0032 1480534e3 table 1 performance newtons method example 1 example 1 rst present results c table conrms superlinear convergence inexact newton method second order methods flow control 21 achieve accuracy newtons method gradient algorithm requires iterations computing time newtons method approximately minutes whereas gradient method requires 110 minutes demonstrates superiority newtons method gradient algorithm example larger values coarser time space grids dierence computing time less drastic fact dierence increases decreasing increasing mesh renement expected signicant amount computing time spent readwrite actions variables harddisc subproblems figures 2 3 4 evolution cost functional dierence stokes ow control function time documented observed newtons method tends overestimate control rst iteration step whereas gradient algorithm approximates optimal control see figure 4 graphically signicant change second iteration newtons method comments hold quite wide range values fig 5 uncontrolled ow together controlled ow control action end time interval presented28e02 figure 2 newtons method 6 iterations top versus gradient algorithm 96 iterations re400 evolution cost functional relative previous example observation volume control volume c cover whole spatial domain practical point view feasible however numerical standpoint complicated situation since inhomogeneities primal adjoint equations large 22 michael hinze karl kunisch14e02 figure 3 newtons method 6 iterations top versus gradient algorithm 96 iterations re400 evolution dier ence stokes ow relative next present two numerical examples dierent observation control volumes results smaller control observation volumes example 1 thus primal adjoint equations numerically simpler solve example 2 1 spatial temporal discretizations well parameter example 1 newtons method takes 15 minutes cputime convergence statistics presented tab 2 gradient algorithm needs 25 iterations 26 minutes cpu reduce value cost functional iteration cgsteps table 2 performance newtons method example 2 example 3 07 discretization spatial time domain well parameter second order methods flow control 2318e00 figure 4 newtons method 6 iterations top versus gradient algorithm 96 iterations re400 evolution control relative example 2 gradient algorithm needs 38 iterations reduce value cost functional takes 80 minutes cputime also implemented polakribiere variant conjugate gradient algorithm converges 37 iterations yields slightly better reduction residual amount cputime needed nearly equal taken gradient algorithm newtons method faster converges within 7 iterations approximate solution needs cputime average cputime inner iteration loop 75 minutes previous examples average cost conjugate gradient iteration inner loop decreases decreasing residual outeriteration loop results depicted tab 3 appendix proof proposition 23 proof proposition 23 make frequent use following lemma lemma a1 z figure 5 results top bottom uncontrolled controlled ow control force iteration cgsteps table 3 performance newtons method example 3 exists positive constant c positive constant c proof t1 lemma a2 exists positive constant c proof since proof identical lemma 31 note power 43 previous estimate cannot improved requiring sucient conditions ru v u rv 2 l 2 v given requiring addition u v 2 l 1 v proof proposition 23 existence uniqueness solution 25 shown following lines existence uniqueness proof instationary twodimensional navierstokes equations chapiii following sketch derivation necessary apriori estimates test respect time use bu v estimate using youngs inequality rst estimate a1 results integration 0 gronwalls inequality gives using a3 a2 cauchyschwarz inequality yields 26 michael hinze karl kunisch combining a3 a4 yields rst claim ii test 25 2 v pointwise time estimate using cauchyschwarz inequality rst estimate a1 gives z implies together 2 w l 1 h estimates a3 a4 gives ii combining ii implies iii 2 w introduce bounded linear operator ay 2 lwz note ay coincides e x section 3 due operator admits continuous inverse ay 1 2 lz w adjoint ay 2 exists every solution w ii together fact lemma a2 assumption g 2 l 43 v mapping element l v 2 1 43 a8 therefore conclude w 2 l v together w dl5 a8 deduce rst equation 26 well dened l v referring a8 third time utilizing fact wt well dened h follows wt exists constant c combining estimate a9 implies estimate iii iv utilizing a8 nd w 2 l 2 v moreover a1 together a9 gives desired estimate iv v test 25 v pointwise time utilize youngs inequality last estimate a1 obtain integration 0 together a4 results gronwalls inequality gives using a11 yields estimate jv j l 2 h test 25 2 v use last estimate a1 gives z 2 together a13 a14 implies therefore v estimation jwj l 1 2 similar order cope b estimation utilizes third estimate a1 obtain estimate vi 28 michael hinze karl kunisch r lagrangenewton method state constrained optimal control problems control problems uid mechanics numerical solution owcontrol problem vorticity reduction dynamic boundary action instantaneous control backwardfacingstep ows preprint feedback control unsteady ow application stochastic burgers equation mathematical analysis numerical methods science technology boundary value problems optimal boundary control navierstokes system twodimensional case numerical methods nonlinear variational problems optimal control twoand threedimensional incompressible navierstokes flows local convergence broydenlike methods lipschitzean problems hilbert spaces velocity tracking problem navierstokes ows bounded distributed controls formulation analysis sequential quadratic programming method optimal dirichlet boundary control navierstokes ow control strategies uid ows optimal versus suboptimal control augmented lagrangiansqpmethods nonlinear optimal control problems tracking type optimal control thermally convected uid ow reduced sqp methods parameter identi mesh independence gradient projection method optimal control problems control burgers equation reduced order approach using proper orthogonal decomposition bericht nr mathematical topics fluid mechanics modelling control physical processes using proper orthogonal decomposition tr ctr kerstin brandes roland griesse quantitative stability analysis optimal solutions pdeconstrained optimization journal computational applied mathematics v206 n2 p908926 september 2007