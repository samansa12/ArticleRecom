efficient mining association rules distributed databases abstractmany sequential algorithms proposed mining association rules however little work done mining association rules distributed databases direct application sequential algorithms distributed databases effective requires large amount communication overhead study efficient algorithm dma proposed generates small number candidate sets requires messages support count exchange candidate set n number sites distributed database algorithm implemented experimental test bed performance studied results show dma superior performance comparing direct application popular sequential algorithm distributed databases b introduction database mining recently attracted tremendous amount attention database research applicability many areas including decision support marketing strategy financial forecast research community observed data mining together data warehousing data repositories three new uses database technology considered important areas database research 20 many interesting efficient data mining algorithms proposed eg see 2 3 4 5 6 7 8 10 12 13 15 16 17 19 21 databaseoriented mining algorithms classified research authors supported part rgc hong kong research grants council grant 3380650026 two categories concept generalizationbased discovery discovery primitive concept levels former relies generalization concepts attribute values stored databases one example dbminer system 7 12 latter discovers strong regularities rules database without concept generalization association rule 4 6 16 important type rules latter approach algorithms mining association rules proposed far sequential algorithms algorithm pdm proposed recently parallel mining association rules 17 adaptation dhp algorithm parallel environment 16 another algorithm count distribution cd adaptation apriori algorithm also proposed parallel mining environment implementation ibm sp2 5 best knowledge little work done mining association rules distributed database environment paper developed distributed algorithm dma distributed mining association rules used solve problem distributed database model horizontally partitioned database database schema partitions ie records transactions set items dma modified case schema different sites completely identical many distributed databases horizontally partitioned example retail chain may several regional data centers manages transaction records region important mine association rules based data centers distributed mining applied many applications data sources located different places sequential environment many algorithms proposed mining association rules popular apriori dhp partition algorithms 6 16 19 candidate set generation function apriorigen adopted apriori algorithm supports efficient method candidate set generation dhp applies hashing technique prune away size2 candidate sets improve efficiency partition divides database small partitions processed efficiently memory independently find large itemsets large itemsets partitions combined form set candidate sets following one scan database required find large itemsets candidates parallel environment pdm algorithm proposed 17 tries parallelize dhp algorithm node computes globally large itemsets exchanging support counts counts referred literatures candidate sets order apply hashing technique nodes broadcast hashing result causes huge amount communication 17 technique proposed decrease number messages amount hash buckets total count larger threshold selected bucket count exchange buckets broadcasted node receives partial count selected buckets polls sites get total counts however two unfavourable features proposal firstly reduction candidate sets done second iteration number candidate sets iterations could also quite large secondly find large candidate sets 2 messages required support count exchange candidate set n number nodes another algorithm proposed parallel mining association rules cd algorithm 5 adaptation apriori algorithm parallel case iteration generates candidate sets every site applying apriorigen function set large itemsets found previous iteration every site computes local support counts candidate sets broadcasts sites subsequently sites find globally large itemsets iteration proceed next iteration algorithm simple communication scheme count exchange however also similar problems higher number candidate sets larger amount communication overhead efficiency algorithm dma developed attributed mainly following two features 1 apriori dhp generate candidate sets applying apriorigen function large itemsets found previous iteration cd pdm use technique parallel environment dma uses new technique generate much smaller set candidate sets either apriori dhp explained section 32 2 dma determine whether candidate set large messages needed support count exchange much less straight adaptation apriori requires 2 messages support count exchange distributed database intrinsic data skewness property distribution itemsets different partitions identical many items occur frequently partitions others example distributed database national supermarket chain expected consumers purchasing patterns new york city quite different los angles result many itemsets may large locally sites necessarily sites skewness property poses new requirement design mining algorithm furthermore dma applied mining association rules large centralized database partitioning database nodes distributed system particularly useful data set large sequential mining extensive experiments conducted study performance dma compare algorithm count distribution cd direct application apriori algorithm distributed databases remaining paper organized follows brief summary mining association rules sequential environment discussed section 2 section 3 problem mining association rules distributed database defined important results discussed algorithm dma presented section 4 performance study discussed section 5 discussion conclusions presented sections 6 7 2 sequential mining association rules 21 association rules set items let db database transactions transaction set items given itemset x transaction contains x x association rule implication form x association rule x holds db confidence c c transactions db contain x also contain association rule x support db transactions db contain x given minimum confidence threshold minconf minimum support threshold minsup problem mining association rules find association rules whose confidence support larger respective thresholds also call association rule strong rule distinguish weak ones ie meet thresholds 13 itemset x support defined similarly percentage transactions db contains x also use xsup denote support count number transactions db containing x given minimum support threshold minsup itemset x large support less minsup moreover presentation purpose call itemset sizek kitemset shown problem mining association rules reduced two subproblems 4 1 find large itemsets predetermined minimum support 2 generate association rules large itemsets found crucial factor affects performance mining association rules find efficient method resolve first problem 6 22 apriori algorithm apriori algorithm one popular algorithm mining association rules centralized database main idea apriori outlined following 6 1 large itemsets computed iterations iteration database scanned large itemsets size computed large itemsets computed ascending order sizes 2 first iteration size1 large itemsets computed scanning database subsequently kth iteration k 1 set candidate sets c k created applying candidate set generating function apriorigen l set large 1itemsets found iteration k gamma 1 apriorigen generates kitemset whose every 1itemset subset l kgamma1 support counts candidate itemsets c k computed scanning database sizek large itemsets extracted candidates two interesting extensions apriori algorithm dhp 17 partition algorithms 19 first iteration computing support counts size1 itemsets dhp stores support counts size2 candidate itemsets hash table upper bounds support counts size2 candidates deduced hash table used prune away size2 candidates second iteration result hashing pruning cost computing support counts size2 candidate sets reduced substantially dhp partition algorithm divides database partitions processed efficiently memory find itemsets large set consists itemsets becomes candidate set finding large itemsets database advantage partition algorithm one scan database required candidate sets found partitions 3 mining association rules distributed databases 31 problem description let db partitioned database located n sites database partitions sites g following adopt convention attaching superscript notation denote corresponding distributed notation site let size db partitions db respectively given itemset x let xsup xsup respective support counts x db db call xsup global support count xsup local support count x site given minimum support x globally large xsup theta correspondingly x locally large site following use l denote globally large itemsets db l k denote globally large kitemsets l problem mining association rules distributed database db reduced finding globally large itemsets 32 generate smaller set candidate sets discuss generate small set candidate sets first present interesting useful observations first found many candidate sets generated applying apriorigen function needed search large itemsets fact natural effective method every site generate set candidate sets typically much smaller set candidate sets following every site needs find large itemsets among candidate sets using technique achieved effective division mining task amongst sites database following several lemmas theorem described illustrate observations lemma 1 itemset x locally large site subsets also locally large proof follows definition locally large 2 similar result lemma 1 centralized database first appeared 4 itemset x globally large exists site 1 n x subsets locally large site proof cannot globally large therefore x must locally large site follows lemma 1 subsets x must locally large 2 site itemset x locally large site globally large say x heavy site use hl denote set heavy itemsets site hl k denote set heavy kitemsets site dma heavy itemsets site play important role generation candidate sets lemma 3 itemset x globally large exists site 1 n x heavy site proof since x globally large follows lemma 2 x must locally large site heavy site 2 lemma 4 itemset x heavy site 1 n subsets also heavy proof x heavy site must globally large therefore subsets globally large moreover since x locally large site follows lemma 1 subsets x must locally large site hence subsets heavy site 2 lemma 4 interesting property shows heavy itemsets site monotonic subset relationship among relationship also exists among large itemsets centralized case necessary condition large itemsets computed iteratively lemma 5 x 2 l k ie x globally large kitemset exists site 1 n x size heavy site proof follows lemmas 3 4 2 lemma 5 equivalent combination lemma 3 lemma 4 basis design effective method generate smaller set candidate sets distributed environment general straightforward adaptation apriori kth iteration set candidate sets would generated applying apriorigen function l kgamma1 denote set candidate sets ca k stands sizek candidate sets apriori order words site let ch k set candidates sets generated applying apriorigen ch stands candidate sets generated heavy itemsets hence ch k generated subset l kgamma1 according lemma 5 every large itemset x 2 l k exists site subsets x heavy site k site therefore use ch k denote set n k theorem 1 every k 1 set large kitemsets l k subset ch hence ch k set candidate sets sizek large itemsets proof proof follows lemma 5 discussion 2 since every hl theorem 1 subset l kgamma1 number candidate sets ch k general smaller ca k dma use result theorem 1 generate set candidate sets ch k site iteration seen set candidate sets typically much smaller direct application apriorigen l k following example 1 used illustrate reduction candidate sets using theorem 1 example 1 assuming 3 sites database db partitions db 1 db 2 db 3 first iteration suppose set large 1itemsets bcdefgg bc locally large site 1 bcd locally large site 2 e f g locally large site 3 therefore hl 1 gg follows theorem 1 set size2 candidate sets site 1 equal ch 1 fabbcacg similarly ch 2 feffgegg hence set candidate sets large 2itemsets ch 8 candidates however apriorigen applied l 1 set candidate sets would 21 candidates shows technique theorem 1 effective reducing candidate sets33 local pruning candidate sets previous subsection shown set ch k typically much smaller set candidate sets ca k find globally large itemsets subsequent generation ch k support count exchange done however observed candidate sets ch k pruned away using local information count exchange starts lemma 5 x globally large kitemset must exist site k x heavy site consequence x must locally large site therefore site prune away candidates ch k locally large words compute large kitemsets site dma restrict search domain sets k locally large site convenience use k denote candidate sets ch k locally large site follows discussion every iteration loop counter k dma computes heavy kitemsets site according following procedure 1 candidate sets generation generate candidate sets ch based heavy itemsets found site site actually responsible generating set candidate sets hence computing set large itemsets 2 local partition scanning x 2 ch k scan partition db compute local support count xsup 3 local pruning x 2 ch k x locally large site pruned away remaining candidate sets stored k pruning removes x candidate set site x could still candidate set site 4 support count exchange broadcast candidate sets k sites collect support counts compute global support counts find heavy kitemsets site j 6 received request support counts need scan partition compute support counts counts computed advance step 2 detail discussion section 41 5 broadcast mining result broadcast heavy kitemsets found sites following extend example 1 example 2 illustrate execution procedure clarity purpose list notations used far discussion table 1 number transactions database db support threshold minsup k set globally large kitemsets k set candidate sets generated l k xsup global support count itemset x number transactions partition db k set heavy kitemsets site k set candidate sets generated hl k set locally large kitemsets ch xsup local support count itemset x site table 1 notation table example 2 example 1 assume database 150 transactions one 3 partitions 50 transactions also assume support threshold illustrated example 1 second iteration candidate sets generated site 2 ch 2 site 3 fefegfgg order compute large 2itemsets dma first computes local support counts site result recorded table 2 last three rows local support counts candidate sets corresponding sites example candidate sets site 1 listed first column local support counts listed second column table 2 seen acsup therefore ac locally large hence candidate set ac pruned away site 1 hand ab bc table 2 locally large itemsets enough local support counts survive local pruning hence 1 fabbcg similarly bd pruned away site 2 2 fbccdg remaining candidate set site 3 ef ie 3 fefg local pruning number size2 candidate sets reduced half original size local pruning completed site broadcasts messages containing remaining candidate sets sites collect support counts result count support exchange recorded table 3 locally large request broadcast xsup 1 xsup 2 xsup 3 xsup candidate sets sites table 3 globally large itemsets request support count ab broadcast 1 site 2 3 counts sent back recorded site 1 second row table 3 rows record similar count exchange activities sites end iteration site 1 finds heavy hence heavy 2itemset site 1 hl 1 fbcg similarly hl 2 fefg broadcast heavy itemsets sites return large 2itemsets fbccdefg terms message communication example candidate sets locally large one site one one broadcast receive needed however candidate set bc messages broadcast 1 2 efficient single broadcast case section 34 optimization technique eliminate duplication discussed 34 message optimization finding large itemsets straight adaptation sequential apriori algorithm number candidate sets generated larger number messages count exchange candidate set also large due broadcast every candidate set sites requires 2 messages total candidate set n number partitions dma candidate set x locally large site needs messages collect support counts x general candidate sets locally large sites data skewness property percentage overlappings locally large candidate sets different sites small therefore cases dma requires much less messages candidate set ensure dma requires messages every candidate set cases optimization technique introduced achieve single broadcast dma uses simple assignment functions could hash function determine polling site candidate set candidate set x polling site responsible broadcasting polling request collecting support counts determine whether x large since one polling site candidate set x number messages required count exchange x kth iteration local pruning phase completed site dma uses following procedure polling 1 candidates sent polling sites acts home site candidate sets every polling finds candidate sets whose polling site j stores k ie candidates divided groups according polling sites local support counts candidate sets also stored corresponding set ij sends k corresponding polling site j 2 polling site send polling requests acts polling site receives ji k sent sites every candidate set x received finds list originating sites x sent broadcasts polling requests sites list collect support counts 3 remote site reply polling requests acts remote site reply polling requests sent every polling request pi k polling site p sends local support counts candidates pi k back p need scan partition find local support counts found already local pruning please see section 41 details 4 polling site compute acts polling site compute heavy itemsets receives support counts sites computes global support counts candidates k finds heavy itemsets eventually broadcasts heavy itemsets together global support counts sites example 3 example 2 assuming 1 assigned polling site ab bc 2 assigned polling site cd 3 assigned polling site ef following assignment site 1 responsible polling ab bc simple case ab 1 sends polling requests 2 3 collect support counts bc locally large 1 2 pair hbcbcsup sent 1 2 receives message sends polling request remaining site 3 support count received 3 1 finds heavy itemset 1 using polling site dma eliminated double polling messages bc4 algorithm distributed mining association rules section present dma algorithm dma detail based discussion description algorithm discuss technique computing local support counts candidate itemsets different sites performing one single scan partition 41 optimizing partition scanning count exchanges site dma find two sets support counts order local pruning count exchange first set local support counts candidate sets generated site candidate sets sets ch k described theorem 1 hash tree used store support counts candidate sets 6 scan partition db needed compute counts store hash tree hand order answer polling requests sites second set support counts candidate sets generated sites needed counts computed requests received second scan partition unavoidable order avoid two scans dma required find two sets support counts one scan partition store counts hash tree possible heavy sets candidate set generation available sites end iteration according theorem 1 site set candidate sets generated kth iteration hand generated site j ch j available compute candidate sets put hash tree scan local support counts starts words every site needs scan partition find local support counts itemsets ch technique two sets support counts required local pruning count exchange found single scan partition therefore number scans dma minimized comparable sequential case since every site set candidate sets ch k need send itemset names polling request positions ordered list itemsets ch k required would optimize message size needed count exchange 42 dma algorithm section present dma algorithm details algorithm 1 dma distributed mining association rules algorithm input 1 db database partition site size equal 2 minimum support threshold submitted site output l set large itemsets db returned every site method iterates following program fragment distributively site starting iteration loop counter algorithm terminates either l k returned empty set candidate sets ch k empty scan db compute 1 array containing size1 itemsets db local support counts site else f generate sizek candidate sets scan db built hash tree k contains candidate sets ch k support counts site n polling compute locally large candidates divide according polling sites send candidates polling sites send ij k site receive candidates polling site receive ji x 2 ji f store x lp update xlarge sites lp k record sites x locally large g send polling requests polling site collect support counts x 2 lp f broadcast polling requests x sites j j 62 xlarge sites receive xsup j sites j j 62 xlarge sites compute global support counts heavy itemsets x 2 lp f xsup theta insert x h filter heavy kitemsets broadcast h receive h j k sites j j 6 return performance study dma done indepth performance study dma confirm analysis efficiency dma implemented sharenothing distributed system using pvm parallel virtual machine 11 10mb lan used connect six rs6000 workstations running aix system perform study database experiment composed synthetic data order study performance dma also implemented algorithm cd test bed iteration cd generates candidate sets every site applying apriori gen function set large itemsets found previous iteration every site computes local support counts candidate sets broadcasts sites sites find globally large itemsets iteration performed two experiments compare performance dma cd first experiment test bed fixed number sites aim perform comparison respect different support thresholds database sizes second experiment threshold database size fixed performance two algorithms compared respect different number sites result first experiment described detail section 51 second experiment presented section 52 databases used experiments synthetic data generated using techniques introduced 6 16 parameters used similar 16 table 4 list parameters values used synthetic databases readers familiar parameters refer 6 16 following use notation txiydm denote database 51 performance comparison different thresholds database sizes first experiment test bed consists three sites purpose experiment compare performance dma cd respect different thresholds database parameter interpretation value number transactions database db size transactions 10 j mean size maximal potentially large itemsets 4 number potentially large itemsets 2000 n number items 1000 c r correlation level 05 multiplying factor 1260 2400 table 4 parameter table sizes site local disk partition loaded local disk experiments start three partitions generated separately using parameters values table 4 order control skewness partitions two control parameters introduced two parameters primary range r p secondary range r primary range interval items secondary range subinterval primary range items range 1 1000 possible pair primary secondary ranges could r described 16 itemsets generated groups similar itemsets size group controlled clustering size q size itemsets poisson distribution synthesizing model first itemset group picked randomly primary range r p itemsets group contain two parts head tail head random extraction first itemset generated head cannot fill itemset size tail picked randomly secondary range r itemsets generated within primary range clustering secondary range therefore generate databases certain skewness towards secondary range data skewness distributed database controlled using different primary secondary ranges different partitions table 5 primary secondary ranges three partitions first experiment listed first two partitions skewed towards ranges 1 700 300 1000 respectively third partition db 3 generated two clustering ranges two disjoint pools large itemsets used synthesizing db 3 first one range pair 1 550 1 400 second one range pair 450 1000 600 1000 half transactions picked first pool half second pool together three partitions exhibit certain degree skewness experiment sizes databases range 100k 900k transactions minimum support threshold ranges 075 2 number candidate sets dma different site number cd remains sites comparing dma cd experienced average 65 reduction number candidate sets every site figure 1 average number candidate sets generated partition primary range secondary range table 5 partition primary secondary ranges t10i4d500k501502 150 1 075 minimum support average number candidate sets per site dma cd t10i4d500k030342 150 1 075 minimum support average number candidate sets dmacd figure 1 candidate sets reduction dma cd site database size 500k transactions plotted support thresholds dma much less candidate sets cases difference increases support decreases database ratios number candidate sets dma cd presented also figure 1 figure shows reduction number candidate sets dma cd 65 gamma 70 comparison number candidate sets per site result direct implication reduction total number messages required one site generate messages candidate set polling reduction total messages required bigger candidate sets comparing dma cd experienced reduction 90 total message size cases figure 2 database 500k total message size needed dma cd plotted support thresholds moreover ratios total message sizes dma cd presented figure reduction larger support threshold smaller ie large itemsets bar chart figure 2 seen dma requires 6 gamma 12 messages cd also compared execution time dma cd database 500k dma 7 25 faster cd depending support threshold figure 3 execution time dma cd plotted thresholds 500k database ratios speedup presented figure bar chart database sizes experiment best speedup reach 55 150 1 075 minimum support total size message transmitted dma cd t10i4d500k0040122 150 1 075 minimum support message size transmitted dmacd figure 2 message size reduction even though speedup experiment substantial seem significant reduction message size main reason overhead communication relatively small test bed dma running distributed database whose partitions placed far apart locations speedup significant t10i4d500k40012002 150 1 075 minimum support execution time seconds dma cd t10i4d500k1122 150 1 075 minimum support execution time cddma figure 3 execution time speed experiment also compared dma cd series 5 databases 100k 900k transactions terms candidate sets total message size reduction improvement dma cd steady figure 4 average number candidate sets per site dma compared cd 5 databases threshold 075 ratios plotted figure result shows percentage reduction 70 cases figure 5 total size message communication dma compared cd 5 databases threshold 075 ratios presented figure shows reduction 88 89 cases figure 6 execution time dma compared cd 5 databases threshold 075 ratios plotted figure dma 18 55 faster cd database size average number candidate sets dmacd figure 4 candidate sets reduction t10i4d100k1m s075011012100k 300k 500k 700k 900k database size message size transmitted dmacd figure 5 message size reduction 52 performance comparison different number sites second experiment test bed consists six rs6000 workstations synthetic database generated similar first experiment aim experiment compare dma cd number sites changes following describe result comparison number sites varies three six size database 200k transactions partitioned equally across sites minimum support threshold 3 similar first experiment found significant reduction number candidate sets total message sizes cases number sites 3 4 5 6 respectively figure 7 average number candidate sets per sites compared dma cd reduction 75 gamma 90 witnessed dma figure 8 ratios total message sizes two algorithms presented dma 85 gamma 90 reduction message sizes cases lastly execution time ratios described figure 9 dma shown 25 gamma 35 faster cd cases general performance dma depends distribution data across partitions itemsets distributed higher skewness among partitions techniques local pruning candidate set generation reduction dma would powerful comparing database size execution time cddma figure execution time speed number nodes average number candidate sets dmacd figure 7 candidate sets reduction results two different experiments observed dma performs better number nodes higher could consequence higher data skewness due increased number partitions 6 discussion efficiency dma attributed three techniques 1 candidate sets generation 2 local pruning 3 messages optimization described dma local information available partition considered local pruning take advantage global information available pruning support count exchange starts fact end iteration polling site candidate set x knows global support count x also local support counts x set local support counts broadcasted sites together x end iteration discuss optimization technique makes use global information prune candidate sets x kitemset respect partition db 1 n use maxsup x number nodes message size transmitted dmacd figure 8 message size reduction number nodes execution time dmacd figure 9 execution time denote minimum value local support counts size subsets x ie 1g follows subset relationship upper bound local support count xsup hence sum upper bounds partitions denoted maxsupx upper bound xsup ie xsup x note maxsupx computed every site beginning kth iteration since maxsupx upper bound global support count used pruning ie cannot candidate set call technique global pruning global pruning combined local pruning form different pruning strategies following outline three possible strategies 1 local pruning followed global pruning local pruning site apply global pruning remaining candidate sets upper bound maxsupx candidate set x fine tuned since xsup available local pruning upper bound computed site effective value maxsupx global pruning 2 global pruning followed local pruning use upper bound maxsupx prune away candidate sets site apply local pruning remaining candidate sets extreme case may use global pruning without local pruning 3 global pruning polling site local pruning done site pruning phase candidate set x additional pruning done polling site let p polling site x gamma set originating sites requests polling sent sites gamma local support counts x sent p already site j gamma since x locally large j polling site deduce local support count xsup j bounded value minmaxsup j x theta j therefore upper bound xsup computed upper bound x used prune away candidate sets polling site starts collect support counts effectiveness global pruning depends data distribution example let ab candidate set size1 subset locally large 1 small locally large 2 subset b small 1 large 2 global pruning deduced ab globally large hand b large 1 small 2 cannot deduced global pruning ab small fact choice appropriate global pruning strategy depend data distribution additional cost global pruning storage required store local support counts message communication broadcast support counts tradeoff cost reduction candidate sets depend data distribution well number partitions believe globalpruning pay distribution data certain degree skewness additional performance study required order investigate technique hashing technique relaxation factor proposed pdm integrated techniques dma 17 example selection hash buckets broadcasting local pruning technique used also relaxation factor support threshold used increase amount information available polling site global pruning another point worthy mention original count distribution algorithm proposed 5 designed high performance parallel environment improved introducing polling sites decrease amount message communication required merit requires less synchronization fact high performance parallel environment dma cd combined form hybrid algorithm less candidate sets cd slightly message communication dma less synchronization investigate future study another issue related performance mining association rules distributed database difference partition sizes algorithms dma cd require synchronization iteration large size difference partitions would favourable performance possible solution would divide large partitions equalize sizes would reduce time synchronization however tradeoff would message communication 7 conclusion studied efficient algorithm mining association rules distributed databases developed method reduces number candidate sets partition effectively using local pruning communication scheme count exchange optimized using polling sites method implemented performance studied compared direct application popular sequential algorithm study shows proposed technique superior performance mining association rules distributed databases efficiency local pruning enhanced global pruning local support counts stored sites also discussed possibility integrating techniques dma pdm recently interesting studies finding multiplelevel generalized association rules large transaction databases 13 21 extension techniques dma mining multiplelevel generalized association rules distributed database interesting problems research experimental purposes planning implement dma related algorithms ibm sp2 system 32 nodes study problem mining association rules parallel system high speed communication r efficient similarity search sequence databasess interval classifier database mining applications database mining performance perspective mining association rules sets items large databases parallel mining association rules design implementation experience fast algorithms mining association rules knowledge discovery databases rulebased attributeoriented approach maintenance discovered association rules large databases incremental updating technique advances knowledge discovery data mining knowledge discovery databases overview pvm parallel virtual machine datadriven discovery quantitative rules relational databases discovery multiplelevel association rules large databases finding interesting rules large sets discovered association rules efficient effective clustering method spatial data mining effective hashbased algorithm mining association rules efficient parallel data mining association rules knowledge discovery databases efficient algorithm mining association rules large databases database achievements opportunities 21st century mining generalized association rules principles database knowledgebase systems tr ctr takahiko shintani masaru kitsuregawa parallel mining algorithms generalized association rules classification hierarchy acm sigmod record v27 n2 p2536 june 1998 murat kantarcioglu chris clifton privacypreserving distributed mining association rules horizontally partitioned data ieee transactions knowledge data engineering v16 n9 p10261037 september 2004 satoshi morinaga kenji yamanishi junichi takeuchi distributed cooperative mining information consortia proceedings ninth acm sigkdd international conference knowledge discovery data mining august 2427 2003 washington dc r j miller yang association rules interval data acm sigmod record v26 n2 p452461 june 1997 jianning dong william perrizo qin ding jingkai zhou application association rule mining remotely sensed data proceedings 2000 acm symposium applied computing p340345 march 2000 como italy thomas legler wolfgang lehner andrew ross data mining sap netweaver bi accelerator proceedings 32nd international conference large data bases september 1215 2006 seoul korea shichao zhang chengqi zhang jeffrey xu yu efficient strategy mining exceptions multidatabases information sciences international journal v165 n12 p120 3 september 2004 jian tang using incremental pruning increase efficiency dynamic itemset counting mining association rules proceedings seventh international conference information knowledge management p273280 november 0207 1998 bethesda maryland united states new distributed data mining model based similarity proceedings acm symposium applied computing march 0912 2003 melbourne florida vincent cho beat wthrich distributed mining classification rules knowledge information systems v4 n1 p130 january 2002 mohammed javeed zaki srinivasan parthasarathy wei li localized algorithm parallel association mining proceedings ninth annual acm symposium parallel algorithms architectures p321330 june 2325 1997 newport rhode island united states mohammed j zaki srinivasan parthasarathy mitsunori ogihara wei li parallel algorithms discovery association rules data mining knowledge discovery v1 n4 p343373 december 1997 wenchih peng mingsyan chen developing data allocation schemes incremental mining user moving patterns mobile computing system ieee transactions knowledge data engineering v15 n1 p7085 january jaideep vaidya chris clifton privacy preserving association rule mining vertically partitioned data proceedings eighth acm sigkdd international conference knowledge discovery data mining july 2326 2002 edmonton alberta canada w cheung lee v xiao effect data skewness workload balance parallel data mining ieee transactions knowledge data engineering v14 n3 p498514 may 2002 john holt soon chung parallel mining association rules text databases journal supercomputing v39 n3 p273299 march 2007 xindong wu shichao zhang synthesizing highfrequency rules different data sources ieee transactions knowledge data engineering v15 n2 p353367 february hongjun lu ling feng jiawei han beyond intratransaction association analysis mining multidimensional intertransaction association rules acm transactions information systems tois v18 n4 p423454 oct 2000 frans coenen paul leng partitioning strategies distributed association rule mining knowledge engineering review v21 n1 p2547 march 2006 ling feng jeffrey xu yu hongjun lu jiawei han template model multidimensional intertransactional association rules vldb journal international journal large data bases v11 n2 p153175 october 2002 miroslav kubat alaaeldin hafez vijay v raghavan jayakrishna r lekkala wei kian chen itemset trees targeted association querying ieee transactions knowledge data engineering v15 n6 p15221534 november boris rozenberg ehud gudes association rules mining vertically partitioned databases data knowledge engineering v59 n2 p378396 november 2006 jaideep vaidya chris clifton secure set intersection cardinality application association rule mining journal computer security v13 n4 p593622 july 2005 euihong sam han george karypis vipin kumar scalable parallel data mining association rules ieee transactions knowledge data engineering v12 n3 p337352 may 2000 qing li ling feng allan wong intratransaction generalized intertransaction landscaping multidimensional contexts association rule mining information sciencesinformatics computer science international journal v172 n34 p361395 9 june 2005 antonin rozsypal miroslav kubat association mining timevarying domains intelligent data analysis v9 n3 p273288 may 2005 kubat searching highsupport itemsets itemset trees intelligent data analysis v10 n2 p105120 march 2006 xindong wu chengqi zhang shichao zhang database classification multidatabase mining information systems v30 n1 p7188 march 2005 parag c pendharkar girish subramanian connectionist evolutionary models learning discovering forecasting software effort managing data mining technologies organizations techniques applications idea group publishing hershey pa ycel saygin zgr ulusoy exploiting data mining techniques broadcasting data mobile computing environments ieee transactions knowledge data engineering v14 n6 p13871399 november 2002 b park h kargupta e johnson e sanseverino hershberger l silvestre distributed collaborative data analysis heterogeneous sites using scalable evolutionary technique applied intelligence v16 n1 p1942 januaryfebruary 2002 vipin kumar mohammed zaki high performance data mining tutorial pm3 tutorial notes sixth acm sigkdd international conference knowledge discovery data mining p309425 august 2023 2000 boston massachusetts united states