increasing power efficiency multicore network processors data filtering propose evaluate data filtering method reduce power consumption highend processors multiple execution cores although proposed method applied wide variety multiprocessor systems including mpps smps type singlechip multiprocessor concentrate network processors proposed method uses execution unit called data filtering engine processes data low temporal locality placed system bus execution cores use locality decide load instructions low temporal locality portion surrounding code offloaded data filtering engineour technique reduces power consumption low temporal data processed data filtering engine placed onto high capacitance system bus b conflict misses caused low temporal data reduced resulting fewer accesses l2 cache specifically show technique reduces bus accesses representative applications much 468 265 average reduces overall power much 156 86 average singlecore processor also improves performance much 767 297 average processor execution cores b introduction power traditionally limited resource one important design criteria mobile processors embedded systems due part increased logic density power dissipation high performance processors also becoming major design factor increased number transistors causes processors dissipate heat return reduces processor performance reliability network processors npus processors optimized networking applications recently processing elements networks either generalpurpose processors asic designs since generalpurpose processors software programmable flexible implementing different networking tasks asics hand typically better performance however change protocol application hard reflect change design increasing number new protocols increasing link speeds need processing elements satisfy processing flexibility requirements modern networks npus fill gap combining network specific processing elements software programmability paper present architectural technique reduce power consumption multicore processors specifically present simulation results showing misses networking applications caused instructions b devise power reduction technique combination locality detection mechanism execution engine c discuss finegrain technique offload code segments engine conduct simulation experiments evaluate effectiveness technique although technique efficiently employed variety multiprocessor systems concentrate npus npus follow singlechip multiprocessor design methodology 18 eg intel ixp 1200 11 7 execution cores ibm powernp 12 17 execution cores addition chips consume significant power ibm powernp 12 consumes 20w typical operations whereas cport c5 7 consumes 15w multiple execution cores often connected global system bus shown figure 1 capacitive load processors inputoutput drivers usually much larger orders magnitude internal nodes processor 23 consequently significant portion power consumed bus figure 1 generic network processor design location proposed dfe system bus permission make digital hard copies part work personal classroom use granted without fee provided copies made distributed profit commercial advantage copies bear notice full citation first page copy otherwise republish post servers redistribute lists requires prior specific permission andor fee cases 2002 october 811 2002 grenoble france technique uses two structures achieve desired reduction power consumption first shared global memory connected system bus execution unit named data filtering engine dfe activated dfe passes data bus hence effect execution processors dfe activated execution core processes data places results bus goal process lowtemporal data within dfe number bus accesses reduced reducing bus accesses cache misses caused low temporal accesses technique achieves significant power reduction processor second low temporal accesses determined execution cores using locality prediction table lpt table stores information prior loads lpt stores program counter pc instruction well misshit behavior recent executions instruction lpt also used determine section code offloaded dfe details lpt explained section 21 following subsection present simulation numbers motivating use data filtering power reduction instructions causing dl1 misses 0 10 20 30 40 50 70 80 90 100 crc dh drr md5 nat url ipc rou tl avg fraction inst 5 inst 25 inst 125 inst 625 inst figure 2 number instructions causing dl1 misses 11 motivation many applications majority misses l1 data cache caused instructions 1 performed set simulations see cache miss distribution different instructions networking applications simulated several applications netbench suite 20 simulations performed using simplescalar simulator 17 processor configuration explained section 41 results presented figure 2 figure gives percentage data misses caused different number instructions example crc application approximately 30 misses caused single instruction 80 misses caused five instructions highest number misses average 58 misses caused five instructions 87 misses caused 25 instructions addition performed another set experiments see type data accessed causes misses simulations reveal 66 misses occur processor reading packet data advantage instructions causing majority misses offload instructions dfe reduce number cache misses l2 accesses hence bus accesses significantly next section present results combined split cache cache bypassing mechanism discuss disadvantages locality enhancement techniques section 21 explains details lpt section 3 discuss design dfe show design options varied section 4 presents experimental results section 5 give overview related work section 6 concludes paper summary 2 reducing l1 cache accesses several cache localityenhancing mechanisms proposed literature 9 13 14 25 26 power implications mechanisms examined bahar et al 4 techniques try improve performance reducing l1 cache misses since l1 misses reduced intuitively techniques also reduce power consumption due less l1 l2 cache activity section first examine power implications representative cache locality enhancing mechanism combined split cache cache bypassing mechanism proposed gonzalez et al 9 study split cache technique proposed mechanism uses advanced version mechanism detect code segments offloaded mechanism l1 data cache split two structures one storing data temporal locality temporal cache storing data spatial locality spatial cache processor uses locality prediction table categorize instructions accessing data temporal locality temporal instructions b accessing data spatial locality spatial instructions c accessing data locality bypass instructions detailed simulation results explained section 4 although technique reduces number execution cycles applications impact overall power consumption applications technique increases power consumption data caches reasons increase twofold lpt structure accessed every data access two smaller caches one larger linesize accessed parallel instead single cache nevertheless overall power consumption reduced technique due significant reduction bus switching activity following first explain lpt presented gonzalez et al 9 discuss enhancements required implement proposed technique 21 locality prediction table lpt small table used data accesses processor conjunction pc instructions stores information past behavior access last address accessed size access misshit history prediction made using fields lpt considering stride past behavior decision made whether data accessed instruction placed temporal cache spatial cache uncached modified original lpt accommodate information required dfe added three fields lpt start address code offloaded dfe sadd field end address code offloaded eadd field variables registers required execute offloaded code lreg field assuming 32bit address register file size 32 three fields require additional 128 1 bits line lpt functions fields explained following section discuss design dfe 1 lreg field 2bits register explained section 31 assumed 32 registers execution cores hence 64 bits required lreg field 3 figure 4 presents dfe design dfe execution core additional features control passing memory data bus memory request originated execution core pass gate opened request transfers bus usual request originated dfe controller closes pass gate forwards data either dfe data cache dfe instruction cache experiments explained section 4 dfe equipped 2 kb data instruction caches compared 4 kb data instruction caches execution cores two differences generalpurpose execution cores dfe first dfe controller equipped additional logic check whether code executed requests register value generated dfe communicated execution cores case dfe communicates master create interrupt execution core offloaded code segment dfe second dfe codemanagement unit cmu keep track origin code executed sum arrayi figure 3 code segment showing effectiveness dfe figure 4 dfe design dfe located onchip l2 cache execution cores activateddeactivated execution cores active executes code segment determined core communicates necessary results back core consider code segment figure 3 code segment executed one cores core read complete array means system bus accessed several times array structure going used accesses result unnecessary power consumption processor code segment executed dfe hand bus accessed initiate execution get result sum dfe besides reducing number bus accesses offloading segment also positive impact execution cores cache replacements due accessing array prevented note code segments executed dfe limited loops discuss code segments offloaded dfe section 31 31 determining dfe code execution core detects instruction accesses low temporal data instructions categorized spatial bypass instructions lpt starts gathering information code segment containing instruction first run loop contains candidate pc detected executing load execution core checks branch instructions jump examined pc destination branch start containing loop procedure callreturn instructions pc instruction stored eadd field lpt corresponds last instruction code offloaded core found containing loop destination jump stored sadd field corresponds start code offloaded containing loop core detected procedure callreturn instruction examined pc stored sadd field start end addresses detected execution core gathers information register values required code offloaded source registers instruction marked required lpt generated within code destination register marked generated marked required marking performed help lreg field lpt 2 code segment completely processed stored lpt execution reaches start address code directly offloaded dfe achieve use comparator stores five last offloaded code segments pc becomes equal one entries corresponding code automatically offloaded note process looking code offload turned segment executed several times example code segment figure 3 method first determines load accesses array candidates loop containing load case whole loop captured observing branches back start loop finally whole loop migrated dfe third iteration loop limitations code offloaded dfe offload segments contain store instructions avoid cache coherency problem addition offloaded code must contained within single procedure achieved checking procedure callreturn instructions also limitations related performance offloading offload segment number required registers certain registerlimit code segment offloaded dfe going access l2 cache code segment might beneficial offload large code segments therefore offload code segments larger codesizelimit changing parameters modify aggressiveness technique 32 executing dfe code execution core decides offload code segment sends start end addresses segment dfe along values required registers communication achieved extension isa instructions initiate communication cores dfe therefore additional ports required register file receiving request dfe first accesses l2 cache retrieve instructions starts execution execution segment dfe uses register neither generated segment 2 lreg field 2bits represent states required generated required generated invalid used inst cache func unit controller reg file pass gate cmu master pass data cache system bus memory communicated dfe generates interrupt core offloaded segment access necessary register values exception possible register values required dfe determined previous executions code segment hence different input data possible execute segment executed determination dfe code execution ends execution tries go eadd sadd dfe communicates necessary register values core ie values generated dfe table 1 netbench applications properties arguments execution arguments inst number instructions executed cycle number cycles required il1 dl1 acc number accesses level 1 instruction data cache l2 acc level 2 cache accesses application arguments inst cycle il1 acc crc crc 10000 1458 2620 2190 598 06 dh dh 5 64 7783 16631 10091 3647 384 drr drr 128 10000 129 335 228 79 11 drrl drr 1024 10000 347 802 601 233 50 ipchains ipchains 10 10000 617 1602 1039 262 36 nat nat 128 10000 114 267 173 56 12 natl nat 1024 10000 332 742 550 211 51 rou route 128 10000 142 320 233 71 09 roul route 1024 10000 368 817 626 228 50 snortl snort r defcon n 10000 dev l log b 3430 9256 5150 1322 334 snortn snort r defcon n 10000 v l log c sncnf 5459 16541 8937 2197 562 sslw opensll netbench weak 10000 3290 8321 4411 1520 318 tl tl 128 10000 69 157 118 39 07 tll tl 1024 10000 303 671 522 199 47 url url smallinputs 10000 4970 9567 7689 2491 100 average 1931 4587 2845 868 130 table 2 dfe code information number different dfe code segments dfe codes average number instructions segments size average number register values transferred segment reg req total number register values transferred tofrom dfe trans fraction instructions executed dfe ratio app dfe codes size reg req trans ratio dh drr 96 289 22 322 588 drrl 96 289 22 1825 571 ipchains 113 778 18 123 186 natl 67 201 23 1801 577 nat 67 201 23 289 620 roul rou snortl 146 194 34 531 435 snortn 161 165 31 3833 962 sslw 95 327 16 905 166 tll 24 431 22 1799 589 tl 24 431 22 280 703 url average execution core also sends taskid code activated taskid consists coreid unique identification number execution core segmentid id process offloading code segment taskid uniquely determines process offloaded segment task complete dfe uses taskid stored cmu send results back execution core cores process similar accessing data l2 cache might context switches execution core sends request results propagated correct process eventually execution segment dfe data cache flushed prevent possible coherence problems 4 experiments performed several experiments measure effectiveness technique simulations simplescalararm 17 simulator used necessary modifications simulator implemented measure effects lpt dfe due limitations simulation environment execution core becomes idle offloads code segment actual implementation core might start executing processes increase system utilization simulate applications netbench suite 20 important characteristics applications explained table 1 report reduction execution cycles bus accesses power consumption caches overall power consumption dfe able reduce power consumption due smaller number bus accesses reduction cache accesses discussed following 41 simulation parameters first report results singlecore processor base processor similar strongarm 110 4 kb directmapped l1 data instruction caches 32byte linesize 128 kb 4 way setassociative unified l2 cache 128byte linesize lpt split cache experiments technique 2way 64 entry cache 05k split cache technique temporal cache 4k 32byte lines spatial cache 2k 128byte lines technique execution core single cache equal temporal cache 4k 32byte lines dfe data cache similar spatial cache 2k 64byte lines dfe also level 1 instruction cache size 2kb 32byte lines latency l1 caches set 1 cycle l2 cache latencies set 10 cycles dfe activated using l2 calls simulator hence delay 10 cycles start dfe execution calculate power consumption caches using simulation numbers cache power consumption values obtained cacti tool 27 modified simplescalar gather information switching activity bus bus power consumption calculated using information model developed zhang irwin 28 find total power consumption processor use power consumption section strongarm presented montanaro et al 21 power consumptions caches modified represent caches simulations overall power consumption sum power execution cores shared resources dfe experiments register limit set four maximum offloadable code size 200 instructions 42 singlecore results first set experiments compare performance four systems base processor explained section 41 system 1 processor 4 kb 2way set associative level data cache 2way system processor directmapped 8 kb level 1 data cache 8kb system processor split cache lpt system processor using proposed dfe method dfe table 2 gives number different code segments offloaded size segment number registers required call total number register values transferred tofrom dfe execute offloaded code fraction instructions executed dfe note code size number instructions first instruction segment last instruction segment correspond number instructions executed figure 5 summarizes results execution cycles presents relative performance systems respect base processor proposed method increases execution cycles 029 average none systems positive impact performance dh application data cache misses application l1 data cache miss rate almost 0 hence effected techniques see dfe approach increase execution cycles 16 mainly due imperfect decision making code segments offload specifically dfe might activated code segment contains loads data used ie data temporal locality case execution cycles improved data accessed cores dfe generating redundant accesses data processed dfe segment used performance might degraded figure 6 presents effects techniques total power consumption data caches proposed technique corresponds power consumption level 1 data cache lpt structures execution core level 1 data cache dfe level 2 cache proposed technique able reduce power consumption 098 average reduction number bit switches bus accesses presented figure 7 see proposed mechanism reduce switching activity much 468 265 average reduction bus activity lpt mechanism 186 average similarly 8 kb 2way level 1 caches reduce bus activity 946 121 respectively energydelay product 10 presented figure 8 dfe technique improves energydelay product much 155 83 average split cache mechanism hand 54 lower energydelay product base processor see almost every category split cache technique performs better system 8 kb cache 2way associative 4 kb cache networking applications exhibit mixture accesses spatial temporal locality hence using split cache achieves significant improvement cache performance class applications also amenable proposed structure spatial accesses efficiently utilized dfe 113crc dh drr drrl ipchains md5 natl nat rou roul snortl snortn ssl tll tl url avg reduction exec cycles 2way 8k lpt dfe figure 5 reduction execution cycles 2way 8k lpt dfe figure 6 reduction power consumption data caches sum level 1 level 2 dfe data caches applicable crc dh drr drrl ipchains md5 natl nat rou roul snortl snortn ssl tll tl url avg reduction 2way 8k lpt dfe figure 7 reduction number bit switches system bus 5515crc dh drr drrl ipchains md5 natl nat rou roul snortl snortn ssl tll tl url avg reduction 2way 8k lpt dfe figure 8 reduction energydelay product 10103050crc dh drr drrl ipchains md5 natl nat rou roul snortl snortn ssl tll tl url avg reduction exec cycles 2way 8k lpt dfe figure 9 reduction execution cycles processor 4 execution cores exec cycles 2way 8k lpt dfe figure 10 reduction execution cycles processor execution cores 43 multiplecore results two important issues need considered regarding use dfe multicore design first since dfe shared resource might become performance bottleneck several cores contending hand dfe reduces bus accesses significantly another shared resource might improve performance due less contention bus note system bus one important bottlenecks chip multiprocessors although techniques small effect performance singlecore processors performance improvement much dramatic multi core systems due reduction bus accesses shown figure 7 see effects multiple execution cores first designed tracedriven multicore simulator mcs simulator processes event traces generated simplescalararm framework simplescalar used generate events singlecore processed mcs account effects global events bus accesses mcs finds new execution time using number cores employed system events traces execution core runs single application hence communication execution cores although represent workload multicore systems single application control execution cores realistic workload npus experiments simulate application execution core application random startup time aware cores system hence simulations realistically measure activity shared resources report results processors 4 execution cores 16 execution cores note cores execute code wait results performed dfe call improvement execution cycles processor 4 execution cores presented figure 9 proposed dfe mechanism reduces number execution cycles much 508 201 average split cache technique reducing execution cycles 172 average figure presents results processor execution cores dfe able improve performance much 767 297 average technique increases execution cycles snort url applications url application execution cores utilize dfe 513 able reduce bus activity therefore number execution cores increased cores stall bus contention resulting increase execution cycles snort application hand dfe usage high 962 hence although bus contention reduced contention dfe increases execution cycles note overcome contention problem using adaptive offloading mechanism dfe occupied execution core continues execution hence code offloaded dfe idle dfe becomes overloaded overhead approach would checking dfe state execution cores another solution contention problem might use multiple dfes however techniques scope paper also measured energy consumption multiple cores however increasing number execution cores significant effect overall power consumption additional execution core increases bus dfe level 2 cache activity linearly hence ratios different system change significantly 5 related work streaming data ie data accessed known fixed displacements successive elements studied literature mckee et al 19 propose using special stream buffer unit sbu store stream accesses special scheduling unit reordering accesses stream data benitez davidson 6 present compiler framework detect streaming data proposed architecture require compiler support tasks addition displacement accesses networking applications fixed due unknown nature packet distribution memory several techniques proposed reduce power consumption highperformance processors 2 5 15 24 techniques use small energyefficient structures capture portion working set thereby filtering accesses larger structures others concentrate restructuring caches 2 context multiple processor systems moshovos et al 22 propose filtering technique snoop accesses smp servers plethora techniques improve cache locality 9 13 14 25 26 techniques reduce l1 data cache misses either intelligently placing data using external structures study however change location computation lowtemporal data accesses techniques could used detect low temporal data proposed method active smart memories extensively studied 3 8 16 however techniques concentrate offchip active memory contrast methods improve performance onchip caches therefore finegrain offloading feasible systems 6 conclusion network processors powerful computing engines usually combine several execution cores consume significant amount power hence prone performance limitations due excessive power dissipation proposed technique reducing power multicore network processors technique reduces power consumption bus caches power consuming entities highperformance processors shown networking applications l1 data cache misses caused instructions motivates specific technique proposed technique uses locality prediction table detect load accesses low temporal locality novel data filtering engine processes code segment surrounding low temporal accesses presented simulation numbers showing effectiveness technique specifically technique able reduce overall power consumption processor 86 singlecore processor able reduce energydelay product 83 358 average singlecore processor processor respectively currently investigating compiler techniques determine code segments offloaded dfe static techniques advantage determining exact communication requirements code remaining execution cores segments offloaded dfe however determining locality dynamically advantages therefore believe better optimizations possible utilizing static dynamic techniques integrated approach r predictability loadstore instruction latencies selective cache ways towards programming environment computer intelligent memory power performance tradeoffs using various caching strategies architectural compiler support energy reduction memory hierarchy high performance processors code generation streaming accessexecute mechanism case intelligent ram iram data cache multiple caching strategies tuned different types locality energy dissipation general purpose microprocessors intel network processor targets routers improving directmapped cache performance addition small fullyassociative cache prefetch buffers filter cache energy efficient memory structure combined dram logic chip massively parallel applications simplescalar home page improving bandwidth streamed references benchmarking suite network processors jetty snoop filtering reduced power smp servers reducing address bus transitions low power memory mapping circuit technique reduce leakage cache memories reducing conflicts directmapped caches temporalitybased design managing data caches using selective cache line replacement enhanced access cycle time model onchip caches tr code generation streaming accessexecute mechanism data cache multiple caching strategies tuned different types locality predictability loadstore instruction latencies managing data caches using selective cache line replacement runtime adaptive cache hierarchy management via reference analysis filter cache architectural compiler support energy reduction memory hierarchy high performance microprocessors improving directmapped cache performance addition small fullyassociative cache prefetch buffers power performance tradeoffs using various caching strategies selective cache ways gatedvsubscrptddsubscrpt netbench smarter memory case intelligent ram towards programming environment computer intelligent memory combined dram logic chip massively parallel systems reducing address bus transitions low power memory mapping energydelay analysis onchip interconnect system level ctr mary jane irwin compilerdirected proactive power management networks proceedings 2005 international conference compilers architectures synthesis embedded systems september 2427 2005 san francisco california usa yan luo jia yu jun yang laxmi n bhuyan conserving network processor power consumption exploiting traffic variability acm transactions architecture code optimization taco v4 n1 p4es march 2007