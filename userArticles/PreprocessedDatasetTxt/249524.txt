runtime parallel scheduling processor load balancing abstractparallel scheduling new approach load balancing parallel scheduling processors cooperate schedule work parallel scheduling able accurately balance load using global load information compiletime runtime provides highquality load balancing paper presents overview parallel scheduling technique scheduling algorithms tree hypercube mesh networks presented algorithms fully balance load maximize locality runtime communication costs significantly reduced compared existing algorithms b introduction static scheduling balances workload runtime applied problems predictable structure called static problems dynamic scheduling performs scheduling activities concurrently runtime applies problems unpredictable structure called dynamic problems static scheduling utilizes knowledge problem characteristics reach wellbalanced load 1 2 3 4 however able balance load dynamic problems addition requirement large memory space store task graph restricts scalability static scheduling dynamic scheduling general approach suitable wide range applications 5 6 7 adjust load distribution based runtime system load information however runtime scheduling algorithms utilize neither characteristics information application problems global load information load balancing decisions system stability usually sacrifices quality quickness load balancing parallel scheduling promising technique processor load balancing parallel schedul ing processors cooperate schedule work parallel scheduling utilizes global load information able accurately balance load provides highquality scalable load balancing parallel scheduling algorithms introduced 8 9 10 11 parallel scheduling applied static problems existing scheduling algorithms static problems running single processor scalable massively parallel computers storing task graph requires large memory space speed scheduling relax demand memory space static scheduling parallelized kwok ahmad developed parallel algorithm 12 wu parallelized mcp algorithm 13 parallel scheduling also applied dynamic problems parallel scheduling applied runtime becomes incremental collective scheduling applied whenever load becomes unbalanced processors collectively schedule workload system described 11 starts system phase schedules initial tasks followed user computation phase execute scheduled tasks possibly generate new tasks next system phase old tasks executed scheduled together newly generated tasks system phase parallel scheduling algorithm applied balance load paper discuss parallel scheduling methodology paper devoted particularly kind scheduling schedules ready jobs tasks objects scheduled set jobs tasks ready execute scheduling algorithms tree hypercube mesh networks presented algorithms primarily designed dynamic problems randomly arrived dynamically generated jobs tasks algorithms fully balance load maximize locality significantly reduce communication overhead compared existing algorithms paper organized follows section 2 discuss optimal scheduling problem parallel scheduling algorithms tree hypercube mesh topologies presented section 3 properties algorithms described section 4 performance presented section 5 previous works discussed section 6 section 7 concludes paper 2 optimal scheduling problem objective scheduling schedule works processor work load thus need estimate task execution time estimation application specific leading less general approach sometimes estimation difficult obtain due difficulties task presumed require equal execution time objective algorithm becomes schedule tasks processor number tasks inaccuracy caused grainsize variation corrected next system phase algorithm estimated time tasks could improve load balancing extent however since algorithm complex scheduling overhead increases may overwrite benefit 11 scheduling problem described follows parallel system n computing nodes connected given topology node w tasks parallel scheduling applied scheduling algorithm redistribute tasks number tasks node equal assume sum w nodes evenly divided n average number tasks w avg calculated node w avg tasks executing scheduling algorithm w node must determine send tasks communication step many communications performed simultaneously time spent load balancing activity depends number communication steps time taken step parallel scheduling algorithm utilizes global information number communication steps order log n n number processors 14 average time communication step depends total number tasks migrated traveling distances objective function minimize number taskhops x e k number tasks transmitted edge k general problem converted minimumcost maximumflow problem 15 follows edge treated bidirectional arc given tuple capacity cost capacity capacity edge cost cost edge set edges processor network add source node edge node w w avg sink node edge j node j w cost capacity cost minimumcost maximumflow algorithm yields solution problem figure 1 shows load distribution eightnode hypercube network graph constructed figure 1 given figure 2 w 8 minimum cost algorithm 15 generates solution shown figure 3 complexity minimum cost algorithm 2 v n number nodes v desired flow value 15 complexity corresponding parallel algorithm n nodes least onv high complexity realistic runtime scheduling certain topology trees complexity reduced olog n n nodes topology trees need find heuristic algorithm 3 parallel scheduling algorithms section present parallel scheduling algorithms tree hypercube mesh topologies common feature algorithms total number tasks obtained figure 1 load distribution 3dimensional hypercube edges figure 2 graph optimal scheduling problem figure 1 figure 3 optimal solution figure 1 parallel reduction operation average number tasks per node calculated 14 node send tasks nodes unless number tasks exceeds average therefore necessary tasks migrated discussing individual algorithms different topologies give generic algorithm shown figure 4 first step collects global information using sum reduction 14 step 2 average number tasks per node calculated number tasks cannot evenly divided number nodes remaining r tasks evenly distributed first r nodes one task others values w avg r available node step 3 node calculates quota node knows overloaded underloaded quotas subsets nodes also computed particular topology step 4 tasks exchanged meet quotas minimum communication different algorithms designed different topologies let w number tasks node 1 global information collection perform sum reduction w compute information total number tasks 2 average load calculation w 3 quota calculation node computes quota q w avg otherwise quotas subsets nodes also computed 4 task exchange overloaded node determines send excess tasks figure 4 generic parallel scheduling algorithm following subsections present three parallel scheduling algorithms tree walking algorithm twa cube walking algorithm cwa mesh walking algorithm mwa tree algorithm optimal algorithm terms number taskhops hypercube mesh algorithms heuristic algorithms 31 tree walking algorithm network topology tree complexity optimal scheduling reduced tree walking algorithm twa shown figure 5 essentially one presented 11 step 1 total number tasks counted parallel reduction operation node records number tasks subtree childrens subtrees step 2 root calculates average number tasks per node broadcasts number every node step 3 subtree rooted node calculates quota q indicates many tasks scheduled subtree q calculated directly follows node keeps records q q j node j node child step 4 workload exchanged end system phase node number tasks quota tree walking algorithm twa assign node order according preorder traversal n number nodes subtree total number nodes system node parent node p also child vector c i0 c i1 c im gamma1 give childrens node numbers 1 global information collection perform sum reduction w 2 average load calculation 3 quota calculation quota node q computed also quota subtree computed 4 task exchange node computes 41 node j l receive tasks node p 42 node receive tasks node c ij 43 node j l tasks node p 44 node ij tasks node c ij figure 5 tree walking algorithm lemma 1 execution twa number tasks node equal quota proof assume node j child node c j node j equal gammaj r il node thus j l receive jj l tasks node similarly j r il node equal gammaj l j node j thus j r receive jj r tasks node j therefore execution twa number tasks node j child j child j child j child j child steps 1 2 spend 2m communication steps depth tree communication steps step 4 distance leaf node another leaf node 2m therefore total number communication steps algorithm 4m balanced tree number communication steps parallel algorithm n nodes ologn example 1 example shown figure 6 nodes tree numbered preorder traversal beginning scheduling node w tasks ready scheduled values w calculated step 1 root calculates value w avg r node calculates value q step 3 values w shown follows figure example tree walking algorithm numbers tasks exchanged nodes shown figure 6 end scheduling nodes 04 five tasks nodes 58 four tasks 32 cube walking algorithm subsection study two algorithms designed hypercube topology dem algorithm 8 9 proposed cube walking algorithm cwa dem small domains balanced first combined form larger domains ultimately entire system balanced integer version dem described figure 7 node pairs first dimension whose addresses differ least significant bit balance load next node pairs second dimension balance load forth node balanced load neighbors number communication steps dem algorithm 3d number dimensions 16 dem node exchanges node j current values w w j w tasks node j w tasks node j update value w figure 7 dem algorithm example 2 dem algorithm illustrated figure 8 load distribution execution b c figure 8 example dem algorithm dem algorithm shown figure 8a first step nodes exchange load information balance load dimension 0 shown figure 8b load balanced dimension 1 shown figure 8c load balancing dimension 2 figure 8d final result shown figure 8e load fully balanced integer numbers tasks transmitted nodes total 33 taskhops whereas optimal scheduling shown figure 3 21 taskhops execution dem algorithm load difference dimension hypercube 17 figure 9 shows example 4dimensional hypercube figure 9 example shows number tasks differs 4 resulting dem dem algorithm simple low complexity load balancing step node pairs exchange load information global information collected without global load information impossible node make correct decision many tasks sent node pairs attempt average number tasks anyway node may send excessive tasks neighbor dem unable fully balance load minimize communication cost good heuristic algorithm designed utilizing global load information present new parallel scheduling algorithm hypercube topology algorithm called cube walking algorithm cwa shown figure 10 let w 0 number tasks node algorithm applied first step collects system load information exchanging values w k obtain values w k1 node records w vector w k total number tasks kdimensional subcube kdimensional subcube node defined nodes whose numbers gamma kbit prefix node value node equal total number tasks entire cube step 2 node calculates average number tasks per node quota vector q calculated step 3 node knows kdimensional subcubes overloaded underloaded vector q computed directly follows bitwise bitwise ffi vector difference w q stand number tasks sent received subcubes cube walking algorithm cwa assume cube dimension number nodes let phi denote bitwise exclusive bitwise 1 global information collection perform sum reductions node computes w vector 2 average load calculation 3 quota calculation node computes vectors q k 4 task exchange 41 node compute number tasks sent tasks well vector node phi 2 k update w ffi vectors 42 node receive tasks well vector node phi 2 k update w ffi vectors dimension figure 10 cube walking algorithm step 4 task exchanges conducted among dimension start cube dimension gamma 1 recursively partition cube dimension k two subcubes dimension gamma1 node ni paired corresponding node ni subcube particular step exchange tasks ni ni 0 send tasks one direction overloaded subcube way overloaded node necessarily commit send tasks since may postpone action decision made globally within subcube calculating vector every node overloaded subcube calculation local operation without communication value ffi n 0 calculated fl vector records number tasks reserved subcubes lower dimensions following lemma shows end algorithm node number tasks quota lemma 2 execution cwa number tasks node equal quota proof show iteration 0 number tasks node equal quota q 0 need show iteration k kdimensional subcube q k tasks subcube needs send ffi k tasks subcube tasks sent one direction number tasks sent overloaded subcube underloaded subcube must equal ffi k proven showing three cases assigning value case 1 hence case 2 j1 since since hence case 3 since since hence algorithm step 1 spends 2d communication steps exchanging load information dimension cube step 4 spends communication steps load balancing therefore total number communication steps algorithm 3d example 3 running example cwa shown figure 11 beginning scheduling node w 0 tasks ready scheduled values w k calculated step 1 values w avg r follows node calculates values q k step 3 every node quota vector step 4 2 subcube f0123g overloaded one values w k follows node d0 d1 d2 thus node 0 sends six tasks node 4 node 1 sends three tasks node 5 loads subcubes f0 1 2 3g f4 5 6 7g balanced subcube tasks subcubes f01g f45g overloaded values w k follows node d0 d1 thus node 0 sends five tasks node 2 node 5 sends two tasks node 7 loads subcubes f0 1g f2 3g f4 5g f6 7g balanced subcube tasks overloaded values w k follows b c figure 11 running example cwa node d0 finally node 3 sends one task node 2 node 5 sends two tasks node 4 node 6 sends two tasks node 7 results balanced load node eight tasks total number taskhops 21 33 mesh walking algorithm parallel scheduling algorithm mesh topology named mesh walking algorithm mwa shown figure 12 first scan partial vector w along every row node records w vector w 0 node mod calculates sum ij scan operation performed along nodes keeps another vector c values w avg r calculated node n gamma 1 spread nodes mod consequently nodes spread values w avg igamman 2 along row vectors q 0 ffi calculated node well q 0 igamman 2 values q calculated directly mod r mod r otherwise step 4 first iteration load among rows nodes calculate values j i0l j i0r receive jj i0l j tasks row r gamma 1 receive jj i0r j tasks row 1 j i0r 0 submesh row 0 row overloaded j i0r tasks need sent row r 1 similarly submesh row underloaded j i0l tasks need sent row r gamma 1 vector calculated determine many tasks node need sent calculation j local operation without communication variable fl ijl indicates many tasks reserved previous j nodes row variable j ijl tells many tasks remain sent values w ffi updated iteration 0 mesh walking algorithm mwa assume n 1 theta n 2 mesh number nodes 1 global information collection perform scan operations compute w vectors 2 average load calculation 3 quota calculation compute vectors q k 4 task exchange let initialize 42 receive tasks well vector node n l update w vector 43 calculate number tasks sent ij j l tasks well vector node n l update w ffi vectors figure 12 mesh walking algorithm balances load row following lemma shows end algorithm node number tasks quota lemma 3 execution mwa number tasks node equal quota proof iteration 1 ij 2 larger equal 0 1 3 v 1 assume 1 equal 0 assume exists ffi 0 therefore 1 2 fl 4 3 6 7 8 j iv 1 1 0r j receive jj i0l j tasks node receive jj i0r j tasks node therefore iteration 1 weight w 0 updated iteration 0 therefore iteration 0 number tasks row algorithm step 1 spends n 2 communication steps collect load information along row n 1 communication steps collect load information across rows broadcasting spreading operations spend communication steps step 4 spends n 1 communication steps load balancing therefore total communication steps algorithm 3n 1 example 4 running example mwa shown figure 13 total number tasks computed parallel reduction values w avg r calculated every node q 0 8 32 respectively values also calculated values w 0 listed follows nodes row values w 1 listed follows 9 3 b c figure 13 running example mwa iteration 1 every j i0l 0 therefore tasks sent one direction example values ffi 0 listed follows row 0 node 1 sends three tasks node 5 node 3 sends six tasks node 7 nodes row 1 receive tasks vectors update ffi calculate vectors node 4 sends nine tasks node 8 node 7 sends three tasks node 11 finally nodes row 2 update ffi calculate vectors node 8 sends three tasks node 12 node 9 sends two tasks node 13 task exchange shown figure 13b number tasks row equal quota q 1 32 iteration 0 w 0 calculated updated values w values q values j vectors follows nodes exchange tasks shown figure 13c according values equal j results balanced load node eight tasks total number taskhops 48 4 properties scheduling algorithms section discuss scheduling quality locality communication costs twa cwa mwa algorithms next theorem shows algorithms able fully balance load number tasks equally divided number nodes node equal number tasks otherwise number tasks node differs one theorem 1 difference number tasks node one execution twa cwa mwa proof lemmas 1 2 3 number tasks node equal quota execution twa cwa mwa since quota either w avg w avg 1 difference number tasks node one 2 algorithms also maximize locality local tasks tasks migrated nodes nonlocal tasks migrated nodes maximum locality implies maximum number local tasks minimum number nonlocal tasks lemmas 4 5 theorems 2 3 assume number tasks evenly divided n number nodes evenly divided n algorithms nearlyoptimal following lemma gives minimum number nonlocal tasks lemma 4 reach balanced load minimum number nonlocal tasks proof node w must receive w tasks nodes balanced load therefore total tasks must migrated nodesthe next theorem proves three algorithms maximize locality theorem 2 number nonlocal tasks twa cwa mwa algorithm proof time executing twa cwa mwa algorithm number tasks node less minw w avg twa node receives tasks sending tasks cwa mwa node sends tasks weight larger w avg w tasks sent thus nodes least tasks local therefore number nonlocal tasks n theta w avg gamma w stated lemma 4 algorithms minimize number nonlocal tasks maximize locality 2 twa optimal scheduling algorithm next theorem proves twa minimizes number taskhops communication theorem 3 twa algorithm minimizes total number taskhops total number communications proof edge k connects subtree parent minimum number tasks transmitted parent subtree similarly minimum number tasks transmitted subtree parent therefore total number taskhops minimized subtree q 6 w least one communication subtree parent minimum number communications q communication subtree parent therefore total number communications minimized 2 cwa mwa heuristic algorithms general able minimize communication cost however system less equal four nodes algorithms minimize communication cost lemma 5 cwa mwa algorithms minimize communication cost system two four nodes proof communication cost system minimized negative cycle 15 system two nodes cycle system four nodes path consisting least three edges form negative cycle either cwa mwa longest path two edges therefore negative cycle 2 dem algorithm minimize communication cost four nodes may path consisting three edges 5 performance study twa optimal algorithm minimizes communication maximizes locality balancing load optimality heuristic algorithms cwa mwa needs studied simulation purpose consider test set load distributions test set load processor randomly selected mean equal specified average number tasks number processors varies 4 256 average number tasks average weight per processor varies 2 100 average weight made integer load fully balanced first study cwa compare performance dem cwa fully balance load dem cannot cases table shows percentage fullybalanced cases dem algorithm run dem algorithm different numbers processors different weights result 1000 test cases number processors increases less fullybalanced cases 32 processors cases 64 processors fullybalanced case test set important measure scheduling algorithm locality cwa algorithm sends necessary tasks processors maximizes locality dem algorithm results unnecessary task migration study locality dem algorithm dem able fully balance load cases fullybalanced cases selected result average fullybalanced cases 1000 test cases normalized locality table percentage fullybalanced cases dem number average weight processors 4 7430 7530 7570 7510 7450 7460 measured tdem total number nonlocal tasks dem algorithm topt minimum number nonlocal tasks figure 14 shows normalized locality 4 8 16 processors fullybalanced cases exist 16 processors reported 0 5 10 15 20 30 40 normalized locality weight processors processors processors figure 14 normalized locality dem next compare load balancing overhead dem simple runtime overhead load balancing decision small however unnecessary task migration leads large communication overhead compared time spent load balancing decision communication time dominate factor cwa hand although needing time make accurate load balancing decision involves less communication overhead normalized communication cost measured copt normalized cost weight dem 4 processors 0 5 10 15 20 30 normalized cost weight dem b 8 processors 0 5 10 15 20 30 40 45 50 normalized cost weight dem c processors figure 15 normalized communication costs dem cwa normalized cost weight 64 processors 0 5 10 15 20 30 40 45 normalized cost weight b 256 processors figure normalized communication costs cwa copt cdem ccwa copt number taskhops dem cwa optimal algorithms respectively figure 15 compares normalized communication costs 4 8 processors result average dem fullybalanced cases 1000 test cases number taskhops cwa four processors minimum seen communication costs dem much larger cwa figure 16 shows normalized communication costs cwa 64 256 processors data presented average 100 different test cases method assumptions used performance study mwa algorithm cwa algorithm mwa able fully balance load maximize locality however communication minimized cases normalized communication cost mwa respect optimal algorithm measured cmwa copt numbers taskhops mwa optimal algorithms spectively mentioned lemma 3 number taskhops mwa two four processors minimum figure 17 shows normalized communication costs 8 256 processors mesh organization either theta theta m2 data presented average 100 different test cases small meshes mwa provides nearly optimal result cost increases number processors 0 1 2 3 4 5 6 7 8 9 normalized cost weight processors processors processors 8 16 processors 0 5 10 15 20 30 40 45 50 normalized cost weight processors 128 processors processors b 64 128 256 processors figure 17 normalized communication cost mwa 6 previous works parallel scheduling static scheduling share common ideas 1 2 3 4 18 utilize global information achieve high quality load balancing parallel scheduling different static scheduling three aspects first scheduling activity performed runtime therefore deal dynamic problems second possible load imbalance caused inaccurate grain size estimation corrected next turn scheduling third eliminates requirement large memory space store task graphs scheduling conducted incremental fashion leads better scalability massively parallel machines large size applications large research efforts directed towards process allocation distributed systems 7 5 6 19 20 21 22 23 recent comparison study dynamic load balancing strategies highly parallel computers given willebeeklemair reeves 16 eager et al compared senderinitiated algorithm receiverinitiated algorithm 6 work similar assumption includes gradient model developed lin keller 24 randomized allocation algorithms developed different authors quite simple effective 25 5 26 27 adaptive contracting within neighborhood acwn 22 receiverinitiated diffusion rid 16 effective algorithms runtime parallel scheduling similar dynamic scheduling certain degree methods schedule tasks runtime instead compiletime scheduling decisions principle depend adapt runtime system information however substantial differences make appear two separate categories first system functions user computation mixed together dynamic scheduling clear cutoff system user phases runtime parallel scheduling potentially offers easy management low overhead second placement task dynamic scheduling basically individual action processor based partial system information whereas parallel scheduling scheduling activity always aggregate operation based global system information category scheduling sometimes referred prescheduling closely related idea presented paper prescheduling schedules workload according problem input fore problems whose load distribution depends input cannot balanced static scheduling balanced prescheduling applying prescheduling periodically load balanced runtime fox et al first adapted prescheduling application problems geometric structures 28 29 works also deal type problem 30 31 32 project parti automates prescheduling nonuniform problems 33 dimension exchange method dem applied application problems without geometric structure 9 conceptually designed hypercube system may applied topologies kary ncubes 34 balances load independent tasks equal grain size method extended willebeeklemair reeves 16 algorithm run incrementally correct unbalanced load caused varied grain sizes nicol proposed direct mapping algorithm computes total number tasks using sumreduction 10 however minimize communication cost eliminate communication conflict incremental scheduling nbody simulation presented 35 task graph rescheduled periodically correct load imbalance however runtime scheduling yet parallelized 7 conclusion recent research demonstrated runtime parallel scheduling provide lowoverhead load balancing global load information parallel scheduling synchronous approach removes stability problem able balance load quickly accurately parallel scheduling combines advantages static scheduling dynamic scheduling processors cooperate collect load information exchange workload parallel parallel schedul ing possible obtain high quality load balancing fullybalanced load maximized locality communication costs reduced significantly three algorithms tree hypercube mesh networks presented paper difficult develop algorithm kary ncube combining cwa mwa algorithms acknowledgments author would like thank xin helpful discussion r scheduling parallel program tasks onto arbitrary target machines hypertool programming aid messagepassing systems pyrros static task scheduling code generation messagepassing multiprocessors applications performance analysis compiletime optimization approach list scheduling algorithms distributed memory multiprocessors adaptive load sharing homogeneous distributed systems comparison receiverinitiated senderinitiated adaptive load sharing load distributing locally distributed sys tems programming hypercube multicomputer dynamic load balancing distributed memory multiprocessors communication efficient global load balancing runtime incremental parallel scheduling rips distributed memory computers parallel approach multiprocessor scheduling efficient parallel scheduling algorithm vector models dataparallel computing networks matroids strategies dynamic load balancing highly parallel computers analysis graph coloring based distributed load balancing algorithm dynamic criticalpath scheduling effective technique allocating task graphs multiprocessors simulations three adaptive decentralized controlled job scheduling algorithms analysis three dynamic distributed loadbalancing strategies varying global information requirements load sharing distributed systems adaptive dynamic process scheduling distributed memory parallel computers scheduling multithreaded computations work stealing gradient model load balancing method fine grain concurrent computations randomized parallel branchandbound procedure randomized load balancing tree structured computation solving problems concurrent processors parallel hierarchical nbody methods lowcost hypercube load balance algorithm partitioning strategy nonuniform problems multipro cessors dynamic load balancing vortex calculation running multiprocessors party parallel runtime system generalized dimension exchange method load balancing kary ncubes variants experience graph scheduling mapping irregular scientific computation tr ctr chen ximing lu xianliang runtime incremental concentrated scheduling nownrics acm sigops operating systems review v34 n2 p8496 april 2000 hwakyung rim juwook jang sungchun kim simple reduction nonuniformity dynamic load balancing quantized loads hypercube multiprocessors hiding balancing overheads journal computer system sciences v67 n1 p125 august janez brest viljem umer milan ojsterek dynamic scheduling pc cluster proceedings 1999 acm symposium applied computing p496500 february 28march 02 1999 san antonio texas united states wan yeon lee sung je hong jong kim sunggu lee dynamic load balancing switchbased networks journal parallel distributed computing v63 n3 p286298 march heejun park byung kook kim optimal task scheduling algorithm cyclic synchronous tasks general multiprocessor networks journal parallel distributed computing v65 n3 p261274 march 2005 k antonis j garofalakis mourtos p spirakis hierarchical adaptive distributed algorithm load balancing journal parallel distributed computing v64 n1 p151162 january 2004 arnaud legrand hlne renard yves robert frdric vivien mapping loadbalancing iterative computations ieee transactions parallel distributed systems v15 n6 p546558 june 2004 chingjung liao yehching chung treebased parallel loadbalancing methods solutionadaptive finite element graphs distributed memory multicomputers ieee transactions parallel distributed systems v10 n4 p360370 april 1999 yehching chung chingjung liao donlin yang prefix code matching parallel loadbalancing method solutionadaptive unstructured finite element graphs distributed memory multicomputers journal supercomputing v15 n1 p2549 jan 2000