scalesensitive dimensions uniform convergence learnability learnability valiants pac learning model shown strongly related existence uniform laws large numbers laws define distributionfree convergence property means expectations uniformly classes random variables classes realvalued functions enjoying property also known uniform glivenkocantelli classes paper prove generalization sauers lemma may interesting right new characterization uniform glivenkocantelli classes characterization yields dudley gine zinns previous characterization corollary furthermore first based gine zinns previous characterization corollary furthermore first based simple combinatorial quantity generalizing vapnikchervonenkis dimension apply result obtain weakest combinatorial condition known imply pac learnability statistical regression agnostic framework furthermore find characterization learnability probabilistic concept model solving open problem posed kearns schapire results show accuracy parameter plays crucial role determining effective complexity learners hypothesis class b introduction typical learning problems learner presented finite sample data generated unknown source find within given class model yielding best predictions future data generated source realistic scenario information provided sample incomplete therefore learner might settle approximating actual best model class within given accuracy data source probabilistic hypothesis class consists functions sample size sufficient given accuracy shown dependent different combinatorial notions dimension measuring certain sense complexity learners hypothesis class whenever learner allowed low degree accuracy complexity hypothesis class might measured coarse scale since case need full power entire set models position related rissanens mdl principle 17 vapniks structural minimization method 22 guyon et als notion effective dimension 11 intuitively dimension class functions decreases coarseness scale measured increases thus measuring complexity right scale ie proportional accuracy sample size sufficient finding best model within given accuracy might dramatically shrink example philosophy consider following scenario 1 suppose meteorologist requested compute daily prediction next days temperature forecast based set presumably relevant data temperature barometric pressure relative humidity past days special events day launching space shuttle prediction high degree accuracy therefore analyzes larger amount data finely tune parameters favorite mathematical meteorological model regular days smaller precision tolerated thus afford tune parameters model coarser scale saving data computational resources paper demonstrate quantitatively accuracy parameter plays crucial role determining effective complexity learners hypothesis class 2 work within decisiontheoretic extension pac framework introduced 12 also known agnostic learning model finite sample pairs x obtained independent draws fixed distribution p x theta 0 1 goal learner able estimate conditional expectation given x quantity defined called regression function statistics learner given class h candidate regression functions may may include true regression function f class h called ffllearnable learner property distribution p corresponding regression function f given large enough random sample p learner find fflclose approximation 3 f within class h f h fflclose approximation function h best approximates f analysis learnability purely informationtheoretic take account computational complexity throughout 1 adapted 14 philosophy compared approach studied 13 range functions hypothesis class discretized number elements proportional accuracy case one interested bounding complexity discretized class dimension original class part results builds discretization technique 3 notions approximation respect mean square error paper assume h later f satisfies mild measurability conditions suitable condition image admissible suslin property see 8 section 1031 page 101 special case distribution p taken x theta f0 1g studied 14 kearns schapire called setting probabilistic concept learning demand functions h take values f0 1g turns reduces one standard pac learning frameworks learning deterministic concepts case well known learnability h completely characterized finiteness simple combinatorial quantity known vapnikchervonenkis vc dimension h 24 6 analogous combinatorial quantity probabilistic concept case introduced kearns schapire call quantity p fl dimension h fl 0 parameter measures scale dimension class h measured able show finiteness parameter necessary probabilistic concept learning leaving converse open solve problem showing condition also sufficient learning harder agnostic model last result recently complemented bartlett long williamson 4 shown p fl dimension characterizes agnostic learnability respect mean absolute error 20 simon independently proven partial characterization nonagnostic learnability using slightly different notion dimension pioneering work vapnik chervonenkis 24 analysis learnability begins establishing appropriate uniform laws large numbers main theorem establish first combinatorial characterization classes random variables whose means uniformly converge expectations distributions classes random variables called glivenkocantelli classes empirical processes literature 9 given usefulness related uniform convergence results combinatorics randomized algorithms feel result may many applications beyond give addition results rely combinatorial result generalizes sauers lemma 18 19 new lemma considerably extends previously known results concerning f0 1 lambdag tournament codes 21 7 related variants sauers lemma proven useful different areas geometry banach space theory see eg 15 1 also hope apply result uniform distributionfree convergence empirical means true expectations classes realvalued functions studied dudley gine pollard talagrand vapnik zinn others area empirical processes results go general name uniform laws large numbers give new combinatorial characterization phenomenon using methods related pioneered vapnik chervonenkis let f class functions set x 0 1 results presented section generalized classes functions taking values bounded real range let p denote probability distribution x f p measurable f 2 f p f denote pmean f ie integral wrt p p n f denote random variable 1 drawn independently random according p following dudley gine zinn 9 say f ffluniform glivenkocantelli class lim sup pr sup mn sup 0 1 pr denotes probability respect points x 1 drawn independently random according p 4 supremum understood respect distributions p x respect suitable oealgebra subsets x see 9 say f satisfies distributionfree uniform strong law large numbers briefly f uniform glivenkocantelli class f ffluniform glivenkocantelli class recall notion vcdimension characterizes uniform glivenkocantelli classes f0 1gvalued functions let f class f0 1gvalued functions domain set x say f cshatters set x every e exists f e 2 f satisfying every x 2 n e 1 let v cdimension f denoted v cdimf maximal cardinality set x v cshattered f f v cshatters sets unbounded finite sizes let v following established vapnik chervonenkis 24 part stronger version assouad dudley 2 see 9 proposition 11 page 504 theorem 21 let f class functions x f0 1g f uniform glivenko cantelli class v cdimf finite several generalizations v cdimension classes realvalued functions previously proposed let f class 0 1valued functions domain set x ffl pollard 16 see also 12 say f p shatters set x exists function r every e exists f e 2 f satisfying every let pdimension denoted pdim maximal cardinality set x shattered f f p shatters sets unbounded finite sizes let shatters set x exists constant ff 2 r every e exists f e 2 f satisfying every x 2 n e f every x 2 let v dimension denoted v dim maximal cardinality set x shattered f f v shatters sets unbounded finite sizes let v easily verified see finiteness neither combinatorial quantities provides characterization uniform glivenkocantelli classes precisely provide sufficient condition kearns schapire 14 introduced following parametrized variant pdimension let f class 0 1valued functions domain set x let fl positive real number say f shatters set x exists function 0 1 every actually dudley et al use outer measure avoid measurability problems certain cases exists f e 2 f satisfying every x 2 n e f e every let p fl dimension f denoted p fl dimf maximal cardinality set x p fl shattered f f p fl shatters sets unbounded finite sizes let p parametrized version v dimension well call v fl dimension defined way defined p fl dimension pdimension first lemma follows directly definitions second lemma proven pigeonhole principle lemma 21 f fl 0 p lemma 22 class f 0 1valued functions fl 0 p fl v fl dimensions advantage sensitive scale differences function values considered significant main result section following new characterization uniform glivenkocantelli classes exploits scalesensitive quality p fl v fl dimensions theorem 22 let f class functions x 0 1 1 exist constants b 0 independent f fl 0 p fl dimf finite f afluniform glivenkocantelli class b v fl dimf finite f bfluniform glivenkocantelli class c p fl dimf infinite f fl gamma uniform glivenkocantelli class v fl dimf infinite f 2fl gamma uniform glivenkocantelli class 0 2 following equivalent f uniform glivenkocantelli class b p fl dimf finite fl 0 c v fl dimf finite fl 0 proof actually show 24 b 48 however values likely improved careful analysis proof theorem deferred next section note however part 1 trivially implies part 2 following simple example special case 9 example 4 page 508 adapted pur poses shows finiteness neither pdim v dim yields characterization glivenko cantelli classes throughout paper use ln denote natural logarithm log denote logarithm base 2 example 21 let f class 0 1valued functions f defined positive integers fx e gammax x 2 n f 2 f observe therefore f uniform glivenkocantelli class theorem 22 hand hard show pdimension v dimension f infinite theorem 22 provides first characterization glivenkocantelli classes terms simple combinatorial quantity generalizing vapnikchervonenkis dimension realvalued functions results extend previous work dudley gine zinn equivalent characterization shown depend asymptotic properties metric entropy stating metric entropy characterization glivenkocantelli classes recall basic notions theory metric spaces let x pseudo metric space let subset x ffl 0 ffl set b fflcover every 2 exists b 2 b ffl fflcovering number n ffl minimal cardinality fflcover finite cover defined 1 ffl set x fflseparated distinct b 2 ffl fflpacking number ffl maximal size fflseparated subset following simple wellknown fact lemma 23 every pseudo metric space x every x ffl 0 sequence n points x class f realvalued functions defined xn f g denote l 1 distance f g 2 f points x n l 1 xn 1in often use l 1 xn distance let us introduce notation n ffl f x n mffl f x n stand respectively fflcovering fflpacking number f respect l 1 xn notion metric entropy h n defined log n ffl f x n used dudley gine zinn prove following theorem 23 9 theorem 6 page 500 let f class functions x 0 1 1 f uniform glivenkocantelli class lim n1 h n ffl 2 ffl 0 lim n1 h n ffl 8ffluniform glivenkocantelli class results dudley et al also give similar characterizations using l p norms place l 1 norm related results proved earlier vapnik chervonenkis 24 25 particular proved analogue theorem 23 convergence means expectations characterized single distribution p characterization based h n ffl f averaged respect samples drawn p 3 proof main theorem wish obtain characterization uniform glivenkocantelli classes terms p dimension using standard techniques need bound flpacking numbers sets realvalued functions appropriate function p cfl dimension positive constant c line attack reduce problem analogous problem realm finite valued functions classes functions discrete finite range analyzed using combinatorial tools shall first introduce discrete counterparts definitions next step show realvalued problem reduced combinatorial problem final technical part proof analysis combinatorial problem new generalization sauers lemma let x set let bg consider classes f functions f x b two functions f g separated 2separated l 1 metric ie exists x 2 x 2 class f pairwise separated f g separated f 6 g f f strongly shatters set x nonempty exists function every e exists f e 2 f satisfying every x 2 n e f e every x 2 e f e x sx 1 function witnessing shattering f shall also say f strongly shatters according let strong dimension f sdimf maximal cardinality set x strongly shattered f f strongly shatters sets unbounded finite size let function f real number ae 0 aediscretization f denoted f ae function f ae x def ae c ie f ae fxg class f nonnegative realvalued functions let fg need following lemma lemma 31 class f 0 1valued functions set x ae 0 1 every 2 every ffl 2ae every x proof prove part 1 show set strongly shattered f ae also p ae2 shattered f x strongly shattered f ae exists function every e exists f e 2 f satisfying every x 2 n e f ae every x 2 e assume first f ae holds definition f ae f e definition f ae f e x aef ae x implies f e x ae delta sxae thus p ae2 shattered f seen using function defined prove part 2 lemma enough observe definition f ae f 2 2 prove main combinatorial result gives new generalization sauers lemma result extends previous work concerning f0 1 lambdag tournament codes proven completely different way see 21 7 lemma concerns l 1 packing numbers classes functions finite range shows class finite strong dimension 2packing number bounded subexponential function cardinality domain simplicity arbitrarily fix sequence x n n points x consider restriction f domain dropping subscript x n notation lemma 32 f class functions finite domain x cardinality n finite range note fixed bound lemma 32 n olog n even b constant polynomial n proof lemma 32 fix b 3 case b 3 trivial let us say class f strongly shatters pair nonempty subset x function f strongly shatters according integers h 2 n 1 let th n denote maximum number every set f h pairwise separated functions f x b f strongly shatters least pairs x 6 b f exists th n infinite note number possible pairs cardinality exceed size 0 strictly less b possibilities choose follows th n h l 12 f h sets f functions x b sdimf therefore finish proof suffices show claim t2 2 first part claim readily verified second part first note set 2mnb 2 pairwise separated functions x b exists t2mnb claim holds assume set f 2mnb 2 pairwise separated functions x b split arbitrarily mnb 2 pairs pair f g find coordinate x 2 x 1 pigeonhole principle coordinate x picked least mb 2 pairs pigeonhole principle least mb 2m pairs f g unordered set ffx gxg means two subclasses f call f 1 f 2 x 2 x g 2 f 2 obviously members f 1 pairwise separated x n fxg holds members f 2 hence definition function f 1 strongly shatters least t2m holds f 2 clearly f strongly shatters pairs strongly shattered f 1 f 2 moreover pair strongly shattered f 1 f 2 f also strongly shatters pair follows establishing claim suppose n r 1 let repeated application claim follows th n 2 r since clearly monotone first argument h implies t2nb 2 however since total number functions b b n sets pairwise separated functions size larger hence case hand result yields t2nb 2 thus either case completing proof 2 proving theorem 22 need two lemmas first one straightforward adaptation 22 section a6 p 223 lemma 33 let f class functions x 0 1 let p distribution x ffl 0 n 2ffl 2 pr sup theta n ffl6 f x 0 pr denotes probability wrt sample x drawn independently random according p e expectation wrt second sample x 0 2n also drawn independently random according p proof wellknown result see eg 8 lemma 1115 10 lemma 25 shows pr sup sup ffl combine result vapnik 22 pp 225228 showing ffl 0 pr sup theta n ffl3 f x 0 concludes proof 2 next result applies lemma 32 bound expected covering number class f terms p fl dimf lemma 34 let f class functions x 0 1 p distribution x choose expectation e taken wrt sample x drawn independently random according p proof lemma 23 lemmas 31 32 stirlings approximation xn xn xn ready prove characterization uniform glivenkocantelli classes proof theorem 22 begin part 1d v fl show f 2fl gamma uniform glivenkocantelli class 0 see assume 1 sample size n n find x set points shattered f exists ff 0 every e exists f e 2 f satisfying every x 2 n e f e uniform distribution sample x function f 2 f fx g thus large enough find f 2 f jp proves part 1d part 1c follows lemma 22 prove part 1a use inequality 2 lemma 33 bound expected covering number apply lemma 34 shows lim sup pr sup 0 whenever p fl dimf finite equation 4 shows p n f p f probability f 2 f distributions p furthermore lemma 33 lemma 34 imply 1 one may apply borelcantelli lemma strengthen 4 almost sure convergence ie lim sup pr sup mn sup 0 completes proof part 1a proof part 1b follows immediately lemma 22the proof theorem 22 addition simpler proof 9 see theorem 23 paper also provides new insights behaviour metric entropy used characterization shows large gap growth rate metric entropy either f uniform glivenkocantelli class hence 3 definition h n f uniform glivenkocantelli class hence exists ffl 0 p ffl easily seen imply h n ffl n unknown log 2 n replaced log ff n 1 ff 2 proof theorem 22 obtain bounds sample size sufficient guarantee high probability class 0 1valued random variables mean close expectation theorem 31 let f class functions x 0 1 distributions p x ffl pr sup p ffl24 dimension f theorem 31 proven applying lemma 33 lemma 34 along standard approximations omit proof theorem mention instead improved sample size bound shown bartlett long 3 equation 5 theorem 9 particular show p 14gamma ffl dimension 0 f finite 0 sample size order sufficient 5 hold 4 applications learning section define notion learnability accuracy ffl ffllearnability statistical regression functions model originally introduced 12 also known agnostic learning learning task approximate regression function unknown distribution probabilistic concept learning kearns schapire 14 realvalued function learning noise investigated bartlett long williamson 4 special cases framework show class functions ffllearnable whenever p affl dimension finite constant 0 moreover combining result kearns schapire show similar condition necessary weaker probabilistic concept learning conclude finiteness p fl dimension fl 0 characterizes learnability probabilistic concept framework solves open problem 14 let us begin briefly introducing learning model model examines learning problems involving statistical regression 0 1valued data assume x arbitrary set unknown distribution z let x random variables respectively distributed according marginal p x regression function f distribution p defined x 2 x general goal regression approximate f mean square sense ie l 2 norm distribution p unknown given z independently generated distribution p general cannot hope approximate regression function f arbitrary distribution therefore choose hypothesis space h family mappings settle function h close best approximation f hypothesis space h end hypothesis h 2 h let function defined mean square loss h goal learning present context find function b h 2 h given accuracy ffl 0 easily verified inf h2h p h achieved h 2 h h function h closest true regression function f l 2 norm learning procedure mapping finite sequences z h learning procedure produces hypothesis b training sample z n given accuracy parameter ffl say h ffllearnable exists learning procedure lim sup pr ae oe 0 7 pr denotes probability respect random sample z n 2 z n z drawn independently according p supremum distributions p defined suitable oealgebra subsets z thus h ffllearnable given large enough training sample reliably find hypothesis b h 2 h mean square error close best hypothesis h finally say h learnable ffllearnable ffl 0 1g definitions learnability yield probabilistic concept learning model case 7 holds ffl 0 class h say h ffllearnable pconcept model state prove main results section start establishing sufficient conditions ffllearnability learnability terms p fl dimension theorem 41 exist constants b 0 fl 0 1 p fl dimh finite h afllearnable 2 v fl dimh finite h bfllearnable 3 p fl dimh finite fl 0 v fl dimh finite fl 0 h learnable prove following characterizes pconcept learnability theorem 42 1 p fl dimh infinite h fl 2 8 gamma learnable pconcept model 2 v fl dimh infinite h fl 2 2 gamma learnable pconcept model 3 following equivalent h learnable pconcept model b p fl dimh finite fl 0 c v fl dimh finite fl 0 h uniform glivenkocantelli class proof theorem 41 clear part 3 follows part 1 using theorem 22 also lemma 22 part 1 equivalent part 2 thus prove theorem 41 suffices establish part 1 via next two lemmas hg lemma 41 h ffluniform glivenkocantelli class h 3ffllearnable proof proof uses method empirical risk minimization analyzed vapnik 22 empirical loss given sample z learning procedure ffl fflminimizes empirical risk ffl z n b us show procedure guaranteed 3ffllearn h fix n 2 n h 2 h thus p ffl zn hence since chose n ffl arbitrarily lim sup pr sup mn sup implies lim sup pr ae oe 0the following lemma shows bounds covering numbers family functions h applied induced family loss functions h formulate lemma terms square loss may readily generalized loss functions similar result independently proven bartlett long williamson 4 absolute loss lx respect l 1 metric rather l 1 metric used lemma 42 ffl 0 h z proof suffices show f g 2 h 1 follows noting every w 2 0 1 end proof theorem 41 proving part 1 lemma 41 suffices show h afluniform glivenkocantelli 0 use 2 lemma 33 bound expected covering number apply first lemma 42 lemma 34 establishes lim sup pr sup 0 whenever p fl dimh finite application borelcantelli lemma get almost sure convergence yields proof 2 conclude section proving characterization pconcept learnability proof theorem 42 ffllearnability implies ffllearnability pconcept model part 3 follows part 1 part 2 theorem 41 using theorem 22 proof part 2 uses arguments similar used prove part 1d theorem 22 finally note part 1 follows part 2 lemma 22 remark restricted version part 1 proven theorem 11 14 2 5 conclusions open problems work shown characterization uniform glivenkocantelli classes based combinatorial notion generalizing vapnikchervonenkis dimension result applied show notion dimension provides weakest combinatorial condition known imply agnostic learnability furthermore characterizes learnability model probabilistic concepts square loss analysis demonstrates accuracy parameter learning plays central role determining effective dimension learners hypothesis class open problem notions dimension may characterize uniform glivenkocantelli classes fact classes functions finite range characterization achieved member family several notions dimension see 5 second open problem asymptotic behaviour metric entropy already shown ffl 0 h n ffl uniform glivenkocantelli class conjecture ffl 0 h n ffl uniform glivenkocantelli class positive solution conjecture would also affect sample complexity bound 6 bartlett long fact suppose lemma 34 improved showing sup xn mffl f x n delta cd positive constant c note implies conjecture combining 3 lemma 1011 easily show sample complexity bound 0 18 finite clear bring constant 18 14 6 proven using l 1 packing numbers acknowledgments would like thank michael kearns yoav freund ron aharoni ron holzman fruitful discussions alon itai useful comments concerning presentation results thanks also anonymous referee many valuable comments suggestions references r embedding minimax nonparametric estimation classes sets theorems scalesensitive dimensions learning characterizations learnability classes f0 learnability vapnikchervonenkis dimension lower bound f0 course empirical processes uniform universal glivenkocantelli classes limit theorems empirical processes structural risk minimization character recognition decision theoretic generalizations pac model neural net learning applications generalization sauers lemma efficient distributionfree learning probabilistic concepts remarks embedding empirical processes modeling shortest data description density families sets combinatorial problem stability order models theories infinitary languages bounds number examples needed learning functions estimation dependences based empirical data inductive principles search empirical dependencies uniform convergence relative frequencies events probabilities necessary sufficient conditions uniform convergence means mathematical expectations tr lower bound 01 tournament codes learnability vapnikchervonenkis dimension inductive principles search empirical dependences methods based weak convergence probability measures decision theoretic generalizations pac model neural net learning applications efficient distributionfree learning probabilistic concepts characterizations learnability classes 0 myampersandhellip italicnitalicvalued functions generalization sauers lemma bounds number examples needed learning functions theorems scalesensitive dimensions learning fatshattering learnability realvalued functions ctr philip long sample complexity learning functions bounded variation proceedings eleventh annual conference computational learning theory p126133 july 2426 1998 madison wisconsin united states martin anthony peter l bartlett function learning interpolation combinatorics probability computing v9 n3 p213225 may 2000 massimiliano pontil note different covering numbers learning theory journal complexity v19 n5 p665671 october john shawetaylor robert c williamson pac analysis bayesian estimator proceedings tenth annual conference computational learning theory p29 july 0609 1997 nashville tennessee united states john shawetaylor nello cristianini results margin distribution proceedings twelfth annual conference computational learning theory p278285 july 0709 1999 santa cruz california united states olivier bousquet andr elisseeff stability generalization journal machine learning research 2 p499526 312002 shahar mendelson size convex hulls small sets journal machine learning research 2 p118 312002 tong zhang covering number bounds certain regularized linear function classes journal machine learning research 2 p527550 312002 hush clint scovel fatshattering affine functions combinatorics probability computing v13 n3 p353360 may 2004 hush clint scovel vc dimension bounded margin classifiers machine learning v45 n1 p3344 october 1 2001 martin anthony generalization error bounds threshold decision lists journal machine learning research 5 p189217 1212004 shahar mendelson petra philips importance small coordinate projections journal machine learning research 5 p219238 1212004 kristin p bennett nello cristianini john shawetaylor donghui wu enlarging margins perceptron decision trees machine learning v41 n3 p295313 dec 2000 philip long efficient algorithms learning functions bounded variation information computation v188 n1 p99115 10 january 2004 john shawetaylor peter l bartlett robert c williamson martin anthony framework structural risk minimisation proceedings ninth annual conference computational learning theory p6876 june 28july 01 1996 desenzano del garda italy barbara hammer generalization ability folding networks ieee transactions knowledge data engineering v13 n2 p196206 march 2001 alberto bertoni carlo mereghetti beatrice palano small size quantum automata recognizing regular languages theoretical computer science v340 n2 p394407 27 june 2005 andrs antos balzs kgl tams linder gbor lugosi datadependent marginbased generalization bounds classification journal machine learning research 3 p7398 312003 yiming ying dingxuan zhou learnability gaussians flexible variances journal machine learning research 8 p249276 512007 bernhard schlkopf alexander j smola short introduction learning kernels advanced lectures machine learning springerverlag new york inc new york ny shahar mendelson notes statistical learning theory advanced lectures machine learning springerverlag new york inc new york ny shahar mendelson learnability hilbert spaces reproducing kernels journal complexity v18 n1 p152170 march 2002 bin zou luoqing li performance bounds learning machines based exponentially strongly mixing sequences computers mathematics applications v53 n7 p10501058 april 2007