processor mapping techniques toward efficient data redistribution abstractruntime data redistribution enhance algorithm performance distributedmemory machines explicit redistribution data performed algorithm phases different data decomposition expected deliver increased performance subsequent phase computation redistribution however represents increased program overhead algorithm computation discontinued data exchanged among processor memories paper present technique minimizes amount data exchange block cyclicc viceversa redistributions arbitrary number dimensions preserving semantics target destination distribution pattern technique manipulates data logical processor mapping target pattern implemented ibm sp mapping technique demonstrates redistribution performance improvements approximately 40 traditional data processor mapping relative traditional mapping technique proposed method affords greater flexibility specifying precisely data elements redistributed elements remain onprocessor b introduction effort standardize dataparallel fortran programming distributedmemory machines high performance fortran forum composed forty academic industrial governmental agencies proposed hpf high performance fortran 1 many concepts originally proposed fortran 2 vienna fortran 3 dataparallel fortran languages incorporated hpf fundamental component hpf specification distribution alignment data arrays compiler directives due nonuniform memory access times characteristic distributedmemory machines determining appropriate data decomposition critical performance dataparallel programs machines data distribution deals data arrays distributed among processor memories data alignment specifies collocation data arrays goal data decomposition maximize system performance balancing computational load among processors minimizing remote memory accesses communication messages data distribution wellsuited one phase algorithm may good terms performance subsequent phase therefore hpf supports explicit runtime data redistribution redistribution may also occur implicitly subprogram boundaries result runtime operations eg data realignment use data redistribution represents performance tradeoff expected higher efficiency new distribution subsequent computation communication cost redistributing data among processor memories consequently minimizing execution time data redistribution obvious merit reducing amount data exchanged among processor memories one possible optimization toward reducing overall redistribution execution time subject paper present technique minimizes amount data exchanged among processor memories block cyclicc viceversa redistributions arbitrary number dimensions preserving semantics target destination distribution pattern technique manipulates data logical processor mapping target pattern clearer presentation mapping technique view data static manipulate processor mapping data implementation technique redistribution operation would specify mapping data processors since mapping technique works within realm logical processors architectureindependent thus could incorporated different redistribution implementations various distributedmemory architectures section 2 provides summary data redistribution issues related work section 3 presents mapping technique applied onedimensional data prove mapping technique achieves minimal data movement section 4 shows natural extension technique mdimensional data section 5 discuss impact using technique programmer compiler respectively additionally compare data redistribution execution time performance proposed mapping technique traditional dataprocessor mappings section 6 concludes paper 2 data redistribution many researchers espoused utility incorporating redistribution capability dataparallel languages kali language 4 one first incorporate runtime data redistribution mechanisms dino 5 addresses implicit redistribution data subprogram boundaries hall et al 6 discuss global optimizations employed set redistribution calls fortran program hypertasking compiler 7 dataparallel c programs incorporated runtime redistribution facility chapman et al 8 introduce vienna fortrans dynamic data distribution capability discuss highlevel implementation vienna fortran engine vfe figure presents segment hpf code illustrating use data redistribution hpf array real numbers initially distributed onto threedimensional processor configuration p distribution patterns block cyclic applied dimensions respectively p contains forty processors processor owns 20theta12theta10 data elements following amount computation redistributed new set patterns cyclic cyclic block onto different shape logical processor configuration q figure 2 illustrates initial distribution left subsequent redistribution right shaded portions figure depict data elements owned processor 000 whereas 000 owned data globally contiguous locations initially owns data globally noncontiguous locations following redistribution note processor configuration q also consists forty processors however p q vary shape example illustrates one redistribution possibility many distinct combinations source destination distribution patterns possible hpf requires however data processor distribution pattern declarations equal dimensionality eg fig 1 data processor pattern declarations threedimensional hpf require source target processor configurations equal size henceforth denote initial source distribution patterns program destination target distribution patterns similarly shall refer source target processor configurations p p respectively note p embedded primitive fig 1 real computation computation figure 1 example hpf figure 2 redistribution blockcyclic cycliccyclicblock previous example illustrates data redistribution results changing mapping data processors thus necessitating data exchange among processors perspective sending processors data exchange viewed simultaneous scattering data elements p p ie processor p performs onetomany send operation different data recipient p alternately perspective receiving processors data exchange viewed simultaneous gathering data elements p p ie processor performs manytoone receive operation distinct data sender p figure 3 illustrates communication pattern jp processor participates scatter gather operations data exchange viewed alltoall personalized communication 9 order perform data redistribution processor must determine identity processors receive data well identity processors must send data sets computed processors exchange data recall defined redistribution logical processors therefore logical physical processor mapping dictates actual data movement among memories machine example logical processors mapped physical node would obviously require messagepassing exchange data figure 3 redistribution communication pattern several research efforts focused efficient methods determining send receive processor sets redistribution gupta et al 10 derive closed form expressions communication sets based fortran ds block cyclic blockcyclic distribution patterns approach global array template indices combined knowledge used compute send receive sets alternative approach node scan local data array determine destination processor element place element message packet bound processor 11 onedimensional data redistribution performed distinct algorithms different combinations distribution patterns multidimensional redistributions implemented series sequential onedimensional redis tributions stichnoth et al 12 propose methods computing ownership sets array assignment statements due similarity determining send receive sets advocate computing together sending processor communicating information together data receiver approach chiefly intended communicating righthand side operands array assignment statements incorporated data redistribution pitfalls 13 mathematical representation regular distributions facilitates determining processor sets data redistribution pitfalls robustly handles arbitrary source target processor configurations arbitrary number data array dimensions pitfalls developed inclusion paradigm 14 compiler project university illinois research presented 10 11 12 13 focus efficiency computing sendreceive processor sets rather efficiency actual data exchange portion redistribution found latter operation several orders magnitude costly terms execution time send receive set determination 15 data exchange distributedmemory machines performed either pointtopoint collective communication messagepassing primitives techniques efficient multiple scatter gather operations presented 16 mckinley et al 17 survey issues regarding alltoall communication wormholerouted machines efficiency simultaneous redistribution data among physical processors affected topology routing switching mechanisms underlying machine technique communicationefficient data redistribution addresses message contention certain topologies presented 18 authors propose data redistribution communication cost model parameterizes number messages sizes network contention modeled expressing communication sequence permutations may executed fixed number contentionfree steps multiphase redistribution defined redistributing data intermediate distribution patterns eventually arriving destination distribution model used conjunction multiphase redistribution show lower overall cost achieved compared singlephase redistribution reshaping perfect poweroftwo sized arrays hypercubes discussed 19 earliest work optimizing data redistribution minimization data exchange regular hpf distribution patterns presented 20 related redistribution optimization strategy proposed wakatani wolfe 21 explore logical physical processor mapping redistribution reduce communication overhead technique assumes underlying torus topology maps data way communicating processors partitioned nonoverlapping sets thus keeping communication local group message contention physical network reduced drawback approach logical physical data mapping based local information words technique imparts mapping based solely best optimize redistribution however redistribution part larger dataparallel program logical physical data mapping must remain consistent throughout execution program logical physical processor mapping determined redistribution operation may best mapping overall performance entire application assert compiler privy global information ought determine logical physical processor mapping possible however manipulate data elementlogical processor mapping target ie mapping p long semantics target distribution pattern violated topic remainder paper onedimensional logical processor mapping begin illustrating utility proposed mapping technique onedimensional data figure 4 illustrates initial block distribution sixteen element data array onto eight logical processors subsequent redistribution array using cyclic pattern mapping initial distribution data onto physical nodes machine established program initialization see arrow labeled 1 subsequent redistribution among processors must retain consistent logical physical processor mapping see arrow labeled 2 logical processor ids lpids bold italics superimposed data processor 1 owns two contiguous elements block two noncontiguous elements stride p number processors cyclic lpids mapped increasing numerical order specified 1 claim unnecessary restriction permutation lpids 07 conform semantics cyclic pattern since data distribution processors hpf require specific ordering processors cyclic requires global data elements owned processor global indices separated stride p figure 5 illustrates benefit permuting lpids mapping data elements using sixteen element array fig 4 show two alternatives redistributing array cyclically choice 1 shows conventional cyclic mapping lpids data results two sixteen data elements marked filled rectangles remaining processor following redistribution call number data hits among processors choice 2 shows alternative cyclic mapping lpids permuted 04152637 results total eight data elements one per processor remaining original memories eliminates exchange six data elements among processors another advantage processors 16 must send data two processors using choice 1 one processor using choice 2 thus reducing number destinations factors reduces interprocessor communication overhead reduction redistribution cost example fig 5 small given size example extend sixteen million data elements permutation lpids eliminate exchange six million data elements across processors 2 31 processor mapping technique consistently minimize size data transfer arbitrary data block processor set sizes develop systematic method determining permutation lpids map data tech 1 henceforth logical processor unless otherwise stated 2 assuming number processors kept block size block two million block size cyclicc one million physical nodes141012145715141012145715 logical processor ids onto p8 onto p8 initial distribution data physical nodes performed compiler135713502466 redistribution data performed runtime bold italics 1 2 figure 4 data distribution redistribution onto physical nodes nique ensures processor retains maximum amount data possible conforming semantics source target distribution patterns establish upper bound ratio amount data retained p processors total number data elements n present function determining lpid data mapping redistribution block cyclicc 3 achieves upper bound make following assumptions 1 let p number processors numbered 0p gamma 1 n total number data elements numbered 0n gamma 1 distributed p assume processor owns b elements thus 2 consider redistribution block cycliccpatterns blockb pattern b variable considered assumption 1 cyclicc assume c divides b ie integer z 3 redistribution symmetric terms amount data movement among processors ie redistribution block cyclicc redistribution cyclicc block results equal amount data movement direction redistribution choice 1 choice 2 figure 5 logical processor data mapping alternatives 3 data initially distributed among p processors assume data redistributed among p processors r number data elements global array remain data hits original processors following redistribution block cyclicc define hitratio r n maxhitratio upper bound hitratio c block sizes block cyclicc patterns respectively z integer maxhitratio integer every cycle cp data elements maps c contiguous data elements cyclicc since complete cycles lpids map one complete data block owned lpid j block pattern processor lpid j map exactly ic elements ie ic data hits p processors number data hits cp ecp hitratio b case 2 integer since b icp cannot complete cycles lpids mapped lpid 0 original block data block thus number hits greater lpid consequently number hits across processors greater icp remainder proof follows case 1 2 maxhitratio achieved permutation lpids z integer multiple number processors ie z ip however goal achieve upper bound values z ip order satisfy aim must consider different permutations lpids maximize number data hits figure 5 demonstrates permutations lpids yield maxhitratio next using semantics define function determining permutation lpids map data array 32 mapping function define ptuple q place holders let f function maps lpid place holder q j 0 assignment lpid place holder specifies permutation lpids represents mapping lpids data elements cyclicc distribution pattern p possible permutations p processors may case many permutations yield ratio maxhitratio however exhaustively testing permutation determine whether produces ratio would impractical since would require exponential amount computation general p therefore present function determining permutation achieves maxhitratio b p c lpid maps unique q fi equation 1 specifies function maps lpid q fi intuition behind mapping function first view place holders q j circular list function maps lpid 0 place holder q 0 maps lpid 1 z places lpid 0 maps lpid 2 z places lpid 1 general map lpid i1 z mod p places lpid figure 6 illustrates behavior f applied different values z p simplicity choose reduces cyclic part shows example case 4 mapping broken rows better illustrate distance consecutive lpids distinction two examples onetoone part f onetoone part b part b one lpid maps locations lpids map place holders formally depending values z p possible f yields one permutation part produces six possible permutations 4 part b since lpid 0 lpid 3 map q 0 turns 4 optimal terms maxhitratio arbitrarily map two lpids place holders q 0 q 1 holds true lpid 2 lpid 5 place holders q 2 q 3 lpid 1 lpid 4 place holders q 4 q 5 shall prove result later lemma figure 6 mapping lpids place holders 33 optimality mapping function given arbitrary z p gcdz p determines whether f onetoone lemmata 2 3 establish lemmata 4 5 establish f achieves maxhitratio whether onetoone z natural numbers z bc let gcdz p greatest common divisor z p gcdz establishes onetoone mapping place holder q words gcdz proof proof contradiction assume gcdz choose k j recall mapping function f maps lpid i1 z mod p places lpid let mapped distance rz mod p place holders lpid j since map place holder distance modp zero ie rz mod must z mod implies assumption lemma 3 let p z natural numbers z bc gcdz q fi proof show divides z jz p mp mod arbitrary thus second term sum jz p left established two lemmas capture behavior f equation 1 natural shown relationship gcdz p function f fig 6 thus f onetoone part b gcdz maps two lpids place holders q next establish two lemmas show f produce permutations always yield maxhitratio lemma 4 let p z natural numbers z bc gcdz determines single permutation lpids achieves maxhitratio proof since f maps lpid i1 z places lpid lpid map cyclic first data element data block block figure 7 illustrates situation arbitrary lpid thus always least one data hit per lpid case 1 z p exactly c data hits per lpid mapping cycle begins ensuring c data hits since z p equivalent b cp lpid cannot map another c data elements bsized data block would violate semantics cyclic mapping thus exactly c hits per processor cp hits processors follows case 2 z p least c data hits per lpid established since lpid maps first element data block also map cp 1th element well since z p see fig 7 let integer division lpid map elements numbered total cj elements 5 thus cp ec data hits per lpid b cp ecpdata hits lpids b c cp elements data figure 7 lpid mapped first element data block lemma 5 let p z natural numbers z bc k lpids map q ik f remapped k ways place holders permutations yield maxhitratio proof lemma 3 established k 1 k lpids namely lpid ijpk place holder q fi consequently k lpids map first data element lpid 0 data block block case 1 z p b cp exactly c data hits per lpid figure 8 illustrates situation z p k z thus ck cz b furthermore k lpids c data elements fit lpid 0 bsized data block lpid mapped place holder q fi remapped one first ck data elements regardless permutation k lpids since lpid group c data hits lpid cannot appear within data block since b cp therefore exactly c data hits lpid cp b cp ecp data hits total among p processors case 2 z p b cp least c data hits per lpid established integer mcp 1cp mc data hits lpid since p lpids 5 data elements numbered 0b gamma 1 data block owned lpid c data data elements elements figure 8 z p map least cycles bsized data block claim 1c data hits even figure 9 illustrates situation groups p processors mapping csized blocks lpid 0 data block block must show lpid maps one last elements data block words show ck b gamma mcp proof contradiction assume ck exist two integers z rk ck c z z z z since pk 1 contradiction thus cgcdz mcp given result lpid 0 csized block must appear within last b gamma mcp elements data block regardless permutation order therefore data hits per lpid 1cp p processors 2 cp cp b mcp cp groups elements data figure 9 z p lemmata 1 5 prove following result theorem 1 redistribution block cyclicc b c respective block integer z p number processors number global data array elements logical processor mapping function equation 1 achieves maxhitratio multidimensional logical processor mapping section extends logical processor mapping technique presented section 3 mdimensional data arrays specifically extend technique optimizing logical processor mapping redistributing block block block cyclicc 0 additionally demonstrate approach redistribution twodimensional data 1 initially distributed dimensions subsequently redistributed one dimension eg blockblock cyclic 2 initially distributed one dimension redistributed dimensions eg block 41 mdimensional redistribution extend technique applying onedimensional lpid mapping dimension global data array let mdimensional data array n 0 theta n 1 theta n 2 theta theta nmgamma1 data elements processors directive hpf declares processor arrangement specifying name rank extent dimension let processor arrangement distributed set block sizes cyclic block sizes respectively dimension extend equation 1 equation 2 map rectilinear set lpids p j n j data elements jth dimension mdimensional data redistribution maxhitratio similar ratio presented section 3 product maxhitratios dimensions present lemma substantiate result lemma 6 maxhitratio mdimensional global array defined b 0 c mapping function equation 2 achieves upper bound proof lemmata 1 5 established b maxhitratio onedimensional data mapping function f achieved upper bound mdimensional data product upper bounds dimension yields maximum data hit ratio mdimensional array function g generalization f applying g respectively dimension upper bounds achieved 2 figure applied data matrix distributed across 3 theta 4 processor grid lpids bold italics 6 redistribute data matrix blockblockto cyclic3cyclic2 lpid data mappings dimensions marked block traditional cyclic mapping lpids data indicated trad optimized technique using g shown opt key right figure indicates data hits following redistribution traditional optimized mappings traditional mapping yields hitratio 12 mapping using g results hitratio 6 reduces 1 4 threefold increase number data hits traditional mapping theorem 1 lemma 6 prove following result 6 subscripts figure label denote number processors given dimension theorem 2 redistribution block block block cyclicc 0 respective block sizes z integer z number processors n number data elements dimension logical processor mapping function equation 2 achieves theta b c block opt trad0000 mappings data hits data hits data elements trad opt figure 10 block 3 block 4 cyclic 3 3cyclic 4 2 redistribution 42 twodimensional onedimensional redistribution redistribution mdimensional array need necessarily involve redistribution dimen sions instance data matrix may initially distributed blockblock redistributed cyclic row dimension distributed extend mapping technique cases define vector equation 3 defines new function maps cartesian lpid placeholder lemma 7 establishes maxhitratio twodimensional onedimensional redistribution based value gcdz p 0 lemmata 8 9 establish mapping function h achieves max hitratio lemma 7 redistribution block 7 cyclicc 0 let b 0 block size row dimension c 0 cyclic block size n 0 theta n 1 data matrix proof proof quite similar proof lemma 1 twodimensional grid processors remapped vector place holders since distributed one dimension substitute p 0 p 1 p lemma 1 remainder proof follows point thus b 0 counts number hits row dimension number multiplied block size column dimension b 1 obtain total number data hits 2 prove mapping function h yields maxhitratio first establishing correspondence lpids place holders showing mapping yields given ratio since column coordinate lpid relevant value computed h lpids row coordinate r map place holder q hr since exactly lpids map place holder thus greater p 0 place holders mapped h lemma 8 establishes h achieves maxhitratio lpids row coordinate r map place holder lemma 9 establishes result kp 1 lpids map place holder lemma 8 let z natural number exactly p 0 place holders mapped hr p 1 lpids map place holder q hrs remapped ways place holders q hrs q hrs1 q hrs2 q permutations yield proof first conjecture lemma follows directly lemma 2 proof second part lemma similar proof lemma 5 instead mapping place holder lemma 5 p 1 lpids mapping place 7 denotes hpf distribution pattern holder established previously remainder proof follows making substitution maxhitratio result lemma 5 except b 1 must factor since number data elements row contribute number data hits 2 lemma 9 let z natural number place holders mapped hr hr maps lpid rjp 0 ks place holder q hrs 1 lpids map place holder q hrs remapped p 1 k ways place holders q permutations yield proof first portion lemma follows directly lemma 3 proof second part lemma similar proofs lemmata 5 8 current situation lpids map place holder remainder proof follows making substitution maxhitratio lemma 8 2 figure 11 demonstrates mapping function h applied 24 theta 16 matrix matrix originally distributed blockblock 3 theta 2 processor grid redistributed cyclic2 six processor vector lpids bold italics lpid rs identifies processor cartesian system 1 block size row dimension blockblock eight b cyclic block size cyclic2 two fig 11 show traditional mapping since mapping set cartesian lpids vector lpids undefined hpf using h map lpids produces hitratio 4 distribution columns inconsequential number data hits achieved since rows distributed words redistribution block cyclicc cyclic would result number data hits redistribution blockblock cyclic additionally lpids row index r could permuted data hit ratio achieved eg lpid 00 lpid 01 figure 12 shows example gcdz processors lpid 0s lpid 2s could permuted 6 possible ways optimal data hit ratio would obtained lemmata 7 9 prove following result theorem 3 redistribution block cyclicc 0 b 0 c 0 respective block sizes row dimension integer z b 1 block size column dimension defines grid processors n 0 theta n 1 number global data array block data elements data hits figure 11 block 3 block 2 cyclic 6 2 redistribution elements logical processor mapping function equation 3 achieves 43 onedimensional twodimensional redistribution another possibility data matrix initially distributed one dimension redistributed two dimensions derive new mapping function equation 4 redistribution block cyclicc 0 since specifies twodimensional data distribution declare twodimensional grid place holders q 1 function maps processor lpid place holder q rs thus computes ordered pair establishes maxhitratio onedimensional twodimensional redistribution based value gcdz p 0 lemmata 11 12 prove achieves maxhitratio 106 214 203 022 0001216200246811141822 block data elements data hits figure 12 block 4 block 3 cyclic 12 1 redistribution redistribution block cyclicc 0 let b 0 block size row dimension c 0 cyclic block size n 0 theta n 1 data matrix proof proof quite similar proofs lemmata 1 7 single dimension case lemma 1 redistribution block cyclicc considered current situation first dimension block cyclicc thus proof lemma 1 applied lemma 7 must include factor b 1 second dimension 2 lemma 11 let z natural number place holders mapped mapping yields maxhitratio b 0 proof proof similar proof lemma 4 since maps lpid i1 z places thus lpid mapped first row data block block pattern since lpid owns b 1 elements second dimension factor contributes maxhitratio 2 lemma 12 let z natural number place holders mapped place holders p 1 dimension mapped place holders p 0 dimension mapped exactly k lpids map place holder k lpids map place holder q rs remapped place holders q k ways achieve maxhitratio b 0 proof proof similar proof lemma 5 lemma 9 current situation second p 1 processor dimension place holders dimension mapped dimension case onedimensional mapping lemma 5 since lpid owns b 1 elements second dimension factor contributes maxhitratio 2 figure 13 illustrates lpid mapping function applied 24 theta 24 matrix matrix initially distributed block across six processors redistributed cyclic2 block across 3 theta 2 processor grid lpids bold italics superimposed matrix data hit ratio using mapping function 4 example gcdz fig 14 illustrates situation gcdz 2 latter figure demonstrates flexibility permuting lpids instance processors lpid 0 lpid 2 could permuted data hit ratio would achieved lemmata prove following result theorem 4 redistribution block cyclicc 0 b 0 c 0 respective block sizes row dimension integer z b 1 block size column dimension defines grid processors n 0 theta n 1 number global data array elements logical processor mapping function equation 4 achieves 5 analysis performance section discuss possible impacts dataparallel programmer compiler respectively using optimal mapping technique present execution time benefit block data data hits figure 13 block 6 cyclic 3 2block 2 redistribution optimal mapping technique compared traditional mapping technique number data redistribution cases 51 effect optimal mapping programmer permuting lpids optimal mapping functions sections 3 4 may incur undesired side effects thus may suitable redistribution instances instance programmer may lose neighboring data relationships fig 5 traditional cyclic mapping lpids 1 neighbors 8 optimized mapping lpids 0 4 neighbors suppose programmer imparts particular logical physical processor mapping given program utilizes information maintaining neighboring data relationships physical processors order preserve relationships programmer may favor traditional processordata 8 processor neighbor owns data elements adjacent perspective global data data elements data hits figure 14 block 12 cyclic 4 block 3 redistribution mappings optimal mapping call data redistribution figure 15 illustrates static ie varying one program execution another logical physical processor mappings logical processor always maps physical node pi traditional mapping ie 0123 preserves neighboring processor relationships b neighboring data relationships physical nodes corrupted using optimal mapping ie 0213 makes p0 p2 become neighbors previously p0 p1 neighbors compiler runtime system uniquely determines logical physical processor mappings however programmer unable maintain neighboring data relationships physical nodes indeed indirectnetwork multicomputers eg ibm spx workstation clusters concept neighboring nodes furthermore allocation parallel jobs thus data may vary distinct executions program since different physical processors may allocated job figure 16 illustrates number program executions allocation logical processors physical nodes different invocation top part fig logical physical13 p3 traditional logical physical3 p3 b optimal figure 15 traditional optimal mappings physical node mapping static traditional processordata mapping three distinct program executions bottom portion fig 16 illustrates optimal processordata mapping three distinct program executions perspective physical nodes machine permutation lpids whether traditional optimal transparent programmer argue situations use optimal mapping always justified since neighboring data relationships physical nodes cannot maintained exploited programmer logical physical3 p3 b second execution logical physical13 p3 first execution logical physical32 c third execution p3 optimal logical physical p3 first execution logical physical c third execution p3 logical physical p3 b second execution figure 16 traditional optimal mappings physical node mapping dynamic permuting lpids also facilitate greater flexibility programmer may want influence data elements redistributed processors remain onprocessor traditional mapping technique programmer one choice ie lpids mapped increasing numerical order optimized mapping technique often several options since number lpids may map place holder see fig 6b compiler supports programmerspecification lpid permutations user inherently greater control data processor mapping flexibility may become even significant data alignments introduced situation number data elements may map local indices fewer data elements map indices therefore able influence indices redistributed indices remain onprocessor could enhance overall performance 52 effect optimal mapping compiler loss neighboring data relationships may complicate role compiler generating spmd node programs hpf source code let us examine simple example let 16element array initially distributed block redistributed cyclic see fig 5 assume following reference pattern appears compilergenerated spmd code following call redistributing cyclic using traditional mapping technique compiler generates following communication paradigm obtain offprocessor elements excluding boundary processors lpid communicates lpid igamma1 lpid i1 optimal mapping technique neighboring processor relationships maintained thus communication paradigm cannot used example lpid 3 communicates lpid 6 lpid 7 lpid 4 communicates lpid 0 lpid 1 problem easily overcome however compiler utilizes place holder mapping information determined optimal mapping function let 7 array place holders lpids generated equation 1 optimal mapping compiler would specify neighboring communication nonboundary processors follows lpid place holder q j communicate lpids q j gamma1 q j1 essentially compiler performs table lookup determine neighboring lpid relationships extension boundary processors straightforward excluded present discussion 53 performance integrated optimal mapping technique data redistribution library darel 15 written c using mpif 22 performance results obtained ibm spx argonne national laboratory 23 assess runtime performance optimal mapping technique compare data redistribution execution times using optimal technique redistribution execution times using traditional mapping figures 19 illustrate various performance comparisons two mapping tech niques figure 17 shows block cyclicc redistributions 8 nodes range matrix sizes 32thousand 34million 4byte floating point elements block size cyclicc pattern row dimension maintained onehalf block size b block distribution optimal mapping technique applied row dimension matrix demonstrates significantly lower execution times traditional mapping optimal technique achieves redistribution result roughly 60 time needed redistribution using traditional technique larger value c respect b greater effect optimized mapping technique overall execution time significantly data hits occur relative traditional mapping smaller values c relative b benefit optimal technique lessened since relative difference number data hits reduced redistribution instances regardless block sizes b c global data size find optimal technique outperforms equals traditional mapping figures 19 demonstrate blockblock cyclicccyclicc redistributions 12 logically 4 theta 3 24 logically 6 theta 4 processor configurations respectively matrix sizes ranged 91million floating point numbers performance plots mapping technique applied dimensions matrices optimal mapping redistributions outperform traditional mapping cases block size c maintained onesixth oneeighth respective block sizes row column dimensions matrices execution time advantage optimal mapping technique remains consistent larger 24 processor configurations well computation send receive processor sets included execution time plots shown execution time attributable calculations represented small fraction overall total ie order hundreds microseconds small data sizes order tens milliseconds largest data sizes plotted see 15 details 6 conclusion distinct computational phases algorithm perform efficiently different data distribution patterns efficient runtime redistribution enhance overall algorithm performance effective use data redistribution dataparallel programs promoted minimizing runtime cost performing data exchange among nodes machine important aspect reducing redistribution cost minimizing amount data moved among processor memories without violating semantics distribution patterns paper presented technique mapping logical processor ids data elements data redistribution proved technique maximizes ratio data retained locally traditional optimal matrix size 4byte floats time sec figure 17 block cyclicc 8 theta 1 processors traditional optimal matrix size 4byte floats time sec figure 18 blockblock cyclicccyclicc 4 theta 3 processors traditional optimal matrix size 4byte floats time sec figure 19 blockblock cyclicccyclicc 6 theta 4 processors total amount data exchanged among processors performing redistributions block cyclicc arbitrary number dimensions architectureindependent technique improves data redistribution execution time performance approximately 40 wide range data sizes examined impact technique dataparallel programmer compiler respectively believe minimizing amount interprocessor data exchange effective optimization data redistribution dataparallel programs work remains generalizing technique arbitrary block sizes necessarily integer multiples number processors padding data array becomes integer multiple number processors one possibility additionally extending technique redistribution different size processor sets redistribution cyclicc 1 areas future study acknowledgment grateful argonne national laboratory staff use ibm spx also thank anonymous referees constructive comments r high performance fortran language specification version 10 draft fortran language specification language specification version 11 programming distributed memory architectures using kali dino parallel programming language interprocedural compilation fortran mimd machines hypertasking support dynamically redistributable resizeable arrays ipsc dynamic data distributions distributed routing algorithms broadcasting personalized communication hypercubes generation efficient data communications distributedmemory machines runtime array redistribution hpf programs generating communication array statements design implementation evaluation automatic generation efficient array redistribution routines distributed memory multicomputers communication optimizations used paradigm compiler distributedmemory multicomputers darel portable data redistribution library distributedmemory machines efficient algorithms index operation messagepassing systems survey collective communication wormholerouted massively parallel computers approach communicationefficient data redistribution complexity reshaping arrays boolean cubes processor mapping techniques toward efficient data redistribution optimization redistribution arrays distributed memory multicomputers mpi ibm sp1sp2 current status future directions tr ctr jihwoei huang chihping chu efficient communication scheduling method processor mapping technique applied data redistribution journal supercomputing v37 n3 p297318 september 2006 wang minyi guo daming wei divideandconquer algorithm irregular redistribution parallelizing compilers journal supercomputing v29 n2 p157170 august 2004 chinghsien hsu shengwen bai yehching chung chusing yang generalized basiccycle calculation method efficient array redistribution ieee transactions parallel distributed systems v11 n12 p12011216 december 2000 frddric desprez cyril randriamaro jack dongarra antonie petitet yves robert scheduling blockcyclic array redistribution ieee transactions parallel distributed systems v9 n2 p192205 february 1998 chinghsien hsu shihchang chen chaoyang lan scheduling contentionfree irregular redistributions parallelizing compilers journal supercomputing v40 n3 p229247 june 2007 yehching chung chinghsien hsu shengwen bai basiccycle calculation technique efficient dynamic data redistribution ieee transactions parallel distributed systems v9 n4 p359377 april 1998 chinghsien hsu sparse matrix blockcyclic realignment distributed memory machines journal supercomputing v33 n3 p175196 september 2005 chinghsien hsu yehching chung efficient methods kr r r kr array redistribution1 journal supercomputing v12 n3 p253276 may 1 1998 chinghsien hsu yehching chung donlin yang chyiren dow generalized processor mapping technique array redistribution ieee transactions parallel distributed systems v12 n7 p743757 july 2001 chinghsien hsu yehching chung chyiren dow efficient methods multidimensional array redistribution journal supercomputing v17 n1 p2346 aug 2000 minyi guo yi pan improving communication scheduling array redistribution journal parallel distributed computing v65 n5 p553563 may 2005 minyi guo ikuo nakata framework efficient data redistribution distributed memory multicomputers journal supercomputing v20 n3 p243265 november 2001 chinghsien hsu kunming yu compressed diagonals remapping technique dynamic data redistribution banded sparse matrix journal supercomputing v29 n2 p125143 august 2004 peizong lee wenyao chen generating communication sets array assignment statements blockcyclic distribution distributed memory parallel computers parallel computing v28 n9 p13291368 september 2002 stavros souravlas manos roumeliotis pipeline technique dynamic data transfer multiprocessor grid international journal parallel programming v32 n5 p361388 october 2004 antoine p petitet jack j dongarra algorithmic redistribution methods blockcyclic decompositions ieee transactions parallel distributed systems v10 n12 p12011216 december 1999