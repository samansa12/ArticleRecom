highlevel language support userdefined reductions optimized handling reductions parallel supercomputers clusters workstations critical high performance reductions common scientific codes potential source bottlenecks yet many highlevel languages mechanism writing efficient reductions remains surprisingly absent mechanisms exist often provide flexibility programmer needs achieve desirable level performance paper present new language construct arbitrary reductions lets programmer achieve level performance equal achievable highly flexible lowlevel combination fortran mpi implemented construct zpl language evaluate context initialization nas mg benchmark show 45 times speedup code written zpl without construct addition performance large number processors surpasses achieved nas implementation showing mechanism provides programmers needed flexibility b introduction reductions require careful compilation two reasons first abound scientic codes example used algorithms image processing computational geometry kernels matrix multiplication sorting test convergence iterative algorithms often test convergence repeated frequent intervals throughout program second performance often whole program performance suffers reduction poorly optimized left unoptimized unoptimized reduction sequential source signicant unnecessary communication reductions nontrivial parallelize dependences loop iterations violated known operation function associative reduction mapping array n dimensions array less n dimensions scalar mapping collisions two array elements mapping part result must occur result necessarily smaller original array resolve collisions elements mapping location combined operator almost always binary moreover binary operator usually associative commutative operator associative parallelprex method 11 used parallelize reduction operator commutative computation might optimizeable parallel computers essential take advantage associativity compiling parallel computer otherwise potential parallelism left unexploited examples reductions consider following two full sum reduction array integers operator addition result sum every integer array partial sum reduction often called histogram reduction 2d array integers operator addition result either column integers sum integers row row integers sum integers column addition summation common reductions include following determining minimum maximum value array nding location minimum maximum value array calculating logical bitwise elements array paper introduce parallel language construct lets programmer explicitly specify arbitrary reductions compiler cannot fail exploit associativity parallelizing reduction report implementation construct context zpl language 18 highlevel parallel languages include reductions repertoire devices fewer allow specication arbitrary reductions languages like nesl 3 zpl prior introduction mechanism supply number builtin reductions mentioned let programmers dene though languages like c 12 13 sac 16 userdened reductions mechanisms allow reductions written efciently mechanism introduce rest paper organized follows section 2 consider tradeoffs various programming approaches reductions section 3 introduce zpl language section 4 present mechanism efciently support arbitrary userdened reductions context zpl section 5 quantitatively evaluate mechanism sections 6 7 discuss related work conclude programming approaches reductions section concerns various approaches one takes program parallel supercomputers clusters workstations particular focus easy write reduction using given approach level performance likely achieved approaches consider follows using automatic parallelizer relying parallelizing compiler assisted programmerinserted directives employing message passing library writing code highlevel language 21 automatic semiautomatic parallelization research related improving stateoftheart programming practices regard reductions lies umbrella automatic parallelization approach programming easy writing code single processor programmer compiler solely responsible exploiting parallelism traditionally pattern matching idiom recognition used parallelize reductions 4 14 sophisticated techniques recognizing broader classes reductions also examined 8 19 commutativity analysis 15 promises yet another effective technique however undecidable problem determine whether function associative 10 moreover even function technically associative salient part calculation might exploit associativity function sufcient necessary function associative automatic parallelization invaluable technique quickly improving performance large legacy codes written sequential processors one cannot expect achieve consistently high performance many uses different reductions compiler ever able identify parallelize reductions moreover compiler might justiably determine reduction parallelizeable even programmer able determine new observation compiler needs assistance parallelizing codes observation lead development high performance fortran hpf 9 hpf limits risk critical section code left unpar allelized relying programmer insert directives code programmer writes sequential program ordinary fortran adds data layout directives thereby creating hpf program programmers hpf achieved successes still suffer many problems encountered programmers relying fully automatic parallelization compilers supposed recognize reductions parallelize accordingly 22 message passing libraries message passing libraries shift responsibility exploiting parallelism programmer details communication account large portion code 6 valuable time must spent writing addition programmer must write computation based perprocessor view system negative impact code readability maintainability underestimated spite problems approach programming parallel computers standard method employed scientists demand high performance reductions difcult write using message passing library mpi 17 comes rich set builtin reductions occasionally reduction programmer wants write set case mpi mechanism allows userdened reductions programmer must write function used processors combine local results function associated datatype used standard reduction function call example see code appendix assumed reduction return minimum element array well location absent builtin set reductions mpi library disadvantage using message passing libraries means programming lowlevel perprocessor view system even though simplies needs added support userdened reductions overall complexity level higher next approach consider 23 highlevel parallel languages highlevel parallel languages like message passing libraries used programmers want guarantee parallelization reductions written high level languages guaranteed parallel advantage message passing libraries twofold details communication hidden programmer view computation global ie perprocessor level disadvantage certain level control lost languages provide userdened reductions like nesl zpl mechanism paper implemented programmer often must write grossly inefcient code relies simple builtin reductions accomplish could easily done complicated reduction example suppose programmer wants determine two smallest elements large array ideally processor would compute two smallest elements part array owned processor reduction could computed pairs processors compare four elements two smallest elements processor determine smallest two elements processors associative operation parallel prex method used reduce two smallest elements processors parallel highlevel language supplies builtin reductions including one single smallest element array well location array identied efcient solution unworkable instead must nd smallest element array replace element maximum value nd second smallest value using reduction finally replace smallest value array left original state clearly inefcient motivates need powerful language construct allows arbitrary userdened reductions note case zpl mechanism implemented best reduction could use nd two smallest elements reduction determined smallest element array location even less efcient algorithm nding smallest two elements array would used paper improve upon userdened reduction mechanisms previously proposed highlevel parallel languages language construct add zpl language lets programmer achieve performance hitherto achievable message passing libraries 3 brief introduction zpl zpl highlevel dataparallel arraybased language used program parallel computers high performance desired even development time limited relative simplicity pascallike feel make easy read un derstand yet also retains sophisticated model parallelism reasons implemented userdened reductions zpl choose introduce language construct context zpl noted however construct sufciently general apply highlevel parallel languages section introduce features zpl language relevant paper interested readers referred users guide 18 31 regions arrays central zpl region 7 regions index sets associated data declare two regions r bigr bigr set r n n index set refers nonborder portion bigr write following region regions used two contexts first used declare parallel arrays arrays declared regions parallel distributed processors manner speciable runtime declare three integer arrays b c index set given bigr writing nonparallel arrays also called indexed arrays declared using keyword array arrays replicated processor guaranteed contain data processor declare ten element array integers replicated consistent processors write note region associated declaration index set instead specied array keyword second use regions implicitly signal parallel computation example sum corresponding values nonborder portion parallel arrays b store result c write following line code corresponds doubly nested loop nn index set communication since interacting parallel arrays distributed way 32 parallel operators communication communication arises certain zpl operators used since communication major overhead parallel computing programmers avoid operators whenever possible additionally operators correspond less communication used instead operators correspond communication classica tion operators gives zpl performance model lets programmers determine fast slow algorithms 5 basic operator operator operator allows programmer refer elements offset elements array assigned important note implies possibility communication particular pointtopoint nearestneighbor communication write computation sum four adjacent elements array assigned elements array write following code note dataparallel computation signaled region old values array values computation line code occur used update array expensive communication operation reduce operator entails broadcast andor parallel prex communication number builtin reductions zpl language summation minimum value maximum value etc example calculate sum every value array store rst element array write following code indexed array directly indexed region apply applying instead computation parallel arrays cannot indexed since would allow arbitrary communication patterns indexed arrays indexed hence name arbitrary indexing parallel arrays done bulk using permute operator expensive communication operator zpl language 4 mechanism userdened reductions added ability write userdened reductions zpl language overloading functions overloaded functions corresponds different piece reduction section describe works first present simple example using userdened reduction mechanism dene one simple builtin reductions describe two complicated reductions implemented way illustrate every aspect mechanism finally discuss miscellaneous issues related userdened reductions associativity commutativity aggregation 41 basic userdened reductions userdened reductions zpl easy write overloading function reduction operator used ensures performance model remains intact 5 example suppose zpls builtin reductions include sum reduction could realize computation writing code appearing figure 1 3 return 0 8 return 9 end sum figure 1 userdened sum reduction zpl overloaded function sum denition sum lines 14 figure 1 initialization function lines 69 reduction function full reduction nding sum every integer array parallel implementation works following way processor single variable type integer used accumulate local sum processor variable initialized initialization function processor variable repeatedly assigned result reduction function applied every element array residing local processor accumulating variable finally function used combine accumulating values pairs processors using parallelprex method last step done number steps order logarithm number processors 42 generalized userdened reductions minten reduction general three functions must used correspond three phases reduction initialization local reduction global reduction initialization phase local accumulating values initialized local phase local function applied local accumulating value values array residing local processor global phase global function applied accumulating values different processors many cases sum reduction local global phases described single function illustrate three distinct functions might desirable consider minten reduction minten reduction given array values must nd ten smallest values zpl write reduction figure 2 note efciency write procedures result returned instead overwrites parameter technique also applies sum reduction less important case since result type small general specify userdened reduction takes array type elements returns lower rank array type b elements overloaded functions following types must constructed b b b b efciency functions may specied form b ab b b done minten reduction symbol corresponds either argument result use denote parameter passed reference advantage allowing programmer distinguish local global functions twofold first greater efciency achieved requiring type translated b type reduction second global operation computeintensive local function better use faster local function used often course reduction assuming many array elements reside processor realize advantages minten code would inefcient translate array element array ten elements containing array element nine maximum values global function nding ten 6 1 10 tmpval bestvi 19 bestvi newval newval tmpval 22 end 26 28 1 10 29 mintenbestvec1i bestvec2 minten figure 2 userdened minten reduction zpl smallest values two arrays ten values computeintensive checking see single element smaller element array ten elements replacing element 43 extensions minloc reduction addition letting programmer write two functions efciency letting programmer pass extra parameters local function adds potential efciency example write minloc reduction using mechanism userdened reductions described point would need translate array values new type includes location alternatively pass extra information local function minloc reduction similar basic minimum reduction along minimum value reduction returns location minimum value array figure 3 contains efcient minloc reduction zpl note use index1 index2 arrays arrays storage associated however thought readonly arrays general variable indexd contains value ith position dth dimension dened current region variables used local portion reduction passed local function specied user ordinary overloaded function resolution techniques still apply nal argument accumulating value appendix contains code implements minloc reduction c mpi zpl allows programmer level expressibility three functions correspond exactly points lowlevel cmpi code 13 bestd 14 bestd 22 min1d min2d 26 minloc index1 index2 figure 3 userdened minloc reduction zpl reasonable condition work done offers zpl programmer enough exibility write efcient code 44 associativity commutativity aggregation userdened reductions zpl must associative guarantee determinism correct answer programmer responsible verifying case consistent languages support userdened reductions c sac c sac userdened reductions must also commutative zpl weakened condition assume instead userdened reduction commutative decision pending results performance study whereas associativity necessary use parallelprex method exploit parallelism commutativity commutativity advantageous certain parallel computers take advantage values arriving different orders noncommutative reductions common example given onedimensional array ones zeroes length longest sequence ones determined parallel using associative noncommutative reduction aggregation important method limiting number messages sent message passing systems presence many similar reductions proven vital achieving high performance 14 given parallel array element corresponds list k elements want nd smallest elements reside position list would write k reductions loop k reductions would aggregated zpl compiler aggregation occurs zpl case userdened reductions builtin reductions code appendix signicant difference seen global functions cmpi zpl implementations cmpi implementation global function takes array reduction elements rather single one zpl global function passed single element however zpl compiler transforms global function one takes array elements aggregation done automatically applicable 5 evaluation determine effect performance userdened reductions ran three versions nas mg benchmark 1 2 272 processor t3e900 processor runs 450mhz 256 mb ram per pro cessor three versions nas mg benchmark original nas implementation f77 mpi zpl implementation using builtin reductions zpl implementation using userdened reduction mechanism described paper focus initialization array nas mg benchmark works follows first array lled random numbers second ten largest ten smallest values identied third twenty values replaced values 1 1 respectively values array set zero assumed ten largest ten smallest values unique timings focus second step process figure 4 contains results experiment three large classes note classes b identical regards initialization array processors3296speedup best 2processor time 0404 seconds initialization linear speedup zpl builtin zpl userdefined processors412speedup best 16processor time 0405 seconds mg class c initialization linear speedup zpl builtin zpl userdefined figure 4 parallel speedup nas mgs initialization graphs figure 4 show us addition userdened reductions zpl language critical performance although zpl implementation using builtin reductions scales almost linearly respect overhead high implementation compute 20 reductions reduction returns minimum maximum value array reductions processor scans portion array nd reduced value found location determined information broadcast processors amount computation overwhelming array traversed total 40 times zpl implementation userdened reductions well f77mpi implementation array traversed another point note graphs zpl implementation userdened reductions slightly slower f77mpi implementation small number processors within 10 large number zpl implementation continues scale whereas f77mpi implementation continued scaling result implementation differences reect limitation expressibility f77mpi implementation f77mpi implementation avoids userdened reductions taking advantage local view com putation single traversal array processor nds ten smallest ten largest values reside portion array twenty reductions used nd values largest smallest entire array location information need broadcast processors locally largest values globally largest processor knows replace values 1 contrast done zpl zpl implementations location information largest smallest values entire array must broadcast processor maintain global view computation zpl implementation user dened reductions scales better f77mpi reduction use single large reduction nd twenty globally largest smallest values array smaller processors zpl implementations suffer extra overhead involved communicating using location information global scale 6 related work idea construct userdened reductions new though remains surprisingly absent many highlevel languages supported often efcient possible supported sac 16 limited form one function speciable global local parts reduction makes reductions like minten reduction difcult write efcient manner reasons discussed section 42 viswanathan larus 20 developed powerful mechanism userdened reductions context c language closely resembles construct described paper however provide mechanism passing extra parameters local function unclear initialization phase done whether another overloaded function addition due language differences mechanism userdened reductions lead data races higherlevel global view computation zpl eliminates worry 7 conclusion optimized handling reductions parallel supercomputers clusters workstations critical high performance reductions common scientic codes potential source bottlenecks consequently researchers worked diligently techniques compilers programmers use reductions execute efciently great strides made domain automatic parallelization remains hitandmiss approach high performance semiautomatic techniques relying directives improved hit rate performance still often suffers use language like fortran 77 coupled message passing library like mpi remains popular standard consistently high performance crucial scientic programmers willing expend considerable effort necessary program fortran 77 mpi message passing libraries difcult use force programmer write programs perprocessor basis tediously engineer interprocessor communication lose track problem whole highlevel parallel languages promising alternative popular standard even wellstudied idiom reductions performance suffers directly done programmer fortran 77 mpi often done given highlevel language either mechanism userdened reductions one forces programmer sacrice amount efciency paper presented new language construct arbitrary reductions lets programmer achieve level high performance equal achievable fortran mpi evaluated approach context nas mg benchmark showed performance closely resembles achieved lowlevel fortran plus mpi approach construct vital high performance makes highlevel languages viable choice scientists acknowledgments rst author supported doe highperformance computer science fellowship completed portion work los alamos national laboratory would like thank sungeun choi anonymous reviewers many insightful comments earlier drafts paper work supported part grant hpc resources arctic region supercomputing center r nas parallel benchmarks 20 nesl nested dataparallel language version 31 zpls wysiwyg performance model comparative study nas mg benchmark across parallel languages architectures regions abstraction expressing array computation parallelizing complex scans reductions high performance fortran forum complexity commutativity analysis parallel pre parallel programming c compiler optimization implicit reductions distributed memory multipro cessors commutativity analysis new analysis framework parallelizing compilers mpi complete reference programming guide zpl detection global optimization reduction operations distributed parallel machines tr parallelizing complex scans reductions commutativity analysis detection global optimization reduction operations distributed parallel machines programmers guide zpl regions defining applicationspecific highlevel array operations means shapeinvariant programming facilities comparative study nas mg benchmark across parallel languages architectures mpi polaris complexity commutativity analysis zpls wysiwyg performance model nesl nested dataparallel language ctr paul van der mark lex wolters gerard cats using semilagrangian formulations automatic code generation environmental modeling proceedings 2004 acm symposium applied computing march 1417 2004 nicosia cyprus steven j deitz david callahan bradford l chamberlain lawrence snyder globalview abstractions userdefined reductions scans proceedings eleventh acm sigplan symposium principles practice parallel programming march 2931 2006 new york new york usa steven j deitz bradford l chamberlain sungeun choi lawrence snyder design implementation parallel array operator arbitrary remapping data acm sigplan notices v38 n10 october lawrence snyder design development zpl proceedings third acm sigplan conference history programming languages p81837 june 0910 2007 san diego california